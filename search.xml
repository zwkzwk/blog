<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[分布式技术面试题]]></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Fdesign%2Farchitecture%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[分布式技术面试题 1. 分布式缓存 1.1. Redis 有什么数据类型？分别用于什么场景？ 1.2. Redis 的主从复制是如何实现的？ 1.3. Redis 的 key 是如何寻址的？ 1.4. Redis 的集群模式是如何实现的？ 1.5. Redis 如何实现分布式锁？ZooKeeper 如何实现分布式锁？比较二者优劣？ 1.6. Redis 的持久化方式？有什么优缺点？持久化实现原理？ 1.7. Redis 过期策略有哪些？ 1.8. Redis 和 Memcached 有什么区别？ 1.9. 为什么单线程的 Redis 性能反而优于多线程的 Memcached？ 2. 分布式消息队列（MQ） 2.1. 为什么使用 MQ？ 2.2. 如何保证 MQ 的高可用？ 2.3. MQ 有哪些常见问题？如何解决这些问题？ 2.4. Kafka, ActiveMQ, RabbitMQ, RocketMQ 各有什么优缺点？ 3. 分布式服务（RPC） 3.1. Dubbo 的实现过程？ 3.2. Dubbo 负载均衡策略有哪些？ 3.3. Dubbo 集群容错策略 ？ 3.4. 动态代理策略？ 3.5. Dubbo 支持哪些序列化协议？Hessian？Hessian 的数据结构？ 3.6. Protoco Buffer 是什么？ 3.7. 注册中心挂了可以继续通信吗？ 3.8. ZooKeeper 原理是什么？ZooKeeper 有什么用？ 3.9. Netty 有什么用？NIO/BIO/AIO 有什么用？有什么区别？ 3.10. 为什么要进行系统拆分？拆分不用 Dubbo 可以吗？ 3.11. Dubbo 和 Thrift 有什么区别？ 1. 分布式缓存 1.1. Redis 有什么数据类型？分别用于什么场景？ 数据类型 可以存储的值 操作 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 LIST 列表 从两端压入或者弹出元素 读取单个或者多个元素 进行修剪，只保留一个范围内的元素 SET 无序集合 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 ZSET 有序集合 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 What Redis data structures look like 1.2. Redis 的主从复制是如何实现的？ 从服务器连接主服务器，发送 SYNC 命令； 主服务器接收到 SYNC 命名后，开始执行 BGSAVE 命令生成 RDB 文件并使用缓冲区记录此后执行的所有写命令； 主服务器 BGSAVE 执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令； 从服务器收到快照文件后丢弃所有旧数据，载入收到的快照； 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令； 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令； 1.3. Redis 的 key 是如何寻址的？ 背景 （1）redis 中的每一个数据库，都由一个 redisDb 的结构存储。其中： redisDb.id 存储着 redis 数据库以整数表示的号码。 redisDb.dict 存储着该库所有的键值对数据。 redisDb.expires 保存着每一个键的过期时间。 （2）当 redis 服务器初始化时，会预先分配 16 个数据库（该数量可以通过配置文件配置），所有数据库保存到结构 redisServer 的一个成员 redisServer.db 数组中。当我们选择数据库 select number 时，程序直接通过 redisServer.db[number] 来切换数据库。有时候当程序需要知道自己是在哪个数据库时，直接读取 redisDb.id 即可。 （3）redis 的字典使用哈希表作为其底层实现。dict 类型使用的两个指向哈希表的指针，其中 0 号哈希表（ht[0]）主要用于存储数据库的所有键值，而 1 号哈希表主要用于程序对 0 号哈希表进行 rehash 时使用，rehash 一般是在添加新值时会触发，这里不做过多的赘述。所以 redis 中查找一个 key，其实就是对进行该 dict 结构中的 ht[0] 进行查找操作。 （4）既然是哈希，那么我们知道就会有哈希碰撞，那么当多个键哈希之后为同一个值怎么办呢？redis 采取链表的方式来存储多个哈希碰撞的键。也就是说，当根据 key 的哈希值找到该列表后，如果列表的长度大于 1，那么我们需要遍历该链表来找到我们所查找的 key。当然，一般情况下链表长度都为是 1，所以时间复杂度可看作 o(1)。 寻址 key 的步骤 当拿到一个 key 后，redis 先判断当前库的 0 号哈希表是否为空，即：if (dict-&gt;ht[0].size == 0)。如果为 true 直接返回 NULL。 判断该 0 号哈希表是否需要 rehash，因为如果在进行 rehash，那么两个表中者有可能存储该 key。如果正在进行 rehash，将调用一次_dictRehashStep 方法，_dictRehashStep 用于对数据库字典、以及哈希键的字典进行被动 rehash，这里不作赘述。 计算哈希表，根据当前字典与 key 进行哈希值的计算。 根据哈希值与当前字典计算哈希表的索引值。 根据索引值在哈希表中取出链表，遍历该链表找到 key 的位置。一般情况，该链表长度为 1。 当 ht[0] 查找完了之后，再进行了次 rehash 判断，如果未在 rehashing，则直接结束，否则对 ht[1]重复 345 步骤。 1.4. Redis 的集群模式是如何实现的？ Redis Cluster 是 Redis 的分布式解决方案，在 Redis 3.0 版本正式推出的。 Redis Cluster 去中心化，每个节点保存数据和整个集群状态，每个节点都和其他所有节点连接。 Redis Cluster 节点分配 Redis Cluster 特点： 所有的 redis 节点彼此互联(PING-PONG 机制)，内部使用二进制协议优化传输速度和带宽。 节点的 fail 是通过集群中超过半数的节点检测失效时才生效。 客户端与 redis 节点直连,不需要中间 proxy 层。客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可。 redis-cluster 把所有的物理节点映射到[0-16383] 哈希槽 (hash slot)上（不一定是平均分配）,cluster 负责维护 node、slot、value。 Redis 集群预分好 16384 个桶，当需要在 Redis 集群中放置一个 key-value 时，根据 CRC16(key) mod 16384 的值，决定将一个 key 放到哪个桶中。 Redis Cluster 主从模式 Redis Cluster 为了保证数据的高可用性，加入了主从模式。 一个主节点对应一个或多个从节点，主节点提供数据存取，从节点则是从主节点拉取数据备份。当这个主节点挂掉后，就会有这个从节点选取一个来充当主节点，从而保证集群不会挂掉。所以，在集群建立的时候，一定要为每个主节点都添加了从节点。 Redis Sentinel Redis Sentinel 用于管理多个 Redis 服务器，它有三个功能： 监控（Monitoring） - Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification） - 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover） - 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。 Redis 集群中应该有奇数个节点，所以至少有三个节点。 哨兵监控集群中的主服务器出现故障时，需要根据 quorum 选举出一个哨兵来执行故障转移。选举需要 majority，即大多数哨兵是运行的（2 个哨兵的 majority=2，3 个哨兵的 majority=2，5 个哨兵的 majority=3，4 个哨兵的 majority=2）。 假设集群仅仅部署 2 个节点 +----+ +----+| M1 |---------| R1 || S1 | | S2 |+----+ +----+ 如果 M1 和 S1 所在服务器宕机，则哨兵只有 1 个，无法满足 majority 来进行选举，就不能执行故障转移。 1.5. Redis 如何实现分布式锁？ZooKeeper 如何实现分布式锁？比较二者优劣？ 分布式锁的三种实现： 基于数据库实现分布式锁； 基于缓存（Redis 等）实现分布式锁； 基于 Zookeeper 实现分布式锁； 数据库实现 Redis 实现 获取锁的时候，使用 setnx 加锁，并使用 expire 命令为锁添加一个超时时间，超过该时间则自动释放锁，锁的 value 值为一个随机生成的 UUID，通过此在释放锁的时候进行判断。 获取锁的时候还设置一个获取的超时时间，若超过这个时间则放弃获取锁。 释放锁的时候，通过 UUID 判断是不是该锁，若是该锁，则执行 delete 进行锁释放。 ZooKeeper 实现 创建一个目录 mylock； 线程 A 想获取锁就在 mylock 目录下创建临时顺序节点； 获取 mylock 目录下所有的子节点，然后获取比自己小的兄弟节点，如果不存在，则说明当前线程顺序号最小，获得锁； 线程 B 获取所有节点，判断自己不是最小节点，设置监听比自己次小的节点； 线程 A 处理完，删除自己的节点，线程 B 监听到变更事件，判断自己是不是最小的节点，如果是则获得锁。 实现对比 ZooKeeper 具备高可用、可重入、阻塞锁特性，可解决失效死锁问题。 但 ZooKeeper 因为需要频繁的创建和删除节点，性能上不如 Redis 方式。 1.6. Redis 的持久化方式？有什么优缺点？持久化实现原理？ RDB 快照（snapshot） 将存在于某一时刻的所有数据都写入到硬盘中。 快照的原理 在默认情况下，Redis 将数据库快照保存在名字为 dump.rdb 的二进制文件中。你可以对 Redis 进行设置， 让它在“N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动保存一次数据集。你也可以通过调用 SAVE 或者 BGSAVE，手动让 Redis 进行数据集保存操作。这种持久化方式被称为快照。 当 Redis 需要保存 dump.rdb 文件时， 服务器执行以下操作: Redis 创建一个子进程。 子进程将数据集写入到一个临时快照文件中。 当子进程完成对新快照文件的写入时，Redis 用新快照文件替换原来的快照文件，并删除旧的快照文件。 这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益。 快照的优点 它保存了某个时间点的数据集，非常适用于数据集的备份。 很方便传送到另一个远端数据中心或者亚马逊的 S3（可能加密），非常适用于灾难恢复。 快照在保存 RDB 文件时父进程唯一需要做的就是 fork 出一个子进程，接下来的工作全部由子进程来做，父进程不需要再做其他 IO 操作，所以快照持久化方式可以最大化 redis 的性能。 与 AOF 相比，在恢复大的数据集的时候，DB 方式会更快一些。 快照的缺点 如果你希望在 redis 意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么快照不适合你。 快照需要经常 fork 子进程来保存数据集到硬盘上。当数据集比较大的时候，fork 的过程是非常耗时的，可能会导致 Redis 在一些毫秒级内不能响应客户端的请求。 AOF AOF 持久化方式记录每次对服务器执行的写操作。当服务器重启的时候会重新执行这些命令来恢复原始的数据。 AOF 的原理 Redis 创建一个子进程。 子进程开始将新 AOF 文件的内容写入到临时文件。 对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾，这样样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。 当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。 搞定！现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。 AOF 的优点 使用默认的每秒 fsync 策略，Redis 的性能依然很好(fsync 是由后台线程进行处理的,主线程会尽力处理客户端请求)，一旦出现故障，使用 AOF ，你最多丢失 1 秒的数据。 AOF 文件是一个只进行追加的日志文件，所以不需要写入 seek，即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令，你也也可使用 redis-check-aof 工具修复这些问题。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写：重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。整个重写操作是绝对安全的。 AOF 文件有序地保存了对数据库执行的所有写入操作，这些写入操作以 Redis 协议的格式保存。因此 AOF 文件的内容非常容易被人读懂，对文件进行分析（parse）也很轻松。 AOF 的缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 根据所使用的 fsync 策略，AOF 的速度可能会慢于快照。在一般情况下，每秒 fsync 的性能依然非常高，而关闭 fsync 可以让 AOF 的速度和快照一样快，即使在高负荷之下也是如此。不过在处理巨大的写入载入时，快照可以提供更有保证的最大延迟时间（latency）。 1.7. Redis 过期策略有哪些？ noeviction - 当内存使用达到阈值的时候，所有引起申请内存的命令会报错。 allkeys-lru - 在主键空间中，优先移除最近未使用的 key。 allkeys-random - 在主键空间中，随机移除某个 key。 volatile-lru - 在设置了过期时间的键空间中，优先移除最近未使用的 key。 volatile-random - 在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl - 在设置了过期时间的键空间中，具有更早过期时间的 key 优先移除。 1.8. Redis 和 Memcached 有什么区别？ 两者都是非关系型内存键值数据库。有以下主要不同： 数据类型 Memcached 仅支持字符串类型； 而 Redis 支持五种不同种类的数据类型，使得它可以更灵活地解决问题。 数据持久化 Memcached 不支持持久化； Redis 支持两种持久化策略：RDB 快照和 AOF 日志。 分布式 Memcached 不支持分布式，只能通过在客户端使用像一致性哈希这样的分布式算法来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。 Redis Cluster 实现了分布式的支持。 内存管理机制 Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题，但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。 在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘。而 Memcached 的数据则会一直在内存中。 1.9. 为什么单线程的 Redis 性能反而优于多线程的 Memcached？ Redis 快速的原因： 绝大部分请求是纯粹的内存操作（非常快速） 采用单线程,避免了不必要的上下文切换和竞争条件 非阻塞 IO 内部实现采用 epoll，采用了 epoll+自己实现的简单的事件框架。epoll 中的读、写、关闭、连接都转化成了事件，然后利用 epoll 的多路复用特性，绝不在 io 上浪费一点时间。 2. 分布式消息队列（MQ） 2.1. 为什么使用 MQ？ 异步处理 - 相比于传统的串行、并行方式，提高了系统吞吐量。 应用解耦 - 系统间通过消息通信，不用关心其他系统的处理。 流量削锋 - 可以通过消息队列长度控制请求量；可以缓解短时间内的高并发请求。 日志处理 - 解决大量日志传输。 消息通讯 - 消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯。比如实现点对点消息队列，或者聊天室等。 2.2. 如何保证 MQ 的高可用？ 数据复制 将所有 Broker 和待分配的 Partition 排序 将第 i 个 Partition 分配到第（i mod n）个 Broker 上 将第 i 个 Partition 的第 j 个 Replica 分配到第（(i + j) mode n）个 Broker 上 选举主服务器 2.3. MQ 有哪些常见问题？如何解决这些问题？ MQ 的常见问题有： 消息的顺序问题 消息的重复问题 消息的顺序问题 消息有序指的是可以按照消息的发送顺序来消费。 假如生产者产生了 2 条消息：M1、M2，假定 M1 发送到 S1，M2 发送到 S2，如果要保证 M1 先于 M2 被消费，怎么做？ 解决方案： （1）保证生产者 - MQServer - 消费者是一对一对一的关系 缺陷： 并行度就会成为消息系统的瓶颈（吞吐量不够） 更多的异常处理，比如：只要消费端出现问题，就会导致整个处理流程阻塞，我们不得不花费更多的精力来解决阻塞的问题。 （2）通过合理的设计或者将问题分解来规避。 不关注乱序的应用实际大量存在 队列无序并不意味着消息无序 所以从业务层面来保证消息的顺序而不仅仅是依赖于消息系统，是一种更合理的方式。 消息的重复问题 造成消息重复的根本原因是：网络不可达。 所以解决这个问题的办法就是绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？ 消费端处理消息的业务逻辑保持幂等性。只要保持幂等性，不管来多少条重复消息，最后处理的结果都一样。 保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现。利用一张日志表来记录已经处理成功的消息的 ID，如果新到的消息 ID 已经在日志表中，那么就不再处理这条消息。 2.4. Kafka, ActiveMQ, RabbitMQ, RocketMQ 各有什么优缺点？ 3. 分布式服务（RPC） 3.1. Dubbo 的实现过程？ 节点角色： 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 调用关系： 务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 3.2. Dubbo 负载均衡策略有哪些？ Random 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin 轮循，按公约后的权重设置轮循比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash 一致性 Hash，相同参数的请求总是发到同一提供者。 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 算法参见：http://en.wikipedia.org/wiki/Consistent_hashing 缺省只对第一个参数 Hash，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.arguments&quot; value=&quot;0,1&quot; /&gt; 缺省用 160 份虚拟节点，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.nodes&quot; value=&quot;320&quot; /&gt; 3.3. Dubbo 集群容错策略 ？ Failover - 失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries=“2” 来设置重试次数(不含第一次)。 Failfast - 快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 Failsafe - 失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback - 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking - 并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=“2” 来设置最大并行数。 Broadcast - 播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息。 3.4. 动态代理策略？ Dubbo 作为 RPC 框架，首先要完成的就是跨系统，跨网络的服务调用。消费方与提供方遵循统一的接口定义，消费方调用接口时，Dubbo 将其转换成统一格式的数据结构，通过网络传输，提供方根据规则找到接口实现，通过反射完成调用。也就是说，消费方获取的是对远程服务的一个代理(Proxy)，而提供方因为要支持不同的接口实现，需要一个包装层(Wrapper)。调用的过程大概是这样： 消费方的 Proxy 和提供方的 Wrapper 得以让 Dubbo 构建出复杂、统一的体系。而这种动态代理与包装也是通过基于 SPI 的插件方式实现的，它的接口就是ProxyFactory。 @SPI("javassist")public interface ProxyFactory &#123; @Adaptive(&#123;Constants.PROXY_KEY&#125;) &lt;T&gt; T getProxy(Invoker&lt;T&gt; invoker) throws RpcException; @Adaptive(&#123;Constants.PROXY_KEY&#125;) &lt;T&gt; Invoker&lt;T&gt; getInvoker(T proxy, Class&lt;T&gt; type, URL url) throws RpcException;&#125; ProxyFactory 有两种实现方式，一种是基于 JDK 的代理实现，一种是基于 javassist 的实现。ProxyFactory 接口上定义了@SPI(“javassist”)，默认为 javassist 的实现。 3.5. Dubbo 支持哪些序列化协议？Hessian？Hessian 的数据结构？ dubbo 序列化，阿里尚不成熟的 java 序列化实现。 hessian2 序列化：hessian 是一种跨语言的高效二进制的序列化方式，但这里实际不是原生的 hessian2 序列化，而是阿里修改过的 hessian lite，它是 dubbo RPC 默认启用的序列化方式。 json 序列化：目前有两种实现，一种是采用的阿里的 fastjson 库，另一种是采用 dubbo 中自已实现的简单 json 库，一般情况下，json 这种文本序列化性能不如二进制序列化。 java 序列化：主要是采用 JDK 自带的 java 序列化实现，性能很不理想。 Kryo 和 FST：Kryo 和 FST 的性能依然普遍优于 hessian 和 dubbo 序列化。 Hessian 序列化与 Java 默认的序列化区别？ Hessian 是一个轻量级的 remoting on http 工具，采用的是 Binary RPC 协议，所以它很适合于发送二进制数据，同时又具有防火墙穿透能力。 Hessian 支持跨语言串行 比 java 序列化具有更好的性能和易用性 支持的语言比较多 3.6. Protoco Buffer 是什么？ Protocol Buffer 是 Google 出品的一种轻量 &amp; 高效的结构化数据存储格式，性能比 Json、XML 真的强！太！多！ Protocol Buffer 的序列化 &amp; 反序列化简单 &amp; 速度快的原因是： 编码 / 解码 方式简单（只需要简单的数学运算 = 位移等等） 采用 Protocol Buffer 自身的框架代码 和 编译器 共同完成 Protocol Buffer 的数据压缩效果好（即序列化后的数据量体积小）的原因是： 采用了独特的编码方式，如 Varint、Zigzag 编码方式等等 采用 T - L - V 的数据存储方式：减少了分隔符的使用 &amp; 数据存储得紧凑 3.7. 注册中心挂了可以继续通信吗？ 可以。Dubbo 消费者在应用启动时会从注册中心拉取已注册的生产者的地址接口，并缓存在本地。每次调用时，按照本地存储的地址进行调用。 3.8. ZooKeeper 原理是什么？ZooKeeper 有什么用？ ZooKeeper 是一个分布式应用协调系统，已经用到了许多分布式项目中，用来完成统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等工作。 每个 Server 在内存中存储了一份数据； Zookeeper 启动时，将从实例中选举一个 leader（Paxos 协议）； Leader 负责处理数据更新等操作（Zab 协议）； 一个更新操作成功，当且仅当大多数 Server 在内存中成功修改数据。 3.9. Netty 有什么用？NIO/BIO/AIO 有什么用？有什么区别？ Netty 是一个“网络通讯框架”。 Netty 进行事件处理的流程。Channel是连接的通道，是 ChannelEvent 的产生者，而ChannelPipeline可以理解为 ChannelHandler 的集合。 参考：https://github.com/code4craft/netty-learning/blob/master/posts/ch1-overview.md IO 的方式通常分为几种： 同步阻塞的 BIO 同步非阻塞的 NIO 异步非阻塞的 AIO 在使用同步 I/O 的网络应用中，如果要同时处理多个客户端请求，或是在客户端要同时和多个服务器进行通讯，就必须使用多线程来处理。 NIO 基于 Reactor，当 socket 有流可读或可写入 socket 时，操作系统会相应的通知引用程序进行处理，应用再将流读取到缓冲区或写入操作系统。也就是说，这个时候，已经不是一个连接就要对应一个处理线程了，而是有效的请求，对应一个线程，当连接没有数据时，是没有工作线程来处理的。 与 NIO 不同，当进行读写操作时，只须直接调用 API 的 read 或 write 方法即可。这两种方法均为异步的，对于读操作而言，当有流可读取时，操作系统会将可读的流传入 read 方法的缓冲区，并通知应用程序；对于写操作而言，当操作系统将 write 方法传递的流写入完毕时，操作系统主动通知应用程序。 即可以理解为，read/write 方法都是异步的，完成后会主动调用回调函数。 参考：https://blog.csdn.net/skiof007/article/details/52873421 3.10. 为什么要进行系统拆分？拆分不用 Dubbo 可以吗？ 系统拆分从资源角度分为：应用拆分和数据库拆分。 从采用的先后顺序可分为：水平扩展、垂直拆分、业务拆分、水平拆分。 是否使用服务依据实际业务场景来决定。 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)是关键。 3.11. Dubbo 和 Thrift 有什么区别？ Thrift 是跨语言的 RPC 框架。 Dubbo 支持服务治理，而 Thrift 不支持。]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
        <tag>distributed</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React 快速入门]]></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Ffrontend%2Fmvc%2Freact%2F</url>
    <content type="text"><![CDATA[React 快速入门 安装 Introducing JSX JSX 中嵌入表达式 JSX 也是一个表达式 用 JSX 指定属性值 用 JSX 指定子元素 JSX 防止注入攻击 JSX 表示对象 渲染元素 渲染一个元素到 DOM 更新已渲染的元素 React 只更新必需要更新的部分 组件(Components) 和 属性(Props) 函数式组件和类组件 渲染一个组件 构成组件 提取组件 Props 是只读的 把函数式组件转化为类组件 在类组件中添加本地状态(state) 在类中添加生命周期方法 正确地使用 State(状态) 不要直接修改 state(状态) state(状态) 更新可能是异步的 state(状态)更新会被合并 数据向下流动 元素变量 使用逻辑 &amp;&amp; 操作符的内联 if 用法 使用条件操作符的内联 If-Else 防止组件渲染 多组件渲染 基本列表组件 键(Keys) 使用 keys 提取组件 keys 在同辈元素中必须是唯一的 在 JSX 中嵌入 map() 受控组件(Controlled Components) textare 标签 select 标签 处理多个输入元素 受控组件的替代方案 添加第二个输入 编写转换函数 状态提升(Lifting State Up) 经验总结 包含 特例 如何看待？ 安装 直接下载使用 React 可以直接下载使用，下载包中也提供了很多学习的实例。 你可以在官网 http://facebook.github.io/react/ 下载最新版。 通过 npm 使用 React $ npm install -S react react-dom 通过 yarn 使用 React $ yarn add react react-dom 使用各种快速构建工具 目前最流行的构建工具应该是 create-react-app，它使得用户可以通过命令就能快速构建 React 开发环境。 create-react-app 自动创建的项目是基于 Webpack + ES6 。 $ npm install -g create-react-app$ create-react-app my-app$ cd my-app/$ npm start Introducing JSX 考虑一下这个变量的声明： const element = &lt;h1&gt;Hello, world!&lt;/h1&gt;; 这种有趣的标签语法既不是字符串也不是 HTML。 这就是 JSX ，他是 JavaScrip 的一种扩展语法。我们推荐在 React 中使用这种语法来描述 UI 信息。JSX 可能会让你想起某种模板语言，但是它具有 JavaScrip 的全部能力。 JSX 可以生成 React “元素”。我们将在下一章探索如何把它渲染到 DOM 上。下面你可以找到 JSX 的基础知识，以帮助您开始使用。 JSX 中嵌入表达式 你可以用 花括号 把任意的 JavaScript 表达式 嵌入到 JSX 中。 例如，2 + 2， user.firstName， 和 formatName(user)，这些都是可用的表达式。 function formatName(user) &#123; return user.firstName + ' ' + user.lastName;&#125;const user = &#123; firstName: 'Harper', lastName: 'Perez'&#125;;const element = ( &lt;h1&gt; Hello, &#123;formatName(user)&#125;! &lt;/h1&gt;);ReactDOM.render( element, document.getElementById('root')); 在 CodePen 中尝试。 为便于阅读，我们将 JSX 分割成多行。我们推荐使用括号将 JSX 包裹起来，虽然这不是必须的，但这样做可以避免分号自动插入的陷阱。 JSX 也是一个表达式 编译之后，JSX 表达式就变成了常规的 JavaScript 对象。 这意味着你可以在 if 语句或者是 for 循环中使用 JSX，用它给变量赋值，当做参数接收，或者作为函数的返回值。 function getGreeting(user) &#123; if (user) &#123; return &lt;h1&gt;Hello, &#123;formatName(user)&#125;!&lt;/h1&gt;; &#125; return &lt;h1&gt;Hello, Stranger.&lt;/h1&gt;;&#125; 用 JSX 指定属性值 您可以使用双引号来指定字符串字面量作为属性值： const element = &lt;div tabIndex="0"&gt;&lt;/div&gt;; 您也可以用花括号嵌入一个 JavaScript 表达式作为属性值: const element = &lt;img src=&#123;user.avatarUrl&#125;&gt;&lt;/img&gt;; 在属性中嵌入 JavaScript 表达式时，不要使用引号来包裹大括号。否则，JSX 将该属性视为字符串字面量而不是表达式。对于字符串值你应该使用引号，对于表达式你应该使用大括号，但两者不能同时用于同一属性。 用 JSX 指定子元素 如果是空标签，您应该像 XML 一样，使用 /&gt;立即闭合它： const element = &lt;img src=&#123;user.avatarUrl&#125; /&gt;; JSX 标签可能包含子元素： const element = ( &lt;div&gt; &lt;h1&gt;Hello!&lt;/h1&gt; &lt;h2&gt;Good to see you here.&lt;/h2&gt; &lt;/div&gt;); 警告： 比起 HTML ， JSX 更接近于 JavaScript ， 所以 React DOM 使用驼峰(camelCase)属性命名约定, 而不是 HTML 属性名称。 例如，class 在 JSX 中变为className，tabindex 变为 tabIndex。 JSX 防止注入攻击 在 JSX 中嵌入用户输入是安全的： const title = response.potentiallyMaliciousInput;// This is safe:const element = &lt;h1&gt;&#123;title&#125;&lt;/h1&gt;; 默认情况下， 在渲染之前, React DOM 会格式化(escapes) JSX 中的所有值. 从而保证用户无法注入任何应用之外的代码. 在被渲染之前，所有的数据都被转义成为了字符串处理。 以避免 XSS(跨站脚本) 攻击。 JSX 表示对象 Babel 将 JSX 编译成 React.createElement() 调用。 下面的两个例子是是完全相同的： const element = ( &lt;h1 className="greeting"&gt; Hello, world! &lt;/h1&gt;); const element = React.createElement( 'h1', &#123;className: 'greeting'&#125;, 'Hello, world!'); React.createElement() 会执行一些检查来帮助你编写没有 bug 的代码，但基本上它会创建一个如下所示的对象： // 注意: 这是简化的结构const element = &#123; type: 'h1', props: &#123; className: 'greeting', children: 'Hello, world' &#125;&#125;; 这些对象被称作“React 元素”。你可以把他们想象成为你想在屏幕上显示内容的一种描述。React 会读取这些对象，用他们来构建 DOM，并且保持它们的不断更新。 我们将在下一节中来探索如何将 React 元素渲染到 DOM 上。 提示: 我们建议你去搜一下你用的编辑器的 “Babel” 语法方案, 以便 ES6 和 JSX 代码都能够被正确高亮的显示。 渲染元素 元素(Elements)是 React 应用中最小的建造部件（或者说构建块，building blocks）。 一个元素用于描述你在将在屏幕上看到的内容： const element = &lt;h1&gt;Hello, world&lt;/h1&gt;; 不同于浏览器的 DOM 元素， React 元素是普通的对象，非常容易创建。React DOM 会负责更新 DOM ，以匹配 React 元素（愚人码头注：DOM 元素与 React 元素保持一致）。 注意： 有人可能会将元素与更广为人知的 “组件(Components)” 概念相混淆。我们将在下一节介绍组件。元素是构成组件的&quot;材料&quot;， 所以我们建议你看完本节再进入下一节。 渲染一个元素到 DOM 我们假设你的 HTML 文件中的什么地方有这么一个``： &lt;div id="root"&gt;&lt;/div&gt; 我们称这个是一个 “根” DOM 节点，因为该节点内的所有内容都由 React DOM 管理。 单纯用 React 构建的应用程序通常只有一个单独的 根 DOM 节点。但如果你要把 React 整合进现有的 app 中 ，那你可能会有多个相互独立的根 DOM 节点。 要渲染一个 React 元素到一个 根 DOM 节点，吧它们传递给 ReactDOM.render() 方法： const element = &lt;h1&gt;Hello, world&lt;/h1&gt;;ReactDOM.render( element, document.getElementById('root')); 在 CodePen 中尝试。 上面代码会在页面上显示 “Hello, world” 。 更新已渲染的元素 React 元素是 不可突变（immutable） 的. 一旦你创建了一个元素, 就不能再修改其子元素或任何属性。一个元素就像电影里的一帧: 它表示在某一特定时间点的 UI 。 就我们所知, 更新 UI 的唯一方法是创建一个新的元素, 并将其传入ReactDOM.render()方法. 思考以下时钟例子: function tick() &#123; const element = ( &lt;div&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;h2&gt;It is &#123;new Date().toLocaleTimeString()&#125;.&lt;/h2&gt; &lt;/div&gt; ); ReactDOM.render( element, document.getElementById('root') );&#125;setInterval(tick, 1000); 在 CodePen 中尝试。 以上代码每隔 1 秒, 就会通过 setInterval() 回调 ReactDOM.render() 方法来重新渲染元素。 注意： 实际上，大多数 React 应用只会调用 ReactDOM.render() 一次。在接下来的章节中，我们将学习如何将这些代码封装到有状态的组件中。 我们建议您不要跳过任何一节，因为每一节之间都是彼此有联系的。 React 只更新必需要更新的部分 React DOM 会将元素及其子元素与之前版本逐一对比, 并只对有必要更新的 DOM 进行更新, 以达到 DOM 所需的状态。 你可以用浏览器工具对 上一个例子 进行检查来验证这一点: 即使我们我们每隔 1 秒都重建了整个元素, 但实际上 React DOM 只更新了修改过的文本节点. 在我们的经验中, 关注每个时间点 UI 的表现, 而不是关注随着时间不断更新 UI 的状态, 可以减少很多奇怪的 bug 。 组件(Components) 和 属性(Props) 组件使你可以将 UI 划分为一个一个独立，可复用的小部件，并可以对每个部件进行单独的设计。 从定义上来说， 组件就像 JavaScript 的函数。组件可以接收任意输入(称为&quot;props&quot;)， 并返回 React 元素，用以描述屏幕显示内容。 愚人码头注：Props ， 即属性(Property)， 在代码中写作 props ， 故可用 props 指代 properties . 函数式组件和类组件 最简单的定义组件的方法是写一个 JavaScript 函数: function Welcome(props) &#123; return &lt;h1&gt;Hello, &#123;props.name&#125;&lt;/h1&gt;;&#125; 这个函数是一个合法的 React 组件，因为它接收一个 props 参数, 并返回一个 React 元素。 我们把此类组件称为&quot;函数式(Functional)&quot;组件， 因为从字面上看来它就是一个 JavaScript 函数。 你也可以用一个 ES6 的 class 来定义一个组件: class Welcome extends React.Component &#123; render() &#123; return &lt;h1&gt;Hello, &#123;this.props.name&#125;&lt;/h1&gt;; &#125;&#125; 上面两个组件从 React 的角度来看是等效的。 类组件有一些额外的特性，我们将在下一节讨论。在此之前, 我们先用函数式组件，因为它们更加简洁。 渲染一个组件 在前面, 我们只遇到代表 DOM 标签的 React 元素： const element = &lt;div /&gt;; 然而，元素也可以代表用户定义的组件： const element = &lt;Welcome name="Sara" /&gt;; 当 React 遇到一个代表用户定义组件的元素时，它将 JSX 属性以一个单独对象的形式传递给相应的组件。 我们将其称为 “props” 对象。 比如, 以下代码在页面上渲染 “Hello, Sara” ： function Welcome(props) &#123; return &lt;h1&gt;Hello, &#123;props.name&#125;&lt;/h1&gt;;&#125;const element = &lt;Welcome name="Sara" /&gt;;ReactDOM.render( element, document.getElementById('root')); 在 CodePen 中尝试。 我们简单扼要重述一下上面这个例子: 我们调用了 ReactDOM.render() 方法并向其中传入了 `` 元素。 React 调用 Welcome 组件，并向其中传入了 {name: 'Sara'} 作为 props 对象。 Welcome 组件返回 Hello, Sara。 React DOM 迅速更新 DOM ，使其显示为 Hello, Sara。 警告： 组件名称总是以大写字母开始。 举例来说, 代表一个 DOM 标签，而 则代表一个组件，并且需要在作用域中有一个 Welcome 组件。 构成组件 组件可以在它们的输出中引用其它组件。这使得我们可以使用同样的组件来抽象到任意层级。一个按钮，一个表单，一个对话框，一个屏幕：在 React 应用中，所有这些都通常描述为组件。 例如，我们可以创建一个 App 组件，并在其内部多次渲染 Welcome： function Welcome(props) &#123; return &lt;h1&gt;Hello, &#123;props.name&#125;&lt;/h1&gt;;&#125;function App() &#123; return ( &lt;div&gt; &lt;Welcome name="Sara" /&gt; &lt;Welcome name="Cahal" /&gt; &lt;Welcome name="Edite" /&gt; &lt;/div&gt; );&#125;ReactDOM.render( &lt;App /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 通常，新的 React apps 都有一个单独的顶层 App 组件。然而，如果你在已有的应用中整合 React，你可以需要由下至上地, 从类似于 Button 这样的小组件开始, 逐渐整合到视图层的顶层。 警告： 组件必须返回一个单独的根元素。这就是为什么我们添加一个 来包含所有 元素的原因。 提取组件 不要害怕把一个组件分为多个更小的组件。 举个例子，思考下名 Comment 组件： function Comment(props) &#123; return ( &lt;div className="Comment"&gt; &lt;div className="UserInfo"&gt; &lt;img className="Avatar" src=&#123;props.author.avatarUrl&#125; alt=&#123;props.author.name&#125; /&gt; &lt;div className="UserInfo-name"&gt; &#123;props.author.name&#125; &lt;/div&gt; &lt;/div&gt; &lt;div className="Comment-text"&gt; &#123;props.text&#125; &lt;/div&gt; &lt;div className="Comment-date"&gt; &#123;formatDate(props.date)&#125; &lt;/div&gt; &lt;/div&gt; );&#125; 在 CodePen 中尝试。 它接受 author（一个对象），text（一个字符串）和 date（一个日期）作为 props，并用于在某社交网站中描述一条评论。 这个组件修改起来很麻烦，因为它是被嵌套的，而且很难复用其中的某个部分。让我们从其中提取一些组件。 首先，提取头像 Avatar： function Avatar(props) &#123; return ( &lt;img className="Avatar" src=&#123;props.user.avatarUrl&#125; alt=&#123;props.user.name&#125; /&gt; );&#125; Avatar 组件不用关心它在 Comment 中是如何渲染的。这是为什么我们它的 prop 一个更通用的属性名: user, 而不是 author 的原因。 我们建议从组件本身的角度来命名 props 而不是它被使用的上下文环境。 我们可以稍微简化一下 Comment 组件: function Comment(props) &#123; return ( &lt;div className="Comment"&gt; &lt;div className="UserInfo"&gt; &lt;Avatar user=&#123;props.author&#125; /&gt; &lt;div className="UserInfo-name"&gt; &#123;props.author.name&#125; &lt;/div&gt; &lt;/div&gt; &lt;div className="Comment-text"&gt; &#123;props.text&#125; &lt;/div&gt; &lt;div className="Comment-date"&gt; &#123;formatDate(props.date)&#125; &lt;/div&gt; &lt;/div&gt; );&#125; 接下来，我们提取用户信息 UserInfo 组件， 用于将 Avatar 显示在用户名旁边： function UserInfo(props) &#123; return ( &lt;div className="UserInfo"&gt; &lt;Avatar user=&#123;props.user&#125; /&gt; &lt;div className="UserInfo-name"&gt; &#123;props.user.name&#125; &lt;/div&gt; &lt;/div&gt; );&#125; 这使我们可以进一步简化 Comment 组件： function Comment(props) &#123; return ( &lt;div className="Comment"&gt; &lt;UserInfo user=&#123;props.author&#125; /&gt; &lt;div className="Comment-text"&gt; &#123;props.text&#125; &lt;/div&gt; &lt;div className="Comment-date"&gt; &#123;formatDate(props.date)&#125; &lt;/div&gt; &lt;/div&gt; );&#125; 在 CodePen 中尝试。 提取组件可能看起来是一个繁琐的工作，但是在大型的 Apps 中可以回报给我们的是大量的可复用组件。一个好的经验准则是如果你 UI 的一部分需要用多次 (Button，Panel，Avatar)，或者本身足够复杂(App，FeedStory，Comment)，最好的做法是使其成为可复用组件。 Props 是只读的 无论你用函数或类的方法来声明组件, 它都无法修改其自身 props. 思考下列 sum (求和)函数: function sum(a, b) &#123; return a + b;&#125; 这种函数称为 “纯函数” ，因为它们不会试图改变它们的输入，并且对于同样的输入,始终可以得到相同的结果。 反之， 以下是非纯函数， 因为它改变了自身的输入值： function withdraw(account, amount) &#123; account.total -= amount;&#125; 虽然 React 很灵活，但是它有一条严格的规则： 所有 React 组件都必须是纯函数，并禁止修改其自身 props 。 当然， 应用 UI 总是动态的，并且随时有可以改变。 所以在下一节, 我们会介绍一个新的概念state(状态) 。state 允许 React 组件在不违反上述规则的情况下, 根据用户操作, 网络响应, 或者其他随便什么东西, 来动态地改变其输出。 状态(State) 和生命周期 思考前面章节中提到过的时钟例子. 目前为止我们只学了一种更新 UI 的方式。 我们通过调 ReactDOM.render() 方法来更新渲染的输出: function tick() &#123; const element = ( &lt;div&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;h2&gt;It is &#123;new Date().toLocaleTimeString()&#125;.&lt;/h2&gt; &lt;/div&gt; ); ReactDOM.render( element, document.getElementById('root') );&#125;setInterval(tick, 1000); 在 CodePen 中尝试。 在本节中，我们将学习如何使 Clock 组件变得真正可复用 和 封装的更好。它将设置自己的计时器，并在每秒更新自身。 我们可以从封装时钟开始： function Clock(props) &#123; return ( &lt;div&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;h2&gt;It is &#123;props.date.toLocaleTimeString()&#125;.&lt;/h2&gt; &lt;/div&gt; );&#125;function tick() &#123; ReactDOM.render( &lt;Clock date=&#123;new Date()&#125; /&gt;, document.getElementById('root') );&#125;setInterval(tick, 1000); 在 CodePen 中尝试。 然而，它没有满足一个关键的要求：Clock 设置定时器并每秒更新 UI ，事实上应该是 Clock自身实现的一部分。 理想情况下，我们应该只引用一个 Clock , 然后让它自动计时并更新: ReactDOM.render( &lt;Clock /&gt;, document.getElementById('root')); 要实现这点，我们需要添加 state 到 Clock 组件。 state 和 props 类似，但是它是私有的，并且由组件本身完全控制。 我们之前提到过, 用类定义的组件有一些额外的特性。 这个&quot;类专有的特性&quot;， 指的就是局部状态。 把函数式组件转化为类组件 你可以遵从以下 5 步, 把一个类似 Clock 这样的函数式组件转化为类组件： 创建一个继承自 React.Component 类的 ES6 class 同名类。 添加一个名为 render() 的空方法。 把原函数中的所有内容移至 render() 中。 在 render() 方法中使用 this.props 替代 props。 删除保留的空函数声明。 class Clock extends React.Component &#123; render() &#123; return ( &lt;div&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;h2&gt;It is &#123;this.props.date.toLocaleTimeString()&#125;.&lt;/h2&gt; &lt;/div&gt; ); &#125;&#125; 在 CodePen 中尝试。 Clock 现在被定为类组件，而不是函数式组件。 类允许我们在其中添加本地状态(state)和生命周期钩子。 在类组件中添加本地状态(state) 我们现在通过以下 3 步, 把date从属性(props) 改为 状态(state)： We will move the date from props to state in three steps: 替换 render() 方法中的 this.props.date 为 this.state.date： class Clock extends React.Component &#123; render() &#123; return ( &lt;div&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;h2&gt;It is &#123;this.state.date.toLocaleTimeString()&#125;.&lt;/h2&gt; &lt;/div&gt; ); &#125;&#125; 添加一个 类构造函数(class constructor) 初始化 this.state: class Clock extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;date: new Date()&#125;; &#125; render() &#123; return ( &lt;div&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;h2&gt;It is &#123;this.state.date.toLocaleTimeString()&#125;.&lt;/h2&gt; &lt;/div&gt; ); &#125;&#125; 注意我们如何将 props 传递给基础构造函数： constructor(props) &#123; super(props); this.state = &#123;date: new Date()&#125;;&#125; 类组件应始终使用 props 调用基础构造函数。 移除 `` 元素中的 date 属性： ReactDOM.render( &lt;Clock /&gt;, document.getElementById('root')); 我们稍后再把 计时器代码 添加到组件内部。 现有的结果是这样: class Clock extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;date: new Date()&#125;; &#125; render() &#123; return ( &lt;div&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;h2&gt;It is &#123;this.state.date.toLocaleTimeString()&#125;.&lt;/h2&gt; &lt;/div&gt; ); &#125;&#125;ReactDOM.render( &lt;Clock /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 接下来，我们将使 Clock 设置自己的计时器，并每秒更新一次。 在类中添加生命周期方法 在一个具有许多组件的应用程序中，在组件被销毁时释放所占用的资源是非常重要的。 当 Clock 第一次渲染到 DOM 时，我们要设置一个定时器 。 这在 React 中称为 “挂载(mounting)” 。 当 Clock 产生的 DOM 被销毁时，我们也想清除该计时器。 这在 React 中称为 “卸载(unmounting)” 。 当组件挂载和卸载时，我们可以在组件类上声明特殊的方法来运行一些代码： class Clock extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;date: new Date()&#125;; &#125; componentDidMount() &#123; &#125; componentWillUnmount() &#123; &#125; render() &#123; return ( &lt;div&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;h2&gt;It is &#123;this.state.date.toLocaleTimeString()&#125;.&lt;/h2&gt; &lt;/div&gt; ); &#125;&#125; 这些方法称为 “生命周期钩子”。 componentDidMount() 钩子在组件输出被渲染到 DOM 之后运行。这是设置时钟的不错的位置： componentDidMount() &#123; this.timerID = setInterval( () =&gt; this.tick(), 1000 );&#125; 注意我们把计时器 ID 直接存在 this 中。 this.props 由 React 本身设定, 而 this.state 具有特殊的含义，但如果需要存储一些不用于视觉输出的内容，则可以手动向类中添加额外的字段。 如果在 render() 方法中没有被引用, 它不应该出现在 state 中。 我们在componentWillUnmount()生命周期钩子中取消这个计时器： componentWillUnmount() &#123; clearInterval(this.timerID);&#125; 最后，我们将会实现每秒运行的 tick() 方法。 它将使用 this.setState() 来来周期性地更新组件本地状态： class Clock extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;date: new Date()&#125;; &#125; componentDidMount() &#123; this.timerID = setInterval( () =&gt; this.tick(), 1000 ); &#125; componentWillUnmount() &#123; clearInterval(this.timerID); &#125; tick() &#123; this.setState(&#123; date: new Date() &#125;); &#125; render() &#123; return ( &lt;div&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;h2&gt;It is &#123;this.state.date.toLocaleTimeString()&#125;.&lt;/h2&gt; &lt;/div&gt; ); &#125;&#125;ReactDOM.render( &lt;Clock /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 现在这个时钟每秒都会走了。 我们来快速回顾一下该过程，以及调用方法的顺序： 当 `` 被传入 ReactDOM.render() 时, React 会调用 Clock组件的构造函数。 因为Clock 要显示的是当前时间，所以它将使用包含当前时间的对象来初始化 this.state。我们稍后会更新此状态。 然后 React 调用了 Clock 组件的 render() 方法。 React 从该方法返回内容中得到要显示在屏幕上的内容。然后，React 然后更新 DOM 以匹配 Clock 的渲染输出。 当 Clock 输出被插入到 DOM 中时，React 调用 componentDidMount() 生命周期钩子。在该方法中，Clock 组件请求浏览器设置一个定时器来一次调用 tick()。 浏览器会每隔一秒调用一次 tick()方法。在该方法中， Clock 组件通过 setState() 方法并传递一个包含当前时间的对象来安排一个 UI 的更新。通过 setState(), React 得知了组件 state(状态)的变化, 随即再次调用 render() 方法，获取了当前应该显示的内容。 这次，render() 方法中的 this.state.date 的值已经发生了改变， 从而，其输出的内容也随之改变。React 于是据此对 DOM 进行更新。 如果通过其他操作将 Clock 组件从 DOM 中移除了, React 会调用componentWillUnmount() 生命周期钩子, 所以计时器也会被停止。 正确地使用 State(状态) 关于 setState() 有三件事是你应该知道的。 不要直接修改 state(状态) 例如，这样将不会重新渲染一个组件： // 错误this.state.comment = 'Hello'; 用 setState() 代替： // 正确this.setState(&#123;comment: 'Hello'&#125;); 唯一可以分配 this.state 的地方是构造函数。 state(状态) 更新可能是异步的 React 为了优化性能，有可能会将多个 setState() 调用合并为一次更新。 因为 this.props 和 this.state 可能是异步更新的，你不能依赖他们的值计算下一个 state(状态)。 例如, 以下代码可能导致 counter(计数器)更新失败： // 错误this.setState(&#123; counter: this.state.counter + this.props.increment,&#125;); 要解决这个问题，应该使用第 2 种 setState() 的格式，它接收一个函数，而不是一个对象。该函数接收前一个状态值作为第 1 个参数， 并将更新后的值作为第 21 个参数: 要弥补这个问题，使用另一种 setState() 的形式，它接受一个函数而不是一个对象。这个函数将接收前一个状态作为第一个参数，应用更新时的 props 作为第二个参数： // 正确this.setState((prevState, props) =&gt; (&#123; counter: prevState.counter + props.increment&#125;)); 我们在上面使用了一个箭头函数，但是也可以使用一个常规的函数： // 正确this.setState(function(prevState, props) &#123; return &#123; counter: prevState.counter + props.increment &#125;;&#125;); state(状态)更新会被合并 当你调用 setState()， React 将合并你提供的对象到当前的状态中。 例如，你的状态可能包含几个独立的变量： constructor(props) &#123; super(props); this.state = &#123; posts: [], comments: [] &#125;;&#125; 然后通过调用独立的 setState() 调用分别更新它们: componentDidMount() &#123; fetchPosts().then(response =&gt; &#123; this.setState(&#123; posts: response.posts &#125;); &#125;); fetchComments().then(response =&gt; &#123; this.setState(&#123; comments: response.comments &#125;); &#125;);&#125; 合并是浅合并，所以 this.setState({comments}) 不会改变 this.state.posts 的值，但会完全替换this.state.comments 的值。 数据向下流动 无论作为父组件还是子组件，它都无法获悉一个组件是否有状态，同时也不需要关心另一个组件是定义为函数组件还是类组件。 这就是 state(状态) 经常被称为 本地状态 或 封装状态的原因。 它不能被拥有并设置它的组件 以外的任何组件访问。 一个组件可以选择将 state(状态) 向下传递，作为其子组件的 props(属性)： &lt;h2&gt;It is &#123;this.state.date.toLocaleTimeString()&#125;.&lt;/h2&gt; 同样适用于用户定义组件: &lt;FormattedDate date=&#123;this.state.date&#125; /&gt; FormattedDate 组件通过 props(属性) 接收了 date 的值，但它仍然不能获知该值是来自于Clock的 state(状态) ，还是 Clock 的 props(属性)，或者是直接手动创建的： function FormattedDate(props) &#123; return &lt;h2&gt;It is &#123;props.date.toLocaleTimeString()&#125;.&lt;/h2&gt;;&#125; 在 CodePen 中尝试。 这通常称为一个“从上到下”，或者“单向”的数据流。任何 state(状态) 始终由某个特定组件所有，并且从该 state(状态) 导出的任何数据 或 UI 只能影响树中 “下方” 的组件。 如果把组件树想像为 props(属性) 的瀑布，所有组件的 state(状态) 就如同一个额外的水源汇入主流，且只能随着主流的方向向下流动。 要证明所有组件都是完全独立的， 我们可以创建一个 App 组件，并在其中渲染 3 个``: function App() &#123; return ( &lt;div&gt; &lt;Clock /&gt; &lt;Clock /&gt; &lt;Clock /&gt; &lt;/div&gt; );&#125;ReactDOM.render( &lt;App /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 每个 Clock 都设置它自己的计时器并独立更新。 在 React 应用中，一个组件是否是有状态或者无状态的，被认为是组件的一个实现细节，随着时间推移可能发生改变。你可以在有状态的组件中使用无状态组件，反之亦然。 通过 React 元素处理事件跟在 DOM 元素上处理事件非常相似。但是有一些语法上的区别： React 事件使用驼峰命名，而不是全部小写。 通过 JSX , 你传递一个函数作为事件处理程序，而不是一个字符串。 例如，HTML： &lt;button onclick="activateLasers()"&gt; Activate Lasers&lt;/button&gt; 在 React 中略有不同： &lt;button onClick=&#123;activateLasers&#125;&gt; Activate Lasers&lt;/button&gt; 另一个区别是，在 React 中你不能通过返回 false（愚人码头注：即 return false; 语句） 来阻止默认行为。必须明确调用 preventDefault 。例如，对于纯 HTML ，要阻止链接打开一个新页面的默认行为，可以这样写： &lt;a href="#" onclick="console.log('The link was clicked.'); return false"&gt; Click me&lt;/a&gt; 在 React 中, 应该这么写: function ActionLink() &#123; function handleClick(e) &#123; e.preventDefault(); console.log('The link was clicked.'); &#125; return ( &lt;a href="#" onClick=&#123;handleClick&#125;&gt; Click me &lt;/a&gt; );&#125; 这里， e 是一个合成的事件。 React 根据 W3C 规范 定义了这个合成事件，所以你不需要担心跨浏览器的兼容性问题。查看 SyntheticEvent 参考指南了解更多。 当使用 React 时，你一般不需要调用 addEventListener 在 DOM 元素被创建后添加事件监听器。相反，只要当元素被初始渲染的时候提供一个监听器就可以了。 当使用一个 ES6 类 定义一个组件时，通常的一个事件处理程序是类上的一个方法。例如，Toggle 组件渲染一个按钮，让用户在 “ON” 和 “OFF” 状态之间切换： class Toggle extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;isToggleOn: true&#125;; // 这个绑定是必要的，使`this`在回调中起作用 this.handleClick = this.handleClick.bind(this); &#125; handleClick() &#123; this.setState(prevState =&gt; (&#123; isToggleOn: !prevState.isToggleOn &#125;)); &#125; render() &#123; return ( &lt;button onClick=&#123;this.handleClick&#125;&gt; &#123;this.state.isToggleOn ? 'ON' : 'OFF'&#125; &lt;/button&gt; ); &#125;&#125;ReactDOM.render( &lt;Toggle /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 在 JSX 回调中你必须注意 this 的指向。 在 JavaScript 中，类方法默认没有 绑定 的。如果你忘记绑定 this.handleClick 并将其传递给onClick，那么在直接调用该函数时，this 会是undefined 。 这不是 React 特有的行为；这是 JavaScript 中的函数如何工作的一部分。 一般情况下，如果你引用一个后面没跟 () 的方法，例如 onClick={this.handleClick} ，那你就应该 绑定(bind) 该方法。 如果调用 bind 令你烦恼，有两种方法可以解决这个问题。 如果您使用实验性的 属性初始化语法 ，那么你可以使用属性初始值设置来正确地 绑定(bind) 回调： class LoggingButton extends React.Component &#123; // 这个语法确保 `this` 绑定在 handleClick 中。 // 警告：这是 *实验性的* 语法。 handleClick = () =&gt; &#123; console.log('this is:', this); &#125; render() &#123; return ( &lt;button onClick=&#123;this.handleClick&#125;&gt; Click me &lt;/button&gt; ); &#125;&#125; 这个语法在 创建 React App 中是默认开启的。 如果你没有使用属性初始化语法，可以在回调中使用一个 箭头函数： class LoggingButton extends React.Component &#123; handleClick() &#123; console.log('this is:', this); &#125; render() &#123; // 这个语法确保 `this` 被绑定在 handleClick 中 return ( &lt;button onClick=&#123;(e) =&gt; this.handleClick(e)&#125;&gt; Click me &lt;/button&gt; ); &#125;&#125; 这个语法的问题是，每次 LoggingButton 渲染时都创建一个不同的回调。在多数情况下，没什么问题。然而，如果这个回调被作为 prop(属性) 传递给下级组件，这些组件可能需要额外的重复渲染。我们通常建议在构造函数中进行绑定，以避免这类性能问题。 条件渲染 在 React 中，你可以创建不同的组件封装你所需要的行为。然后，只渲染它们之中的一些，取决于你的应用的状态。 React 中的条件渲染就和在 JavaScript 中的条件语句一样。使用 JavaScript 操作符如 if 或者条件操作符来创建渲染当前状态的元素，并且让 React 更新匹配的 UI 。 思考以下两个组件： function UserGreeting(props) &#123; return &lt;h1&gt;Welcome back!&lt;/h1&gt;;&#125;function GuestGreeting(props) &#123; return &lt;h1&gt;Please sign up.&lt;/h1&gt;;&#125; 我们需要创建一个 Greeting 组件, 用来根据用户是否登录, 判断并显示上述两个组件之一： function Greeting(props) &#123; const isLoggedIn = props.isLoggedIn; if (isLoggedIn) &#123; return &lt;UserGreeting /&gt;; &#125; return &lt;GuestGreeting /&gt;;&#125;ReactDOM.render( // 修改为 isLoggedIn=&#123;true&#125; 试试: &lt;Greeting isLoggedIn=&#123;false&#125; /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 这个例子根据 isLoggedIn prop 渲染了不同的问候语 。 元素变量 你可以用变量来存储元素。这可以帮助您有条件地渲染组件的一部分，而输出的其余部分不会更改。 思考以下两个新组件，分别用于显示登出和登入按钮： function LoginButton(props) &#123; return ( &lt;button onClick=&#123;props.onClick&#125;&gt; Login &lt;/button&gt; );&#125;function LogoutButton(props) &#123; return ( &lt;button onClick=&#123;props.onClick&#125;&gt; Logout &lt;/button&gt; );&#125; 在接下来的例子中，我们将会创建一个有状态组件，叫做 LoginControl 。 它将渲染 或者 ，取决于当前状态。同时渲染前面提到的`` 组件: class LoginControl extends React.Component &#123; constructor(props) &#123; super(props); this.handleLoginClick = this.handleLoginClick.bind(this); this.handleLogoutClick = this.handleLogoutClick.bind(this); this.state = &#123;isLoggedIn: false&#125;; &#125; handleLoginClick() &#123; this.setState(&#123;isLoggedIn: true&#125;); &#125; handleLogoutClick() &#123; this.setState(&#123;isLoggedIn: false&#125;); &#125; render() &#123; const isLoggedIn = this.state.isLoggedIn; let button = null; if (isLoggedIn) &#123; button = &lt;LogoutButton onClick=&#123;this.handleLogoutClick&#125; /&gt;; &#125; else &#123; button = &lt;LoginButton onClick=&#123;this.handleLoginClick&#125; /&gt;; &#125; return ( &lt;div&gt; &lt;Greeting isLoggedIn=&#123;isLoggedIn&#125; /&gt; &#123;button&#125; &lt;/div&gt; ); &#125;&#125;ReactDOM.render( &lt;LoginControl /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 虽然声明一个变量并使用一个 if 语句是一个有条件地渲染组件的好方法，有时你可能想要使用一个更简短的语法。在 JSX 中有几种内联条件的方法，如下所述。 使用逻辑 &amp;&amp; 操作符的内联 if 用法 您可以 在 JSX 中嵌入任何表达式 ，方法是将其包裹在花括号中。这也包括 JavaScript 逻辑&amp;&amp; 运算符。 它有助于有条件地包含一个元素： function Mailbox(props) &#123; const unreadMessages = props.unreadMessages; return ( &lt;div&gt; &lt;h1&gt;Hello!&lt;/h1&gt; &#123;unreadMessages.length &gt; 0 &amp;&amp; &lt;h2&gt; You have &#123;unreadMessages.length&#125; unread messages. &lt;/h2&gt; &#125; &lt;/div&gt; );&#125;const messages = ['React', 'Re: React', 'Re:Re: React'];ReactDOM.render( &lt;Mailbox unreadMessages=&#123;messages&#125; /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 它可以正常运行，因为在 JavaScript 中， true &amp;&amp; expression 总是会评估为 expression ，而false &amp;&amp; expression 总是执行为 false 。 因此，如果条件为 true ，则 &amp;&amp; 后面的元素将显示在输出中。 如果是 false，React 将会忽略并跳过它。 使用条件操作符的内联 If-Else 另一个用于条件渲染元素的内联方法是使用 JavaScript 的条件操作符 condition ? true : false 。 在下面这个例子中，我们使用它来进行条件渲染一个小的文本块： render() &#123; const isLoggedIn = this.state.isLoggedIn; return ( &lt;div&gt; The user is &lt;b&gt;&#123;isLoggedIn ? 'currently' : 'not'&#125;&lt;/b&gt; logged in. &lt;/div&gt; );&#125; 它也可以用于更大的表达式，虽然不太明显发生了什么： render() &#123; const isLoggedIn = this.state.isLoggedIn; return ( &lt;div&gt; &#123;isLoggedIn ? ( &lt;LogoutButton onClick=&#123;this.handleLogoutClick&#125; /&gt; ) : ( &lt;LoginButton onClick=&#123;this.handleLoginClick&#125; /&gt; )&#125; &lt;/div&gt; );&#125; 就像 JavaScript 一样，你可以根据你和你的团队认为更易于阅读的方式选择合适的风格。还要记住，无论何时何地，当条件变得太复杂时，可能是提取组件的好时机。 防止组件渲染 在极少数情况下，您可能希望组件隐藏自身，即使它是由另一个组件渲染的。为此，返回null 而不是其渲染输出。 在下面的例子中，根据名为warn的 prop 值，呈现 `` 。如果 prop 值为 false，则该组件不渲染： function WarningBanner(props) &#123; if (!props.warn) &#123; return null; &#125; return ( &lt;div className="warning"&gt; Warning! &lt;/div&gt; );&#125;class Page extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;showWarning: true&#125; this.handleToggleClick = this.handleToggleClick.bind(this); &#125; handleToggleClick() &#123; this.setState(prevState =&gt; (&#123; showWarning: !prevState.showWarning &#125;)); &#125; render() &#123; return ( &lt;div&gt; &lt;WarningBanner warn=&#123;this.state.showWarning&#125; /&gt; &lt;button onClick=&#123;this.handleToggleClick&#125;&gt; &#123;this.state.showWarning ? 'Hide' : 'Show'&#125; &lt;/button&gt; &lt;/div&gt; ); &#125;&#125;ReactDOM.render( &lt;Page /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 从组件的 render 方法返回 null 不会影响组件生命周期方法的触发。 例如，componentWillUpdate 和 componentDidUpdate 仍将被调用。 列表(Lists) 和 键(Keys) 首先，让我们回顾一下在 JavaScript 中如何转换列表。 给定下面的代码，我们使用 map() 函数使 numbers 数组中的元素值翻倍。我们将 map() 返回的新数组分配给变量 doubled，并且打印这个它： const numbers = [1, 2, 3, 4, 5];const doubled = numbers.map((number) =&gt; number * 2);console.log(doubled); 这段代码在控制台中打印为 [2, 4, 6, 8, 10]。 在 React 中，转换数组为 元素列表 的方式，和上述方法基本相同。 多组件渲染 可以创建元素集合，并用一对大括号 {} 在 JSX 中直接将其引用即可。 下面，我们用 JavaScript 的 map() 函数将 numbers 数组循环处理。对于每一项，我们返回一个 `` 元素。最终，我们将结果元素数组分配给 listItems： const numbers = [1, 2, 3, 4, 5];const listItems = numbers.map((number) =&gt; &lt;li&gt;&#123;number&#125;&lt;/li&gt;); 把整个 listItems 数组包含到一个 `` 元素，并渲染到 DOM： ReactDOM.render( &lt;ul&gt;&#123;listItems&#125;&lt;/ul&gt;, document.getElementById('root')); 在 CodePen 中尝试。 这段代码显示从 1 到 5 的数字列表。 基本列表组件 通常情况下，我们会在一个组件中渲染列表。 我们可以重构前面的例子到一个组件，它接受一个 numbers 数组，并输出一个元素的无序列表。 function NumberList(props) &#123; const numbers = props.numbers; const listItems = numbers.map((number) =&gt; &lt;li&gt;&#123;number&#125;&lt;/li&gt; ); return ( &lt;ul&gt;&#123;listItems&#125;&lt;/ul&gt; );&#125;const numbers = [1, 2, 3, 4, 5];ReactDOM.render( &lt;NumberList numbers=&#123;numbers&#125; /&gt;, document.getElementById('root')); 当运行上述代码的时候，将会收到一个警告：a key should be provided for list items（应该为列表元素提供一个键）（愚人码头注 ：CodeOpen 中没有报警告，是因为其示例中使用的是 min 版本的 React，换成非 min 版本的就可以看到）。当创建元素列表时，“key” 是一个你需要包含的特殊字符串属性。我们将在下一节讨论它的重要性。 我们在 numbers.map() 中赋值一个 key 给我们的列表元素，解决丢失 key 的问题。 function NumberList(props) &#123; const numbers = props.numbers; const listItems = numbers.map((number) =&gt; &lt;li key=&#123;number.toString()&#125;&gt; &#123;number&#125; &lt;/li&gt; ); return ( &lt;ul&gt;&#123;listItems&#125;&lt;/ul&gt; );&#125;const numbers = [1, 2, 3, 4, 5];ReactDOM.render( &lt;NumberList numbers=&#123;numbers&#125; /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 键(Keys) 键(Keys) 帮助 React 标识哪个项被修改、添加或者移除了。数组中的每一个元素都应该有一个唯一不变的键(Keys)来标识： const numbers = [1, 2, 3, 4, 5];const listItems = numbers.map((number) =&gt; &lt;li key=&#123;number.toString()&#125;&gt; &#123;number&#125; &lt;/li&gt;); 挑选 key 最好的方式是使用一个在它的同辈元素中不重复的标识字符串。多数情况你可以使用数据中的 IDs 作为 keys： const todoItems = todos.map((todo) =&gt; &lt;li key=&#123;todo.id&#125;&gt; &#123;todo.text&#125; &lt;/li&gt;); 当要渲染的列表项中没有稳定的 IDs 时，你可以使用数据项的索引值作为 key 的最后选择： const todoItems = todos.map((todo, index) =&gt; // Only do this if items have no stable IDs &lt;li key=&#123;index&#125;&gt; &#123;todo.text&#125; &lt;/li&gt;); 如果列表项可能被重新排序时，我们不建议使用索引作为 keys，因为这导致一定的性能问题，会很慢。如果感兴趣，你可以阅读一下深入的介绍关于为什么 keys 是必须的。 使用 keys 提取组件 keys 只在数组的上下文中存在意义。 例如，如果你提取 一个 ListItem 组件，应该把 key 放置在数组处理的 元素中，不能放在 `ListItem` 组件自身中的 根元素上。 例子：错误的 key 用法 function ListItem(props) &#123; const value = props.value; return ( // 错误！不需要在这里指定 key： &lt;li key=&#123;value.toString()&#125;&gt; &#123;value&#125; &lt;/li&gt; );&#125;function NumberList(props) &#123; const numbers = props.numbers; const listItems = numbers.map((number) =&gt; // 错误！key 应该在这里指定： &lt;ListItem value=&#123;number&#125; /&gt; ); return ( &lt;ul&gt; &#123;listItems&#125; &lt;/ul&gt; );&#125;const numbers = [1, 2, 3, 4, 5];ReactDOM.render( &lt;NumberList numbers=&#123;numbers&#125; /&gt;, document.getElementById('root')); 错误！key 应该在这里指定： function ListItem(props) &#123; // 正确！这里不需要指定 key ： return &lt;li&gt;&#123;props.value&#125;&lt;/li&gt;;&#125;function NumberList(props) &#123; const numbers = props.numbers; const listItems = numbers.map((number) =&gt; // 正确！key 应该在这里被指定 &lt;ListItem key=&#123;number.toString()&#125; value=&#123;number&#125; /&gt; ); return ( &lt;ul&gt; &#123;listItems&#125; &lt;/ul&gt; );&#125;const numbers = [1, 2, 3, 4, 5];ReactDOM.render( &lt;NumberList numbers=&#123;numbers&#125; /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 一个好的经验准则是元素中调用 map() 需要 keys 。 keys 在同辈元素中必须是唯一的 在数组中使用的 keys 必须在它们的同辈之间唯一。然而它们并不需要全局唯一。我们可以在操作两个不同数组的时候使用相同的 keys ： function Blog(props) &#123; const sidebar = ( &lt;ul&gt; &#123;props.posts.map((post) =&gt; &lt;li key=&#123;post.id&#125;&gt; &#123;post.title&#125; &lt;/li&gt; )&#125; &lt;/ul&gt; ); const content = props.posts.map((post) =&gt; &lt;div key=&#123;post.id&#125;&gt; &lt;h3&gt;&#123;post.title&#125;&lt;/h3&gt; &lt;p&gt;&#123;post.content&#125;&lt;/p&gt; &lt;/div&gt; ); return ( &lt;div&gt; &#123;sidebar&#125; &lt;hr /&gt; &#123;content&#125; &lt;/div&gt; );&#125;const posts = [ &#123;id: 1, title: 'Hello World', content: 'Welcome to learning React!'&#125;, &#123;id: 2, title: 'Installation', content: 'You can install React from npm.'&#125;];ReactDOM.render( &lt;Blog posts=&#123;posts&#125; /&gt;, document.getElementById('root')); 在 CodePen 中尝试。 键是 React 的一个内部映射，但其不会传递给组件的内部。如果你需要在组件中使用相同的值，可以明确使用一个不同名字的 prop 传入。 const content = posts.map((post) =&gt; &lt;Post key=&#123;post.id&#125; id=&#123;post.id&#125; title=&#123;post.title&#125; /&gt;); 上面的例子中， Post 组件可以读取 props.id，但是不能读取 props.key 。 在 JSX 中嵌入 map() 在上面的例子中，我们单独声明了一个 listItems 变量，并在 JSX 中引用了该变量： function NumberList(props) &#123; const numbers = props.numbers; const listItems = numbers.map((number) =&gt; &lt;ListItem key=&#123;number.toString()&#125; value=&#123;number&#125; /&gt; ); return ( &lt;ul&gt; &#123;listItems&#125; &lt;/ul&gt; );&#125; JSX 允许在大括号中嵌入任何表达式，因此可以 内联 map() 结果： function NumberList(props) &#123; const numbers = props.numbers; return ( &lt;ul&gt; &#123;numbers.map((number) =&gt; &lt;ListItem key=&#123;number.toString()&#125; value=&#123;number&#125; /&gt; )&#125; &lt;/ul&gt; );&#125; 在 CodePen 中尝试。 有时这可以产生清晰的代码，但是这个风格也可能被滥用。就像在 JavaScript 中，是否有必要提取一个变量以提高程序的可读性，这取决于你。但是记住，如果 map() 体中有太多嵌套，可能是提取组件的好时机。 表单(Forms) HTML 表单元素与 React 中的其他 DOM 元素有所不同，因为表单元素自然地保留了一些内部状态。例如，这个纯 HTML 表单接受一个单独的 name： &lt;form&gt; &lt;label&gt; Name: &lt;input type="text" name="name" /&gt; &lt;/label&gt; &lt;input type="submit" value="Submit" /&gt;&lt;/form&gt; 该表单和 HTML 表单的默认行为一致，当用户提交此表单时浏览器会打开一个新页面。如果你希望 React 中保持这个行为，也可以工作。但是多数情况下，用一个处理表单提交并访问用户输入到表单中的数据的 JavaScript 函数也很方便。实现这一点的标准方法是使用一种称为“受控组件(controlled components)”的技术。 受控组件(Controlled Components) 在 HTML 中，表单元素如 ， 和 `` 表单元素通常保持自己的状态，并根据用户输入进行更新。而在 React 中，可变状态一般保存在组件的 state(状态) 属性中，并且只能通过 setState() 更新。 我们可以通过使 React 的 state 成为 “单一数据源原则” 来结合这两个形式。然后渲染表单的 React 组件也可以控制在用户输入之后的行为。这种形式，其值由 React 控制的输入表单元素称为“受控组件”。 例如，如果我们想使上一个例子在提交时记录名称，我们可以将表单写为受控组件： class NameForm extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;value: ''&#125;; this.handleChange = this.handleChange.bind(this); this.handleSubmit = this.handleSubmit.bind(this); &#125; handleChange(event) &#123; this.setState(&#123;value: event.target.value&#125;); &#125; handleSubmit(event) &#123; alert('A name was submitted: ' + this.state.value); event.preventDefault(); &#125; render() &#123; return ( &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt; &lt;label&gt; Name: &lt;input type="text" value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125; /&gt; &lt;/label&gt; &lt;input type="submit" value="Submit" /&gt; &lt;/form&gt; ); &#125;&#125; 在 CodePen 中尝试。 设置表单元素的 value 属性之后，其显示值将由 this.state.value 决定，以满足 React 状态的同一数据理念。每次键盘敲击之后会执行 handleChange 方法以更新 React 状态，显示值也将随着用户的输入改变。 由于 value 属性设置在我们的表单元素上，显示的值总是 this.state.value，以满足 state 状态的同一数据理念。由于 handleChange 在每次敲击键盘时运行，以更新 React state(状态)，显示的值将更新为用户的输入。 对于受控组件来说，每一次 state(状态) 变化都会伴有相关联的处理函数。这使得可以直接修改或验证用户的输入。比如，如果我们希望强制 name 的输入都是大写字母，可以这样来写handleChange 方法： handleChange(event) &#123; this.setState(&#123;value: event.target.value.toUpperCase()&#125;);&#125; textare 标签 在 HTML 中，`` 元素通过它的子节点定义了它的文本值： &lt;textarea&gt; Hello there, this is some text in a text area&lt;/textarea&gt; 在 React 中，的赋值使用 `value` 属性替代。这样一来，表单中 的书写方式接近于单行文本输入框 ： class EssayForm extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123; value: 'Please write an essay about your favorite DOM element.' &#125;; this.handleChange = this.handleChange.bind(this); this.handleSubmit = this.handleSubmit.bind(this); &#125; handleChange(event) &#123; this.setState(&#123;value: event.target.value&#125;); &#125; handleSubmit(event) &#123; alert('An essay was submitted: ' + this.state.value); event.preventDefault(); &#125; render() &#123; return ( &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt; &lt;label&gt; Name: &lt;textarea value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125; /&gt; &lt;/label&gt; &lt;input type="submit" value="Submit" /&gt; &lt;/form&gt; ); &#125;&#125; 注意，this.state.value 在构造函数中初始化，所以这些文本一开始就出现在文本域中。 select 标签 在 HTML 中，`` 创建了一个下拉列表。例如，这段 HTML 创建一个下拉的口味(flavors)列表： &lt;select&gt; &lt;option value="grapefruit"&gt;Grapefruit&lt;/option&gt; &lt;option value="lime"&gt;Lime&lt;/option&gt; &lt;option selected value="coconut"&gt;Coconut&lt;/option&gt; &lt;option value="mango"&gt;Mango&lt;/option&gt;&lt;/select&gt; 注意，Coconut 选项是初始化选中的，因为它的 selected 属性。React 中，并不使用这个selected 属性，而是在根 select 标签中使用了一个 value 属性。这使得受控组件使用更方便，因为你只需要更新一处即可。例如： class FlavorForm extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;value: 'coconut'&#125;; this.handleChange = this.handleChange.bind(this); this.handleSubmit = this.handleSubmit.bind(this); &#125; handleChange(event) &#123; this.setState(&#123;value: event.target.value&#125;); &#125; handleSubmit(event) &#123; alert('Your favorite flavor is: ' + this.state.value); event.preventDefault(); &#125; render() &#123; return ( &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt; &lt;label&gt; Pick your favorite La Croix flavor: &lt;select value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125;&gt; &lt;option value="grapefruit"&gt;Grapefruit&lt;/option&gt; &lt;option value="lime"&gt;Lime&lt;/option&gt; &lt;option value="coconut"&gt;Coconut&lt;/option&gt; &lt;option value="mango"&gt;Mango&lt;/option&gt; &lt;/select&gt; &lt;/label&gt; &lt;input type="submit" value="Submit" /&gt; &lt;/form&gt; ); &#125;&#125; 在 CodePen 中尝试。 总的来说，这使 ， 和 `` 都以类似的方式工作 —— 它们都接受一个 value 属性可以用来实现一个受控组件。 处理多个输入元素 当您需要处理多个受控的 input 元素时，您可以为每个元素添加一个 name 属性，并且让处理函数根据 event.target.name 的值来选择要做什么。 例如： class Reservation extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123; isGoing: true, numberOfGuests: 2 &#125;; this.handleInputChange = this.handleInputChange.bind(this); &#125; handleInputChange(event) &#123; const target = event.target; const value = target.type === 'checkbox' ? target.checked : target.value; const name = target.name; this.setState(&#123; [name]: value &#125;); &#125; render() &#123; return ( &lt;form&gt; &lt;label&gt; Is going: &lt;input name="isGoing" type="checkbox" checked=&#123;this.state.isGoing&#125; onChange=&#123;this.handleInputChange&#125; /&gt; &lt;/label&gt; &lt;br /&gt; &lt;label&gt; Number of guests: &lt;input name="numberOfGuests" type="number" value=&#123;this.state.numberOfGuests&#125; onChange=&#123;this.handleInputChange&#125; /&gt; &lt;/label&gt; &lt;/form&gt; ); &#125;&#125; 在 CodePen 中尝试。 注意我们如何使用 ES6计算的属性名称语法来更新与给定输入名称相对应的 state(状态) 键： this.setState(&#123; [name]: value&#125;); 这段代码等价于 ES5 代码: var partialState = &#123;&#125;;partialState[name] = value;this.setState(partialState); 此外，由于 setState() 自动将部分状态合并到当前状态，所以我们只需要调用更改的部分即可。 受控组件的替代方案 有时使用受控组件有些乏味，因为你需要为每一个可更改的数据提供事件处理器，并通过 React 组件管理所有输入状态。当你将已经存在的代码转换为 React 时，或将 React 应用程序与非 React 库集成时，这可能变得特别烦人。在这些情况下，您可能需要使用不受控的组件，用于实现输入表单的替代技术。 状态提升(Lifting State Up) 通常情况下，同一个数据的变化需要几个不同的组件来反映。我们建议提升共享的状态到它们最近的祖先组件中。我们看下这是如何运作的。 在本节，我们将会创建一个温度计算器，用来计算水在一个给定温度下是否会沸腾。 我们通过一个称为 BoilingVerdict 的组件开始。它接受 celsius（摄氏温度）作为 prop ，并打印是否足以使水沸腾： function BoilingVerdict(props) &#123; if (props.celsius &gt;= 100) &#123; return &lt;p&gt;The water would boil.&lt;/p&gt;; &#125; return &lt;p&gt;The water would not boil.&lt;/p&gt;;&#125; 接下来，我们将会创建一个 Calculator 组件。它渲染一个 `` 让你输入温度，并在this.state.temperature 中保存它的值。 另外，它会根据当前输入的温度来渲染 BoilingVerdict 。 class Calculator extends React.Component &#123; constructor(props) &#123; super(props); this.handleChange = this.handleChange.bind(this); this.state = &#123;temperature: ''&#125;; &#125; handleChange(e) &#123; this.setState(&#123;temperature: e.target.value&#125;); &#125; render() &#123; const temperature = this.state.temperature; return ( &lt;fieldset&gt; &lt;legend&gt;Enter temperature in Celsius:&lt;/legend&gt; &lt;input value=&#123;temperature&#125; onChange=&#123;this.handleChange&#125; /&gt; &lt;BoilingVerdict celsius=&#123;parseFloat(temperature)&#125; /&gt; &lt;/fieldset&gt; ); &#125;&#125; 在 CodePen 中尝试。 添加第二个输入 我们新的需求是，除了一个摄氏温度输入之外，我们再提供了一个华氏温度输入，并且两者保持自动同步。 我们可以从 Calculator 中提取一个 TemperatureInput 组件开始。我们将添加一个新的 scale属性，值可能是 &quot;c&quot; 或者 &quot;f&quot; ： const scaleNames = &#123; c: 'Celsius', f: 'Fahrenheit'&#125;;class TemperatureInput extends React.Component &#123; constructor(props) &#123; super(props); this.handleChange = this.handleChange.bind(this); this.state = &#123;temperature: ''&#125;; &#125; handleChange(e) &#123; this.setState(&#123;temperature: e.target.value&#125;); &#125; render() &#123; const temperature = this.state.temperature; const scale = this.props.scale; return ( &lt;fieldset&gt; &lt;legend&gt;Enter temperature in &#123;scaleNames[scale]&#125;:&lt;/legend&gt; &lt;input value=&#123;temperature&#125; onChange=&#123;this.handleChange&#125; /&gt; &lt;/fieldset&gt; ); &#125;&#125; 现在我们可以修改 Calculator 来渲染两个独立的温度输入： class Calculator extends React.Component &#123; render() &#123; return ( &lt;div&gt; &lt;TemperatureInput scale="c" /&gt; &lt;TemperatureInput scale="f" /&gt; &lt;/div&gt; ); &#125;&#125; 在 CodePen 中尝试。 我们现在有两个 (input)输入框 了，但是当你输入其中一个温度时，另一个输入并没有更新。这是跟我们的需要不符的：我们希望它们保持同步。 我们也不能在 Calculator 中显示 BoilingVerdict 。 Calculator 不知道当前的温度，因为它是在 TemperatureInput 中隐藏的。 编写转换函数 首先，我们编写两个函数来在摄氏温度和华氏温度之间转换： function toCelsius(fahrenheit) &#123; return (fahrenheit - 32) * 5 / 9;&#125;function toFahrenheit(celsius) &#123; return (celsius * 9 / 5) + 32;&#125; 这两个函数用来转化数字。接下来再编写一个函数用来接收一个字符串 temperature 和一个 转化器函数 作为参数，并返回一个字符串。这个函数用来在两个输入之间进行相互转换。 对于无效的 temperature 值，它返回一个空字符串，输出结果保留 3 位小数： function tryConvert(temperature, convert) &#123; const input = parseFloat(temperature); if (Number.isNaN(input)) &#123; return ''; &#125; const output = convert(input); const rounded = Math.round(output * 1000) / 1000; return rounded.toString();&#125; 例如， tryConvert('abc', toCelsius) 将返回一个空字符串，而 tryConvert('10.22', toFahrenheit) 返回 '50.396' 。 状态提升(Lifting State Up) 目前，两个 TemperatureInput 组件都将其值保持在本地状态中： class TemperatureInput extends React.Component &#123; constructor(props) &#123; super(props); this.handleChange = this.handleChange.bind(this); this.state = &#123;temperature: ''&#125;; &#125; handleChange(e) &#123; this.setState(&#123;temperature: e.target.value&#125;); &#125; render() &#123; const temperature = this.state.temperature; 但是，我们希望这两个输入是相互同步的。当我们更新摄氏温度输入时，华氏温度输入应反映转换后的温度，反之亦然。 在 React 中，共享 state(状态) 是通过将其移动到需要它的组件的最接近的共同祖先组件来实现的。 这被称为“状态提升(Lifting State Up)”。我们将从 TemperatureInput 中移除相关状态本地状态，并将其移动到 Calculator 中。 如果 Calculator 拥有共享状态，那么它将成为两个输入当前温度的“单一数据来源”。它可以指示他们具有彼此一致的值。由于两个 TemperatureInput 组件的 props 都来自同一个父级Calculator组件，两个输入将始终保持同步。 让我们一步一步看看这是如何工作的。 首先，我们将在 TemperatureInput 组件中用 this.props.temperature 替换this.state.temperature 。 现在，我们假装 this.props.temperature 已经存在，虽然我们将来需要从 Calculator 传递过来： render() &#123; // 之前是: const temperature = this.state.temperature; const temperature = this.props.temperature; 我们知道 props(属性) 是只读的。 当 temperature 是 本地 state(状态)时， TemperatureInput可以调用 this.setState() 来更改它。 然而，现在 temperature 来自父级作为 prop(属性) ，TemperatureInput 就无法控制它。 在 React 中，通常通过使组件“受控”的方式来解决。就像 DOM ``一样接受一个 value和一个 onChange prop(属性) ，所以可以定制 TemperatureInput 接受来自其父级 Calculator 的temperature 和 onTemperatureChange 。 现在，当 TemperatureInput 想要更新其温度时，它就会调用this.props.onTemperatureChange： handleChange(e) &#123; // 之前是: this.setState(&#123;temperature: e.target.value&#125;); this.props.onTemperatureChange(e.target.value); 请注意，自定义组件中的 temperature 或 onTemperatureChange prop(属性) 名称没有特殊的含义。我们可以命名为任何其他名称，像命名他们为 value 和 onChange，是一个常见的惯例。 onTemperatureChange prop(属性) 和 temperature prop(属性) 一起由父级的 Calculator 组件提供。它将通过修改自己的本地 state(状态) 来处理变更，从而通过新值重新渲染两个输入。我们将很快看到新的 Calculator 实现。 在修改 Calculator 之前，让我们回顾一下对 TemperatureInput 组件的更改。我们已经从中删除了本地 state(状态) ，不是读取this.state.temperature ，我们现在读取this.props.temperature 。当我们想要更改时， 不是调用 this.setState() ，而是调用this.props.onTemperatureChange()， 这将由 Calculator 提供： class TemperatureInput extends React.Component &#123; constructor(props) &#123; super(props); this.handleChange = this.handleChange.bind(this); &#125; handleChange(e) &#123; this.props.onTemperatureChange(e.target.value); &#125; render() &#123; const temperature = this.props.temperature; const scale = this.props.scale; return ( &lt;fieldset&gt; &lt;legend&gt;Enter temperature in &#123;scaleNames[scale]&#125;:&lt;/legend&gt; &lt;input value=&#123;temperature&#125; onChange=&#123;this.handleChange&#125; /&gt; &lt;/fieldset&gt; ); &#125;&#125; 现在我们来看一下 Calculator 组件。 我们将当前输入的 temperature 和 scale 存储在本地 state(状态) 中。这是我们从输入 “提升” 的 state(状态) ，它将作为两个输入的 “单一数据来源” 。为了渲染两个输入，我们需要知道的所有数据的最小表示。 例如，如果我们在摄氏度输入框中输入 37 ，则 Calculator 组件的状态将是： &#123; temperature: '37', scale: 'c'&#125; 如果我们稍后将华氏温度字段编辑为 212 ，则 Calculator 组件的状态将是： &#123; temperature: '212', scale: 'f'&#125; 我们可以存储两个输入框的值，但事实证明是不必要的。存储最近更改的输入框的值，以及它所表示的度量衡就够了。然后，我们可以基于当前的 temperature(温度) 和 scale(度量衡) 来推断其他输入的值。 输入框保持同步，因为它们的值是从相同的 state(状态) 计算出来的： class Calculator extends React.Component &#123; constructor(props) &#123; super(props); this.handleCelsiusChange = this.handleCelsiusChange.bind(this); this.handleFahrenheitChange = this.handleFahrenheitChange.bind(this); this.state = &#123;temperature: '', scale: 'c'&#125;; &#125; handleCelsiusChange(temperature) &#123; this.setState(&#123;scale: 'c', temperature&#125;); &#125; handleFahrenheitChange(temperature) &#123; this.setState(&#123;scale: 'f', temperature&#125;); &#125; render() &#123; const scale = this.state.scale; const temperature = this.state.temperature; const celsius = scale === 'f' ? tryConvert(temperature, toCelsius) : temperature; const fahrenheit = scale === 'c' ? tryConvert(temperature, toFahrenheit) : temperature; return ( &lt;div&gt; &lt;TemperatureInput scale="c" temperature=&#123;celsius&#125; onTemperatureChange=&#123;this.handleCelsiusChange&#125; /&gt; &lt;TemperatureInput scale="f" temperature=&#123;fahrenheit&#125; onTemperatureChange=&#123;this.handleFahrenheitChange&#125; /&gt; &lt;BoilingVerdict celsius=&#123;parseFloat(celsius)&#125; /&gt; &lt;/div&gt; ); &#125;&#125; 在 CodePen 中尝试。 现在，无论你编辑哪个输入框，Calculator 中的 this.state.temperature 和 this.state.scale都会更新。其中一个输入框获取值，所以任何用户输入都被保留，并且另一个输入总是基于它重新计算值。 让我们回顾一下编辑输入时会发生什么： React 调用在 DOM `` 上的 onChange 指定的函数。在我们的例子中，这是TemperatureInput 组件中的 handleChange 方法。 TemperatureInput 组件中的 handleChange 方法使用 新的期望值 调用this.props.onTemperatureChange()。TemperatureInput 组件中的 props(属性) ，包括onTemperatureChange，由其父组件 Calculator 提供。 当它预先呈现时， Calculator 指定了摄氏 TemperatureInput 的 onTemperatureChange 是Calculator 的 handleCelsiusChange 方法，并且华氏 TemperatureInput 的onTemperatureChange 是 Calculator 的 handleFahrenheitChange 方法。因此，会根据我们编辑的输入框，分别调用这两个 Calculator 方法。 在这些方法中， Calculator 组件要求 React 通过使用 新的输入值 和 刚刚编辑的输入框的当前度量衡 来调用 this.setState() 来重新渲染自身。 React 调用 Calculator 组件的 render 方法来了解 UI 外观应该是什么样子。基于当前温度和激活的度量衡来重新计算两个输入框的值。这里进行温度转换。 React 使用 Calculator 指定的新 props(属性) 调用各个 TemperatureInput 组件的 render方法。 它了解 UI 外观应该是什么样子。 React DOM 更新 DOM 以匹配期望的输入值。我们刚刚编辑的输入框接收当前值，另一个输入框更新为转换后的温度。 每个更新都会执行相同的步骤，以便输入保持同步。 经验总结 在一个 React 应用中，对于任何可变的数据都应该循序“单一数据源”原则。通常情况下，state 首先被添加到需要它进行渲染的组件。然后，如果其它的组件也需要它，你可以提升状态到它们最近的祖先组件。你应该依赖 从上到下的数据流向 ，而不是试图在不同的组件中同步状态。 提升状态相对于双向绑定方法需要写更多的“模板”代码，但是有一个好处，它可以更方便的找到和隔离 bugs。由于任何 state(状态) 都 “存活” 在若干的组件中，而且可以分别对其独立修改，所以发生错误的可能大大减少。另外，你可以实现任何定制的逻辑来拒绝或者转换用户输入。 如果某个东西可以从 props(属性) 或者 state(状态) 得到，那么它可能不应该在 state(状态) 中。例如，我们只保存最后编辑的 temperature 和它的 scale，而不是保存 celsiusValue 和fahrenheitValue 。另一个输入框的值总是在 render() 方法中计算得来的。这使我们对其进行清除和四舍五入到其他字段同时不会丢失用户输入的精度。 当你看到 UI 中的错误，你可以使用 React 开发者工具来检查 props ，并向上遍历树，直到找到负责更新状态的组件。这使你可以跟踪到 bug 的源头： 组合和继承对比（Composition vs Inheritance） React 拥有一个强大的组合模型，我们建议使用组合而不是继承以实现代码的重用。 在本节中，我们将考虑几个问题，即 React 新手经常会使用继承，并展示我们如何通过组合来解决它们。 包含 一些组件在设计前无法获知自己要使用什么子组件，尤其在 Sidebar 和 Dialog 等通用 “容器” 中比较常见。 我们建议这种组件使用特别的 children prop 来直接传递 子元素到他们的输出中： function FancyBorder(props) &#123; return ( &lt;div className=&#123;'FancyBorder FancyBorder-' + props.color&#125;&gt; &#123;props.children&#125; &lt;/div&gt; );&#125; 这允许其他组件通过嵌套 JSX 传递任意子组件给他们： function WelcomeDialog() &#123; return ( &lt;FancyBorder color="blue"&gt; &lt;h1 className="Dialog-title"&gt; Welcome &lt;/h1&gt; &lt;p className="Dialog-message"&gt; Thank you for visiting our spacecraft! &lt;/p&gt; &lt;/FancyBorder&gt; );&#125; 在 CodePen 中尝试。 在 JSX 标签中的任何内容被传递到 `FancyBorder` 组件中，作为一个 `children`prop(属性)。由于 `FancyBorder` 渲染 `{props.children}` 到一个 中，传递的元素会呈现在最终的输出中。 然而这并不常见，有时候，在一个组件中你可能需要多个 “占位符” 。在这种情况下，你可以使用自定义的 prop(属性)，而不是使用 children ： function SplitPane(props) &#123; return ( &lt;div className="SplitPane"&gt; &lt;div className="SplitPane-left"&gt; &#123;props.left&#125; &lt;/div&gt; &lt;div className="SplitPane-right"&gt; &#123;props.right&#125; &lt;/div&gt; &lt;/div&gt; );&#125;function App() &#123; return ( &lt;SplitPane left=&#123; &lt;Contacts /&gt; &#125; right=&#123; &lt;Chat /&gt; &#125; /&gt; );&#125; 在 CodePen 中尝试。 如 和 等 React 元素本质上也是对象，所以可以将其像其他数据一样作为 props(属性) 传递使用。 特例 有时候，我们考虑组件作为其它组件的“特殊情况”。例如，我们可能说一个 WelcomeDialog 是Dialog 的一个特殊用例。 在 React 中，也可以使用组合来实现，一个偏“特殊”的组件渲染出一个偏“通用”的组件，通过 props(属性) 配置它： function Dialog(props) &#123; return ( &lt;FancyBorder color="blue"&gt; &lt;h1 className="Dialog-title"&gt; &#123;props.title&#125; &lt;/h1&gt; &lt;p className="Dialog-message"&gt; &#123;props.message&#125; &lt;/p&gt; &lt;/FancyBorder&gt; );&#125;function WelcomeDialog() &#123; return ( &lt;Dialog title="Welcome" message="Thank you for visiting our spacecraft!" /&gt; );&#125; 在 CodePen 中尝试。 对于用类定义的组件组合也同样适用： function Dialog(props) &#123; return ( &lt;FancyBorder color="blue"&gt; &lt;h1 className="Dialog-title"&gt; &#123;props.title&#125; &lt;/h1&gt; &lt;p className="Dialog-message"&gt; &#123;props.message&#125; &lt;/p&gt; &#123;props.children&#125; &lt;/FancyBorder&gt; );&#125;class SignUpDialog extends React.Component &#123; constructor(props) &#123; super(props); this.handleChange = this.handleChange.bind(this); this.handleSignUp = this.handleSignUp.bind(this); this.state = &#123;login: ''&#125;; &#125; render() &#123; return ( &lt;Dialog title="Mars Exploration Program" message="How should we refer to you?"&gt; &lt;input value=&#123;this.state.login&#125; onChange=&#123;this.handleChange&#125; /&gt; &lt;button onClick=&#123;this.handleSignUp&#125;&gt; Sign Me Up! &lt;/button&gt; &lt;/Dialog&gt; ); &#125; handleChange(e) &#123; this.setState(&#123;login: e.target.value&#125;); &#125; handleSignUp() &#123; alert(`Welcome aboard, $&#123;this.state.login&#125;!`); &#125;&#125; 在 CodePen 中尝试。 如何看待？ 在 Facebook ，我们在千万的组件中使用 React，我们还没有发现任何用例，值得我们建议你用继承层次结构来创建组件。 使用 props(属性) 和 组合已经足够灵活来明确、安全的定制一个组件的外观和行为。切记，组件可以接受任意的 props(属性) ，包括原始值、React 元素，或者函数。 如果要在组件之间重用非 UI 功能，我们建议将其提取到单独的 JavaScript 模块中。组件可以导入它并使用该函数，对象或类，而不扩展它。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Ffrontend%2FREADME%2F</url>
    <content type="text"><![CDATA[前端技术 🎯 所有配套源码整理归档在 frontend-tutorial 项目中。 📝 知识点 Base 前端 web 技术的基石： html + css + js HTML 定义了网页的内容。 CSS 定义了网页的样式。 JavaScript 定义了网页的行为。 Html Css Javascript Nodejs, Npm, Yarn Nodejs 和包管理器 流行： npm, yarn Nodejs - 关键词： nodejs, REPL， require, exports Npm - 关键词： nodejs, 包管理, npm, cnpm, package.json, node_modules Yarn - 关键词： nodejs, 包管理, yarn, yarn.lock ES6, TypeScript, Babel 下一代 Javascript 语言 ES6 - 关键词： ES6, ECMAScript, arrow, this, let, const, class, extends, super, arrow … Babel - 关键词： babel-cli, .babelrc, preset, polyfill TypeScript - 关键词： typescript, tsc Bundlers 捆绑资源管理器 流行： Webpack Webpack 入门 Webpack 概念 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 QA 静态检查工具、代码格式化工具 流行： JSLint, JSHint, ESLint, Prettier, Standard, TSlint JavaScript QA 工具总结 eslint MVC - React, Vue, Angular Angular,React,Vue 比较 React 技术栈 React Vue 技术栈 Angular 技术栈 Angular 🚪 传送门 | 回首頁 |]]></content>
      <tags>
        <tag>hide</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Fjava%2Fjavatool%2Fbuild%2Fmaven%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Fjava%2Fjavacore%2Fconcurrent%2FREADME%2F</url>
    <content type="text"><![CDATA[Java 并发 内容目录 Java 并发面试题集 第一章 并发简介 第二章 线程基础 第三章 并发机制的底层实现 第四章 内存模型 第五章 同步容器和并发容器 第六章 锁 第七章 原子变量类 第八章 并发工具类 第九章 线程池]]></content>
      <tags>
        <tag>hide</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Fbigdata%2FREADME%2F</url>
    <content type="text"><![CDATA[大数据 📝 知识点 MapReduce HDFS YARN HBase HBase 命令 HBase 配置 📚 学习资源 书 Hadoop 权威指南 🚪 传送门 | 回首頁 |]]></content>
      <tags>
        <tag>hide</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Fdatabase%2Fsql%2FREADME%2F</url>
    <content type="text"><![CDATA[关系型数据库 关系数据库，是建立在关系模型基础上的数据库，借助于集合代数等数学概念和方法来处理数据库中的数据。现实世界中的各种实体以及实体之间的各种联系均用关系模型来表示。关系模型由关系数据结构、关系操作集合、关系完整性约束三部分组成。 📝 知识点 通用知识点 关系型数据库 SQL 基本语法 关系型数据库基本原理 关系型数据库面试题 关系型数据库管理系统（RDBMS） Mysql Oracle SQL Server PostgreSQL SQLite DB2 H2 流行数据库中间件 flyway 📚 学习资源 🚪 传送门 | 回首頁 |]]></content>
      <tags>
        <tag>hide</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Fdatabase%2Fnosql%2FREADME%2F</url>
    <content type="text"><![CDATA[非关系型数据库 📝 知识点 非关系型数据库管理系统 Redis MongoDB Cassandra 📚 学习资源 🚪 传送门 | 回首頁 |]]></content>
      <tags>
        <tag>hide</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fblog%2F2019%2F04%2F25%2Fdatabase%2FREADME%2F</url>
    <content type="text"><![CDATA[数据库 🎯 所有配套源码整理归档在 db-tutorial 项目中。 📝 知识点 关系型数据库 非关系型数据库 🚪 传送门 | 回首頁 |]]></content>
      <tags>
        <tag>hide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 通用工具包]]></title>
    <url>%2Fblog%2F2019%2F04%2F16%2Fjava%2Fjavalib%2Fjava-util%2F</url>
    <content type="text"><![CDATA[apache.commons commons-lang commons-collections common-io guava]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[reflections]]></title>
    <url>%2Fblog%2F2019%2F04%2F16%2Fjava%2Fjavalib%2Freflections%2F</url>
    <content type="text"><![CDATA[reflections 引入 &lt;dependency&gt; &lt;groupId&gt;org.reflections&lt;/groupId&gt; &lt;artifactId&gt;reflections&lt;/artifactId&gt; &lt;version&gt;0.9.11&lt;/version&gt;&lt;/dependency&gt; 典型应用 Reflections reflections = new Reflections("my.project");Set&lt;Class&lt;? extends SomeType&gt;&gt; subTypes = reflections.getSubTypesOf(SomeType.class);Set&lt;Class&lt;?&gt;&gt; annotated = reflections.getTypesAnnotatedWith(SomeAnnotation.class); 使用 基本上，使用 Reflections 首先使用 urls 和 scanners 对其进行实例化 //scan urls that contain 'my.package', include inputs starting with 'my.package', use the default scannersReflections reflections = new Reflections("my.package");//or using ConfigurationBuildernew Reflections(new ConfigurationBuilder() .setUrls(ClasspathHelper.forPackage("my.project.prefix")) .setScanners(new SubTypesScanner(), new TypeAnnotationsScanner().filterResultsBy(optionalFilter), ...), .filterInputsBy(new FilterBuilder().includePackage("my.project.prefix")) ...); 然后，使用方便的查询方法 // 子类型扫描Set&lt;Class&lt;? extends Module&gt;&gt; modules = reflections.getSubTypesOf(com.google.inject.Module.class);// 类型注解扫描Set&lt;Class&lt;?&gt;&gt; singletons = reflections.getTypesAnnotatedWith(javax.inject.Singleton.class);// 资源扫描Set&lt;String&gt; properties = reflections.getResources(Pattern.compile(".*\\.properties"));// 方法注解扫描Set&lt;Method&gt; resources = reflections.getMethodsAnnotatedWith(javax.ws.rs.Path.class);Set&lt;Constructor&gt; injectables = reflections.getConstructorsAnnotatedWith(javax.inject.Inject.class);// 字段注解扫描Set&lt;Field&gt; ids = reflections.getFieldsAnnotatedWith(javax.persistence.Id.class);// 方法参数扫描Set&lt;Method&gt; someMethods = reflections.getMethodsMatchParams(long.class, int.class);Set&lt;Method&gt; voidMethods = reflections.getMethodsReturn(void.class);Set&lt;Method&gt; pathParamMethods = reflections.getMethodsWithAnyParamAnnotated(PathParam.class);// 方法参数名扫描List&lt;String&gt; parameterNames = reflections.getMethodParamNames(Method.class)// 方法使用扫描Set&lt;Member&gt; usages = reflections.getMethodUsages(Method.class) 说明： 如果未配置扫描程序，则将使用默认值 - SubTypesScanner 和 TypeAnnotationsScanner。 还可以配置类加载器，它将用于从名称中解析运行时类。 Reflection 默认情况下会扩展超类型。 这解决了传输 URL 不被扫描的一些问题。 ReflectionUtils import static org.reflections.ReflectionUtils.*;Set&lt;Method&gt; getters = getAllMethods(someClass, withModifier(Modifier.PUBLIC), withPrefix("get"), withParametersCount(0));//orSet&lt;Method&gt; listMethodsFromCollectionToBoolean = getAllMethods(List.class, withParametersAssignableTo(Collection.class), withReturnType(boolean.class));Set&lt;Field&gt; fields = getAllFields(SomeClass.class, withAnnotation(annotation), withTypeAssignableTo(type));]]></content>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 基本数据类型]]></title>
    <url>%2Fblog%2F2019%2F04%2F06%2Fdatabase%2Fnosql%2Fredis%2FREADME%2F</url>
    <content type="text"><![CDATA[Redis 📓 本文已归档到：「blog」 简介 Redis 简介 Redis 的优势 Redis 与 Memcached 数据类型 命令 字符串命令 列表命令 集合命令 散列命令 有序集合命令 发布与订阅命令 其它命令 使用场景 Redis 管道 键的过期时间 数据淘汰策略 持久化 快照持久化 AOF 持久化 发布与订阅 事务 EXEC MULTI DISCARD WATCH 事件 文件事件 时间事件 事件的调度与执行 集群 复制 哨兵 分片 资料 简介 Redis 简介 Redis 是一个速度非常快的非关系型数据库（NoSQL）。 Redis 可以存储键（key）和 5 种不同类型的值（value）之间的映射（mapping）。 Redis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。 Redis 的优势 性能极高 – Redis 能读的速度是 110000 次/s,写的速度是 81000 次/s。 丰富的数据类型 - 支持字符串、列表、集合、有序集合、散列表。 原子 - Redis 的所有操作都是原子性的。单个操作是原子性的。多个操作也支持事务，即原子性，通过 MULTI 和 EXEC 指令包起来。 持久化 - Redis 支持数据的持久化。可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 备份 - Redis 支持数据的备份，即 master-slave 模式的数据备份。 丰富的特性 - Redis 还支持发布订阅, 通知, key 过期等等特性。 Redis 与 Memcached Redis 与 Memcached 因为都可以用于缓存，所以常常被拿来做比较，二者主要有以下区别： 数据类型 Memcached 仅支持字符串类型； 而 Redis 支持五种不同种类的数据类型，使得它可以更灵活地解决问题。 数据持久化 Memcached 不支持持久化； Redis 支持两种持久化策略：RDB 快照和 AOF 日志。 分布式 Memcached 不支持分布式，只能通过在客户端使用像一致性哈希这样的分布式算法来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。 Redis Cluster 实现了分布式的支持。 内存管理机制 Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题，但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。 在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘。而 Memcached 的数据则会一直在内存中。 数据类型 扩展阅读：《Redis 实战》之 Redis 数据结构简介 Redis 可以存储键与 5 种不同数据结构类型之间的映射，这 5 种数据结构类型分别为STRING（字符串）、LIST（列表）、SET（集合）、HASH（散列）和ZSET（有序集合）。 命令 更详细内容可以阅读： 《Redis 实战》之 Redis 命令 Redis 官方命令手册 字符串命令 命令 用例和描述 INCR INCR key-name——将键存储的值加上 1 DECR DECR key-name——将键存储的值减去 1 INCRBY INCRBY key-name amount——将键存储的值加上整数amount DECRBY DECRBY key-name amount——将键存储的值减去整数amount INCRBYFLOAT INCRBYFLOAT key-name amount——将键存储的值加上浮点数amount，这个命令在 Redis 2.6 或以上的版本可用 APPEND APPEND key-name value——将提供的值value追加到给定键key-name当前存储的值的末尾 GETRANGE GETRANGE key-name start end——获取一个由偏移量start至偏移量end范围内所有字符组成的子串，包括start和end在内 SETRANGE SETRANGE key-name offset value——将从start偏移量开始的子串设置为给定value GETBIT GETBIT key-name offset——将字节串看作是二进制位串（bit string），并返回位串中偏移量为offset的二进制位的值 SETBIT SETBIT key-name offset value——将字节串看作是二进制位串，并将位串中偏移量为offset的二进制位的值设置为value BITCOUNT BITCOUNT key-name [start end]——统计二进制位串里面值为 1 的二进制位的数量，如果给定了可选的start偏移量和end偏移量，那么只对偏移量指定范围内的二进制位进行统计 BITOP BITOP operation dest-key key-name [key-name ...]——对一个或多个二进制位串执行包括并（AND）、或（OR）、异或（XOR）、 非（NOT）在内的任意一种按位运算操作（bitwise operation），并将计算得出的结果保存在dest-key键里面 列表命令 命令 用例和描述 RPUSH RPUSH key-name value [value ...]——将一个或多个值推入到列表的右端 LPUSH LPUSH key-name value [value ...]——将一个或多个值推入到列表的左端 RPOP RPOP key-name——移除并返回列表最右端的元素 LPOP LPOP key-name——移除并返回列表最左端的元素 LINDEX LINDEX key-name offset——返回列表中偏移量为offset的元素 LRANGE LRANGE key-name start end——返回列表从start偏移量到end偏移量范围内的所有元素，包括start和end LTRIM LTRIM key-name start end——对列表进行修剪，只保留从start偏移量到end偏移量范围内的元素，包括start和end BLPOP BLPOP key-name [key-name ...] timeout——从第一个非空列表中弹出位于最左端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现 BRPOP BRPOP key-name [key-name ...] timeout——从第一个非空列表中弹出位于最右端的元素，或者在timeout秒之内阻塞并等待可弹出的元素出现 RPOPLPUSH RPOPLPUSH source-key dest-key——从source-key列表中弹出位于最右端的元素，然后将这个元素推入到dest-key列表的最左端，并向用户返回这个元素 BRPOPLPUSH BRPOPLPUSH source-key dest-key timeout——从source-key列表中弹出位于最右端的元素，然后将这个元素推入到dest-key列表的最左端， 并向用户返回这个元素；如果source-key为空，那么在timeout秒之内阻塞并等待可弹出的元素出现 集合命令 命令 用例和描述 SADD SADD key-name item [item ...]——将一个或多个元素添加到集合里面，并返回被添加元素当中原本并不存在于集合里面的元素数量 SREM SREM key-name item [item ...]——从集合里面移除一个或多个元素，并返回被移除元素的数量 SISMEMBER SISMEMBER key-name item——检查元素item是否存在于集合key-name里 SCARD SCARD key-name——返回集合包含的元素的数量 SMEMBERS SMEMBERS key-name——返回集合包含的所有元素 SRANDMEMBER SRANDMEMBER key-name [count]——从集合里面随机地返回一个或多个元素。当count为正数时，命令返回的随机元素不会重复； 当count为负数时，命令返回的随机元素可能会出现重复 SPOP SPOP key-name——从集合里面移除并返回一个随机元素 SMOVE SMOVE source-key dest-key item——如果集合source-key包含元素item， 那么从集合source-key里面移除元素item，并将元素item添加到集合dest-key中； 如果item被成功移除，那么命令返回 1，否则返回 0 SDIFF SDIFF key-name [key-name ...]——返回那些存在于第一个集合、但不存在于其他集合中的元素（数学上的差集运算） SDIFFSTORE SDIFFSTORE dest-key key-name [key-name ...]——将那些存在于第一个集合、但并不存在于其他集合中的元素（数学上的差集运算）存储到dest-key中 SINTER SINTER key-name [key-name ...]——返回那些同时存在于所有集合中的元素（数学上的交集运算） SINTERSTORE SINTERSTORE dest-key key-name [key-name ...]——将那些同时存在于所有集合的元素（数学上的交集运算）保存到键dest-key SUNION SUNION key-name [key-name ...]——返回那些至少存在于一个集合中的元素（数学上的并集计算） SUNIONSTORE SUNIONSTORE dest-key key-name [key-name ...]——将那些至少存在于一个集合中的元素（数学上的并集计算）存储到dest-key中 散列命令 命令 用例和描述 HMGET HMGET key-name key [key ...]——从散列里面获取一个或多个键的值 HMSET HMSET key-name key value [key value ...]——为散列里面的一个或多个键设置值 HDEL HDEL key-name key [key ...]——删除散列里面的一个或多个键值对，返回成功找到并删除的键值对数量 HLEN HLEN key-name——返回散列包含的键值对数量 HEXISTS HEXISTS key-name key——检查给定键是否存在于散列中 HKEYS HKEYS key-name——获取散列包含的所有键 HVALS HVALS key-name——获取散列包含的所有值 HGETALL HGETALL key-name——获取散列包含的所有键值对 HINCRBY HINCRBY key-name key increment——将键key保存的值加上整数increment HINCRBYFLOAT HINCRBYFLOAT key-name key increment——将键key保存的值加上浮点数increment 有序集合命令 ZADD ZADD key-name score member [score member ...]——将带有给定分值的成员添加到有序集合里面 ZREM ZREM key-name member [member ...]——从有序集合里面移除给定的成员，并返回被移除成员的数量 ZCARD ZCARD key-name——返回有序集合包含的成员数量 ZINCRBY ZINCRBY key-name increment member——将member成员的分值加上increment ZCOUNT ZCOUNT key-name min max——返回分值介于min和max之间的成员数量 ZRANK ZRANK key-name member——返回成员member在key-name中的排名 ZSCORE ZSCORE key-name member——返回成员member的分值 ZRANGE ZRANGE key-name start stop [WITHSCORES]——返回有序集合中排名介于start和stop之间的成员，如果给定了可选的WITHSCORES选项，那么命令会将成员的分值也一并返回 ZREVRANK ZREVRANK key-name member——返回有序集合里成员member所处的位置，成员按照分值从大到小排列 ZREVRANGE ZREVRANGE key-name start stop [WITHSCORES]——返回有序集合给定排名范围内的成员，成员按照分值从大到小排列 ZRANGEBYSCORE ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]——返回有序集合中，分值介于min和max之间的所有成员 ZREVRANGEBYSCORE ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count]——获取有序集合中分值介于min和max之间的所有成员，并按照分值从大到小的顺序来返回它们 ZREMRANGEBYRANK ZREMRANGEBYRANK key-name start stop——移除有序集合中排名介于start和stop之间的所有成员 ZREMRANGEBYSCORE ZREMRANGEBYSCORE key-name min max——移除有序集合中分值介于min和max之间的所有成员 ZINTERSTORE ZINTERSTORE dest-key key-count key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX]——对给定的有序集合执行类似于集合的交集运算 ZUNIONSTORE ZUNIONSTORE dest-key key-count key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX]——对给定的有序集合执行类似于集合的并集运算 发布与订阅命令 命令 用例和描述 SUBSCRIBE SUBSCRIBE channel [channel ...]——订阅给定的一个或多个频道 UNSUBSCRIBE UNSUBSCRIBE [channel [channel ...]]——退订给定的一个或多个频道，如果执行时没有给定任何频道，那么退订所有频道 PUBLISH PUBLISH channel message——向给定频道发送消息 PSUBSCRIBE PSUBSCRIBE pattern [pattern ...]——订阅与给定模式相匹配的所有频道 PUNSUBSCRIBE PUNSUBSCRIBE [pattern [pattern ...]]——退订给定的模式，如果执行时没有给定任何模式，那么退订所有模式 其它命令 命令 用例和描述 SORT SORT source-key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern ...]] [ASC|DESC] [ALPHA] [STORE dest-key] ——根据给定的选项，对输入列表、集合或者有序集合进行排序，然后返回或者存储排序的结果 PERSIST PERSIST key-name——移除键的过期时间 TTL TTL key-name——返回给定键距离过期还有多少秒 EXPIRE EXPIRE key-name seconds——让键key-name在给定的seconds秒之后过期 EXPIREAT EXPIREAT key-name timestamp——将给定键的过期时间设置为给定的 UNIX 时间戳 PTTL PTTL key-name——返回给定键距离过期时间还有多少毫秒，这个命令在 Redis 2.6 或以上版本可用 PEXPIRE PEXPIRE key-name milliseconds——让键key-name在milliseconds毫秒之后过期，这个命令在 Redis 2.6 或以上版本可用 PEXPIREAT PEXPIREAT key-name timestamp-milliseconds——将一个毫秒级精度的 UNIX 时间戳设置为给定键的过期时间，这个命令在 Redis 2.6 或以上版本可用 使用场景 缓存 - 将热点数据放到内存中，设置内存的最大使用量以及过期淘汰策略来保证缓存的命中率。 计数器 - Redis 这种内存数据库能支持计数器频繁的读写操作。 应用限流 - 限制一个网站访问流量。 消息队列 - 使用 List 数据类型，它是双向链表。 查找表 - 使用 HASH 数据类型。 交集运算 - 使用 SET 类型，例如求两个用户的共同好友。 排行榜 - 使用 ZSET 数据类型。 分布式 Session - 多个应用服务器的 Session 都存储到 Redis 中来保证 Session 的一致性。 分布式锁 - 除了可以使用 SETNX 实现分布式锁之外，还可以使用官方提供的 RedLock 分布式锁实现。 Redis 管道 Redis 是一种基于 C/S 模型以及请求/响应协议的 TCP 服务。 Redis 支持管道技术。管道技术允许请求以异步方式发送，即旧请求的应答还未返回的情况下，允许发送新请求。这种方式可以大大提高传输效率。 使用管道发送命令时，服务器将被迫回复一个队列答复，占用很多内存。所以，如果你需要发送大量的命令，最好是把他们按照合理数量分批次的处理。 键的过期时间 Redis 可以为每个键设置过期时间，当键过期时，会自动删除该键。 对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。 可以使用 EXPIRE 或 EXPIREAT 来为 key 设置过期时间。 注意：当 EXPIRE 的时间如果设置的是负数，EXPIREAT 设置的时间戳是过期时间，将直接删除 key。 示例： redis&gt; SET mykey "Hello""OK"redis&gt; EXPIRE mykey 10(integer) 1redis&gt; TTL mykey(integer) 10redis&gt; SET mykey "Hello World""OK"redis&gt; TTL mykey(integer) -1redis&gt; 数据淘汰策略 可以设置内存最大使用量，当内存使用量超过时施行淘汰策略，具体有 6 种淘汰策略。 策略 描述 volatile-lru 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰 volatile-ttl 从已设置过期时间的数据集中挑选将要过期的数据淘汰 volatile-random 从已设置过期时间的数据集中任意选择数据淘汰 allkeys-lru 从所有数据集中挑选最近最少使用的数据淘汰 allkeys-random 从所有数据集中任意选择数据进行淘汰 noeviction 禁止驱逐数据 如果使用 Redis 来缓存数据时，要保证所有数据都是热点数据，可以将内存最大使用量设置为热点数据占用的内存量，然后启用 allkeys-lru 淘汰策略，将最近最少使用的数据淘汰。 作为内存数据库，出于对性能和内存消耗的考虑，Redis 的淘汰算法（LRU、TTL）实际实现上并非针对所有 key，而是抽样一小部分 key 从中选出被淘汰 key，抽样数量可通过 maxmemory-samples 配置。 持久化 Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。 快照持久化 将某个时间点的所有数据都存放到硬盘上。 可以将快照复制到其它服务器从而创建具有相同数据的服务器副本。 如果系统发生故障，将会丢失最后一次创建快照之后的数据。 如果数据量很大，保存快照的时间会很长。 AOF 持久化 将写命令添加到 AOF 文件（Append Only File）的末尾。 对硬盘的文件进行写入时，写入的内容首先会被存储到缓冲区，然后由操作系统决定什么时候将该内容同步到硬盘，用户可以调用 file.flush() 方法请求操作系统尽快将缓冲区存储的数据同步到硬盘。可以看出写入文件的数据不会立即同步到硬盘上，在将写命令添加到 AOF 文件时，要根据需求来保证何时同步到硬盘上。 有以下同步选项： 选项 同步频率 always 每个写命令都同步 everysec 每秒同步一次 no 让操作系统来决定何时同步 always 选项会严重减低服务器的性能； everysec 选项比较合适，可以保证系统奔溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响； no 选项并不能给服务器性能带来多大的提升，而且也会增加系统奔溃时数据丢失的数量。 随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令。 发布与订阅 订阅者订阅了频道之后，发布者向频道发送字符串消息会被所有订阅者接收到。 某个客户端使用 SUBSCRIBE 订阅一个频道，其它客户端可以使用 PUBLISH 向这个频道发送消息。 发布与订阅模式和观察者模式有以下不同： 观察者模式中，观察者和主题都知道对方的存在；而在发布与订阅模式中，发布者与订阅者不知道对方的存在，它们之间通过频道进行通信。 观察者模式是同步的，当事件触发时，主题会去调用观察者的方法；而发布与订阅模式是异步的； 事务 MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。 事务可以一次执行多个命令， 并且有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 EXEC EXEC 命令负责触发并执行事务中的所有命令： 如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。 MULTI MULTI 命令用于开启一个事务，它总是返回 OK 。 MULTI 执行之后， 客户端可以继续向服务器发送任意多条命令， 这些命令不会立即被执行， 而是被放到一个队列中， 当 EXEC 命令被调用时， 所有队列中的命令才会被执行。 DISCARD 当执行 DISCARD 命令时， 事务会被放弃， 事务队列会被清空， 并且客户端会从事务状态中退出。 示例： &gt; SET foo 1OK&gt; MULTIOK&gt; INCR fooQUEUED&gt; DISCARDOK&gt; GET foo"1" WATCH WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回 nil-reply 来表示事务已经失败。 WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey $valEXEC 使用上面的代码， 如果在 WATCH 执行之后， EXEC 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。 这种形式的锁被称作乐观锁， 它是一种非常强大的锁机制。 并且因为大多数情况下， 不同的客户端会访问不同的键， 碰撞的情况一般都很少， 所以通常并不需要进行重试。 WATCH 使得 EXEC 命令需要有条件地执行：事务只能在所有被监视键都没有被修改的前提下执行，如果这个前提不能满足的话，事务就不会被执行。 WATCH 命令可以被调用多次。对键的监视从 WATCH 执行之后开始生效，直到调用 EXEC 为止。 用户还可以在单个 WATCH 命令中监视任意多个键，例如： redis&gt; WATCH key1 key2 key3OK 当 EXEC 被调用时， 不管事务是否成功执行， 对所有键的监视都会被取消。 另外， 当客户端断开连接时， 该客户端对键的监视也会被取消。 使用无参数的 UNWATCH 命令可以手动取消对所有键的监视。 对于一些需要改动多个键的事务， 有时候程序需要同时对多个键进行加锁， 然后检查这些键的当前值是否符合程序的要求。 当值达不到要求时， 就可以使用 UNWATCH 命令来取消目前对键的监视， 中途放弃这个事务， 并等待事务的下次尝试。 事件 Redis 服务器是一个事件驱动程序。 文件事件 服务器通过套接字与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。 Redis 基于 Reactor 模式开发了自己的网络时间处理器，使用 I/O 多路复用程序来同时监听多个套接字，并将到达的时间传送给文件事件分派器，分派器会根据套接字产生的事件类型调用响应的时间处理器。 时间事件 服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。 时间事件又分为： 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 Redis 将所有时间事件都放在一个无序链表中，通过遍历整个链表查找出已到达的时间事件，并调用响应的事件处理器。 事件的调度与执行 服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能监听太久，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。 事件调度与执行由 aeProcessEvents 函数负责，伪代码如下： def aeProcessEvents(): ## 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() ## 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() ## 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms &lt; 0: remaind_ms = 0 ## 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) ## 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) ## 处理所有已产生的文件事件 procesFileEvents() ## 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下： def main(): ## 初始化服务器 init_server() ## 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() ## 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下： 集群 复制 通过使用 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。 一个从服务器只能有一个主服务器，并且不支持主主复制。 12.1. 连接过程 主服务器创建快照文件，发送给从服务器，并在发送期间使用缓冲区记录执行的写命令。快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令； 从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令； 主服务器每执行一次写命令，就向从服务器发送相同的写命令。 12.2. 主从链 随着负载不断上升，主服务器可能无法很快地更新所有从服务器，或者重新连接和重新同步从服务器将导致系统超载。为了解决这个问题，可以创建一个中间层来分担主服务器的复制工作。中间层的服务器是最上层服务器的从服务器，又是最下层服务器的主服务器。 哨兵 Sentinel（哨兵）可以监听主服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。 分片 分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，也可以从多台机器里面获取数据，这种方法在解决某些问题时可以获得线性级别的性能提升。 假设有 4 个 Reids 实例 R0，R1，R2，R3，还有很多表示用户的键 user:1，user:2，… 等等，有不同的方式来选择一个指定的键存储在哪个实例中。最简单的方式是范围分片，例如用户 id 从 0~1000 的存储到实例 R0 中，用户 id 从 1001~2000 的存储到实例 R1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。还有一种方式是哈希分片，使用 CRC32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。 主要有三种分片方式： 客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。 代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。 服务器分片：Redis Cluster。 资料 📓 本文已归档到：「blog」 官网 redis 官网 redis github Sentinel 官方文档 最全 官方文档翻译 翻译,排版一般,新 官方文档翻译 翻译有段时间了,但主要部分都包含,排版好 redis sentinel 实战 简要实战,能快速看出来是怎么回事 redis client jedis redisson lettuce spring-data-redis 官方文档 CRUG | Redisson PRO vs. Jedis: Which Is Faster? 翻译 redis 分布锁 Redisson 性能测试 站点 awesome-redis 书 Redis 实战 Redis 实战在线版 Redis 设计与实现]]></content>
      <categories>
        <category>database</category>
        <category>nosql</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>nosql</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员玩转 Windows]]></title>
    <url>%2Fblog%2F2019%2F03%2F22%2Fos%2Fwindows%2F</url>
    <content type="text"><![CDATA[程序员玩转 Windows 软件 视频音频 压缩 文件管理 开发 编辑器 文档 效率提升 办公 个性化 参考资料 软件 扩展阅读： Awesome Windows best-windows-apps 视频音频 Musicbee - 类似 iTunes，但比 iTunes 更好用。 ScreenToGif - 它允许你录制屏幕的一部分区域并保存为 gif 或视频。 PotPlayer - 多媒体播放器，具有广泛的编解码器集合，它还为用户提供大量配置选项。 射手影音播放器 - 来自射手网，小巧开源，首创自动匹配字幕功能。 压缩 7-Zip - 用于处理压缩包的开源 Windows 实用程序。完美支持 7z，ZIP，GZIP，BZIP2 和 TAR 的全部特性，其他格式也可解压缩。 WinRAR - 强大的归档管理器。 它可以备份您的数据并减小电子邮件附件的大小，解压缩 RAR，ZIP 和其他文件。 文件管理 Clover - 为资源管理器加上多标签功能。 Total Commander - 老牌、功能异常强大的文件管理增强软件。 Q-Dir - 轻量级的文件管理器，各种布局视图切换灵活，默认四个小窗口组成一个大窗口，操作快捷。软件虽小，粉丝忠诚。 WoX - 新一代文件定位工具，堪称 Windows 上的 Alfred。 Everything - 最快的文件/文件夹搜索工具， 通过名称搜索。 Listary - 非常优秀的 Windows 文件浏览和搜索增强工具。 Beyond Compare - 好用又万能的文件对比工具。 CCleaner - 如果你有系统洁癖，那一定要选择一款干净、良心、老牌的清洁软件。 chocolatey - 包管理器 Ninite - 最简单，最快速的更新或安装软件的方式。 Recuva - 来自 piriform 梨子公司产品，免费的数据恢复工具。 Launchy：自由的跨平台工具，帮助你忘记开始菜单、桌面图标甚至文件管理器。 开发 Fiddler - web 调试代理工具。 Postman - 适合 API 开发的完整工具链，最常用的 REST 客户端。 SourceTree - 一个免费的 Git &amp; Mercurial 客户端。 TortoiseSVN - Subversion(SVN)的图形客户端 Wireshark - 一个网络协议分析工具。 Switchhosts Cmder - 控制台模拟器包。扩展阅读：Win 下必备神器之 Cmder Babun - 基于 Cygwin，用于替代 Windows shell。 编辑器 JetBrain IDE 系列 - 真香！ Visual Studio Code - 用于构建和调试现代 Web 和云应用程序。 Eclipse - 一款功能强大的 IDE。 Visual Studio - 微软官方的 IDE，通过插件可支持大量编程语言。 NetBeans IDE - 免费开源的 IDE。 Typora - 个人觉得最好用的 Markdown 编辑器。 Cmd Markdown - 跨平台优秀 Markdown 编辑器，本文即用其所写。 Notepad++ - 一款支持多种编程语言的源码编辑器。 Notepad2 - 用于替代默认文本编辑器的轻量快速的编辑器，拥有众多有用的功能。 Sublime Text 3 - 高级文本编辑器。 Atom - 面向 21 世纪的极客文本编辑器。 文档 Microsoft Office - 微软办公软件。 WPS Office - 金山免费办公软件。 Calibre - 用于电子书管理和转换的强大软件。 福昕阅读器 - 在全球拥有大量用户，最优秀的国产软件之一。Ribbon 界面，支持手写签名、插入印章等。 效率提升 【笔记】 XMind - 优秀的思维导图。 OneNote - Windows 下综合评价非常高的笔记应用。 印象笔记 - 老牌跨平台笔记工具，国际版 Evernote。一家立志于做百年公司的企业，安全、可靠。 为知笔记 - 越来越好的笔记应用，记录、查阅一切有价值的信息，同样跨平台支持。 有道云笔记 - 网易旗下笔记工具，同样跨主流平台支持，文字、手写、录音、拍照多种记录方式，支持任意附件格式。 ShareX - 你要的所有与截图、录屏相关的功能，这里都有了。 【快捷键】 AutoHotkey - Windows 平台的终极自动化脚本语言。 技巧： https://www.jeffjade.com/2016/03/11/2016-03-11-autohotkey/ https://www.autohotkey.com/boards/viewtopic.php?f=29&amp;t=4296 办公 有道词典 - 最好用的免费全能翻译软件。 Outlook - 大名鼎鼎的 Microsoft Office 组件之一，除了电子邮件，还包含了日历、任务管理、联系人、记事本等功能。 Gmail - 功能上可以称为业界标杆，用户数量世界第一，或许你真的找不到比它更好的邮件系统。 Chrome - 最好的浏览器。 Teamviewer - 专业、功能强大的远程控制软件。使用简单，对个人用户免费。 个性化 TranslucentTB - 透明化你的 Windows 任务栏。 QTTabBar - 通过多标签和额外的文件夹视图扩展资源管理器的功能。 Fences - 管理桌面快捷方式。 参考资料 https://github.com/Awesome-Windows/Awesome/blob/master/README-cn.md https://love.appinn.com/ https://github.com/stackia/best-windows-apps]]></content>
      <categories>
        <category>os</category>
      </categories>
      <tags>
        <tag>os</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 中的 JSON 应用]]></title>
    <url>%2Fblog%2F2019%2F03%2F18%2Fjava%2Fjavalib%2Fjava-json%2F</url>
    <content type="text"><![CDATA[Java 中的 JSON 应用 📓 本文已归档到：「blog」 JSON 简介 Java JSON 库 Fastjson 添加 maven 依赖 JavaBean 的序列化和反序列化 注解 Jackson 添加 maven 依赖 API 注解 参考资料 JSON 简介 JSON（JavaScript Object Notation）是一种基于文本的数据交换格式。几乎所有的编程语言都有很好的库或第三方工具来提供基于 JSON 的 API 支持，因此你可以非常方便地使用任何自己喜欢的编程语言来处理 JSON 数据。 JSON 建构于两种结构： “名称/值”对的集合。不同的语言中，它被理解为对象（object），纪录（record），结构（struct），字典（dictionary），哈希表（hash table），有键列表（keyed list），或者关联数组 （associative array）。 值的有序列表（An ordered list of values）。在大部分语言中，它被理解为数组（array）。 扩展阅读： http://www.json.org/json-zh.html - 图文并茂介绍 json 数据形式 json 的 RFC 文档 Java JSON 库 Java 中比较流行的 JSON 库有： Fastjson Jackson Gson Fastjson fastjson 是阿里巴巴的开源 JSON 解析库。 添加 maven 依赖 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;x.x.x&lt;/version&gt;&lt;/dependency&gt; Fastjson API JavaBean 的序列化和反序列化 String text = JSON.toJSONString(obj); //序列化VO vo = JSON.parseObject("&#123;...&#125;", VO.class); //反序列化 Fastjson 注解 @JSONField 可以配置在属性（setter、getter）和字段（必须是 public field）上。 扩展阅读：JSONField 用法 @JSONField(name="ID")public int getId() &#123;return id;&#125;// 配置date序列化和反序列使用yyyyMMdd日期格式@JSONField(format="yyyyMMdd")public Date date1;// 不序列化@JSONField(serialize=false)public Date date2;// 不反序列化@JSONField(deserialize=false)public Date date3;// 按ordinal排序@JSONField(ordinal = 2)private int f1;@JSONField(ordinal = 1)private int f2; @JSONType 自定义序列化：ObjectSerializer 子类型处理：SeeAlso JSONType.alphabetic 属性: fastjson 缺省时会使用字母序序列化，如果你是希望按照 java fields/getters 的自然顺序序列化，可以配置 JSONType.alphabetic，使用方法如下： @JSONType(alphabetic = false)public static class B &#123; public int f2; public int f1; public int f0;&#125; Jackson 以下仅列举个人认为比较常用的特性。 添加 maven 依赖 &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.8&lt;/version&gt;&lt;/dependency&gt; Jackson API 扩展阅读：更多 API 使用细节可以参考 jackson-databind 官方说明 JavaBean 的序列化和反序列化 反序列化示例： ObjectMapper mapper = new ObjectMapper();MyValue value = mapper.readValue(new File("data.json"), MyValue.class);// or:value = mapper.readValue(new URL("http://some.com/api/entry.json"), MyValue.class);// or:value = mapper.readValue("&#123;\"name\":\"Bob\", \"age\":13&#125;", MyValue.class); 序列化示例： mapper.writeValue(new File("result.json"), myResultObject);// or:byte[] jsonBytes = mapper.writeValueAsBytes(myResultObject);// or:String jsonString = mapper.writeValueAsString(myResultObject); 容器的序列化和反序列化 扩展阅读：更多 API 使用细节可以参考 [jackson-databind 官方说明](https://github.com/FasterXML/jackson-databind)javaPerson p = new Person(&quot;Tom&quot;, 20);Person p2 = new Person(&quot;Jack&quot;, 22);Person p3 = new Person(&quot;Mary&quot;, 18);List&lt;Person&gt; persons = new LinkedList&lt;&gt;();persons.add(p);persons.add(p2);persons.add(p3);Map&lt;String, List&gt; map = new HashMap&lt;&gt;();map.put(&quot;persons&quot;, persons);String json = null;try &#123; json = mapper.writeValueAsString(map);&#125; catch (JsonProcessingException e) &#123; e.printStackTrace();&#125; Jackson 注解 扩展阅读：更多注解使用细节可以参考 jackson-annotations 官方说明 @JsonProperty public class MyBean &#123; private String _name; // without annotation, we'd get "theName", but we want "name": @JsonProperty("name") public String getTheName() &#123; return _name; &#125; // note: it is enough to add annotation on just getter OR setter; // so we can omit it here public void setTheName(String n) &#123; _name = n; &#125;&#125; @JsonIgnoreProperties 和 @JsonIgnore // means that if we see "foo" or "bar" in JSON, they will be quietly skipped// regardless of whether POJO has such properties@JsonIgnoreProperties(&#123; "foo", "bar" &#125;)public class MyBean &#123; // will not be written as JSON; nor assigned from JSON: @JsonIgnore public String internal; // no annotation, public field is read/written normally public String external; @JsonIgnore public void setCode(int c) &#123; _code = c; &#125; // note: will also be ignored because setter has annotation! public int getCode() &#123; return _code; &#125;&#125; @JsonCreator public class CtorBean &#123; public final String name; public final int age; @JsonCreator // constructor can be public, private, whatever private CtorBean(@JsonProperty("name") String name, @JsonProperty("age") int age) &#123; this.name = name; this.age = age; &#125;&#125; @JsonPropertyOrder alphabetic 设为 true 表示，json 字段按自然顺序排列，默认为 false。 @JsonPropertyOrder(alphabetic = true)public class JacksonAnnotationBean &#123;&#125; 参考资料 http://www.json.org/json-zh.html json 的 RFC 文档 fastjson jackson 官方文档 jackson-databind JSON 最佳实践 【简明教程】JSON]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 常用工具类]]></title>
    <url>%2Fblog%2F2019%2F03%2F14%2Fjava%2Fjavacore%2Fbasics%2FJava%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[Java 常用工具类 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「javacore」 并发、IO、容器的工具类不会在本文提及，后面会有专题一一道来。 字符串 String StringBuffer StringBuilder 日期时间 Date SimpleDateFormat Calendar 数学 Number Math 参考资料 字符串 String StringBuffer StringBuilder 日期时间 Date SimpleDateFormat Calendar 数学 Number Math 参考资料 Java 编程思想 JAVA 核心技术（卷 1）]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Electron]]></title>
    <url>%2Fblog%2F2019%2F03%2F14%2Ffrontend%2Felectron%2F</url>
    <content type="text"><![CDATA[Electron 📓 本文已归档到：「blog」 简介 入门 参考资料 简介 Electron 是什么？ Electron 基于 Chromium 和 Node.js, 让你可以使用 HTML, CSS 和 JavaScript 等 Web 技术构建跨平台的应用（兼容 Mac, Windows 和 Linux）。 入门 快速启动 应用程序，看看 Electron 是如何运转的： # 克隆示例项目的仓库$ git clone https://github.com/electron/electron-quick-start# 进入这个仓库$ cd electron-quick-start# 安装依赖并运行$ npm install &amp;&amp; npm start 参考资料 electron electron 官方文档 awesome-electron electron-quick-start]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java 基础语法特性]]></title>
    <url>%2Fblog%2F2019%2F03%2F10%2Fjava%2Fjavacore%2Fbasics%2FJava%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[Java 基础语法特性 📓 本文已归档到：「blog」 注释 基本数据类型 变量 变量修饰符 数组 枚举 操作符 方法 控制语句 异常 序列化 泛型 反射 注释 空白行，或者注释的内容，都会被 Java 编译器忽略掉。 Java 支持多种注释方式，下面的示例展示了各种注释的使用方式： public class HelloWorld &#123; /* * JavaDoc 注释 */ public static void main(String[] args) &#123; // 单行注释 /* 多行注释： 1. 注意点a 2. 注意点b */ System.out.println("Hello World"); &#125;&#125; 基本数据类型 👉 扩展阅读：深入理解 Java 基本数据类型 变量 Java 支持的变量类型有： 局部变量 - 类方法中的变量。 实例变量（也叫成员变量） - 类方法外的变量，不过没有 static 修饰。 类变量（也叫静态变量） - 类方法外的变量，用 static 修饰。 特性对比： 局部变量 实例变量（也叫成员变量） 类变量（也叫静态变量） 局部变量声明在方法、构造方法或者语句块中。 实例变量声明在方法、构造方法和语句块之外。 类变量声明在方法、构造方法和语句块之外。并且以 static 修饰。 局部变量在方法、构造方法、或者语句块被执行的时候创建，当它们执行完成后，变量将会被销毁。 实例变量在对象创建的时候创建，在对象被销毁的时候销毁。 类变量在第一次被访问时创建，在程序结束时销毁。 局部变量没有默认值，所以必须经过初始化，才可以使用。 实例变量具有默认值。数值型变量的默认值是 0，布尔型变量的默认值是 false，引用类型变量的默认值是 null。变量的值可以在声明时指定，也可以在构造方法中指定。 类变量具有默认值。数值型变量的默认值是 0，布尔型变量的默认值是 false，引用类型变量的默认值是 null。变量的值可以在声明时指定，也可以在构造方法中指定。此外，静态变量还可以在静态语句块中初始化。 对于局部变量，如果是基本类型，会把值直接存储在栈；如果是引用类型，会把其对象存储在堆，而把这个对象的引用（指针）存储在栈。 实例变量存储在堆。 类变量存储在静态存储区。 访问修饰符不能用于局部变量。 访问修饰符可以用于实例变量。 访问修饰符可以用于类变量。 局部变量只在声明它的方法、构造方法或者语句块中可见。 实例变量对于类中的方法、构造方法或者语句块是可见的。一般情况下应该把实例变量设为私有。通过使用访问修饰符可以使实例变量对子类可见。 与实例变量具有相似的可见性。但为了对类的使用者可见，大多数静态变量声明为 public 类型。 实例变量可以直接通过变量名访问。但在静态方法以及其他类中，就应该使用完全限定名：ObejectReference.VariableName。 静态变量可以通过：ClassName.VariableName 的方式访问。 无论一个类创建了多少个对象，类只拥有类变量的一份拷贝。 类变量除了被声明为常量外很少使用。 变量修饰符 访问级别修饰符 如果变量是实例变量或类变量，可以添加访问级别修饰符（public/protected/private） 静态修饰符 如果变量是类变量，需要添加 static 修饰 final 如果变量使用 fianl 修饰符，就表示这是一个常量，不能被修改。 数组 👉 扩展阅读：深入理解 Java 数组 枚举 👉 扩展阅读：深入理解 Java 数组 操作符 Java 中支持的操作符类型如下： 👉 扩展阅读：Java 操作符 方法 👉 扩展阅读：深入理解 Java 方法 控制语句 👉 扩展阅读：Java 控制语句 异常 👉 扩展阅读：深入理解 Java 异常 序列化 👉 扩展阅读：深入理解 Java 序列化 泛型 👉 扩展阅读：深入理解 Java 泛型 反射 👉 扩展阅读：深入理解 Java 反射和动态代理]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Title]]></title>
    <url>%2Fblog%2F2019%2F03%2F08%2Fdesign%2Farchitecture%2FREADME%2F</url>
    <content type="text"><![CDATA[架构设计 架构设计是为业务服务，脱离业务实际的架构设计都是纸上谈兵。 架构设计需要根据架构师自身的经验，在实现业务功能、性能、扩展性、系统复杂度等维度上综合考量以及权衡。而架构设计的经验需要架构师不断的学习、不断的积累。性能、扩展性、系统复杂度等方面有很多个专题，有必要针对每个专题由浅入深的去理解、掌握。 专题 如何设计 第一步：需求分析 需求分析阶段，要做的就是分析使用场景，约束和假设。 这个阶段，应该以审视的角度，不断提问、求证，以挖掘用户真实的需求。 系统是什么？系统有什么功能？ 谁是系统的用户群体？用户群体的规模是多大？ 系统的输入输出分别是什么？ 系统希望处理多少数据？ 系统希望每秒钟处理多少请求？ 系统希望的读写比率？ 第二步：概要设计 创造一个高层级的设计 第三步：详细设计 数据库选型：SQL 还是 NOSQL 数据库模型 API 和面向对象设计 第四步：扩展设计 负载均衡 水平扩展 缓存 数据库分片 消息队列 扩展阅读 参考资料 文章 系统设计入门]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Falgorithm%2Fdata-structure%2FREADME%2F</url>
    <content type="text"><![CDATA[数据结构 数据结构 是指相互之间存在着一种或多种关系的数据元素的集合和该集合中数据元素之间的关系组成。 记为：Data_Structure=(D,R)。其中 D 是数据元素的集合，R 是该集合中所有元素之间的关系的有限集合。 常用结构 数组 栈 队列 链表 树 - 树、二叉树、红黑树 图 堆 散列表 结构算法 查找 排序 - 冒泡排序、快速排序、直接插入排序、希尔排序、简单选择排序、堆排序、归并排序、基数排序]]></content>
      <tags>
        <tag>hide</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Falgorithm%2FREADME%2F</url>
    <content type="text"><![CDATA[算法和数据结构 🎯 所有配套源码整理归档在 algorithm-tutorial 项目中。 📝 知识点 数据结构 数据结构 是指相互之间存在着一种或多种关系的数据元素的集合和该集合中数据元素之间的关系组成。 记为：Data_Structure=(D,R)。其中 D 是数据元素的集合，R 是该集合中所有元素之间的关系的有限集合。 常用结构 数组 栈 队列 链表 树 - 树、二叉树、红黑树 图 堆 散列表 结构算法 查找 排序 - 冒泡排序、快速排序、直接插入排序、希尔排序、简单选择排序、堆排序、归并排序、基数排序 📚 学习资源 书 刷题必备 《剑指 offer》 《编程之美》 《编程之法:面试和算法心得》 《算法谜题》 都是思维题 基础 《编程珠玑（第 2 版）》 《编程珠玑（续）》 《数据结构与算法分析 : C++描述（第 4 版）》 《数据结构与算法分析 : C 语言描述（第 2 版）》 《数据结构与算法分析 : Java 语言描述（第 2 版）》 《算法（第 4 版）》- 这本近千页的书只有 6 章,其中四章分别是排序，查找，图，字符串，足见介绍细致 算法设计 《算法设计与分析基础（第 3 版）》 《算法引论》 - 告诉你如何创造算法 断货 《Algorithm Design Manual》 - 算法设计手册 红皮书 《算法导论》 - 是一本对算法介绍比较全面的经典书籍 《Algorithms on Strings,Trees and Sequences》 《Advanced Data Structures》 - 各种诡异高级的数据结构和算法 如元胞自动机、斐波纳契堆、线段树 600 块 参考链接和学习网站 https://github.com/nonstriater/Learn-Algorithms https://github.com/trekhleb/javascript-algorithms https://github.com/kdn251/interviews/blob/master/README-zh-cn.md#数据结构 July 博客 《数学建模十大经典算法》 《数据挖掘领域十大经典算法》 《十道海量数据处理面试题》 《数字图像处理领域的二十四个经典算法》 《精选微软等公司经典的算法面试 100 题》 The-Art-Of-Programming-By-July 微软面试 100 题 程序员编程艺术 基本算法演示 http://sjjg.js.zwu.edu.cn/SFXX/sf1/sfys.html http://www.cs.usfca.edu/\~galles/visualization/Algorithms.html 编程网站 leetcode openjudge 开放在线程序评测平台，可以创建自己的 OJ 小组 九度 OJ 这有个ACM 训练方案 其它 高级数据结构和算法 北大教授张铭老师在 coursera 上的课程。完成这门课之时，你将掌握多维数组、广义表、Trie 树、AVL 树、伸展树等高级数据结构，并结合内排序、外排序、检索、索引有关的算法，高效地解决现实生活中一些比较复杂的应用问题。当然 coursera 上也还有很多其它算法方面的视频课程。 算法设计与分析 Design and Analysis of Algorithms 由北大教授 Wanling Qu 在 coursera 讲授的一门算法课程。首先介绍一些与算法有关的基础知识，然后阐述经典的算法设计思想和分析技术，主要涉及的算法设计技术是：分治策略、动态规划、贪心法、回溯与分支限界等。每个视频都配有相应的讲义（pdf 文件）以便阅读和复习。 🚪 传送门 | 回首頁 |]]></content>
      <tags>
        <tag>hide</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK 安装]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Fjdk%2F</url>
    <content type="text"><![CDATA[JDK 安装 关键词：JDK, JAVA_HOME, CLASSPATH, PATH JDK 安装步骤 Windows 系统安装方法 Linux 系统安装方法 RedHat 发行版本使用 rpm 安装方法 参考资料 JDK 安装步骤 JDK 安装步骤： （1）下载 JDK a. 进入 Java 官网下载页面； b. 选择需要的版本： c. 选择对应操作系统的安装包： Windows 系统选择 exe 安装包；Mac 系统选择 dmp 安装包；Linux 系统选择 tar.gz 压缩包（RedHat 发行版可以安装 rpm 包）。 （2）运行安装包，按提示逐步安装 （3）配置系统环境变量：JAVA_HOME, CLASSPATH, PATH （4）验证 Java 是否安装成功 Windows 系统安装方法 （1）下载 JDK 需要根据 Windows 系统实际情况，选择 exe 安装文件： 32 位计算机选择 Windows x86 64 位计算机选择 Windows x64 （2）运行安装包，按提示逐步安装 （3）配置系统环境变量 a. 安装完成后，右击&quot;我的电脑&quot;，点击&quot;属性&quot;，选择&quot;高级系统设置&quot;； b. 选择&quot;高级&quot;选项卡，点击&quot;环境变量&quot;； 然后就会出现如下图所示的画面： 在&quot;系统变量&quot;中设置 3 项属性，JAVA_HOME,PATH,CLASSPATH(大小写无所谓),若已存在则点击&quot;编辑&quot;，不存在则点击&quot;新建&quot;。 变量设置参数如下： 变量名：JAVA_HOME 变量值：C:\Program Files (x86)\Java\jdk1.8.0_91 // 要根据自己的实际路径配置 变量名：CLASSPATH 变量值：.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar; //记得前面有个&quot;.&quot; 变量名：Path 变量值：%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin; （4）验证 Java 是否安装成功 a. “开始”-&gt;“运行”，键入&quot;cmd&quot;； b. 键入命令: java -version、java、javac 几个命令，出现以下信息，说明环境变量配置成功； Linux 系统安装方法 （1）下载 JDK 需要根据 Linux 系统实际情况，选择 tar.gz 压缩包： 32 位计算机选择 Linux x86 64 位计算机选择 Linux x64 （2）解压压缩包到本地 $ tar -zxf jdk-8u162-linux-x64.tar.gz （3）配置系统环境变量 执行 /etc/profile 命令，添加以下内容： # JDK 的根路径export JAVA_HOME=/opt/java/jdk1.8.0_162export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/libexport PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH 执行 source /etc/profile ，立即生效 （4）验证 Java 是否安装成功 执行 java -version 命令，验证安装是否成功。 RedHat 发行版本使用 rpm 安装方法 （1）下载 JDK 下载 rpm 安装包 （2）选择一个合适的版本安装 $ rpm -ivh jdk-8u181-linux-x64.rpm 安装成功后，默认安装路径在 /usr/local 下： （3）设置环境变量，同压缩包安装。 （4）检验是否安装成功，执行 java -version 命令 更多内容 引申 操作系统、运维部署总结系列 引用 http://www.runoob.com/java/java-environment-setup.html https://blog.csdn.net/deliciousion/article/details/78046007]]></content>
  </entry>
  <entry>
    <title><![CDATA[Gitlab]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Fgitlab%2Fgitlab-ci%2F</url>
    <content type="text"><![CDATA[Gitlab]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mac]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fmac%2F</url>
    <content type="text"><![CDATA[Mac 基本操作 软件管理 dmg 格式：双击安装包，然后拖到 applications 文件夹下即可。 浏览器 更改默认搜索引擎 选择「偏好设置–&gt;搜索–&gt;搜索引擎–&gt;Google」。 导入 chrome 浏览器的书签 选择「文件–&gt;导入自–&gt; Google Chrome」，然后选择要导入的项目。 快捷键 Command + R 刷新 上方显示书签栏／收藏栏 选择「显示–&gt; 显示个人收藏栏」。 关闭软件的右上角通知 在 Mac 系统中有对通知的设置，打开系统偏好设置 — 通知 找到 QQ，然后将 QQ 提示样式设置成无即可。 复制文件/文件夹路径 OS X 10.11 系统，选中文件夹，「cmd +Option +c」 复制文件夹路径，cmd+v 粘贴。 之前的系统，利用 Administrator 创建一个到右键菜单，然后到设置里面设置快捷键。具体操作请百度。 打开来自身份不明的开发者的应用程序 在应用程序文件夹，按住 control 键的同时打开应用程序。 复制文件路径 选择文件／文件夹按 Command+C 复制，在终端中 Command+V 粘贴即可。 如果只是想在 Finder 中看到文件的路径, 并方便切换层级, Finder 内置了「显示路径栏」的功能, 并配置了快捷键(Option+Cmd+P). 如下图所示： 20161124-184148.png 参考链接： [https://www.zhihu.com/question/22883229] 隐藏和取消隐藏 Mac App Store 中的已购项目 Mac 同时登陆两个 QQ 在已经打开的 QQ 中，按住「command + N」即可。 系统便好设置 语音播报 打开「系统便好设置–&gt;辅助功能–&gt;语音」，即可设置不同国家的语言。 勾选上图中的红框部分，可以设置全局快捷键。这样的话，在任何一个软件当中，按下「 option+esc」时，就会朗读选中的文本。 调整字体大小 Mac 调整字体大小：「系统偏好设置 -&gt; 显示器 -&gt; 缩放」。如下图： 如何分别设置 Mac 的鼠标和触控板的滚动方向 很多人习惯鼠标使用相反的滚动方向，而触控板类似 iPad 那样的自然滚动，问如何设置，当时我的回答是不知道，因为目前 OS X 的系统设置里，鼠标和触控板的设置是统一 的。今天发现了一个免费的软件 Scroll Reverser，可以实现鼠标和触控板的分别设置。下载地址：https://pilotmoon.com/scrollreverser/ 启动后程序显示在顶部菜单栏，设置简单明了，有需要的用户体验一下吧。 Touch Bar 自定义 打开「系统偏好设置-键盘」，下面有个自定义控制条。 色温调节：夜间模式 iOS9.3 的最明显变化，莫过于苹果在发布会上特意提到的 Night Shift 夜间护眼模式。 iCloud 邮箱 如果您用于设置 iCloud 的 Apple ID 不以“@icloud.com”、“@me.com”或“@mac.com”结尾，您必须先设置一个“@icloud.com”电子邮件地址，然后才能使用 iCloud“邮件”。 如果您拥有以“@mac.com”或“@me.com”结尾的电子邮件地址，则您已经拥有了名称相同但以“@icloud.com”结尾的等效地址。如果您使用的电子邮件别名以“@mac.com”或“@me.com”结尾，您也将拥有以“@icloud.com”结尾的等效地址。 操作如下： 在 iOS 设备上，前往“设置”&gt;“iCloud”，开启“邮件”，然后按照屏幕上的说明操作。 在 Mac 上，选取 Apple 菜单 &gt;“系统偏好设置”，点按“iCloud”，再选择“邮件”，然后按照屏幕上的说明操作。 PS：创建 iCloud 电子邮件地址后，您无法对其进行更改。 设置 @icloud.com 电子邮件地址后即可用其登录 iCloud。您也可以用创建 iCloud 帐户时所用的 Apple ID 登录。 您可以从以下任意地址发送 iCloud 电子邮件： 您的 iCloud 电子邮件地址（您的帐号名称@icloud.com） 别名 参考链接： 直接注册以@icloud.com 结尾的 Apple ID： 参考链接： PodCast PodCast 中文翻译为播客，是一种特殊的音频 or 视频节目。PodCast 这个单词是由 iPod+Broadcast 这两个单词组成的。 PodCast 可以在 iTunes 中收听。 others 词典 系统有一个自带应用「词典」，可以进行单词的查询。 如何解决 MAC 软件（dmg，akp，app）出现程序已损坏的提示 「xxx.app 已损坏,打不开.你应该将它移到废纸篓」，并非你安装的软件已损坏，而是 Mac 系统的安全设置问题，因为这些应用都是破解或者汉化的,那么解决方法就是临时改变 Mac 系统安全设置。 出现这个问题的解决方法：修改系统配置：系统偏好设置… -&gt; 安全性与隐私。修改为任何来源。 如果没有这个选项的话（macOS Sierra 10.12）,打开终端，执行： sudo spctl --master-disable 即可。 参考链接： Max OS-[xxx.app 已损坏,打不开.你应该将它移到废纸篓] 如何解决 MAC 软件（dmg，akp，app）出现程序已损坏的提示 备注：这个链接里的各种资源都很不错啊。 终端 在 Finder 的当前目录打开终端 在 Finder 打开 terminal 终端这个功能其实是有的，但是系统默认没有打开。我们可以通过如下方法将其打开： 进入系统偏好设置-&gt;键盘-&gt;快捷键-&gt;服务。 在右边新建位于文件夹位置的终端窗口上打勾。 如此设置后，在 Finder 中右击某文件，在出现的菜单中找到服务，然后点击新建位于文件夹位置的终端窗口即可！ Mac 常用快捷键 Finder 快捷键 作用 备注 Shift + Command + G 前往指定路径的文件夹 包括隐藏文件夹 Shift + Command + . 显示隐藏文件、文件夹 再按一次，恢复隐藏 Command + ↑ 返回上一层 Command + ↓ 进入当前文件夹 编辑 删除文字： 快捷键 作用 备注 delete 删除光标的前一个字符 相当于 Windows 键盘上的退格键 fn + delete 删除光标的后一个字符 option + delete 删除光标之前的一个单词 英文有效 command + delete 删除光标之前的整行内容 【荐】 command + delete 在 finder 中删掉该文件 shift + command + delete 清空回收站 剪切文件： 首先选中文件，按 Command+C 复制文件；然后按「Command ＋ Option ＋ V」剪切文件。 备注：Command+X 只能剪切文字文本，不要混淆了。 Mac 用户必须知道的 15 组快捷键 参考链接：《轻松玩 Mac》第 6 期：Mac 用户必须知道的 15 组快捷键 「space」键：快速预览 选中文件后， 不需要启动任何应用程序，使用「space」空格键可进行快速预览，再次按下「space」空格键取消预览。 可以预览 mp3、视频、pdf 等文件。 我们还可以选中多张图片， 然后按「space」键，就可以同时对比预览多张图片。这一点，很赞。 改名 选中文件/文件夹后，按 enter 键，就可以改名了。 「command + I」键：查看文件属性 选中文件后，按「command + I」键，可以查看文件的各种属性。 选中文件夹后，按「command + I」键，可以查看文件夹的大小。【荐】 切换输入法 「control + space」 打开 spotlight 搜索框 spotlight 是系统自带的软件，搜索功能不是很强大。我们一般都会用第三方的 Alfred 软件。 编辑相关 Cmd+C、Cmd+V、Cmd+X、Cmd+A、Cmd+Z。 翻页和光标 「control + ↑」：将光标定位到文章的最开头（翻页到文档的最上方） 「control + ↓」：将光标定位到文章的最末尾（翻页到文档的最下方） 「control + ←」：将光标定位到当前行的最左侧 「control + →」：将光标定位到当前行的最右侧 「command + shift + Y」：将文字快速保存到便笺 选中你想要的内容（例如文字、链接等），然后按下 command + shift + Y」，那么你选中的内容就会快速保存到系统自带的「便笺」软件中。 如果你想临时性的保存一段内容，这个操作很实用。 程序相关 「command + Q」：快速退出程序 「command + tab」：切换程序 「command + H」：隐藏当前应用程序。这是一个有趣的快捷键。 「command + ，」：打开当前应用程序的「偏好设置」。 窗口相关 「command + N」：新建一个当前应用程序的窗口 「command + `」：在当前应用程序的不同窗口之间切换【很实用】 我们知道，「command + tab」是在不同的软件之间切换。但你不知道的是，「command + `」是在同一个软件的不同窗口之间切换。 「command + M」：将当前窗口最小化 「command + W」：关闭当前窗口 浏览器相关 「command + T」：浏览器中，新建一个标签 「command + W」：关闭当前标签 「command + R」：强制刷新。 「command + L」：定位到地址栏。【重要】 截图相关 「command + shift + 3」：截全屏（对整个屏幕截图）。 声音相关 选中文字后，按住「ctrl + esc」键，会将文字进行朗读。（我发现，在触控条版的 mac 上，并没有生效） Dock 栏相关 「option + command + D」：隐藏 dock 栏 强制推出 强制退出的快捷键非常重要 「option + command + esc」：打开强制退出的窗口 option 相关 强烈推荐 「option + command + H」：隐藏除当前应用程序之外的其他应用程序 在文本中，按住「option」键，配合鼠标的选中，可以进行块状文字选取。 「option + command + W」：快速关闭当前应用程序的所有窗口。【很实用】 比如说，你一次性打开了很多文件的详情，然后就可以通过此快捷键，将这些窗口一次性关闭。 「option + command + I」：查看多个文件的总的属性。 打开 launchpad，按住「option」键，可以快速卸载应用程序。 在 dock 栏，右键点击软件图标，同时按住「option」键，就可以强制退出该软件。【重要】 在 Safari 浏览器中，按住「option + command + Q」退出 Safari。等下次进入 Safari 的时候，上次退出时的网址会自动被打开。【实用】 推荐一个软件：CheatSheet 打开 CheatSheet 后，长按 command 键，会弹出当前应用程序的所有快捷键。我们还可以对这些快捷键进行保存。 📚 学习资源 Awesome Mac awesome-macos-command-line 🚪 传送门 | 回首頁 |]]></content>
  </entry>
  <entry>
    <title><![CDATA[Apollo]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Fapollo%2FREADME%2F</url>
    <content type="text"><![CDATA[Apollo Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。 官方 Github：https://github.com/ctripcorp/apollo 由于官方示例不能直接使用，所以我 Fork 后，略作修改：https://github.com/dunwu/apollo]]></content>
  </entry>
  <entry>
    <title><![CDATA[合理编排你的技术文档]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fmethod%2Fdoc-style%2F</url>
    <content type="text"><![CDATA[技术文档规范 文档采用 Markdown 语法书写。 📚 「参考」Markdown 语法可以参考： https://github.com/guodongxiaren/README https://github.com/tchapi/markdown-cheatsheet 1. 标题 1.1. 标题层级 1.2. 标题原则 2. 文本 2.1. 字间距 2.2. 句子 2.3. 写作风格 2.4. 英文处理 3. 段落 3.1. 段落原则 3.2. 引用 3.3. 强调 4. 数值 4.1. 半角数字 4.2. 千分号 4.3. 货币 4.4. 数值范围 4.5. 变化程度的表示法 5. 符号 5.1. 符号原则 5.2. 句号 5.3. 逗号 5.4. 顿号 5.5. 分号 5.6. 引号 5.7. 圆括号 5.8. 冒号 5.9. 省略号 5.10. 感叹号 5.11. 破折号 5.12. 连接号 6. 结构 6.1. 目录结构 6.2. 文件名 7. Emoji 8. 参考 1. 标题 1.1. 标题层级 标题分为四级。 一级标题：文章的标题 二级标题：文章内容的大标题 三级标题：二级标题下一级的标题 四级标题：三级标题下一级的标题 1.2. 标题原则 一篇文章中应该尽力避免同名标题。 一级标题下，不能直接出现三级标题。 标题要避免孤立编号（即同级标题只有一个）。 下级标题不重复上一级标题的内容。 谨慎使用四级标题，尽量避免出现，保持层级的简单和防止出现过于复杂的章节。如果三级标题下有并列性的内容，建议只使用项目列表（Item list）。 2. 文本 2.1. 字间距 全角中文字符与半角英文字符之间，应有一个半角空格。 反例：本文介绍如何快速启动Windows系统。正例：本文介绍如何快速启动 Windows 系统。 全角中文字符与半角阿拉伯数字之间，有没有半角空格都可，但必须保证风格统一，不能两种风格混杂。 正例：2011年5月15日，我订购了5台笔记本电脑与10台平板电脑。正例：2011 年 5 月 15 日，我订购了 5 台笔记本电脑与 10 台平板电脑。 半角的百分号，视同阿拉伯数字。 英文单位若不翻译，单位前的阿拉伯数字与单位间不留空格。 反例：一部容量为 16 GB 的智能手机正例：一部容量为 16GB 的智能手机 半角英文字符和半角阿拉伯数字，与全角标点符号之间不留空格。 反例：他的电脑是 MacBook Air 。正例：他的电脑是 MacBook Air。 2.2. 句子 避免使用长句。一个句子建议不超过 100 字或者正文的 3 行。 尽量使用简单句和并列句，避免使用复合句。 2.3. 写作风格 尽量不使用被动语态，改为使用主动语态。 反例：假如此软件尚未被安装，正例：假如尚未安装这个软件， 不使用非正式的语言风格。 反例：Lady Gaga 的演唱会真是酷毙了，从没看过这么给力的表演！！！正例：无法参加本次活动，我深感遗憾。 用对“的”、“地”、“得”。 她露出了开心的笑容。（形容词＋的＋名词）她开心地笑了。（副词＋地＋动词）她笑得很开心。（动词＋得＋副词） 使用代词时（比如“其”、“该”、“此”、“这”等词），必须明确指代的内容，保证只有一个含义。 反例：从管理系统可以监视中继系统和受其直接控制的分配系统。正例：从管理系统可以监视两个系统：中继系统和受中继系统直接控制的分配系统。 名词前不要使用过多的形容词。 反例：此设备的使用必须在接受过本公司举办的正式的设备培训的技师的指导下进行。正例：此设备必须在技师的指导下使用，且指导技师必须接受过由本公司举办的正式设备培训。 单个句子的长度尽量保持在 20 个字以内；20 ～ 29 个字的句子，可以接受；30 ～ 39 个字的句子，语义必须明确，才能接受；多于 40 个字的句子，在任何情况下都不能接受。 反例：本产品适用于从由一台服务器进行动作控制的单一节点结构到由多台服务器进行动作控制的并行处理程序结构等多种体系结构。正例：本产品适用于多种体系结构。无论是由一台服务器（单一节点结构），还是由多台服务器（并行处理结构）进行动作控制，均可以使用本产品。 同样一个意思，尽量使用肯定句表达，不使用否定句表达。 反例：请确认没有接通装置的电源。正例：请确认装置的电源已关闭。 避免使用双重否定句。 反例：没有删除权限的用户，不能删除此文件。正例：用户必须拥有删除权限，才能删除此文件。 2.4. 英文处理 英文原文如果使用了复数形式，翻译成中文时，应该将其还原为单数形式。 英文：⋯information stored in random access memory (RAMs)⋯中文：……存储在随机存取存储器（RAM）里的信息…… 外文缩写可以使用半角圆点(.)表示缩写。 U.S.A.Apple, Inc. 表示中文时，英文省略号（⋯）应改为中文省略号（……）。 英文：5 minutes later⋯中文：5 分钟过去了⋯⋯ 英文书名或电影名改用中文表达时，双引号应改为书名号。 英文：He published an article entitled "The Future of the Aviation".中文：他发表了一篇名为《航空业的未来》的文章。 第一次出现英文词汇时，在括号中给出中文标注。此后再次出现时，直接使用英文缩写即可。 IOC（International Olympic Committee，国际奥林匹克委员会）。这样定义后，便可以直接使用“IOC”了。 专有名词中每个词第一个字母均应大写，非专有名词则不需要大写。 “American Association of Physicists in Medicine”（美国医学物理学家协会）是专有名词，需要大写。“online transaction processing”（在线事务处理）不是专有名词，不应大写。 3. 段落 3.1. 段落原则 一个段落只能有一个主题，或一个中心句子。 段落的中心句子放在段首，对全段内容进行概述。后面陈述的句子为核心句服务。 一个段落的长度不能超过七行，最佳段落长度小于等于四行。 段落的句子语气要使用陈述和肯定语气，避免使用感叹语气。 段落之间使用一个空行隔开。 段落开头不要留出空白字符。 3.2. 引用 引用第三方内容时，应注明出处。 One man’s constant is another man’s variable. — Alan Perlis 如果是全篇转载，请在全文开头显著位置注明作者和出处，并链接至原文。 本文转载自 WikiQuote 使用外部图片时，必须在图片下方或文末标明来源。 本文部分图片来自 Wikipedia 3.3. 强调 一些特殊的强调内容可以按照如下方式书写： 🔔 『注意』 💡 『提示』 📚 『参考』 4. 数值 4.1. 半角数字 数字一律使用半角形式，不得使用全角形式。 反例： 这件商品的价格是１０００元。正例： 这件商品的价格是 1000 元。 4.2. 千分号 数值为千位以上，应添加千分号（半角逗号）。 XXX 公司的实收资本为 RMB1,258,000。 对于 4 ～ 6 位的数值，千分号是选用的，比如1000和1,000都可以接受。对于 7 位及以上的数值，千分号是必须的。 多位小数要从小数点后从左向右添加千分号，比如4.234,345。 4.3. 货币 货币应为阿拉伯数字，并在数字前写出货币符号，或在数字后写出货币中文名称。 $1,0001,000 美元 4.4. 数值范围 表示数值范围时，用～连接。参见《标点符号》一节的“连接号”部分。 带有单位或百分号时，两个数字都要加上单位或百分号，不能只加后面一个。 反例：132～234kg正例：132kg～234kg反例：67～89%正例：67%～89% 4.5. 变化程度的表示法 数字的增加要使用“增加了”、“增加到”。“了”表示增量，“到”表示定量。 增加到过去的两倍（过去为一，现在为二）增加了两倍（过去为一，现在为三） 数字的减少要使用“降低了”、“降低到”。“了”表示增量，“到”表示定量。 降低到百分之八十（定额是一百，现在是八十）降低了百分之八十（原来是一百，现在是二十） 不能用“降低 N 倍”或“减少 N 倍”的表示法，要用“降低百分之几”或“减少百分之几”。因为减少（或降低）一倍表示数值原来为一百，现在等于零。 5. 符号 5.1. 符号原则 中文语句的标点符号，均应该采取全角符号，这样可以保证视觉的一致。 如果整句为英文，则该句使用英文/半角标点。 句号、问号、叹号、逗号、顿号、分号和冒号不得出现在一行之首。 5.2. 句号 中文语句中的结尾处应该用全角句号（。）。 句子末尾用括号加注时，句号应在括号之外。 反例：关于文件的输出，请参照第 1.3 节（见第 26 页。）正例：关于文件的输出，请参照第 1.3 节（见第 26 页）。 5.3. 逗号 逗号，表示句子内部的一般性停顿。 注意避免“一逗到底”，即整个段落除了结尾，全部停顿都使用逗号。 5.4. 顿号 句子内部的并列词，应该用全角顿号(、) 分隔，而不用逗号，即使并列词是英语也是如此。 反例：我最欣赏的科技公司有 Google, Facebook, 腾讯, 阿里和百度等。正例：我最欣赏的科技公司有 Google、Facebook、腾讯、阿里和百度等。 英文句子中，并列词语之间使用半角逗号（,）分隔。 例句：Microsoft Office includes Word, Excel, PowerPoint, Outlook and other components. 5.5. 分号 分号；表示复句内部并列分句之间的停顿。 5.6. 引号 引用时，应该使用全角双引号（“ ”），注意前后双引号不同。 例句：许多人都认为客户服务的核心是“友好”和“专业”。 引号里面还要用引号时，外面一层用双引号，里面一层用单引号（‘ ’），注意前后单引号不同。 例句：鲍勃解释道：“我要放音乐，可萨利说，‘不行！’。” 5.7. 圆括号 补充说明时，使用全角圆括号（），括号前后不加空格。 例句：请确认所有的连接（电缆和接插件）均安装牢固。 5.8. 冒号 全角冒号（：）常用在需要解释的词语后边，引出解释和说明。 例句：请确认以下几项内容：时间、地点、活动名称，以及来宾数量。 表示时间时，应使用半角冒号（:）。 例句：早上 8:00 5.9. 省略号 省略号……表示语句未完、或者语气的不连续。它占两个汉字空间、包含六个省略点，不要使用。。。或...等非标准形式。 省略号不应与“等”这个词一起使用。 反例：我们为会餐准备了香蕉、苹果、梨…等各色水果。正例：我们为会餐准备了各色水果，有香蕉、苹果、梨……正例：我们为会餐准备了香蕉、苹果、梨等各色水果。 5.10. 感叹号 应该使用平静的语气叙述，尽量避免使用感叹号！。 不得多个感叹号连用，比如！！和!!!。 5.11. 破折号 破折号————一般用于做进一步解释。破折号应占两个汉字的位置。 例句：直觉————尽管它并不总是可靠的————告诉我，这事可能出了些问题。 5.12. 连接号 连接号用于连接两个类似的词。 以下场合应该使用直线连接号（-），占一个半角字符的位置。 两个名词的复合 图表编号 例句：氧化-还原反应例句：图 1-1 以下场合应该使用波浪连接号（～），占一个全角字符的位置。 数值范围（例如日期、时间或数字） 例句：2009 年～2011 年 注意，波浪连接号前后两个值都应该加上单位。 波浪连接号也可以用汉字“至”代替。 例句：周围温度：-20°C 至 -10°C 6. 结构 6.1. 目录结构 技术手册目录结构是一部完整的书，建议采用下面的结构。 简介（Introduction） - [必选][目录|文件] 提供对产品和文档本身的总体的、扼要的说明 入门篇（Quickstart） - [可选][文件] 如何最快速地使用产品 基础篇（Basics） - [必选][目录] 又称”使用篇“，提供初级的使用教程 环境准备（Prerequisite） - [可选][文件] 软件使用需要满足的前置条件 安装（Installation） - [可选][文件] 软件的安装方法 配置（Configuration） - [可选][目录|文件] 软件的配置 特性（Feature） - [必选][目录|文件] 软件的功能特性 进阶篇（Advanced） - [可选][目录] 又称”开发篇“，提供中高级的开发教程 原理（Principle） - [可选][目录|文件] 软件的原理 设计（Design） - [可选][目录|文件] 软件的设计，如：架构、设计思想等 实战篇（Action） - [可选][目录] 提供一些具有实战意义的示例说明 API（API） - [可选][目录|文件] 软件 API 的逐一介绍 常见问题（FAQ） - [可选][目录|文件] 常见问题解答 附录（Appendix） - [可选][目录] 不属于教程本身、但对阅读教程有帮助的内容 命令（Command） - [可选][目录] 命令 资源（Resource） - [必选][文件] 资源 术语（Glossary） - [可选][文件] 名词解释 技巧（Recipe） - [可选][文件] 最佳实践 版本（Changelog） - [可选][文件] 版本说明 反馈（Feedback） - [可选][文件] 反馈方式 下面是两个真实范例，可参考。 Redux 手册 Atom 手册 6.2. 文件名 文档的文件名不得含有空格。 文件名必须使用半角字符，不得使用全角字符。这也意味着，中文不能用于文件名。 反例： 名词解释.md正例： glossary.md 文件名建议只使用小写字母，不使用大写字母。 反例：TroubleShooting.md正例：troubleshooting.md 为了醒目，某些说明文件的文件名，可以使用大写字母，比如README、LICENSE。 文件名包含多个单词时，单词之间建议使用半角的连词线（-）分隔。 反例：advanced_usage.md正例：advanced-usage.md 7. Emoji 在 markdown 文档中，普遍会使用 emoji，帮助理解内容。但是，如果滥用 emoji，可能会适得其反。 这里，将一些比较约定俗成的 emoji 表情使用场景列举一下： 💡 提示 - [推荐] 🔔 注意、警告 - [推荐] ✔️ 正确 - [推荐] ❌ 错误 - [推荐] 🚧 未完待续、有待补充 - [推荐] 📚 参考、参考资料 - [可选] ⌨️ 源码 - [可选] 8. 参考 产品手册中文写作规范, by 华为 写作规范和格式规范, by DaoCloud 技术写作技巧在日汉翻译中的应用, by 刘方 简体中文规范指南, by lengoo 文档风格指南, by LeanCloud 豌豆荚文案风格指南, by 豌豆荚 中文文案排版指北, by sparanoid 中文排版需求, by W3C 为什么文件名要小写？, by 阮一峰]]></content>
      <categories>
        <category>method</category>
      </categories>
      <tags>
        <tag>method</tag>
        <tag>doc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic 技术栈]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Felastic%2FREADME%2F</url>
    <content type="text"><![CDATA[Elastic 技术栈 Elastic 技术栈通常被用来作为日志中心。 ELK 是 elastic 公司旗下三款产品 ElasticSearch 、Logstash 、Kibana 的首字母组合。 ElasticSearch 是一个基于 Lucene 构建的开源，分布式，RESTful 搜索引擎。 Logstash 传输和处理你的日志、事务或其他数据。 Kibana 将 Elasticsearch 的数据分析并渲染为可视化的报表。 Elastic 技术栈，在 ELK 的基础上扩展了一些新的产品，如：Beats 、X-Pack 。 目录 Elastic 技术栈之快速指南 Elastic 技术栈之 Logstash 基础 资源 官方资源 Elastic 官方文档 第三方工具 logstash-logback-encoder 教程 Elasticsearch 权威指南（中文版） ELK Stack权威指南 博文 Elasticsearch+Logstash+Kibana教程 ELK（Elasticsearch、Logstash、Kibana）安装和配置]]></content>
  </entry>
  <entry>
    <title><![CDATA[HBase]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fbigdata%2Fhbase%2FREADME%2F</url>
    <content type="text"><![CDATA[HBase 简介 基础 原理 数据模型 HBase 架构 HBase 和 RDBMS API 附录 命令行 更多内容 扩展阅读 参考资料 简介 HBase 是建立在 HDFS 基础上的面向列的分布式数据库。 HBase 参考了谷歌的 BigTable 建模，实现的编程语言为 Java。 它是 Hadoop 项目的子项目，运行于 HDFS 文件系统之上。 HBase 适用场景：实时地随机访问超大数据集。 在 CAP 理论中，HBase 属于 CP 类型的系统。 基础 HBase 维护 原理 数据模型 HBase 是一个面向列的数据库，在表中它由行排序。 HBase 表模型结构为： 表（table）是行的集合。 行（row）是列族的集合。 列族（column family）是列的集合。 列（row）是键值对的集合。 HBase 表的单元格（cell）由行和列的坐标交叉决定，是有版本的。默认情况下，版本号是自动分配的，为 HBase 插入单元格时的时间戳。单元格的内容是未解释的字节数组。 行的键也是未解释的字节数组，所以理论上，任何数据都可以通过序列化表示成字符串或二进制，从而存为 HBase 的键值。 HBase 架构 和 HDFS、YARN 一样，HBase 也采用 master / slave 架构： HBase 有一个 master 节点。master 节点负责将区域（region）分配给 region 节点；恢复 region 节点的故障。 HBase 有多个 region 节点。region 节点负责零个或多个区域（region）的管理并相应客户端的读写请求。region 节点还负责区域的划分并通知 master 节点有了新的子区域。 HBase 依赖 ZooKeeper 来实现故障恢复。 Regin HBase 表按行键范围水平自动划分为区域（region）。每个区域由表中行的子集构成。每个区域由它所属的表、它所含的第一行及最后一行来表示。 区域只不过是表被拆分，并分布在区域服务器。 Master 服务器 区域分配、DDL(create、delete)操作由 HBase master 服务器处理。 master 服务器负责协调 region 服务器 协助区域启动，出现故障恢复或负载均衡情况时，重新分配 region 服务器 监控集群中的所有 region 服务器 支持 DDL 接口（创建、删除、更新表） Regin 服务器 区域服务器运行在 HDFS 数据节点上，具有以下组件 WAL - Write Ahead Log 是 HDFS 上的文件。WAL 存储尚未持久存储到永久存储的新数据，它用于在发生故障时进行恢复。 BlockCache - 是读缓存。它将频繁读取的数据存储在内存中。至少最近使用的数据在完整时被逐出。 MemStore - 是写缓存。它存储尚未写入磁盘的新数据。在写入磁盘之前对其进行排序。每个区域每个列族有一个 MemStore。 Hfiles - 将行存储为磁盘上的排序键值对。 ZooKeeper HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。Zookeeper 维护哪些服务器是活动的和可用的，并提供服务器故障通知。集群至少应该有 3 个节点。 HBase 和 RDBMS HBase RDBMS HBase 无模式，它不具有固定列模式的概念;仅定义列族。 RDBMS 有它的模式，描述表的整体结构的约束。 它专门创建为宽表。 HBase 是横向扩展。 这些都是细而专为小表。很难形成规模。 没有任何事务存在于 HBase。 RDBMS 是事务性的。 它反规范化的数据。 它具有规范化的数据。 它用于半结构以及结构化数据是非常好的。 用于结构化数据非常好。 API Java API 归纳总结在这里：Hbase Java API 附录 命令行 HBase 命令行可以参考这里：HBase 命令行 更多内容 扩展阅读 HBase 命令 HBase 配置 参考资料 官方 HBase 官网 HBase 官方文档 HBase 官方文档中文版 HBase API 文章 Bigtable: A Distributed Storage System for Structured Data https://mapr.com/blog/in-depth-look-hbase-architecture/]]></content>
  </entry>
  <entry>
    <title><![CDATA[HBase Java API]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fbigdata%2Fhbase%2Fhbase-api-java%2F</url>
    <content type="text"><![CDATA[HBase Java API 更多内容 参考资料 更多内容 📓 本文已归档到：「blog」 参考资料 https://blog.csdn.net/jiyiqinlovexx/article/details/36526433 https://blog.csdn.net/u010967382/article/details/38046821]]></content>
  </entry>
  <entry>
    <title><![CDATA[HBase 维护]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fbigdata%2Fhbase%2Fhbase-ops%2F</url>
    <content type="text"><![CDATA[HBase 维护 配置文件 环境要求 引用和引申 引申 配置文件 backup-masters - 默认情况下不存在。列出主服务器应在其上启动备份主进程的主机，每行一个主机。 hadoop-metrics2-hbase.properties - 用于连接 HBase Hadoop 的 Metrics2 框架。 hbase-env.cmd and hbase-env.sh - 用于 Windows 和 Linux / Unix 环境的脚本，用于设置 HBase 的工作环境，包括 Java，Java 选项和其他环境变量的位置。 hbase-policy.xml - RPC 服务器用于对客户端请求进行授权决策的默认策略配置文件。仅在启用 HBase 安全性时使用。 hbase-site.xml - 主要的 HBase 配置文件。此文件指定覆盖 HBase 默认配置的配置选项。您可以在 docs / hbase-default.xml 中查看（但不要编辑）默认配置文件。您还可以在 HBase Web UI 的 HBase 配置选项卡中查看群集的整个有效配置（默认值和覆盖）。 log4j.properties - log4j 日志配置。 regionservers - 包含应在 HBase 集群中运行 RegionServer 的主机列表。默认情况下，此文件包含单个条目 localhost。它应包含主机名或 IP 地址列表，每行一个，并且如果群集中的每个节点将在其 localhost 接口上运行 RegionServer，则应仅包含 localhost。 环境要求 Java HBase 2.0+ 要求 JDK8+ HBase 1.2+ 要求 JDK7+ SSH - 环境要支持 SSH DNS - 环境中要在 hosts 配置本机 hostname 和本机 IP NTP - HBase 集群的时间要同步，可以配置统一的 NTP 平台 - 生产环境不推荐部署在 Windows 系统中 Hadoop - 依赖 Hadoop 配套版本 Zookeeper - 依赖 Zookeeper 配套版本 运行模式 单点 hbase-site.xml 配置如下： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://namenode.example.org:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 分布式 hbase-site.xm 配置如下： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://namenode.example.org:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node-a.example.com,node-b.example.com,node-c.example.com&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 引用和引申 扩展阅读 Apache HBase Configuration]]></content>
  </entry>
  <entry>
    <title><![CDATA[HBase 命令]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fbigdata%2Fhbase%2Fhbase-cli%2F</url>
    <content type="text"><![CDATA[HBase 命令 1. 连接 HBase 2. 查询帮助 3. 创建表 4. 查看表信息 5. 查看表详细信息 6. 向表中写数据 7. 扫描表 8. 查询 row 9. 禁用、启用表 10. 删除表 11. 停止 HBase 1. 连接 HBase $ ./bin/hbase shellhbase(main):001:0&gt; 2. 查询帮助 help 3. 创建表 create 'table1','columnFamliy1','columnFamliy2' 说明： 创建一张名为 table1 的 HBase 表，columnFamliy1、columnFamliy2 是 table1 表的列族。 4. 查看表信息 list 'table1' 5. 查看表详细信息 describe 'table1' 6. 向表中写数据 put 'table1', 'row1', 'columnFamliy1:a', 'valueA'put 'table1', 'row1', 'columnFamliy1:b', 'valueB'put 'table1', 'row1', 'columnFamliy1:c', 'valueC'put 'table1', 'row2', 'columnFamliy1:a', 'valueA'put 'table1', 'row2', 'columnFamliy1:b', 'valueB'put 'table1', 'row2', 'columnFamliy1:c', 'valueC'put 'table1', 'row1', 'columnFamliy2:a', 'valueA'put 'table1', 'row1', 'columnFamliy2:b', 'valueB'put 'table1', 'row1', 'columnFamliy2:c', 'valueC' 7. 扫描表 hbase&gt; scan 'hbase:meta'hbase&gt; scan 'hbase:meta', &#123;COLUMNS =&gt; 'info:regioninfo'&#125;hbase&gt; scan 'ns1:hbase&gt; scan 't1', &#123;COLUMNS =&gt; ['c1', 'c2'], LIMIT =&gt; 10, STARTROW =&gt; 'xyz'&#125;hbase&gt; scan 't1', &#123;COLUMNS =&gt; 'c1', TIMERANGE =&gt; [1303668804, 1303668904]&#125;hbase&gt; scan 't1', &#123;REVERSED =&gt; true&#125;hbase&gt; scan 't1', &#123;ALL_METRICS =&gt; true&#125;hbase&gt; scan 't1', &#123;METRICS =&gt; ['RPC_RETRIES', 'ROWS_FILTERED']&#125;hbase&gt; scan 't1', &#123;ROWPREFIXFILTER =&gt; 'row2', FILTER =&gt; " (QualifierFilter (&gt;=, 'binary:xyz')) AND (TimestampsFilter ( 123, 456))"&#125;hbase&gt; scan 't1', &#123;FILTER =&gt; org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)&#125;hbase&gt; scan 't1', &#123;CONSISTENCY =&gt; 'TIMELINE'&#125;For setting the Operation Attributes hbase&gt; scan 't1', &#123; COLUMNS =&gt; ['c1', 'c2'], ATTRIBUTES =&gt; &#123;'mykey' =&gt; 'myvalue'&#125;&#125;hbase&gt; scan 't1', &#123; COLUMNS =&gt; ['c1', 'c2'], AUTHORIZATIONS =&gt; ['PRIVATE','SECRET']&#125;For experts, there is an additional option -- CACHE_BLOCKS -- whichswitches block caching for the scanner on (true) or off (false). Bydefault it is enabled. Examples:hbase&gt; scan 't1', &#123;COLUMNS =&gt; ['c1', 'c2'], CACHE_BLOCKS =&gt; false&#125; 8. 查询 row get 'table1', 'row1'get 'table1', 'row1', 'columnFamliy1'get 'table1', 'row1', 'columnFamliy1:a' 9. 禁用、启用表 hbase(main):008:0&gt; disable 'test'0 row(s) in 1.1820 secondshbase(main):009:0&gt; enable 'test'0 row(s) in 0.1770 seconds 10. 删除表 hbase(main):011:0&gt; drop 'test'0 row(s) in 0.1370 seconds 11. 停止 HBase $ ./bin/stop-hbase.shstopping hbase....................$]]></content>
  </entry>
  <entry>
    <title><![CDATA[HDFS 命令]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fbigdata%2Fhdfs%2Fhdfs-cli%2F</url>
    <content type="text"><![CDATA[HDFS 命令 列出目录的内容： $ hdfs dfs -ls / 将文件从本地文件系统加载到HDFS： $ hdfs dfs -put songs.txt /user/adam 从HDFS读取文件内容： $ hdfs dfs -cat /user/adam/songs.txt 更改文件的权限： $ hdfs dfs -chmod 700 /user/adam/songs.txt 将文件的复制因子设置为4： $ hdfs dfs -setrep -w 4 /user/adam/songs.txt 检查文件的大小： $ hdfs dfs -du -h /user/adam/songs.txt Create a subdirectory in your home directory.$ hdfs dfs -mkdir songs 注意，相对路径总是引用执行命令的用户的主目录。HDFS上没有“当前”目录的概念（换句话说，没有“CD”命令）： 将文件移到新创建的子目录： $ hdfs dfs -mv songs.txt songs 从HDFS中删除一个目录： $ hdfs dfs -rm -r songs]]></content>
  </entry>
  <entry>
    <title><![CDATA[事务管理]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring%2Fdata%2Ftransaction%2F</url>
    <content type="text"><![CDATA[事务管理 理解事务 在软件开发领域，全有或全无的操作被称为事务（transaction）。事务允许你将几个操作组合成一个要么全部发生要么全部不发生的工作单元。 事务的特性 事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为 ACID。 原子性（Atomic）：一个事务是一个不可分割的工作单位，事务中包括的诸操作要么都做，要么都不做。 一致性（Consistent）：事务必须是使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。 隔离性（Isolated）：一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 持久性（Durable）：持久性也称永久性（permanence），指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。 Spring 对事务的支持 Spring 通过回调机制将实际的事务实现从事务性的代码中抽象出来。 Spring 提供了对编码式和声明式事务管理的支持。 编码式事务允许用户在代码中精确定义事务的边界 声明式事务（基于 AOP）有助于用户将操作与事务规则进行解耦 核心 API PlatformTransactionManager Spring 并不直接管理事务，而是提供了多种事务管理器，它们将事务管理的职责委托给 JTA 或其他持久化机制所提供的平台相关的事务实现。 Spring 事务管理器的接口是 org.springframework.transaction.PlatformTransactionManager，通过这个接口，Spring为各个平台如 JDBC、Hibernate 等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。 public interface PlatformTransactionManager &#123; TransactionStatus getTransaction( TransactionDefinition definition) throws TransactionException; void commit(TransactionStatus status) throws TransactionException; void rollback(TransactionStatus status) throws TransactionException;&#125; 从这里可知具体的具体的事务管理机制对 Spring 来说是透明的，它并不关心那些，那些是对应各个平台需要关心的，所以Spring 事务管理的一个优点就是为不同的事务 API 提供一致的编程模型，如 JTA、JDBC、Hibernate、JPA。下面分别介绍各个平台框架实现事务管理的机制。 JDBC事务 如果应用程序中直接使用JDBC来进行持久化，DataSourceTransactionManager 会为你处理事务边界。为了使用 DataSourceTransactionManager，你需要使用如下的XML将其装配到应用程序的上下文定义中： &lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource" /&gt;&lt;/bean&gt; 实际上，DataSourceTransactionManager 是通过调用 java.sql.Connection 来管理事务，而后者是通过 DataSource 获取到的。通过调用连接的 commit() 方法来提交事务，同样，事务失败则通过调用 rollback() 方法进行回滚。 Hibernate事务 如果应用程序的持久化是通过 Hibernate 实现的，那么你需要使用 HibernateTransactionManager。对于Hibernate3，需要在 Spring 上下文定义中添加如下的 bean 声明： &lt;bean id="transactionManager" class="org.springframework.orm.hibernate3.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt;&lt;/bean&gt; sessionFactory 属性需要装配一个 Hibernate 的 session 工厂，HibernateTransactionManager 的实现细节是它将事务管理的职责委托给 org.hibernate.Transaction 对象，而后者是从 Hibernate Session 中获取到的。当事务成功完成时，HibernateTransactionManager 将会调用 Transaction 对象的 commit() 方法，反之，将会调用 rollback() 方法。 Java持久化API事务（JPA） Hibernate 多年来一直是事实上的 Java 持久化标准，但是现在 Java 持久化 API 作为真正的 Java 持久化标准进入大家的视野。如果你计划使用 JPA 的话，那你需要使用 Spring 的 JpaTransactionManager 来处理事务。你需要在 Spring 中这样配置 JpaTransactionManager： &lt;bean id="transactionManager" class="org.springframework.orm.jpa.JpaTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt;&lt;/bean&gt; JpaTransactionManager 只需要装配一个JPA实体管理工厂（javax.persistence.EntityManagerFactory 接口的任意实现）。JpaTransactionManager 将与由工厂所产生的 JPA EntityManager 合作来构建事务。 Java原生API事务（JTA） 如果你没有使用以上所述的事务管理，或者是跨越了多个事务管理源（比如两个或者是多个不同的数据源），你就需要使用JtaTransactionManager： &lt;bean id="transactionManager" class="org.springframework.transaction.jta.JtaTransactionManager"&gt; &lt;property name="transactionManagerName" value="java:/TransactionManager" /&gt;&lt;/bean&gt; JtaTransactionManager 将事务管理的责任委托给 javax.transaction.UserTransaction 和 javax.transaction.TransactionManager 对象，其中事务成功完成通过 UserTransaction.commit() 方法提交，事务失败通过 UserTransaction.rollback() 方法回滚。 TransactionDefinition 上面讲到的事务管理器接口 PlatformTransactionManager 通过 getTransaction(TransactionDefinition definition) 方法来得到事务，这个方法里面的参数是 TransactionDefinition 类，这个类就定义了一些基本的事务属性。 那么什么是事务属性呢？事务属性可以理解成事务的一些基本配置，描述了事务策略如何应用到方法上。事务属性包含了5个方面，如图所示： 而 TransactionDefinition 接口内容如下： public interface TransactionDefinition &#123; int getPropagationBehavior(); // 返回事务的传播行为 int getIsolationLevel(); // 返回事务的隔离级别，事务管理器根据它来控制另外一个事务可以看到本事务内的哪些数据 int getTimeout(); // 返回事务必须在多少秒内完成 boolean isReadOnly(); // 事务是否只读，事务管理器能够根据这个返回值进行优化，确保事务是只读的&#125; 我们可以发现 TransactionDefinition 正好用来定义事务属性，下面详细介绍一下各个事务属性。 传播行为 事务的第一个方面是传播行为（propagation behavior）。当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。Spring定义了七种传播行为： 传播行为 含义 PROPAGATION_REQUIRED 表示当前方法必须运行在事务中。如果当前事务存在，方法将会在该事务中运行。否则，会启动一个新的事务 PROPAGATION_SUPPORTS 表示当前方法不需要事务上下文，但是如果存在当前事务的话，那么该方法会在这个事务中运行 PROPAGATION_MANDATORY 表示该方法必须在事务中运行，如果当前事务不存在，则会抛出一个异常 PROPAGATION_REQUIRED_NEW 表示当前方法必须运行在它自己的事务中。一个新的事务将被启动。如果存在当前事务，在该方法执行期间，当前事务会被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager PROPAGATION_NOT_SUPPORTED 表示该方法不应该运行在事务中。如果存在当前事务，在该方法运行期间，当前事务将被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager PROPAGATION_NEVER 表示当前方法不应该运行在事务上下文中。如果当前正有一个事务在运行，则会抛出异常 PROPAGATION_NESTED 表示如果当前已经存在一个事务，那么该方法将会在嵌套事务中运行。嵌套的事务可以独立于当前事务进行单独地提交或回滚。如果当前事务不存在，那么其行为与PROPAGATION_REQUIRED一样。注意各厂商对这种传播行为的支持是有所差异的。可以参考资源管理器的文档来确认它们是否支持嵌套事务 注：以下具体讲解传播行为的内容参考自Spring事务机制详解 PROPAGATION_REQUIRED 如果存在一个事务，则支持当前事务。如果没有事务则开启一个新的事务。 //事务属性 PROPAGATION_REQUIREDmethodA&#123; …… methodB(); ……&#125; //事务属性 PROPAGATION_REQUIREDmethodB&#123; ……&#125; 使用spring声明式事务，spring使用AOP来支持声明式事务，会根据事务属性，自动在方法调用之前决定是否开启一个事务，并在方法执行之后决定事务提交或回滚事务。 单独调用methodB方法： main&#123; metodB(); &#125; 相当于 Main&#123; Connection con=null; try&#123; con = getConnection(); con.setAutoCommit(false); //方法调用 methodB(); //提交事务 con.commit(); &#125; Catch(RuntimeException ex) &#123; //回滚事务 con.rollback(); &#125; finally &#123; //释放资源 closeCon(); &#125; &#125; Spring保证在methodB方法中所有的调用都获得到一个相同的连接。在调用methodB时，没有一个存在的事务，所以获得一个新的连接，开启了一个新的事务。 单独调用MethodA时，在MethodA内又会调用MethodB. 执行效果相当于： main&#123; Connection con = null; try&#123; con = getConnection(); methodA(); con.commit(); &#125; catch(RuntimeException ex) &#123; con.rollback(); &#125; finally &#123; closeCon(); &#125; &#125; 调用MethodA时，环境中没有事务，所以开启一个新的事务.当在MethodA中调用MethodB时，环境中已经有了一个事务，所以methodB就加入当前事务。 PROPAGATION_SUPPORTS 如果存在一个事务，支持当前事务。如果没有事务，则非事务的执行。但是对于事务同步的事务管理器，PROPAGATION_SUPPORTS 与不使用事务有少许不同。 //事务属性 PROPAGATION_REQUIREDmethodA()&#123; methodB();&#125;//事务属性 PROPAGATION_SUPPORTSmethodB()&#123; ……&#125; 单纯的调用methodB时，methodB方法是非事务的执行的。当调用methdA时,methodB则加入了methodA的事务中,事务地执行。 PROPAGATION_MANDATORY 如果已经存在一个事务，支持当前事务。如果没有一个活动的事务，则抛出异常。 //事务属性 PROPAGATION_REQUIREDmethodA()&#123; methodB();&#125;//事务属性 PROPAGATION_MANDATORY methodB()&#123; ……&#125; 当单独调用methodB时，因为当前没有一个活动的事务，则会抛出异常throw new IllegalTransactionStateException(“Transaction propagation ‘mandatory’ but no existing transaction found”);当调用methodA时，methodB则加入到methodA的事务中，事务地执行。 PROPAGATION_REQUIRES_NEW 总是开启一个新的事务。如果一个事务已经存在，则将这个存在的事务挂起。 //事务属性 PROPAGATION_REQUIREDmethodA()&#123; doSomeThingA(); methodB(); doSomeThingB();&#125;//事务属性 PROPAGATION_REQUIRES_NEWmethodB()&#123; ……&#125; 调用A方法： main()&#123; methodA();&#125; 相当于 main()&#123; TransactionManager tm = null; try&#123; //获得一个JTA事务管理器 tm = getTransactionManager(); tm.begin();//开启一个新的事务 Transaction ts1 = tm.getTransaction(); doSomeThing(); tm.suspend();//挂起当前事务 try&#123; tm.begin();//重新开启第二个事务 Transaction ts2 = tm.getTransaction(); methodB(); ts2.commit();//提交第二个事务 &#125; Catch(RunTimeException ex) &#123; ts2.rollback();//回滚第二个事务 &#125; finally &#123; //释放资源 &#125; //methodB执行完后，恢复第一个事务 tm.resume(ts1); doSomeThingB(); ts1.commit();//提交第一个事务 &#125; catch(RunTimeException ex) &#123; ts1.rollback();//回滚第一个事务 &#125; finally &#123; //释放资源 &#125;&#125; 在这里，我把ts1称为外层事务，ts2称为内层事务。从上面的代码可以看出，ts2与ts1是两个独立的事务，互不相干。Ts2是否成功并不依赖于 ts1。如果methodA方法在调用methodB方法后的doSomeThingB方法失败了，而methodB方法所做的结果依然被提交。而除了 methodB之外的其它代码导致的结果却被回滚了。使用PROPAGATION_REQUIRES_NEW,需要使用 JtaTransactionManager作为事务管理器。 PROPAGATION_NOT_SUPPORTED 总是非事务地执行，并挂起任何存在的事务。使用PROPAGATION_NOT_SUPPORTED,也需要使用JtaTransactionManager作为事务管理器。（代码示例同上，可同理推出） PROPAGATION_NEVER 总是非事务地执行，如果存在一个活动事务，则抛出异常。 PROPAGATION_NESTED如果一个活动的事务存在，则运行在一个嵌套的事务中. 如果没有活动事务, 则按TransactionDefinition.PROPAGATION_REQUIRED 属性执行。这是一个嵌套事务,使用JDBC 3.0驱动时,仅仅支持DataSourceTransactionManager作为事务管理器。需要JDBC 驱动的java.sql.Savepoint类。有一些JTA的事务管理器实现可能也提供了同样的功能。使用PROPAGATION_NESTED，还需要把PlatformTransactionManager的nestedTransactionAllowed属性设为true;而 nestedTransactionAllowed属性值默认为false。 //事务属性 PROPAGATION_REQUIREDmethodA()&#123; doSomeThingA(); methodB(); doSomeThingB();&#125;//事务属性 PROPAGATION_NESTEDmethodB()&#123; ……&#125; 如果单独调用methodB方法，则按REQUIRED属性执行。如果调用methodA方法，相当于下面的效果： main()&#123; Connection con = null; Savepoint savepoint = null; try&#123; con = getConnection(); con.setAutoCommit(false); doSomeThingA(); savepoint = con2.setSavepoint(); try&#123; methodB(); &#125; catch(RuntimeException ex) &#123; con.rollback(savepoint); &#125; finally &#123; //释放资源 &#125; doSomeThingB(); con.commit(); &#125; catch(RuntimeException ex) &#123; con.rollback(); &#125; finally &#123; //释放资源 &#125;&#125; 当methodB方法调用之前，调用setSavepoint方法，保存当前的状态到savepoint。如果methodB方法调用失败，则恢复到之前保存的状态。但是需要注意的是，这时的事务并没有进行提交，如果后续的代码(doSomeThingB()方法)调用失败，则回滚包括methodB方法的所有操作。 嵌套事务一个非常重要的概念就是内层事务依赖于外层事务。外层事务失败时，会回滚内层事务所做的动作。而内层事务操作失败并不会引起外层事务的回滚。 PROPAGATION_NESTED 与PROPAGATION_REQUIRES_NEW的区别:它们非常类似,都像一个嵌套事务，如果不存在一个活动的事务，都会开启一个新的事务。使用 PROPAGATION_REQUIRES_NEW时，内层事务与外层事务就像两个独立的事务一样，一旦内层事务进行了提交后，外层事务不能对其进行回滚。两个事务互不影响。两个事务不是一个真正的嵌套事务。同时它需要JTA事务管理器的支持。 使用PROPAGATION_NESTED时，外层事务的回滚可以引起内层事务的回滚。而内层事务的异常并不会导致外层事务的回滚，它是一个真正的嵌套事务。DataSourceTransactionManager使用savepoint支持PROPAGATION_NESTED时，需要JDBC 3.0以上驱动及1.4以上的JDK版本支持。其它的JTA TrasactionManager实现可能有不同的支持方式。 PROPAGATION_REQUIRES_NEW 启动一个新的, 不依赖于环境的 “内部” 事务. 这个事务将被完全 commited 或 rolled back 而不依赖于外部事务, 它拥有自己的隔离范围, 自己的锁, 等等. 当内部事务开始执行时, 外部事务将被挂起, 内务事务结束时, 外部事务将继续执行。 另一方面, PROPAGATION_NESTED 开始一个 “嵌套的” 事务, 它是已经存在事务的一个真正的子事务. 潜套事务开始执行时, 它将取得一个 savepoint. 如果这个嵌套事务失败, 我们将回滚到此 savepoint. 潜套事务是外部事务的一部分, 只有外部事务结束后它才会被提交。 由此可见, PROPAGATION_REQUIRES_NEW 和 PROPAGATION_NESTED 的最大区别在于, PROPAGATION_REQUIRES_NEW 完全是一个新的事务, 而 PROPAGATION_NESTED 则是外部事务的子事务, 如果外部事务 commit, 嵌套事务也会被 commit, 这个规则同样适用于 roll back. PROPAGATION_REQUIRED应该是我们首先的事务传播行为。它能够满足我们大多数的事务需求。 隔离级别 事务的第二个维度就是隔离级别（isolation level）。隔离级别定义了一个事务可能受其他并发事务影响的程度。 并发事务引起的问题 在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务。并发虽然是必须的，但可能会导致一下的问题。 脏读（Dirty reads）——脏读发生在一个事务读取了另一个事务改写但尚未提交的数据时。如果改写在稍后被回滚了，那么第一个事务获取的数据就是无效的。 不可重复读（Nonrepeatable read）——不可重复读发生在一个事务执行相同的查询两次或两次以上，但是每次都得到不同的数据时。这通常是因为另一个并发事务在两次查询期间进行了更新。 幻读（Phantom read）——幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录。 不可重复读与幻读的区别 不可重复读的重点是修改: 同样的条件, 你读取过的数据, 再次读取出来发现值不一样了 例如：在事务1中，Mary 读取了自己的工资为1000,操作并没有完成 con1 = getConnection(); select salary from employee empId ="Mary"; 在事务2中，这时财务人员修改了Mary的工资为2000,并提交了事务. con2 = getConnection(); update employee set salary = 2000; con2.commit(); 在事务1中，Mary 再次读取自己的工资时，工资变为了2000 //con1 select salary from employee empId ="Mary"; 在一个事务中前后两次读取的结果并不一致，导致了不可重复读。 幻读的重点在于新增或者删除： 同样的条件, 第1次和第2次读出来的记录数不一样 例如：目前工资为1000的员工有10人。事务1,读取所有工资为1000的员工。 con1 = getConnection(); Select * from employee where salary =1000; 共读取10条记录 这时另一个事务向employee表插入了一条员工记录，工资也为1000 con2 = getConnection(); Insert into employee(empId,salary) values("Lili",1000); con2.commit(); 事务1再次读取所有工资为1000的员工 //con1 select * from employee where salary =1000; 共读取到了11条记录，这就产生了幻像读。 从总的结果来看, 似乎不可重复读和幻读都表现为两次读取的结果不一致。但如果你从控制的角度来看, 两者的区别就比较大。 对于前者, 只需要锁住满足条件的记录。 对于后者, 要锁住满足条件及其相近的记录。 隔离级别 隔离级别 含义 ISOLATION_DEFAULT 使用后端数据库默认的隔离级别 ISOLATION_READ_UNCOMMITTED 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 ISOLATION_READ_COMMITTED 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 ISOLATION_REPEATABLE_READ 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生 ISOLATION_SERIALIZABLE 最高的隔离级别，完全服从ACID的隔离级别，确保阻止脏读、不可重复读以及幻读，也是最慢的事务隔离级别，因为它通常是通过完全锁定事务相关的数据库表来实现的 只读 事务的第三个特性是它是否为只读事务。如果事务只对后端的数据库进行该操作，数据库可以利用事务的只读特性来进行一些特定的优化。通过将事务设置为只读，你就可以给数据库一个机会，让它应用它认为合适的优化措施。 事务超时 为了使应用程序很好地运行，事务不能运行太长的时间。因为事务可能涉及对后端数据库的锁定，所以长时间的事务会不必要的占用数据库资源。事务超时就是事务的一个定时器，在特定时间内事务如果没有执行完毕，那么就会自动回滚，而不是一直等待其结束。 回滚规则 事务五边形的最后一个方面是一组规则，这些规则定义了哪些异常会导致事务回滚而哪些不会。默认情况下，事务只有遇到运行期异常时才会回滚，而在遇到检查型异常时不会回滚（这一行为与EJB的回滚行为是一致的） 但是你可以声明事务在遇到特定的检查型异常时像遇到运行期异常那样回滚。同样，你还可以声明事务遇到特定的异常不回滚，即使这些异常是运行期异常。 TransactionStatus 上面讲到的调用 PlatformTransactionManager 接口的 getTransaction() 的方法得到的是 TransactionStatus 接口的一个实现，这个接口的内容如下： public interface TransactionStatus extends SavepointManager &#123; boolean isNewTransaction(); // 是否是新的事物 boolean hasSavepoint(); // 是否有恢复点 void setRollbackOnly(); // 设置为只回滚 boolean isRollbackOnly(); // 是否为只回滚 boolean isCompleted; // 是否已完成&#125; 可以发现这个接口描述的是一些处理事务提供简单的控制事务执行和查询事务状态的方法，在回滚或提交的时候需要应用对应的事务状态。 编程式事务 编程式和声明式事务的区别 Spring 提供了对编程式事务和声明式事务的支持。编程式事务允许用户在代码中精确定义事务的边界，而声明式事务（基于AOP）有助于用户将操作与事务规则进行解耦。 简单地说，编程式事务侵入到了业务代码里面，但是提供了更加详细的事务管理；而声明式事务由于基于 AOP，所以既能起到事务管理的作用，又可以不影响业务代码的具体实现。 Spring 提供两种方式的编程式事务管理，分别是：使用 TransactionTemplate 和使用 PlatformTransactionManager。 使用TransactionTemplate 采用TransactionTemplate和采用其他Spring模板，如JdbcTempalte和HibernateTemplate是一样的方法。它使用回调方法，把应用程序从处理取得和释放资源中解脱出来。如同其他模板，TransactionTemplate是线程安全的。代码片段： TransactionTemplate tt = new TransactionTemplate(); // 新建一个TransactionTemplateObject result = tt.execute( new TransactionCallback()&#123; public Object doTransaction(TransactionStatus status)&#123; updateOperation(); return resultOfUpdateOperation(); &#125; &#125;); // 执行execute方法进行事务管理 使用TransactionCallback()可以返回一个值。如果使用TransactionCallbackWithoutResult则没有返回值。 使用PlatformTransactionManager 示例代码如下： DataSourceTransactionManager dataSourceTransactionManager = new DataSourceTransactionManager(); //定义一个某个框架平台的TransactionManager，如JDBC、HibernatedataSourceTransactionManager.setDataSource(this.getJdbcTemplate().getDataSource()); // 设置数据源DefaultTransactionDefinition transDef = new DefaultTransactionDefinition(); // 定义事务属性transDef.setPropagationBehavior(DefaultTransactionDefinition.PROPAGATION_REQUIRED); // 设置传播行为属性TransactionStatus status = dataSourceTransactionManager.getTransaction(transDef); // 获得事务状态try &#123; // 数据库操作 dataSourceTransactionManager.commit(status);// 提交&#125; catch (Exception e) &#123; dataSourceTransactionManager.rollback(status);// 回滚&#125; 声明式事务 配置方式 注：以下配置代码参考自Spring事务配置的五种方式 根据代理机制的不同，总结了五种Spring事务的配置方式，配置文件如下： 每个Bean都有一个代理 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd"&gt; &lt;bean id="sessionFactory" class="org.springframework.orm.hibernate3.LocalSessionFactoryBean"&gt; &lt;property name="configLocation" value="classpath:hibernate.cfg.xml" /&gt; &lt;property name="configurationClass" value="org.hibernate.cfg.AnnotationConfiguration" /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id="transactionManager" class="org.springframework.orm.hibernate3.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt; &lt;!-- 配置DAO --&gt; &lt;bean id="userDaoTarget" class="com.bluesky.spring.dao.UserDaoImpl"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt; &lt;bean id="userDao" class="org.springframework.transaction.interceptor.TransactionProxyFactoryBean"&gt; &lt;!-- 配置事务管理器 --&gt; &lt;property name="transactionManager" ref="transactionManager" /&gt; &lt;property name="target" ref="userDaoTarget" /&gt; &lt;property name="proxyInterfaces" value="com.bluesky.spring.dao.GeneratorDao" /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name="transactionAttributes"&gt; &lt;props&gt; &lt;prop key="*"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; 所有Bean共享一个代理基类 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd"&gt; &lt;bean id="sessionFactory" class="org.springframework.orm.hibernate3.LocalSessionFactoryBean"&gt; &lt;property name="configLocation" value="classpath:hibernate.cfg.xml" /&gt; &lt;property name="configurationClass" value="org.hibernate.cfg.AnnotationConfiguration" /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id="transactionManager" class="org.springframework.orm.hibernate3.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt; &lt;bean id="transactionBase" class="org.springframework.transaction.interceptor.TransactionProxyFactoryBean" lazy-init="true" abstract="true"&gt; &lt;!-- 配置事务管理器 --&gt; &lt;property name="transactionManager" ref="transactionManager" /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name="transactionAttributes"&gt; &lt;props&gt; &lt;prop key="*"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置DAO --&gt; &lt;bean id="userDaoTarget" class="com.bluesky.spring.dao.UserDaoImpl"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt; &lt;bean id="userDao" parent="transactionBase" &gt; &lt;property name="target" ref="userDaoTarget" /&gt; &lt;/bean&gt;&lt;/beans&gt; 使用拦截器 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd"&gt; &lt;bean id="sessionFactory" class="org.springframework.orm.hibernate3.LocalSessionFactoryBean"&gt; &lt;property name="configLocation" value="classpath:hibernate.cfg.xml" /&gt; &lt;property name="configurationClass" value="org.hibernate.cfg.AnnotationConfiguration" /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id="transactionManager" class="org.springframework.orm.hibernate3.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt; &lt;bean id="transactionInterceptor" class="org.springframework.transaction.interceptor.TransactionInterceptor"&gt; &lt;property name="transactionManager" ref="transactionManager" /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name="transactionAttributes"&gt; &lt;props&gt; &lt;prop key="*"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean class="org.springframework.aop.framework.autoproxy.BeanNameAutoProxyCreator"&gt; &lt;property name="beanNames"&gt; &lt;list&gt; &lt;value&gt;*Dao&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="interceptorNames"&gt; &lt;list&gt; &lt;value&gt;transactionInterceptor&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置DAO --&gt; &lt;bean id="userDao" class="com.bluesky.spring.dao.UserDaoImpl"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt;&lt;/beans&gt; 使用tx标签配置的拦截器 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.5.xsd"&gt; &lt;context:annotation-config /&gt; &lt;context:component-scan base-package="com.bluesky" /&gt; &lt;bean id="sessionFactory" class="org.springframework.orm.hibernate3.LocalSessionFactoryBean"&gt; &lt;property name="configLocation" value="classpath:hibernate.cfg.xml" /&gt; &lt;property name="configurationClass" value="org.hibernate.cfg.AnnotationConfiguration" /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id="transactionManager" class="org.springframework.orm.hibernate3.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt; &lt;tx:advice id="txAdvice" transaction-manager="transactionManager"&gt; &lt;tx:attributes&gt; &lt;tx:method name="*" propagation="REQUIRED" /&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;aop:config&gt; &lt;aop:pointcut id="interceptorPointCuts" expression="execution(* com.bluesky.spring.dao.*.*(..))" /&gt; &lt;aop:advisor advice-ref="txAdvice" pointcut-ref="interceptorPointCuts" /&gt; &lt;/aop:config&gt; &lt;/beans&gt; 全注解 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.5.xsd"&gt; &lt;context:annotation-config /&gt; &lt;context:component-scan base-package="com.bluesky" /&gt; &lt;tx:annotation-driven transaction-manager="transactionManager"/&gt; &lt;bean id="sessionFactory" class="org.springframework.orm.hibernate3.LocalSessionFactoryBean"&gt; &lt;property name="configLocation" value="classpath:hibernate.cfg.xml" /&gt; &lt;property name="configurationClass" value="org.hibernate.cfg.AnnotationConfiguration" /&gt; &lt;/bean&gt; &lt;!-- 定义事务管理器（声明式的事务） --&gt; &lt;bean id="transactionManager" class="org.springframework.orm.hibernate3.HibernateTransactionManager"&gt; &lt;property name="sessionFactory" ref="sessionFactory" /&gt; &lt;/bean&gt;&lt;/beans&gt; 此时在DAO上需加上@Transactional注解，如下： package com.bluesky.spring.dao;import java.util.List;import org.hibernate.SessionFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.orm.hibernate3.support.HibernateDaoSupport;import org.springframework.stereotype.Component;import com.bluesky.spring.domain.User;@Transactional@Component("userDao")public class UserDaoImpl extends HibernateDaoSupport implements UserDao &#123; public List&lt;User&gt; listUsers() &#123; return this.getSession().createQuery("from User").list(); &#125; &#125; 一个声明式事务的实例 注：该实例参考自Spring中的事务管理实例详解 首先是数据库表 book(isbn, book_name, price) account(username, balance) book_stock(isbn, stock) 然后是XML配置 &lt;beans xmlns="http://www.springframework.org/schema/beans"xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"xmlns:context="http://www.springframework.org/schema/context"xmlns:aop="http://www.springframework.org/schema/aop"xmlns:tx="http://www.springframework.org/schema/tx"xsi:schemaLocation="http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-3.0.xsdhttp://www.springframework.org/schema/contexthttp://www.springframework.org/schema/context/spring-context-3.0.xsdhttp://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.5.xsdhttp://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.5.xsd"&gt; &lt;import resource="applicationContext-db.xml" /&gt; &lt;context:component-scan base-package="com.springinaction.transaction"&gt; &lt;/context:component-scan&gt; &lt;tx:annotation-driven transaction-manager="txManager"/&gt; &lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;/bean&gt;&lt;/beans&gt; 使用的类 BookShopDao package com.springinaction.transaction;public interface BookShopDao &#123; // 根据书号获取书的单价 public int findBookPriceByIsbn(String isbn); // 更新书的库存，使书号对应的库存-1 public void updateBookStock(String isbn); // 更新用户的账户余额：account的balance-price public void updateUserAccount(String username, int price);&#125; BookShopDaoImpl package com.springinaction.transaction;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.stereotype.Repository;@Repository("bookShopDao")public class BookShopDaoImpl implements BookShopDao &#123; @Autowired private JdbcTemplate JdbcTemplate; @Override public int findBookPriceByIsbn(String isbn) &#123; String sql = "SELECT price FROM book WHERE isbn = ?"; return JdbcTemplate.queryForObject(sql, Integer.class, isbn); &#125; @Override public void updateBookStock(String isbn) &#123; //检查书的库存是否足够，若不够，则抛出异常 String sql2 = "SELECT stock FROM book_stock WHERE isbn = ?"; int stock = JdbcTemplate.queryForObject(sql2, Integer.class, isbn); if (stock == 0) &#123; throw new BookStockException("库存不足！"); &#125; String sql = "UPDATE book_stock SET stock = stock - 1 WHERE isbn = ?"; JdbcTemplate.update(sql, isbn); &#125; @Override public void updateUserAccount(String username, int price) &#123; //检查余额是否不足，若不足，则抛出异常 String sql2 = "SELECT balance FROM account WHERE username = ?"; int balance = JdbcTemplate.queryForObject(sql2, Integer.class, username); if (balance &lt; price) &#123; throw new UserAccountException("余额不足！"); &#125; String sql = "UPDATE account SET balance = balance - ? WHERE username = ?"; JdbcTemplate.update(sql, price, username); &#125;&#125; BookShopService package com.springinaction.transaction;public interface BookShopService &#123; public void purchase(String username, String isbn);&#125; BookShopServiceImpl package com.springinaction.transaction;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Isolation;import org.springframework.transaction.annotation.Propagation;import org.springframework.transaction.annotation.Transactional;@Service("bookShopService")public class BookShopServiceImpl implements BookShopService &#123; @Autowired private BookShopDao bookShopDao; /** * 1.添加事务注解 * 使用propagation 指定事务的传播行为，即当前的事务方法被另外一个事务方法调用时如何使用事务。 * 默认取值为REQUIRED，即使用调用方法的事务 * REQUIRES_NEW：使用自己的事务，调用的事务方法的事务被挂起。 * * 2.使用isolation 指定事务的隔离级别，最常用的取值为READ_COMMITTED * 3.默认情况下 Spring 的声明式事务对所有的运行时异常进行回滚，也可以通过对应的属性进行设置。通常情况下，默认值即可。 * 4.使用readOnly 指定事务是否为只读。 表示这个事务只读取数据但不更新数据，这样可以帮助数据库引擎优化事务。若真的是一个只读取数据库值得方法，应设置readOnly=true * 5.使用timeOut 指定强制回滚之前事务可以占用的时间。 */ @Transactional(propagation=Propagation.REQUIRES_NEW, isolation=Isolation.READ_COMMITTED, noRollbackFor=&#123;UserAccountException.class&#125;, readOnly=true, timeout=3) @Override public void purchase(String username, String isbn) &#123; //1.获取书的单价 int price = bookShopDao.findBookPriceByIsbn(isbn); //2.更新书的库存 bookShopDao.updateBookStock(isbn); //3.更新用户余额 bookShopDao.updateUserAccount(username, price); &#125;&#125; Cashier package com.springinaction.transaction;import java.util.List;public interface Cashier &#123; public void checkout(String username, List&lt;String&gt;isbns);&#125; CashierImpl：CashierImpl.checkout和bookShopService.purchase联合测试了事务的传播行为 package com.springinaction.transaction;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;@Service("cashier")public class CashierImpl implements Cashier &#123; @Autowired private BookShopService bookShopService; @Transactional @Override public void checkout(String username, List&lt;String&gt; isbns) &#123; for(String isbn : isbns) &#123; bookShopService.purchase(username, isbn); &#125; &#125;&#125; BookStockException package com.springinaction.transaction;public class BookStockException extends RuntimeException &#123; private static final long serialVersionUID = 1L; public BookStockException() &#123; super(); // TODO Auto-generated constructor stub &#125; public BookStockException(String arg0, Throwable arg1, boolean arg2, boolean arg3) &#123; super(arg0, arg1, arg2, arg3); // TODO Auto-generated constructor stub &#125; public BookStockException(String arg0, Throwable arg1) &#123; super(arg0, arg1); // TODO Auto-generated constructor stub &#125; public BookStockException(String arg0) &#123; super(arg0); // TODO Auto-generated constructor stub &#125; public BookStockException(Throwable arg0) &#123; super(arg0); // TODO Auto-generated constructor stub &#125;&#125; UserAccountException package com.springinaction.transaction;public class UserAccountException extends RuntimeException &#123; private static final long serialVersionUID = 1L; public UserAccountException() &#123; super(); // TODO Auto-generated constructor stub &#125; public UserAccountException(String arg0, Throwable arg1, boolean arg2, boolean arg3) &#123; super(arg0, arg1, arg2, arg3); // TODO Auto-generated constructor stub &#125; public UserAccountException(String arg0, Throwable arg1) &#123; super(arg0, arg1); // TODO Auto-generated constructor stub &#125; public UserAccountException(String arg0) &#123; super(arg0); // TODO Auto-generated constructor stub &#125; public UserAccountException(Throwable arg0) &#123; super(arg0); // TODO Auto-generated constructor stub &#125;&#125; 测试类 package com.springinaction.transaction;import java.util.Arrays;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class SpringTransitionTest &#123; private ApplicationContext ctx = null; private BookShopDao bookShopDao = null; private BookShopService bookShopService = null; private Cashier cashier = null; &#123; ctx = new ClassPathXmlApplicationContext("config/transaction.xml"); bookShopDao = ctx.getBean(BookShopDao.class); bookShopService = ctx.getBean(BookShopService.class); cashier = ctx.getBean(Cashier.class); &#125; @Test public void testBookShopDaoFindPriceByIsbn() &#123; System.out.println(bookShopDao.findBookPriceByIsbn("1001")); &#125; @Test public void testBookShopDaoUpdateBookStock()&#123; bookShopDao.updateBookStock("1001"); &#125; @Test public void testBookShopDaoUpdateUserAccount()&#123; bookShopDao.updateUserAccount("AA", 100); &#125; @Test public void testBookShopService()&#123; bookShopService.purchase("AA", "1001"); &#125; @Test public void testTransactionPropagation()&#123; cashier.checkout("AA", Arrays.asList("1001", "1002")); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fbasics%2Fdocker-dockerfile%2F</url>
    <content type="text"><![CDATA[Dockerfile Dockerfile 指令 FROM(指定基础镜像) RUN(执行命令) COPY(复制文件) ADD(更高级的复制文件) CMD(容器启动命令) ENTRYPOINT(入口点) ENV(设置环境变量) ARG(构建参数) VOLUME(定义匿名卷) EXPOSE(暴露端口) WORKDIR(指定工作目录) USER(指定当前用户) HEALTHCHECK(健康检查) ONBUILD(为他人作嫁衣裳) 引用和引申 Dockerfile 指令 FROM(指定基础镜像) 作用： FROM 指令用于指定基础镜像。 所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。就像我们之前运行了一个 nginx 镜像的容器，再进行修改一样，基础镜像是必须指定的。而 FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 在 Docker Store 上有非常多的高质量的官方镜像，有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat 等；也有一些方便开发、构建、运行各种语言应用的镜像，如 node、openjdk、python、ruby、golang 等。可以在其中寻找一个最符合我们最终目标的镜像为基础镜像进行定制。 如果没有找到对应服务的镜像，官方镜像中还提供了一些更为基础的操作系统镜像，如 ubuntu、debian、centos、fedora、alpine 等，这些操作系统的软件库为我们提供了更广阔的扩展空间。 除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。 FROM scratch... 如果你以 scratch 为基础镜像的话，意味着你不以任何镜像为基础，接下来所写的指令将作为镜像第一层开始存在。 不以任何系统为基础，直接将可执行文件复制进镜像的做法并不罕见，比如 swarm、coreos/etcd。对于 Linux 下静态编译的程序来说，并不需要有操作系统提供运行时支持，所需的一切库都已经在可执行文件里了，因此直接 FROM scratch 会让镜像体积更加小巧。使用 Go 语言 开发的应用很多会使用这种方式来制作镜像，这也是为什么有人认为 Go 是特别适合容器微服务架构的语言的原因之一。 RUN(执行命令) RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一。其格式有两种： shell 格式：RUN &lt;命令&gt;，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 RUN 指令就是这种格式。 RUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html exec 格式：RUN [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;]，这更像是函数调用中的格式。 既然 RUN 就像 Shell 脚本一样可以执行命令，那么我们是否就可以像 Shell 脚本一样把每个命令对应一个 RUN 呢？比如这样： FROM debian:jessieRUN apt-get updateRUN apt-get install -y gcc libc6-dev makeRUN wget -O redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz"RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install 之前说过，Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。 而上面的这种写法，创建了 7 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误。 Union FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 上面的 Dockerfile 正确的写法应该是这样： FROM debian:jessieRUN buildDeps='gcc libc6-dev make' \ &amp;&amp; apt-get update \ &amp;&amp; apt-get install -y $buildDeps \ &amp;&amp; wget -O redis.tar.gz "http://download.redis.io/releases/redis-3.2.5.tar.gz" \ &amp;&amp; mkdir -p /usr/src/redis \ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \ &amp;&amp; make -C /usr/src/redis \ &amp;&amp; make -C /usr/src/redis install \ &amp;&amp; rm -rf /var/lib/apt/lists/* \ &amp;&amp; rm redis.tar.gz \ &amp;&amp; rm -r /usr/src/redis \ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 首先，之前所有的命令只有一个目的，就是编译、安装 redis 可执行文件。因此没有必要建立很多层，这只是一层的事情。因此，这里没有使用很多个 RUN 对一一对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 &amp;&amp; 将各个所需命令串联起来。将之前的 7 层，简化为了 1 层。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。 并且，这里为了格式化还进行了换行。Dockerfile 支持 Shell 类的行尾添加 \ 的命令换行方式，以及行首 # 进行注释的格式。良好的格式，比如换行、缩进、注释等，会让维护、排障更为容易，这是一个比较好的习惯。 此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 很多人初学 Docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。 COPY(复制文件) 作用： COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。 格式： COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径&gt;... &lt;目标路径&gt; COPY [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;源路径1&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;] 示例： COPY package.json /usr/src/app/ &lt;源路径&gt; 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如： COPY hom* /mydir/COPY hom?.txt /mydir/ &lt;目标路径&gt; 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。 此外，还需要注意一点，使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。 在使用该指令的时候还可以加上 --chown=&lt;user&gt;:&lt;group&gt; 选项来改变文件的所属用户及所属组。 COPY --chown=55:mygroup files* /mydir/COPY --chown=bin files* /mydir/COPY --chown=1 files* /mydir/COPY --chown=10:11 files* /mydir/ ADD(更高级的复制文件) 作用： ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。 比如 &lt;源路径&gt; 可以是一个 URL，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 &lt;目标路径&gt;去。下载后的文件权限自动设置为 600，如果这并不是想要的权限，那么还需要增加额外的一层 RUN 进行权限调整，另外，如果下载的是个压缩包，需要解压缩，也一样还需要额外的一层 RUN 指令进行解压缩。所以不如直接使用 RUN 指令，然后使用 wget 或者 curl 工具下载，处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并不实用，而且不推荐使用。 如果 &lt;源路径&gt; 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 &lt;目标路径&gt; 去。 在某些情况下，这个自动解压缩的功能非常有用，比如官方镜像 ubuntu 中： FROM scratchADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz /... 但在某些情况下，如果我们真的是希望复制个压缩文件进去，而不解压缩，这时就不可以使用 ADD 命令了。 在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。 另外需要注意的是，ADD 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。 因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。 在使用该指令的时候还可以加上 --chown=&lt;user&gt;:&lt;group&gt; 选项来改变文件的所属用户及所属组。 ADD --chown=55:mygroup files* /mydir/ADD --chown=bin files* /mydir/ADD --chown=1 files* /mydir/ADD --chown=10:11 files* /mydir/ CMD(容器启动命令) 作用： 之前介绍容器的时候曾经说过，Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。CMD 指令就是用于指定默认的容器主进程的启动命令的。 CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式：CMD &lt;命令&gt; exec 格式：CMD [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;...] 参数列表格式：CMD [&quot;参数1&quot;, &quot;参数2&quot;...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如，ubuntu 镜像默认的 CMD 是 /bin/bash，如果我们直接 docker run -it ubuntu 的话，会直接进入 bash。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release。这就是用 cat /etc/os-release 命令替换了默认的 /bin/bash 命令了，输出了系统版本信息。 在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 &quot;，而不要使用单引号。 如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如： CMD echo $HOME 在实际执行中，会将其变更为： CMD [ "sh", "-c", "echo $HOME" ] 这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。 提到 CMD 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。 Docker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 upstart/systemd 去启动后台服务，容器内没有后台服务的概念。 一些初学者将 CMD 写为： CMD service nginx start 然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。 对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。 而使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;service nginx start&quot;]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。 正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如： CMD ["nginx", "-g", "daemon off;"] ENTRYPOINT(入口点) ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 --entrypoint 来指定。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为： &lt;ENTRYPOINT&gt; "&lt;CMD&gt;" 那么有了 CMD 后，为什么还要有 ENTRYPOINT 呢？这种 &lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot; 有什么好处么？让我们来看几个场景。 场景一：让镜像变成像命令一样使用 假设我们需要一个得知自己当前公网 IP 的镜像，那么可以先用 CMD 来实现： FROM ubuntu:18.04RUN apt-get update \ &amp;&amp; apt-get install -y curl \ &amp;&amp; rm -rf /var/lib/apt/lists/*CMD [ "curl", "-s", "https://ip.cn" ] 假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 IP，只需要执行： $ docker run myip当前 IP：61.148.226.66 来自：北京市 联通 嗯，这么看起来好像可以直接把镜像当做命令使用了，不过命令总有参数，如果我们希望加参数呢？比如从上面的 CMD 中可以看到实质的命令是 curl，那么如果我们希望显示 HTTP 头信息，就需要加上 -i 参数。那么我们可以直接加 -i 参数给 docker run myip 么？ $ docker run myip -idocker: Error response from daemon: invalid header field value "oci runtime error: container_linux.go:247: starting container process caused \"exec: \\\"-i\\\": executable file not found in $PATH\"\n". 我们可以看到可执行文件找不到的报错，executable file not found。之前我们说过，跟在镜像名后面的是 command，运行时会替换 CMD 的默认值。因此这里的 -i 替换了原来的 CMD，而不是添加在原来的 curl -s https://ip.cn 后面。而 -i 根本不是命令，所以自然找不到。 那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令： $ docker run myip curl -s https://ip.cn -i 这显然不是很好的解决方案，而使用 ENTRYPOINT 就可以解决这个问题。现在我们重新用 ENTRYPOINT 来实现这个镜像： FROM ubuntu:18.04RUN apt-get update \ &amp;&amp; apt-get install -y curl \ &amp;&amp; rm -rf /var/lib/apt/lists/*ENTRYPOINT [ "curl", "-s", "https://ip.cn" ] 这次我们再来尝试直接使用 docker run myip -i： $ docker run myip当前 IP：61.148.226.66 来自：北京市 联通$ docker run myip -iHTTP/1.1 200 OKServer: nginx/1.8.0Date: Tue, 22 Nov 2016 05:12:40 GMTContent-Type: text/html; charset=UTF-8Vary: Accept-EncodingX-Powered-By: PHP/5.6.24-1~dotdeb+7.1X-Cache: MISS from cache-2X-Cache-Lookup: MISS from cache-2:80X-Cache: MISS from proxy-2_6Transfer-Encoding: chunkedVia: 1.1 cache-2:80, 1.1 proxy-2_6:8006Connection: keep-alive当前 IP：61.148.226.66 来自：北京市 联通 可以看到，这次成功了。这是因为当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给 ENTRYPOINT，而这里 -i 就是新的 CMD，因此会作为参数传给 curl，从而达到了我们预期的效果。 场景二：应用运行前的准备工作 启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。 比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。 此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。 这些准备工作是和容器 CMD 无关的，无论 CMD 为什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 &lt;CMD&gt;）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的： FROM alpine:3.4...RUN addgroup -S redis &amp;&amp; adduser -S -G redis redis...ENTRYPOINT ["docker-entrypoint.sh"]EXPOSE 6379CMD [ "redis-server" ] 可以看到其中为了 redis 服务创建了 redis 用户，并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本。 #!/bin/sh...# allow the container to be started with `--user`if [ "$1" = 'redis-server' -a "$(id -u)" = '0' ]; then chown -R redis . exec su-exec redis "$0" "$@"fiexec "$@" 该脚本的内容就是根据 CMD 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如： $ docker run -it redis iduid=0(root) gid=0(root) groups=0(root) ENV(设置环境变量) 作用：ENV 指令用于设置环境变量。无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。 格式： ENV &lt;key&gt; &lt;value&gt; ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... 示例 1： ENV VERSION=1.0 DEBUG=on \ NAME="Happy Feet" 这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 Shell 下的行为是一致的。 示例 2： 定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。比如在官方 node 镜像 Dockerfile 中，就有类似这样的代码： ENV NODE_VERSION 7.2.0RUN curl -SLO "https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz" \ &amp;&amp; curl -SLO "https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc" \ &amp;&amp; gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \ &amp;&amp; grep " node-v$NODE_VERSION-linux-x64.tar.xz\$" SHASUMS256.txt | sha256sum -c - \ &amp;&amp; tar -xJf "node-v$NODE_VERSION-linux-x64.tar.xz" -C /usr/local --strip-components=1 \ &amp;&amp; rm "node-v$NODE_VERSION-linux-x64.tar.xz" SHASUMS256.txt.asc SHASUMS256.txt \ &amp;&amp; ln -s /usr/local/bin/node /usr/local/bin/nodejs 在这里先定义了环境变量 NODE_VERSION，其后的 RUN 这层里，多次使用 $NODE_VERSION 来进行操作定制。可以看到，将来升级镜像构建版本的时候，只需要更新 7.2.0 即可，Dockerfile 构建维护变得更轻松了。 下列指令可以支持环境变量展开： ADD、COPY、ENV、EXPOSE、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD。 可以从这个指令列表里感觉到，环境变量可以使用的地方很多，很强大。通过环境变量，我们可以让一份 Dockerfile 制作更多的镜像，只需使用不同的环境变量即可。 ARG(构建参数) 作用： Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖。 构建参数和 ENV 的效果一样，都是设置环境变量。所不同的是，ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。 格式：ARG &lt;参数名&gt;[=&lt;默认值&gt;] 在 1.13 之前的版本，要求 --build-arg 中的参数名，必须在 Dockerfile 中用 ARG 定义过了，换句话说，就是 --build-arg 指定的参数，必须在 Dockerfile 中使用了。如果对应参数没有被使用，则会报错退出构建。从 1.13 开始，这种严格的限制被放开，不再报错退出，而是显示警告信息，并继续构建。这对于使用 CI 系统，用同样的构建流程构建不同的 Dockerfile 的时候比较有帮助，避免构建命令必须根据每个 Dockerfile 的内容修改。 VOLUME(定义匿名卷) 格式： VOLUME [&quot;&lt;路径1&gt;&quot;, &quot;&lt;路径2&gt;&quot;...] VOLUME &lt;路径&gt; 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中，后面的章节我们会进一步介绍 Docker 卷的概念。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。 VOLUME /data 这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如： docker run -d -v mydata:/data xxxx 在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。 EXPOSE(暴露端口) 作用： EXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 要将 EXPOSE 和在运行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 格式：EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...]。 WORKDIR(指定工作目录) 作用： 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。 格式：WORKDIR &lt;工作目录路径&gt;。 示例 1： 之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误： RUN cd /appRUN echo "hello" &gt; world.txt 如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单，在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。 之前说过每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 RUN cd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。 因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。 USER(指定当前用户) 作用： USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。 当然，和 WORKDIR 一样，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。 格式：USER &lt;用户名&gt;[:&lt;用户组&gt;] 示例 1： RUN groupadd -r redis &amp;&amp; useradd -r -g redis redisUSER redisRUN [ "redis-server" ] 如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su或者 sudo，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu。 # 建立 redis 用户，并使用 gosu 换另一个用户执行命令RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis# 下载 gosuRUN wget -O /usr/local/bin/gosu "https://github.com/tianon/gosu/releases/download/1.7/gosu-amd64" \ &amp;&amp; chmod +x /usr/local/bin/gosu \ &amp;&amp; gosu nobody true# 设置 CMD，并以另外的用户执行CMD [ "exec", "gosu", "redis", "redis-server" ] HEALTHCHECK(健康检查) 格式： HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令 HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令 HEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常，这是 Docker 1.12 引入的新指令。 在没有 HEALTHCHECK 指令前，Docker 引擎只可以通过容器内主进程是否退出来判断容器是否状态异常。很多情况下这没问题，但是如果程序进入死锁状态，或者死循环状态，应用进程并不退出，但是该容器已经无法提供服务了。在 1.12 以前，Docker 不会检测到容器的这种状态，从而不会重新调度，导致可能会有部分容器已经无法提供服务了却还在接受用户请求。 而自 1.12 之后，Docker 提供了 HEALTHCHECK 指令，通过该指令指定一行命令，用这行命令来判断容器主进程的服务状态是否还正常，从而比较真实的反应容器实际状态。 当在一个镜像指定了 HEALTHCHECK 指令后，用其启动容器，初始状态会为 starting，在 HEALTHCHECK 指令检查成功后变为 healthy，如果连续一定次数失败，则会变为 unhealthy。 HEALTHCHECK 支持下列选项： --interval=&lt;间隔&gt;：两次健康检查的间隔，默认为 30 秒； --timeout=&lt;时长&gt;：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒； --retries=&lt;次数&gt;：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。 和 CMD, ENTRYPOINT 一样，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。 在 HEALTHCHECK [选项] CMD 后面的命令，格式和 ENTRYPOINT 一样，分为 shell 格式，和 exec 格式。命令的返回值决定了该次健康检查的成功与否：0：成功；1：失败；2：保留，不要使用这个值。 假设我们有个镜像是个最简单的 Web 服务，我们希望增加健康检查来判断其 Web 服务是否在正常工作，我们可以用 curl 来帮助判断，其 Dockerfile 的 HEALTHCHECK 可以这么写： FROM nginxRUN apt-get update &amp;&amp; apt-get install -y curl &amp;&amp; rm -rf /var/lib/apt/lists/*HEALTHCHECK --interval=5s --timeout=3s \ CMD curl -fs http://localhost/ || exit 1 这里我们设置了每 5 秒检查一次（这里为了试验所以间隔非常短，实际应该相对较长），如果健康检查命令超过 3 秒没响应就视为失败，并且使用 curl -fs http://localhost/ || exit 1 作为健康检查命令。 使用 docker build 来构建这个镜像： $ docker build -t myweb:v1 . 构建好了后，我们启动一个容器： $ docker run -d --name web -p 80:80 myweb:v1 当运行该镜像后，可以通过 docker container ls 看到最初的状态为 (health: starting)： $ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES03e28eb00bd0 myweb:v1 "nginx -g 'daemon off" 3 seconds ago Up 2 seconds (health: starting) 80/tcp, 443/tcp web 在等待几秒钟后，再次 docker container ls，就会看到健康状态变化为了 (healthy)： $ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES03e28eb00bd0 myweb:v1 "nginx -g 'daemon off" 18 seconds ago Up 16 seconds (healthy) 80/tcp, 443/tcp web 如果健康检查连续失败超过了重试次数，状态就会变为 (unhealthy)。 为了帮助排障，健康检查命令的输出（包括 stdout 以及 stderr）都会被存储于健康状态里，可以用 docker inspect 来查看。 $ docker inspect --format '&#123;&#123;json .State.Health&#125;&#125;' web | python -m json.tool&#123; "FailingStreak": 0, "Log": [ &#123; "End": "2016-11-25T14:35:37.940957051Z", "ExitCode": 0, "Output": "&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\n body &#123;\n width: 35em;\n margin: 0 auto;\n font-family: Tahoma, Verdana, Arial, sans-serif;\n &#125;\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n", "Start": "2016-11-25T14:35:37.780192565Z" &#125; ], "Status": "healthy"&#125; ONBUILD(为他人作嫁衣裳) 格式：ONBUILD &lt;其它指令&gt;。 ONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。 Dockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人定制自己而准备的。 假设我们要制作 Node.js 所写的应用的镜像。我们都知道 Node.js 使用 npm 进行包管理，所有依赖、配置、启动信息等会放到 package.json 文件里。在拿到程序代码后，需要先进行 npm install 才可以获得所有需要的依赖。然后就可以通过 npm start来启动应用。因此，一般来说会这样写 Dockerfile： FROM node:slimRUN mkdir /appWORKDIR /appCOPY ./package.json /appRUN [ "npm", "install" ]COPY . /app/CMD [ "npm", "start" ] 把这个 Dockerfile 放到 Node.js 项目的根目录，构建好镜像后，就可以直接拿来启动容器运行。但是如果我们还有第二个 Node.js 项目也差不多呢？好吧，那就再把这个 Dockerfile 复制到第二个项目里。那如果有第三个项目呢？再复制么？文件的副本越多，版本控制就越困难，让我们继续看这样的场景维护的问题。 如果第一个 Node.js 项目在开发过程中，发现这个 Dockerfile 里存在问题，比如敲错字了、或者需要安装额外的包，然后开发人员修复了这个 Dockerfile，再次构建，问题解决。� 第一个项目没问题了，但是第二个项目呢？虽然最初 Dockerfile 是复制、粘贴自第一个项目的，但是并不会因为第一个项目修复了他们的 Dockerfile，而第二个项目的 Dockerfile 就会被自动修复。 那么我们可不可以做一个基础镜像，然后各个项目使用这个基础镜像呢？这样基础镜像更新，各个项目不用同步 Dockerfile的变化，重新构建后就继承了基础镜像的更新？好吧，可以，让我们看看这样的结果。那么上面的这个 Dockerfile 就会变为： FROM node:slimRUN mkdir /appWORKDIR /appCMD [ "npm", "start" ] 这里我们把项目相关的构建指令拿出来，放到子项目里去。假设这个基础镜像的名字为 my-node 的话，各个项目内的自己的 Dockerfile 就变为： FROM my-nodeCOPY ./package.json /appRUN [ "npm", "install" ]COPY . /app/ 基础镜像变化后，各个项目都用这个 Dockerfile 重新构建镜像，会继承基础镜像的更新。 那么，问题解决了么？没有。准确说，只解决了一半。如果这个 Dockerfile 里面有些东西需要调整呢？比如 npm install 都需要加一些参数，那怎么办？这一行 RUN 是不可能放入基础镜像的，因为涉及到了当前项目的 ./package.json，难道又要一个个修改么？所以说，这样制作基础镜像，只解决了原来的 Dockerfile 的前 4 条指令的变化问题，而后面三条指令的变化则完全没办法处理。 ONBUILD 可以解决这个问题。让我们用 ONBUILD 重新写一下基础镜像的 Dockerfile: FROM node:slimRUN mkdir /appWORKDIR /appONBUILD COPY ./package.json /appONBUILD RUN [ "npm", "install" ]ONBUILD COPY . /app/CMD [ "npm", "start" ] 这次我们回到原始的 Dockerfile，但是这次将项目相关的指令加上 ONBUILD，这样在构建基础镜像的时候，这三行并不会被执行。然后各个项目的 Dockerfile 就变成了简单地： FROM my-node 是的，只有这么一行。当在各个项目目录中，用这个只有一行的 Dockerfile 构建镜像时，之前基础镜像的那三行 ONBUILD 就会开始执行，成功的将当前项目的代码复制进镜像、并且针对本项目执行 npm install，生成应用镜像。 引用和引申 Dockerfie 官方文档 Dockerfile 最佳实践文档 Docker 官方镜像 Dockerfile Dockerfile 指令详解]]></content>
  </entry>
  <entry>
    <title><![CDATA[Elastic 技术栈之 Kibana]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Felastic%2Felastic-kibana%2F</url>
    <content type="text"><![CDATA[Elastic 技术栈之 Kibana Discover 单击侧面导航栏中的 Discover ，可以显示 Kibana 的数据查询功能功能。 在搜索栏中，您可以输入Elasticsearch查询条件来搜索您的数据。您可以在 Discover 页面中浏览结果并在 Visualize 页面中创建已保存搜索条件的可视化。 当前索引模式显示在查询栏下方。索引模式确定提交查询时搜索哪些索引。要搜索一组不同的索引，请从下拉菜单中选择不同的模式。要添加索引模式（index pattern），请转至 Management/Kibana/Index Patterns 并单击 Add New。 您可以使用字段名称和您感兴趣的值构建搜索。对于数字字段，可以使用比较运算符，如大于（&gt;），小于（&lt;）或等于（=）。您可以将元素与逻辑运算符 AND，OR 和 NOT 链接，全部使用大写。 默认情况下，每个匹配文档都显示所有字段。要选择要显示的文档字段，请将鼠标悬停在“可用字段”列表上，然后单击要包含的每个字段旁边的添加按钮。例如，如果只添加account_number，则显示将更改为包含五个帐号的简单列表： 查询语义 kibana 的搜索栏遵循 query-string-syntax 文档中所说明的查询语义。 这里说明一些最基本的查询语义。 查询字符串会被解析为一系列的术语和运算符。一个术语可以是一个单词（如：quick、brown）或用双引号包围的短语（如&quot;quick brown&quot;）。 查询操作允许您自定义搜索 - 下面介绍了可用的选项。 字段名称 正如查询字符串查询中所述，将在搜索条件中搜索default_field，但可以在查询语法中指定其他字段： 例如： 查询 status 字段中包含 active 关键字 status:active title 字段包含 quick 或 brown 关键字。如果您省略 OR 运算符，则将使用默认运算符 title:(quick OR brown)title:(quick brown) author 字段查找精确的短语 “john smith”，即精确查找。 author:"John Smith" 任意字段 book.title，book.content 或 book.date 都包含 quick 或 brown（注意我们需要如何使用 \* 表示通配符） book.\*:(quick brown) title 字段包含任意非 null 值 _exists_:title 通配符 ELK 提供了 ? 和 * 两个通配符。 ? 表示任意单个字符； * 表示任意零个或多个字符。 qu?ck bro* 注意：通配符查询会使用大量的内存并且执行性能较为糟糕，所以请慎用。 提示：纯通配符 * 被写入 exsits 查询，从而提高了查询效率。因此，通配符 field：* 将匹配包含空值的文档，如：&gt; **提示**：在一个单词的开头（例如：`*ing`）使用通配符这种方式的查询量特别大，因为索引中的所有术语都需要检查，以防万一匹配。通过将 `allow_leading_wildcard` 设置为 `false`，可以禁用。#### 正则表达式可以通过 `/` 将正则表达式包裹在查询字符串中进行查询例： name:/joh?n(ath[oa]n)/ 支持的正则表达式语义可以参考：[Regular expression syntax](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-regexp-query.html#regexp-syntax)#### 模糊查询我们可以使用 `~` 运算符来进行模糊查询。例：假设我们实际想查询 quick brown forks 但是，由于拼写错误，我们的查询关键字变成如下情况，依然可以查到想要的结果。 quikc~ brwn~ foks~ 这种模糊查询使用 Damerau-Levenshtein 距离来查找所有匹配最多两个更改的项。所谓的更改是指单个字符的插入，删除或替换，或者两个相邻字符的换位。默认编辑距离为 `2`，但编辑距离为 `1` 应足以捕捉所有人类拼写错误的80％。它可以被指定为： quikc~1 #### 近似检索尽管短语查询（例如，`john smith`）期望所有的词条都是完全相同的顺序，但是近似查询允许指定的单词进一步分开或以不同的顺序排列。与模糊查询可以为单词中的字符指定最大编辑距离一样，近似搜索也允许我们指定短语中单词的最大编辑距离：例 “fox quick”~5 字段中的文本越接近查询字符串中指定的原始顺序，该文档就越被认为是相关的。当与上面的示例查询相比时，短语 `"quick fox"` 将被认为比 `"quick brown fox"` 更近似查询条件。#### 范围可以为日期，数字或字符串字段指定范围。闭区间范围用方括号 `[min TO max]` 和开区间范围用花括号 `&#123;min TO max&#125;` 来指定。我们不妨来看一些示例。* 2012 年的所有日子 date:[2012-01-01 TO 2012-12-31] * 数字 1 到 5 count:[1 TO 5] * 在 `alpha` 和 `omega` 之间的标签，不包括 `alpha` 和 `omega` tag:{alpha TO omega} * 10 以上的数字 count:[10 TO *] * 2012 年以前的所有日期 date:{* TO 2012-01-01} 此外，开区间和闭区间也可以组合使用* 数组 1 到 5，但不包括 5 count:[1 TO 5} 一边无界的范围也可以使用以下语法： age:&gt;10 age:&gt;=10 age:&lt;10 age:&lt;=10 当然，你也可以使用 AND 运算符来得到连个查询结果的交集 age:(&gt;=10 AND &lt;20) age:(+&gt;=10 +&lt;20) #### Boosting使用操作符 `^` 使一个术语比另一个术语更相关。例如，如果我们想查找所有有关狐狸的文档，但我们对狐狸特别感兴趣： quick^2 fox 默认提升值是1，但可以是任何正浮点数。 0到1之间的提升减少了相关性。增强也可以应用于短语或组： “john smith”^2 (foo bar)^4 #### 布尔操作默认情况下，只要一个词匹配，所有词都是可选的。搜索 `foo bar baz` 将查找包含 `foo` 或 `bar` 或 `baz` 中的一个或多个的任何文档。我们已经讨论了上面的`default_operator`，它允许你强制要求所有的项，但也有布尔运算符可以在查询字符串本身中使用，以提供更多的控制。首选的操作符是 `+`（此术语必须存在）和 `-` （此术语不得存在）。所有其他条款是可选的。例如，这个查询： quick brown +fox -news 这条查询意味着：* fox 必须存在* news 必须不存在* quick 和 brown 是可有可无的熟悉的运算符 `AND`，`OR` 和 `NOT`（也写成 `&amp;&amp;`，`||` 和 `!`）也被支持。然而，这些操作符有一定的优先级：`NOT` 优先于 `AND`，`AND` 优先于 `OR`。虽然 `+` 和 `-` 仅影响运算符右侧的术语，但 `AND` 和 `OR` 会影响左侧和右侧的术语。#### 分组多个术语或子句可以用圆括号组合在一起，形成子查询 (quick OR brown) AND fox 可以使用组来定位特定的字段，或者增强子查询的结果： status:(active OR pending) title:(full text search)^2 #### 保留字 如果你需要使用任何在你的查询本身中作为操作符的字符（而不是作为操作符），那么你应该用一个反斜杠来转义它们。例如，要搜索（1 + 1）= 2，您需要将查询写为 `\(1\+1\)\=2` 保留字符是：`+ - = &amp;&amp; || &gt; &lt; ! ( ) { } [ ] ^ &quot; ~ * ? : \ /` 无法正确地转义这些特殊字符可能会导致语法错误，从而阻止您的查询运行。 #### 空查询 如果查询字符串为空或仅包含空格，则查询将生成一个空的结果集。 ## Visualize 要想使用可视化的方式展示您的数据，请单击侧面导航栏中的 `Visualize`。 Visualize工具使您能够以多种方式（如饼图、柱状图、曲线图、分布图等）查看数据。要开始使用，请点击蓝色的 `Create a visualization` 或 `+` 按钮。 ![https://www.elastic.co/guide/en/kibana/6.1/images/tutorial-visualize-landing.png](https://www.elastic.co/guide/en/kibana/6.1/images/tutorial-visualize-landing.png) 有许多可视化类型可供选择。 ![https://www.elastic.co/guide/en/kibana/6.1/images/tutorial-visualize-wizard-step-1.png](https://www.elastic.co/guide/en/kibana/6.1/images/tutorial-visualize-wizard-step-1.png) 下面，我们来看创建几个图标示例： ### Pie 您可以从保存的搜索中构建可视化文件，也可以输入新的搜索条件。要输入新的搜索条件，首先需要选择一个索引模式来指定要搜索的索引。 默认搜索匹配所有文档。最初，一个“切片”包含整个饼图： ![https://www.elastic.co/guide/en/kibana/6.1/images/tutorial-visualize-pie-1.png](https://www.elastic.co/guide/en/kibana/6.1/images/tutorial-visualize-pie-1.png) 要指定在图表中展示哪些数据，请使用Elasticsearch存储桶聚合。分组汇总只是将与您的搜索条件相匹配的文档分类到不同的分类中，也称为分组。 为每个范围定义一个存储桶： 1. 单击 `Split Slices`。 2. 在 `Aggregation` 列表中选择 `Terms`。_注意：这里的 Terms 是 Elk 采集数据时定义好的字段或标签。_ 3. 在 `Field` 列表中选择 `level.keyword`。 4. 点击 ![images/apply-changes-button.png](https://www.elastic.co/guide/en/kibana/6.1/images/apply-changes-button.png) 按钮来更新图表。 ![image.png](https://upload-images.jianshu.io/upload_images/3101171-7fb2042dc6d59520.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 完成后，如果想要保存这个图表，可以点击页面最上方一栏中的 `Save` 按钮。 ### Vertical Bar 我们在展示一下如何创建柱状图。 1. 点击蓝色的 `Create a visualization` 或 `+` 按钮。选择 `Vertical Bar` 2. 选择索引模式。由于您尚未定义任何 bucket ，因此您会看到一个大栏，显示与默认通配符查询匹配的文档总数。 3. 指定 Y 轴所代表的字段 4. 指定 X 轴所代表的字段 5. 点击 ![images/apply-changes-button.png](https://www.elastic.co/guide/en/kibana/6.1/images/apply-changes-button.png) 按钮来更新图表。 ![image.png](https://upload-images.jianshu.io/upload_images/3101171-5aa7627284c19a56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 完成后，如果想要保存这个图表，可以点击页面最上方一栏中的 `Save` 按钮。 ## Dashboard `Dashboard` 可以整合和共享 `Visualize` 集合。 1. 点击侧面导航栏中的 Dashboard。 2. 点击添加显示保存的可视化列表。 3. 点击之前保存的 `Visualize`，然后点击列表底部的小向上箭头关闭可视化列表。 4. 将鼠标悬停在可视化对象上会显示允许您编辑，移动，删除和调整可视化对象大小的容器控件。]]></content>
  </entry>
  <entry>
    <title><![CDATA[ESLint]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fqa%2Feslint%2F</url>
    <content type="text"><![CDATA[ESLint ESLint 是一种用于识别和报告 ECMAScript / JavaScript 代码中的模式的工具。在许多方面，它类似于 JSLint 和 JSHint，但有一些例外： ESLint 使用 Espree 进行 JavaScript 解析。 ESLint 使用 AST 来评估代码中的模式。 ESLint 是完全可插拔的，每个规则都是一个插件，您可以在运行时添加更多。 安装 ESLint 配置 指定解析器选项 指定解析器 指定环境 指定全局变量 配置插件 配置规则 使用行注释禁用规则 添加分享配置 使用配置文件 配置文件文件格式 配置的层级和继承 扩展配置文件 基于 glob 模式的配置 在配置文件中注释 指定需要检查的文件扩展名 忽略文件和目录 实战 ESLint 集成 Airbnb ESLint 命令 选项 .eslintignore 文件 更多内容 安装 （1）本地安装 如果要将 ESLint 作为项目构建系统的一部分，我们建议在本地进行安装。你可以使用 npm： $ npm install eslint --save-dev 然后您应该设置一个配置文件： $ ./node_modules/.bin/eslint --init 之后，您可以在项目的根目录中运行 ESLint，如下所示： $ ./node_modules/.bin/eslint yourfile.js 您使用的任何插件或可共享配置也必须在本地安装以与本地安装的 ESLint 配合使用。 （2）全局安装 如果要使 ESLint 可用于跨所有项目运行的工具，我们建议在全局安装 ESLint。你可以使用 npm： $ npm install -g eslint 然后您应该设置一个配置文件： $ eslint --init 之后，您可以在任何文件或目录下运行 ESLint，如下所示： $ eslint yourfile.js 您使用的任何插件或可共享配置也必须全局安装，才能使用全局安装的 ESLint。 注意：eslint --init 旨在根据每个项目设置和配置 ESLint，并将 ESLint 及其插件的本地安装运行在其运行的目录中。如果您希望使用 ESLint 的全局安装，则配置中使用的任何插件也必须全局安装 ESLint 配置 运行 eslint --init 后，您的目录中将有一个 .eslintrc 文件。在其中，您将看到一些如下配置的规则： &#123; "rules": &#123; "semi": ["error", "always"], "quotes": ["error", "double"] &#125;&#125; semi 和 quotes 是 ESLint 中规则的名称。第一个值是规则的错误级别，可以是以下值之一： off or 0 - 关闭规则 warn or 1 - 将规则作为警告（不影响退出代码） error or 2 - 将规则作为错误（退出代码将为 1） 这三个错误级别允许您细分控制 ESLint 如何应用规则（有关更多配置选项和详细信息，请参阅配置文档）。 您的 .eslintrc 配置文件也将包括以下行： "extends": "eslint:recommended" 由于这一行，规则页面上的所有打钩的规则都将被打开。或者，您可以通过在 npmjs.com 上搜索 “eslint-config” 来使用其他人创建的配置。 ESLint 不会删除您的代码，除非您从共享配置中扩展或在配置中明确地打开规则。 ESlint 被设计为完全可配置的，这意味着你可以关闭每一个规则而只运行基本语法验证，或混合和匹配 ESLint 默认绑定的规则和你的自定义规则，以让 ESLint 更适合你的项目。有两种主要的方式来配置 ESLint： Configuration Comments - 使用 JavaScript 注释把配置信息直接嵌入到一个代码源文件中。 Configuration Files - 使用 JavaScript、JSON 或者 YAML 文件为整个目录和它的子目录指定配置信息。可以配置一个独立的 .eslintrc.* 文件，或者直接在 package.json 文件里的 eslintConfig 字段指定配置，ESLint 会查找和自动读取它们，再者，你可以在命令行运行时指定一个任意的配置文件。 有很多信息可以配置： Environments - 指定脚本的运行环境。每种环境都会有一组特定的预定义全局变量。 Globals - 脚本在执行期间访问的额外的全局变量。 Rules - 启用的规则及其各自的错误级别。 所有这些选项让你可以细粒度地控制 ESLint 如何对待你的代码。 指定解析器选项 ESLint 允许你指定你想要支持的 JavaScript 语言选项。默认情况下，ESLint 支持 ECMAScript 5 语法。你可以覆盖该设置，以启用对 ECMAScript 其它版本和 JSX 的支持。 请注意，对 JSX 语法的支持不用于对 React 的支持。React 使用了一些特定的 ESLint 无法识别的 JSX 语法。如果你正在使用 React 并且想要 React 语义支持，我们推荐你使用 eslint-plugin-react。 同样的，支持 ES6 语法并不意味着同时支持新的 ES6 全局变量或类型（比如 Set 等新类型）。使用 { &quot;parserOptions&quot;: { &quot;ecmaVersion&quot;: 6 } } 来启用 ES6 语法支持；要额外支持新的 ES6 全局变量，使用 { &quot;env&quot;:{ &quot;es6&quot;: true } }(这个设置会同时自动启用 ES6 语法支持)。 解析器选项可以在 .eslintrc.* 文件使用 parserOptions 属性设置。可用的选项有： ecmaVersion - 默认设置为 5， 你可以使用 3、5、6、7 或 8 来指定你想要使用的 ECMAScript 版本。你也可以用使用年份命名的版本号指定为 2015（同 6），2016（同 7），或 2017（同 8） sourceType - 设置为 script (默认) 或 module（如果你的代码是 ECMAScript 模块)。 ecmaFeatures - 这是个对象，表示你想使用的额外的语言特性:globalReturn - 允许在全局作用域下使用 return 语句impliedStrict - 启用全局 strict mode (如果 ecmaVersion 是 5 或更高)jsx - 启用 JSXexperimentalObjectRestSpread - 启用实验性的 object rest/spread properties 支持。(**重要：**这是一个实验性的功能,在未来可能会有明显改变。 建议你写的规则 不要 依赖该功能，除非当它发生改变时你愿意承担维护成本。) .eslintrc.json 文件示例： &#123; "parserOptions": &#123; "ecmaVersion": 6, "sourceType": "module", "ecmaFeatures": &#123; "jsx": true &#125; &#125;, "rules": &#123; "semi": 2 &#125;&#125; 设置解析器选项能帮助 ESLint 确定什么是解析错误，所有语言选项默认都是 false。 指定解析器 ESLint 默认使用Espree作为其解析器，你可以在配置文件中指定一个不同的解析器，只要该解析器符合下列要求： 它必须是本地安装的一个 npm 模块。 它必须有兼容 Esprima 的接口（它必须输出一个 parse() 方法） 它必须产出兼容 Esprima 的 AST 和 token 对象。 注意，即使满足这些兼容性要求，也不能保证一个外部解析器可以与 ESLint 正常配合工作，ESLint 也不会修复与其它解析器不兼容的相关 bug。 为了表明使用该 npm 模块作为你的解析器，你需要在你的 .eslintrc 文件里指定 parser 选项。例如，下面的配置指定了 Esprima 作为解析器： &#123; "parser": "esprima", "rules": &#123; "semi": "error" &#125;&#125; 以下解析器与 ESLint 兼容： Esprima Babel-ESLint - 一个对Babel解析器的包装，使其能够与 ESLint 兼容。 typescript-eslint-parser(实验) - 一个把 TypeScript 转换为 ESTree 兼容格式的解析器，这样它就可以在 ESLint 中使用了。这样做的目的是通过 ESLint 来解析 TypeScript 文件（尽管不一定必须通过所有的 ESLint 规则）。 注意，在使用自定义解析器时，为了让 ESLint 在处理非 ECMAScript 5 特性时正常工作，配置属性 parserOptions 仍然是必须的。解析器会被传入 parserOptions，但是不一定会使用它们来决定功能特性的开关。 指定环境 一个“环境”定义了一组预定义的全局变量。可用的环境包括： browser - 浏览器环境中的全局变量。 node - Node.js 全局变量和 Node.js 作用域。 commonjs - CommonJS 全局变量和 CommonJS 作用域 (一般用于 Browserify/WebPack 打包的只在浏览器中运行的代码)。 shared-node-browser - Node 和 Browser 通用全局变量。 es6 - 启用除了 modules 以外的所有 ECMAScript 6 特性（该选项会自动设置 ecmaVersion 解析器选项为 6）。 worker - Web Workers 全局变量。 amd - 将 require() 和 define() 定义为像 amd 一样的全局变量。 mocha - 添加所有的 Mocha 测试全局变量。 jasmine - 添加所有的 Jasmine 版本 1.3 和 2.0 的测试全局变量。 jest - Jest 全局变量。 phantomjs - PhantomJS 全局变量。 protractor - Protractor 全局变量。 qunit - QUnit 全局变量。 jquery - jQuery 全局变量。 prototypejs - Prototype.js 全局变量。 shelljs - ShellJS 全局变量。 meteor - Meteor 全局变量。 mongo - MongoDB 全局变量。 applescript - AppleScript 全局变量。 nashorn - Java 8 Nashorn 全局变量。 serviceworker - Service Worker 全局变量。 atomtest - Atom 测试全局变量。 embertest - Ember 测试全局变量。 webextensions - WebExtensions 全局变量。 greasemonkey - GreaseMonkey 全局变量。 这些环境并不是互斥的，所以你可以同时定义多个。 可以在源文件里、在配置文件中或使用 命令行 的 --env 选项来指定环境。 要在你的 JavaScript 文件中使用注释来指定环境，格式如下： /* eslint-env node, mocha */ 该设置启用了 Node.js 和 Mocha 环境。 要在配置文件里指定环境，使用 env 关键字指定你想启用的环境，并设置它们为 true。例如，以下示例启用了 browser 和 Node.js 的环境： &#123; "env": &#123; "browser": true, "node": true &#125;&#125; 或在 package.json 文件中： &#123; "name": "mypackage", "version": "0.0.1", "eslintConfig": &#123; "env": &#123; "browser": true, "node": true &#125; &#125;&#125; 在 YAML 文件中： --- env: browser: true node: true 如果你想在一个特定的插件中使用一种环境，确保提前在 plugins 数组里指定了插件名，然后在 env 配置中不带前缀的插件名后跟一个 / ，紧随着环境名。例如： &#123; "plugins": ["example"], "env": &#123; "example/custom": true &#125;&#125; 或在 package.json 文件中 &#123; "name": "mypackage", "version": "0.0.1", "eslintConfig": &#123; "plugins": ["example"], "env": &#123; "example/custom": true &#125; &#125;&#125; 在 YAML 文件中： --- plugins: - example env: example/custom: true 指定全局变量 当访问当前源文件内未定义的变量时，no-undef 规则将发出警告。如果你想在一个源文件里使用全局变量，推荐你在 ESLint 中定义这些全局变量，这样 ESLint 就不会发出警告了。你可以使用注释或在配置文件中定义全局变量。 要在你的 JavaScript 文件中，用注释指定全局变量，格式如下： /* global var1, var2 */ 这里定义了两个全局变量：var1 和 var2。如果你想指定这些变量不应被重写（只读），你可以将它们设置为 false： /* global var1:false, var2:false */ 在配置文件里配置全局变量时，使用 globals 指出你要使用的全局变量。将变量设置为 true 将允许变量被重写，或 false 将不允许被重写。比如： &#123; "globals": &#123; "var1": true, "var2": false &#125;&#125; 在 YAML 中： --- globals: var1: true var2: false 在这些例子中 var1 允许被重写，var2 不允许被重写。 注意： 要启用no-global-assign规则来禁止对只读的全局变量进行修改。 配置插件 ESLint 支持使用第三方插件。在使用插件之前，你必须使用 npm 安装它。 在配置文件里配置插件时，可以使用 plugins 关键字来存放插件名字的列表。插件名称可以省略 eslint-plugin- 前缀。 &#123; "plugins": [ "plugin1", "eslint-plugin-plugin2" ]&#125; 在 YAML 中： --- plugins: - plugin1 - eslint-plugin-plugin2 **注意：**全局安装的 ESLint 只能使用全局安装的插件。本地安装的 ESLint 不仅可以使用本地安装的插件，也可以使用全局安装的插件。 配置规则 ESLint 附带有大量的规则。你可以使用注释或配置文件修改你项目中要使用的规则。要改变一个规则设置，你必须将规则 ID 设置为下列值之一： off 或 0 - 关闭规则 warn 或 1 - 开启规则，使用警告级别的错误：warn (不会导致程序退出) error 或 2 - 开启规则，使用错误级别的错误：error (当被触发的时候，程序会退出) 为了在文件注释里配置规则，使用以下格式的注释： /* eslint eqeqeq: "off", curly: "error" */ 在这个例子里，eqeqeq 规则被关闭，curly 规则被打开，定义为错误级别。你也可以使用对应的数字定义规则严重程度： /* eslint eqeqeq: 0, curly: 2 */ 这个例子和上个例子是一样的，只不过它是用的数字而不是字符串。eqeqeq 规则是关闭的，curly 规则被设置为错误级别。 如果一个规则有额外的选项，你可以使用数组字面量指定它们，比如： /* eslint quotes: ["error", "double"], curly: 2 */ 这条注释为规则 quotes 指定了 “double”选项。数组的第一项总是规则的严重程度（数字或字符串）。 还可以使用 rules 连同错误级别和任何你想使用的选项，在配置文件中进行规则配置。例如： &#123; "rules": &#123; "eqeqeq": "off", "curly": "error", "quotes": ["error", "double"] &#125;&#125; 在 YAML 中： ---rules: eqeqeq: off curly: error quotes: - error - double 配置定义在插件中的一个规则的时候，你必须使用 插件名/规则ID 的形式。比如： &#123; "plugins": [ "plugin1" ], "rules": &#123; "eqeqeq": "off", "curly": "error", "quotes": ["error", "double"], "plugin1/rule1": "error" &#125;&#125; 在 YAML 中： ---plugins: - plugin1rules: eqeqeq: 0 curly: error quotes: - error - "double" plugin1/rule1: error 在这些配置文件中，规则 plugin1/rule1 表示来自插件 plugin1 的 rule1 规则。你也可以使用这种格式的注释配置，比如： /* eslint "plugin1/rule1": "error" */ **注意：**当指定来自插件的规则时，确保删除 eslint-plugin- 前缀。ESLint 在内部只使用没有前缀的名称去定位规则。 使用行注释禁用规则 可以在你的文件中使用以下格式的块注释来临时禁止规则出现警告： /* eslint-disable */alert('foo');/* eslint-enable */ 你也可以对指定的规则启用或禁用警告: /* eslint-disable no-alert, no-console */alert('foo');console.log('bar');/* eslint-enable no-alert, no-console */ 如果在整个文件范围内禁止规则出现警告，将 /* eslint-disable */ 块注释放在文件顶部： /* eslint-disable */alert('foo'); 你也可以对整个文件启用或禁用警告: /* eslint-disable no-alert */// Disables no-alert for the rest of the filealert('foo'); 可以在你的文件中使用以下格式的行注释在某一特定的行上禁用所有规则： alert('foo'); // eslint-disable-line// eslint-disable-next-linealert('foo'); 在某一特定的行上禁用某个指定的规则： alert('foo'); // eslint-disable-line no-alert// eslint-disable-next-line no-alertalert('foo'); 在某个特定的行上禁用多个规则： alert('foo'); // eslint-disable-line no-alert, quotes, semi// eslint-disable-next-line no-alert, quotes, semialert('foo'); 上面的所有方法同样适用于插件规则。例如，禁止 eslint-plugin-example 的 rule-name 规则，把插件名（example）和规则名（rule-name）结合为 example/rule-name： foo(); // eslint-disable-line example/rule-name **注意：**为文件的某部分禁用警告的注释，告诉 ESLint 不要对禁用的代码报告规则的冲突。ESLint 仍解析整个文件，然而，禁用的代码仍需要是有效的 JavaScript 语法。 添加分享配置 ESLint 支持在配置文件添加共享设置。你可以添加 settings 对象到配置文件，它将提供给每一个将被执行的规则。如果你想添加的自定义规则而且使它们可以访问到相同的信息，这将会很有用，并且很容易配置。 在 JSON 中： &#123; "settings": &#123; "sharedData": "Hello" &#125;&#125; 在 YAML 中： --- settings: sharedData: "Hello" 使用配置文件 有两种方式可以使用配置文件。第一种是将文件保存到你喜欢的地方，然后将它的位置使用 -c 选项传递命令行，比如： eslint -c myconfig.json myfiletotest.js 第二种方式是通过 .eslintrc.* 和 package.json。ESLint 将自动在要检测的文件目录里寻找它们，紧接着是父级目录，一直到文件系统的根目录。当你想对一个项目的不同部分的使用不同配置，或当你希望别人能够直接使用 ESLint，而无需记住要在配置文件中传递什么，这种方式就很有用。 每种情况，配置文件都会覆盖默认设置。 配置文件文件格式 ESLint 支持几种格式的配置文件： JavaScript - 使用 .eslintrc.js 然后输出一个配置对象。 YAML - 使用 .eslintrc.yaml 或 .eslintrc.yml 去定义配置的结构。 JSON - 使用 .eslintrc.json 去定义配置的结构，ESLint 的 JSON 文件允许 JavaScript 风格的注释。 (不推荐) - 使用 .eslintrc，可以使 JSON 也可以是 YAML。 package.json - 在 package.json 里创建一个 eslintConfig属性，在那里定义你的配置。 如果同一个目录下有多个配置文件，ESLint 只会使用一个。优先级顺序如下： .eslintrc.js .eslintrc.yaml .eslintrc.yml .eslintrc.json .eslintrc package.json 配置的层级和继承 当使用 .eslintrc.* 和 package.json文件的配置时，你可以利用层叠配置。例如，假如你有以下结构： your-project├── .eslintrc├── lib│ └── source.js└─┬ tests ├── .eslintrc └── test.js 层叠配置使用离要检测的文件最近的 .eslintrc文件作为最高优先级，然后才是父目录里的配置文件，等等。当你在这个项目中允许 ESLint 时，lib/下面的所有文件将使用项目根目录里的 .eslintrc 文件作为它的配置文件。当 ESLint 遍历到 test/ 目录，your-project/.eslintrc 之外，它还会用到 your-project/tests/.eslintrc。所以 your-project/tests/test.js 是基于它的目录层次结构中的两个.eslintrc 文件的组合，并且离的最近的一个优先。通过这种方式，你可以有项目级 ESLint 设置，也有覆盖特定目录的 ESLint 设置。 同样的，如果在根目录的 package.json 文件中有一个 eslintConfig 字段，其中的配置将使用于所有子目录，但是当 tests 目录下的 .eslintrc 文件中的规则与之发生冲突时，就会覆盖它。 your-project├── package.json├── lib│ └── source.js└─┬ tests ├── .eslintrc └── test.js 如果同一目录下 .eslintrc 和 package.json 同时存在，.eslintrc 优先级高会被使用，package.json 文件将不会被使用。 **注意：**如果在你的主目录下有一个自定义的配置文件 (\~/.eslintrc) ，如果没有其它配置文件时它才会被使用。因为个人配置将适用于用户目录下的所有目录和文件，包括第三方的代码，当 ESLint 运行时可能会导致问题。 默认情况下，ESLint 会在所有父级目录里寻找配置文件，一直到根目录。如果你想要你所有项目都遵循一个特定的约定时，这将会很有用，但有时候会导致意想不到的结果。为了将 ESLint 限制到一个特定的项目，在你项目根目录下的 package.json 文件或者 .eslintrc.* 文件里的 eslintConfig 字段下设置 root&quot;: true。ESLint 一旦发现配置文件中有 root&quot;: true，它就会停止在父级目录中寻找。 &#123; "root": true&#125; 在 YAML 中： --- root: true 例如，projectA 的 lib/ 目录下的 .eslintrc 文件中设置了 root&quot;: true。这种情况下，当检测 main.js 时，lib/ 下的配置将会被使用，projectA/ 下的 .eslintrc 将不会被使用。 home└── user ├── .eslintrc &lt;- Always skipped if other configs present └── projectA ├── .eslintrc &lt;- Not used └── lib ├── .eslintrc &lt;- &#123; "root": true &#125; └── main.js 完整的配置层次结构，从最高优先级最低的优先级，如下: 行内配置 /*eslint-disable*/ 和 /*eslint-enable*/ /*global*/ /*eslint*/ /*eslint-env*/ 命令行选项： --global --rule --env -c、--config 项目级配置： 与要检测的文件在同一目录下的 .eslintrc.* 或 package.json 文件 继续在父级目录寻找 .eslintrc 或 package.json文件，直到根目录（包括根目录）或直到发现一个有root&quot;: true的配置。 如果不是（1）到（3）中的任何一种情况，退回到 \~/.eslintrc 中自定义的默认配置。 扩展配置文件 一个配置文件可以被基础配置中的已启用的规则继承。 extends 属性值可以是： 在配置中指定的一个字符串 字符串数组：每个配置继承它前面的配置 ESLint 递归地进行扩展配置，所以一个基础的配置也可以有一个 extends 属性。 rules 属性可以做下面的任何事情以扩展（或覆盖）规则： 启用额外的规则 改变继承的规则级别而不改变它的选项： 基础配置：eqeqeq&quot;: [&quot;error&quot;, &quot;allow-null&quot;] 派生的配置：eqeqeq&quot;: &quot;warn 最后生成的配置：eqeqeq&quot;: [&quot;warn&quot;, &quot;allow-null&quot;] 覆盖基础配置中的规则的选项 基础配置：quotes&quot;: [&quot;error&quot;, &quot;single&quot;, &quot;avoid-escape&quot;] 派生的配置：quotes&quot;: [&quot;error&quot;, &quot;single&quot;] 最后生成的配置：quotes&quot;: [&quot;error&quot;, &quot;single&quot;] 使用 eslint:recommended 值为 eslint:recommended 的 extends 属性启用一系列核心规则，这些规则报告一些常见问题，在 规则页面 中被标记为 。这个推荐的子集只能在 ESLint 主要版本进行更新。 如果你的配置集成了推荐的规则：在你升级到 ESLint 新的主版本之后，在你使用命令行的 --fix 选项之前，检查一下报告的问题，这样你就知道一个新的可修复的推荐的规则将更改代码。 eslint --init 命令可以创建一个配置，这样你就可以继承推荐的规则。 JavaScript 格式的一个配置文件的例子： module.exports = &#123; "extends": "eslint:recommended", "rules": &#123; // enable additional rules "indent": ["error", 4], "linebreak-style": ["error", "unix"], "quotes": ["error", "double"], "semi": ["error", "always"], // override default options for rules from base configurations "comma-dangle": ["error", "always"], "no-cond-assign": ["error", "always"], // disable rules from base configurations "no-console": "off", &#125;&#125; 使用可共享的配置包 可共享的配置 是一个 npm 包，它输出一个配置对象。要确保这个包安装在 ESLint 能请求到的目录下。 extends 属性值可以省略包名的前缀 eslint-config-。 eslint --init 命令可以创建一个配置，这样你就可以扩展一个流行的风格指南（比如，eslint-config-standard）。 YAML 格式的一个配置文件的例子： extends: standardrules: comma-dangle: - error - always no-empty: warn 使用插件中的配置 插件 是一个 npm 包，通常输出规则。一些插件也可以输出一个或多个命名的 配置。要确保这个包安装在 ESLint 能请求到的目录下。 plugins 属性值 可以省略包名的前缀 eslint-plugin-。 extends 属性值可以由以下组成： plugin: 包名 (省略了前缀，比如，react) / 配置名称 (比如 recommended) JSON 格式的一个配置文件的例子： &#123; "plugins": [ "react" ], "extends": [ "eslint:recommended", "plugin:react/recommended" ], "rules": &#123; "no-set-state": "off" &#125;&#125; 使用一个配置文件 extends 属性值可以是基本配置文件的绝对路径或相对路径。 ESLint 解析基本配置文件的相对路径相对你你使用的配置文件，除非那个文件在你的主目录或非 ESLint 安装目录的父级目录。在这些情况下，ESLint 解析基本配合文件的相对路径相对于被检测的 项目目录（尤其是当前工作目录）。 JSON 格式的一个配置文件的例子： &#123; "extends": [ "./node_modules/coding-standard/eslintDefaults.js", "./node_modules/coding-standard/.eslintrc-es6", "./node_modules/coding-standard/.eslintrc-jsx" ], "rules": &#123; "eqeqeq": "warn" &#125;&#125; 使用 eslint:all extends 属性值可以是 eslint:all，启用当前安装的 ESLint 中所有的核心规则。这些规则可以在 ESLint 的任何版本进行更改。 **重要：**这些配置 不推荐在产品中使用，因为它随着 ESLint 版本进行更改。使用的话，请自己承担风险。 如果你配置 ESLint 升级时自动地启用新规则，当源码没有任何改变时，ESLint 可以报告新问题，因此任何 ESLint 的新的小版本好像有破坏性的更改。 当你决定在一个项目上使用的规则和选项，尤其是如果你很少覆盖选项或禁用规则，你可能启用所有核心规则作为一种快捷方式使用。规则的默认选项并不是 ESLint 推荐的（例如，quotes 规则的默认选项并不意味着双引号要比单引号好）。 如果你的配置扩展了所有的核心规则：在你升级到一个新的大或小的 ESLint 版本，在你使用命令行的 --fix 选项之前，检查一下报告的问题，这样你就知道一个新的可修复的规则将更改代码。 JavaScript 格式的一个配置文件的例子： module.exports = &#123; "extends": "eslint:all", "rules": &#123; // override default options "comma-dangle": ["error", "always"], "indent": ["error", 2], "no-cond-assign": ["error", "always"], // disable now, but enable in the future "one-var": "off", // ["error", "never"] // disable "init-declarations": "off", "no-console": "off", "no-inline-comments": "off", &#125;&#125; 基于 glob 模式的配置 有时，你可能需要更精细的配置，比如，如果同一个目录下的文件需要有不同的配置。因此，你可以在配置中使用 overrides 键，它只适用于匹配特定的 glob 模式的文件，使用你在命令行上传递的格式 (e.g., app/**/*.test.js)。 怎么工作 Glob 模式覆盖只能在配置文件 (.eslintrc.* 或 package.json) 中进行配置。 模式应用于相对于配置文件的目录的文件路径。 比如，如果你的配置文件的路径为 /Users/john/workspace/any-project/.eslintrc.js 而你要检测的路径为 /Users/john/workspace/any-project/lib/util.js，那么你在 .eslintrc.js 中提供的模式是相对于 lib/util.js 来执行的. 在相同的配置文件中，Glob 模式覆盖比其他常规配置具有更高的优先级。 同一个配置中的多个覆盖将按顺序被应用。也就是说，配置文件中的最后一个覆盖会有最高优先级。 一个 glob 特定的配置几乎与 ESLint 的其他配置相同。覆盖块可以包含常规配置中的除了 extends、overrides 和 root 之外的其他任何有效配置选项， 可以在单个覆盖块中提供多个 glob 模式。一个文件必须匹配至少一个配置中提供的模式。 覆盖块也可以指定从匹配中排除的模式。如果一个文件匹配了任何一个排除模式，该配置将不再被应用。 相对 glob 模式 project-root├── app│ ├── lib│ │ ├── foo.js│ │ ├── fooSpec.js│ ├── components│ │ ├── bar.js│ │ ├── barSpec.js│ ├── .eslintrc.json├── server│ ├── server.js│ ├── serverSpec.js├── .eslintrc.json app/.eslintrc.json 文件中的配置定义了 glob 模式 **/*Spec.js。该模式是相对 app/.eslintrc.json 的基本目录的。因此，该模式将匹配 app/lib/fooSpec.js 和 app/components/barSpec.js 但 不匹配 server/serverSpec.js。如果你在项目根目录下的 .eslintrc.json 文件中定义了同样的模式，它将匹配这三个 *Spec 文件。 配置示例 在你的 .eslintrc.json 文件中： &#123; "rules": &#123; "quotes": [ 2, "double" ] &#125;, "overrides": [ &#123; "files": [ "bin/*.js", "lib/*.js" ], "excludedFiles": "*.test.js", "rules": &#123; "quotes": [ 2, "single" ] &#125; &#125; ]&#125; 在配置文件中注释 JSON 和 YAML 配置文件格式都支持注释 ( package.json 文件不应该包括注释)。你可以在其他类型的文件中使用 JavaScript 风格的注释或使用 YAML 风格的注释，ESLint 会忽略它们。这允许你的配置更加人性化。例如： &#123; "env": &#123; "browser": true &#125;, "rules": &#123; // Override our default settings just for this directory "eqeqeq": "warn", "strict": "off" &#125;&#125; 指定需要检查的文件扩展名 目前，告诉 ESLint 哪个文件扩展名要检测的唯一方法是使用 --ext 命令行选项指定一个逗号分隔的扩展名列表。注意，该标记只在与目录一起使用时有效，如果使用文件名或 glob 模式，它将会被忽略。 忽略文件和目录 你可以通过在项目根目录创建一个 .eslintignore 文件告诉 ESLint 去忽略特定的文件和目录。.eslintignore 文件是一个纯文本文件，其中的每一行都是一个 glob 模式表明哪些路径应该忽略检测。例如，以下将忽略所有的 JavaScript 文件： **/*.js 当 ESLint 运行时，在确定哪些文件要检测之前，它会在当前工作目录中查找一个 .eslintignore 文件。如果发现了这个文件，当遍历目录时，将会应用这些偏好设置。一次只有一个 .eslintignore 文件会被使用，所以，不是当前工作目录下的 .eslintignore 文件将不会被用到。 Globs 匹配使用 node-ignore，所以大量可用的特性有： 以 # 开头的行被当作注释，不影响忽略模式。 路径是相对于 .eslintignore 的位置或当前工作目录。这也会影响通过 --ignore-pattern传递的路径。 忽略模式同 .gitignore 规范 以 ! 开头的行是否定模式，它将会重新包含一个之前被忽略的模式。 除了 .eslintignore 文件中的模式，ESLint 总是忽略 /node_modules/* 和 /bower_components/* 中的文件。 例如：把下面 .eslintignore 文件放到当前工作目录里，将忽略 node_modules，bower_components 以及 build/ 目录下除了 build/index.js 的所有文件。 ## /node_modules/* and /bower_components/* ignored by default## Ignore built files except build/index.jsbuild/*!build/index.js 使用备用文件 如果相比于当前工作目录下 .eslintignore 文件，你更想使用一个不同的文件，你可以在命令行使用 --ignore-path 选项指定它。例如，你可以使用 .jshintignore 文件，因为它有相同的格式： eslint --ignore-path .jshintignore file.js 你也可以使用你的 .gitignore 文件： eslint --ignore-path .gitignore file.js 任何文件只要满足标准忽略文件格式都可以用。记住，指定 --ignore-path 意味着任何现有的 .eslintignore 文件将不被使用。请注意，.eslintignore 中的匹配规则比 .gitignore 中的更严格。 在 package.json 中使用 eslintConfig &#123; "name": "mypackage", "version": "0.0.1", "eslintConfig": &#123; "env": &#123; "browser": true, "node": true &#125; &#125;, "eslintIgnore": ["hello.js", "world.js"]&#125; 忽略文件告警 当您将目录传递给 ESLint 时，文件和目录将被忽略。如果将特定文件传递给 ESLint，则会看到一条警告，指示该文件已被跳过。例如，假设你有一个 .eslintignore 文件，如下所示： foo.js 然后，您执行： eslint foo.js 您将会看到以下告警： foo.js 0:0 warning File ignored because of your .eslintignore file. Use --no-ignore to override.✖ 1 problem (0 errors, 1 warning) 发生此消息是因为 ESLint 不确定是否要检查该文件。如消息所示，您可以使用 --no-ignore 省略使用忽略规则。 实战 React 项目中，通常会使用 React、JSX、ES6 等特性，所以 ESLint 推荐的检查规则可能还不能满足我们对代码检查的需要。 ESLint 集成 Airbnb 业界比较流行的 Airbnb JavaScript Style 可以说是一个不错的选择。 那么如何让 ESLint 支持 Airbnb 呢？ 下面将一一道来。 eslint-config-airbnb eslint-config-airbnb 该包将 Airbnb 的规则作为 ESLint 可扩展的共享配置。 安装方法一 执行 eslint --init 命令，依次选择 Use a popular style guide -&gt; Airbnb ESLint 会自动下载 Airbnb 扩展所需的所有插件。 安装方法二 ESLint 默认的下载地址是 https://registry.npmjs.org/ ，由于是外网，可能会很慢。 如果你使用淘宝的 cnpm，也可以自己下载 Airbnb 扩展所需的插件： eslint-config-airbnb eslint-plugin-import eslint-plugin-jsx-a11y eslint-plugin-react 执行安装命令： $ cnpm i -D eslint-plugin-import eslint-plugin-jsx-a11y eslint-plugin-react eslint-config-airbnb babel-eslint 由于 React 项目往往会使用 ES6 特性。ES6 若想识别 ES6 特性，需要相应的解析器，babel-eslint 是比较流行的选择。 执行安装命令： $ cnpm i -D babel-eslint 配置 ESLint 支持几种格式的配置文件，个人推荐使用 .eslintrc.js ，这是官方优先级最高的配置文件。 配置文件中配置 Airbnb 有几个重点项： &#123; // 指定校验的环境 'env': &#123; 'browser': true, 'node': true, 'es6': true, 'commonjs': true &#125;, // 指定扩展标准 'extends': ['airbnb'], // 指定解析器 'parser': 'babel-eslint', // 指定插件 'plugins': [ 'import', 'jsx-a11y', 'react' ], 'rules': &#123; ... &#125;&#125; Airbnb 的检查规则是比较严格的，如果你认为有些检查项没有必要，也可以自行在 rules 中修改。 ESLint 命令 为了在 Node.js 上运行 ESLint，你必须先安装 npm。如果还没有安装 npm ，按照这里的说明进行安装：https://www.npmjs.com/。 一旦安装了 npm，运行下面的命令 npm i -g eslint 这句命令从 npm 仓库安装了 ESLint CLI。使用以下格式运行 ESLint： eslint [options] [file|dir|glob]* 比如： eslint file1.js file2.js 或者： eslint lib/** 请注意，传递一个 glob 模式作为参数时，它将由你的 shell 进行扩展。扩展的结果取决于你的 shell 及其配置。如果你想使用 node 的 glob 语法，你需要给参数加上引号（在 windows 系统运行时，如果你需要，也可以使用双引号 ），像下面这样： eslint "lib/**" 选项 命令行工具有几个选项，你可以通过运行 eslint -h 查看所有选项。 eslint [options] file.js [file.js] [dir]Basic configuration: -c, --config path::String Use configuration from this file or shareable config --no-eslintrc Disable use of configuration from .eslintrc --env [String] Specify environments --ext [String] Specify JavaScript file extensions - default: .js --global [String] Define global variables --parser String Specify the parser to be used --parser-options Object Specify parser optionsCaching: --cache Only check changed files - default: false --cache-file path::String Path to the cache file. Deprecated: use --cache-location - default: .eslintcache --cache-location path::String Path to the cache file or directorySpecifying rules and plugins: --rulesdir [path::String] Use additional rules from this directory --plugin [String] Specify plugins --rule Object Specify rulesIgnoring files: --ignore-path path::String Specify path of ignore file --no-ignore Disable use of ignore files and patterns --ignore-pattern [String] Pattern of files to ignore (in addition to those in .eslintignore)Using stdin: --stdin Lint code provided on &lt;STDIN&gt; - default: false --stdin-filename String Specify filename to process STDIN asHandling warnings: --quiet Report errors only - default: false --max-warnings Int Number of warnings to trigger nonzero exit code - default: -1Output: -o, --output-file path::String Specify file to write report to -f, --format String Use a specific output format - default: stylish --color, --no-color Force enabling/disabling of colorMiscellaneous: --init Run config initialization wizard - default: false --fix Automatically fix problems --debug Output debugging information -h, --help Show help -v, --version Output the version number --no-inline-config Prevent comments from changing config or rules --print-config path::String Print the configuration for the given file 这些选项可以通过重复该选项或使用逗号分隔的列表进行指定（除了 --ignore-pattern 不允许第二种风格）。 示例： eslint --ext .jsx --ext .js lib/eslint --ext .jsx,.js lib/ 基本配置 -c, --config 该选项允许你为 ESLint (查看 Configuring ESLint 了解更多)指定一个额外的配置文件。 例如： eslint -c ~/my-eslint.json file.js 这个例子使用了 \~/my-eslint.json 作为配置文件。 它还接受 sharable config 的一个模块的 ID。 例如： eslint -c myconfig file.js 这个例子直接使用可共享的配置 eslint-config-myconfig。 --no-eslintrc 禁用 .eslintrc 和 package.json 文件中的配置。 示例： eslint --no-eslintrc file.js --env 这个选项用来指定环境。关于每种环境中定义的全局变量的详细信息请查看 configuration 文档。该选项只能启用环境，不能禁用在其它配置文件中设置的环境。要指定多个环境的话，使用逗号分隔它们，或多次使用这个选项。 例如： eslint --env browser,node file.jseslint --env browser --env node file.js --ext 这个选项允许你指定 ESLint 在指定的目录下查找 JavaScript 文件时要使用的文件扩展名。默认情况下，它使用 .js 作为唯一性文件扩展名。 示例： ## Use only .js2 extensioneslint . --ext .js2## Use both .js and .js2eslint . --ext .js --ext .js2## Also use both .js and .js2eslint . --ext .js,.js2 **注意：**如果你使用了 glob 模式，则 --ext 被忽略 例如，eslint lib/* --ext .js 将匹配 lib/ 下的所有文件，忽略扩展名。 --global 这个选项定义了全局变量，这样它们就不会被 no-undef 规则标记为未定义了。任何指定的全局变量默认是只读的，在变量名字后加上 :true 后会使它变为可写。要指定多个变量，使用逗号分隔它们，或多次使用这个选项。 示例： eslint --global require,exports:true file.jseslint --global require --global exports:true --parser 该选项允许你为 ESLint 指定一个解析器。默认情况下，使用 espree。 --parser-options 该选项允许你指定 ESLint 要使用的解析器选项。注意，可用的解析器选项取决于你所选用的解析器。 示例： echo '3 ** 4' | eslint --stdin --parser-options=ecmaVersion:6 ## will fail with a parsing errorecho '3 ** 4' | eslint --stdin --parser-options=ecmaVersion:7 ## succeeds, yay! 缓存 --cache 存储处理过的文件的信息以便只对有改变的文件进行操作。缓存默认被存储在 .eslintcache。启用这个选项可以显著改善 ESLint 的运行时间，确保只对有改变的文件进行检测。 **注意：**如果你运行 ESLint --cache，然后又运行 ESLint 不带 --cache，.eslintcache 文件将被删除。这是必要的，因为检测的结果可能会改变，使 .eslintcache 无效。如果你想控制缓存文件何时被删除，那么使用 --cache-location 来指定一个缓存文件的位置。 --cache-file 缓存文件的路径。如果没有指定，则使用 .eslintcache。这个文件会在 eslint 命令行被执行的文件目录中被创建。 已弃用： 请使用 --cache-location。 --cache-location 缓存文件的路径。可以是一个文件或者一个目录。如果没有指定，则使用 .eslintcache 。这个文件会在 eslint 命令行被执行的文件目录中被创建。 如果指定一个目录，缓存文件将在指定的文件夹下被创建。文件名将基于当前工作目录（CWD) 的 hash 值，比如：.cache_hashOfCWD。 **重要提示：**如果不存在缓存文件的目录，请确保在尾部添加 /（*nix 系统）或 \（windows 系统）。否则该路径将被假定为是一个文件。 示例： eslint "src/**/*.js" --cache --cache-location "/Users/user/.eslintcache/" 指定规则和插件 --rulesdir 这个选项允许你指定另一个加载规则文件的目录。这允许你在运行时动态加载新规则。当你有自定义规则，而且这些规则不适合绑定到 ESLint 时，这会很有用。 示例： eslint --rulesdir my-rules/ file.js 为了使你自定义的规则目录下的规则正常工作，必须遵照同绑定的规则一样的格式。你也可以通过包含多个 --rulesdir 选项来为自定义规则指定多个位置。 eslint --rulesdir my-rules/ --rulesdir my-other-rules/ file.js 注意，与核心规则和插件规则一样，你仍需要在配置文件或通过 --rule 命令行选项启用这些规则，以便在检测过程中实际运行这些规则。使用 --rulesdir 指定一个规则目录不会自动启用那些目录下的规则。 --plugin 这个选项指定一个要加载的插件。你可以省略插件名的前缀 eslint-plugin-。在你使用插件直接，你必须使用 npm 安装它。 示例： eslint --plugin jquery file.jseslint --plugin eslint-plugin-mocha file.js --rule 这个选项指定要使用的规则。这些规则将会与配制文件中指定的规则合并。（你可以使用 --no-eslintrc 改变这种行为。）要定义多个规则，使用逗号分隔它们，或多次使用这个选项。levn 格式被用来指定规则。 如果这个规则定义在插件内，你必须在规则 ID 前使用插件名和 /，即 插件名/规则ID。 示例： eslint --rule 'quotes: [2, double]'eslint --rule 'guard-for-in: 2' --rule 'brace-style: [2, 1tbs]'eslint --rule 'jquery/dollar-sign: 2' 忽略文件 --ignore-path 这个选项允许你指定一个文件作为 .eslintignore。默认情况下，ESLint 在当前工作目录下查找 .eslintignore。你可以通过提供另一个文件的路径改变这种行为。 示例： eslint --ignore-path tmp/.eslintignore file.jseslint --ignore-path .gitignore file.js --no-ignore 禁止排除 .eslintignore、--ignore-path 和 --ignore-pattern 文件中指定的文件。 示例： eslint --no-ignore file.js --ignore-pattern 该选项允许你指定要忽略的文件模式(除了那些在 .eslintignore 的)。你可以重复该选项已提供多个模式。语法同 .eslintignore 文件中的相同。你应该将你的模式用引号括起来，以避免命令行解析器的解析。 示例： eslint --ignore-pattern '/lib/' --ignore-pattern '/src/vendor/*' . 使用 stdin --stdin 这个选项告诉 ESLint 从 STDIN 而不是从文件中读取和检测源码。你可以使用该选项向 ESLint 来输入代码。 示例： cat myfile.js | eslint --stdin --stdin-filename 这个选项允许你指定一个文件名去处理 STDIN。当你处理从 STDIN 来的文件和有规则依赖于这个文件名时，这会很有用。 示例： cat myfile.js | eslint --stdin --stdin-filename=myfile.js 处理告警 --quiet 这个选项允许你禁止报告警告。如果开启这个选项，ESLint 只会报告错误。 示例： eslint --quiet file.js --max-warnings 这个选项允许你指定一个警告的阈值，当你的项目中有太多违反规则的警告时，这个阈值被用来强制 ESLint 以错误状态退出。 通常情况下，如果 ESLint 运行过程中，没有出现错误（只有警告），它将以成功的状态退出。然而，如果指定了 --max-warnings，而且警告的总数超过了指定的阈值，ESLint 将以错误的状态退出。通过指定一个 -1 的阈值或省略这个选项将会避免这种行为。 示例： eslint --max-warnings 10 file.js 输出 -o, --output-file 将报告写到一个文件。 示例： eslint -o ./test/test.html 当指定这个选项时，就会按给定的格式输出到指定的文件名。 -f, --format 这个选项指定了控制台的输出格式。可用的格式是： codeframe compact html jslint-xml json junit stylish (the default) table tap unix visualstudio 例如： eslint -f compact file.js 你也可以在命令行中通过指定一个自定义的格式的文件路径来使用自定义的格式。 示例： eslint -f ./customformat.js file.js 当指定之后，给定的格式就输出到控制台。如果你想将输出保存到一个文件，你可以在命令行上这样操作： eslint -f compact file.js &gt; results.txt 这会将输出保存到 results.txt 文件。 --color, --no-color 在管道输出中禁用颜色。 示例： eslint --color file.js | cateslint --no-color file.js 杂项 --init 这个选项将会配置初始化向导。它被用来帮助新用户快速地创建 .eslintrc 文件，用户通过回答一些问题，选择一个流行的风格指南，或检查你的源文件，自动生成一个合适的配置。 生成的配置文件将被创建在当前目录。 --fix 该选项指示 ESLint 试图修复尽可能多的问题。修复只针对实际文件本身，而且剩下的未修复的问题才会输出。不是所有的问题都能使用这个选项进行修复，该选项在以下情形中不起作用： 当代码传递给 ESLint 时，这个选项抛出一个错误。 这个选项对使用处理器的代码不起作用。 --debug 这个选项将调试信息输出到控制台。当你看到一个问题并且很难定位它时，这些调试信息会很有用。ESLint 团队可能会通过询问这些调试信息帮助你解决 bug。 -h, --help 这个选项会输出帮助菜单，显示所有可用的选项。当有这个选项时，忽略其他所有选项。 -v, --version 这个选项在控制台输出当前 ESlint 的版本。当有这个标记时，忽略其他所有标记。 --no-inline-config 这个选项会阻止像 /*eslint-disable*/ 或者 /*global foo*/ 这样的内联注释起作用。这允许你在不修改文件的情况下设置一个 ESLint 配置。所有的内联注释都会被忽略，比如： /*eslint-disable*/ /*eslint-enable*/ /*global*/ /*eslint*/ /*eslint-env*/ // eslint-disable-line // eslint-disable-next-line 示例： eslint --no-inline-config file.js --print-config 这个选项输出传递的文件使用的配置。当有这个标记时，不进行检测，只有配置相关的选项才是有效的。 示例： eslint --print-config file.js .eslintignore 文件 当 ESLint 作用于一个目录时，ESLint 支持使用 .eslintignore 文件来避免检测处理。通过特定的命令行参数指定的文件就可以免除被忽略。.eslintignore 文件是个纯文本文件，每一行都包含一种模式。它可以放在目标目录的任何父级目录；它将影响到它所在的当前目录和所有子目录。这里是 .eslintignore 文件的一个简单示例： node_modules/***/vendor/*.js ESLint 默认忽略的模式分解和目录的更多详细信息可以在 Configuring ESLint 中找到。 更多内容 📦 本文归档在 我的前端技术教程系列：frontend-tutorial ESLint]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringBootTutorialLibLog]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Flib%2Fsbe-lib-log%2F</url>
    <content type="text"><![CDATA[SpringBootTutorialLibLog Spring Boot 支持集成 Java 世界主流的日志库。 如果对于 Java 日志库不熟悉，可以参考：细说 Java 主流日志工具库 关键词： log4j, log4j2, logback, slf4j 日志格式 控制台输出 彩色打印 文件输出 日志级别 日志组 日志配置文件 Logback 扩展 profile 指定配置 环境属性 Spring Boot 中的日志配置 源码 更多内容 Spring Boot 内部日志全部使用 Commons Logging 记录，但保留底层日志实现。为 Java Util Logging，Log4J2，和 Logback 提供了默认配置。在每种情况下，记录器都预先配置为使用控制台输出，并且还提供可选的文件输出。 默认情况下，如果使用“Starters”，则使用 Logback 进行日志记录。还包括适当的 Logback 路由，以确保使用 Java Util Logging，Commons Logging，Log4J 或 SLF4J 的依赖库都能正常工作。 日志格式 Spring Boot 日志默认格式类似下面的形式： 2014-03-05 10:57:51.112 INFO 45469 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet Engine: Apache Tomcat/7.0.522014-03-05 10:57:51.253 INFO 45469 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext2014-03-05 10:57:51.253 INFO 45469 --- [ost-startStop-1] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 1358 ms2014-03-05 10:57:51.698 INFO 45469 --- [ost-startStop-1] o.s.b.c.e.ServletRegistrationBean : Mapping servlet: 'dispatcherServlet' to [/]2014-03-05 10:57:51.702 INFO 45469 --- [ost-startStop-1] o.s.b.c.embedded.FilterRegistrationBean : Mapping filter: 'hiddenHttpMethodFilter' to: [/*] 说明： 日期和时间：精确到微秒 日志级别：ERROR, WARN, INFO, DEBUG, or TRACE. 进程 ID --- 分隔符后面是实际的日志内容 线程名 日志名 日志内容 控制台输出 Spring Boot 默认打印信息到控制台，并且仅打印ERROR, WARN, INFO 级别信息。 如果你想打印 debug 级别信息，可以设置 jar 启动参数，如下： $ java -jar myapp.jar --debug 此外，也可以在 application.properties 中设置 debug = true 。 打印 trace 级别信息同上所示。 彩色打印 如果您的终端支持 ANSI，可以使用彩色打印来提高可读性。您可以将 spring.output.ansi.enabled 设置为支持的值以覆盖自动检测。 使用 ％clr 转换字配置颜色编码。在最简单的形式中，转换器根据日志级别对输出进行着色，如以下示例所示： %clr(%5p)%clr(%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;)&#123;yellow&#125; 支持以下的颜色和样式： blue cyan faint green magenta red yellow 文件输出 默认情况下，Spring Boot 仅记录到控制台，不会写入日志文件。如果除了控制台输出之外还要编写日志文件，则需要设置 logging.file 或 logging.path 属性（例如，在 application.properties 中）。 详细配置参考：配置 日志级别 所有支持的日志系统都可以 在 Spring 环境中通过 logging.level.&lt;logger-name&gt;=&lt;level&gt; 属性设置日志级别（例如，在 application.properties 中）。其中 level 是 TRACE、DEBUG、INFO、WARN 、ERROR、FATAL 或 OFF。可以使用 logging.level.root 配置根记录器。 示例： logging.level.root=WARNlogging.level.org.springframework.web=DEBUGlogging.level.org.hibernate=ERROR 日志组 能够将相关记录器组合在一起以便可以同时配置它们通常很有用。例如，您可以更改所有 Tomcat 相关记录器的日志记录级别，但您无法轻松记住顶级软件包。 Spring Boot 通过 logging.group 属性来提供这样的支持。 logging.group.tomcat=org.apache.catalina, org.apache.coyote, org.apache.tomcatlogging.level.tomcat=TRACE 以下是 Spring Boot 预设的日志组： 名称 Loggers web org.springframework.core.codec, org.springframework.http, org.springframework.web sql org.springframework.jdbc.core, org.hibernate.SQL 日志配置文件 可以通过在 classpath 中包含适当的库来激活各种日志记录系统，并且可以通过在 classpath 的根目录中或在以下 Spring Environment 属性指定的位置提供合适的配置文件来进一步自定义：logging.config。 您可以使用 org.springframework.boot.logging.LoggingSystem 系统属性强制 Spring Boot 使用特定的日志记录系统。该值应该是 LoggingSystem 实现的完全限定类名。您还可以使用 none 值完全禁用 Spring Boot 的日志记录配置。 由于在创建 ApplicationContext 之前初始化日志记录，因此无法在 Spring @Configuration 文件中控制来自 @PropertySources 的日志记录。更改日志记录系统或完全禁用它的唯一方法是通过系统属性。 Logback 扩展 profile 指定配置 可以通过 &lt;springProfile&gt; 指定特定的 profile 下的配置，如下： &lt;springProfile name="staging"&gt; &lt;!-- configuration to be enabled when the "staging" profile is active --&gt;&lt;/springProfile&gt;&lt;springProfile name="dev | staging"&gt; &lt;!-- configuration to be enabled when the "dev" or "staging" profiles are active --&gt;&lt;/springProfile&gt;&lt;springProfile name="!production"&gt; &lt;!-- configuration to be enabled when the "production" profile is not active --&gt;&lt;/springProfile&gt; 环境属性 &lt;springProperty&gt; 标签允许指定从 Environment 中获取的属性，并在配置文件中引用。 &lt;springProperty scope="context" name="fluentHost" source="myapp.fluentd.host" defaultValue="localhost"/&gt;&lt;appender name="FLUENT" class="ch.qos.logback.more.appenders.DataFluentAppender"&gt; &lt;remoteHost&gt;$&#123;fluentHost&#125;&lt;/remoteHost&gt; ...&lt;/appender&gt; Spring Boot 中的日志配置 logging.config= # Location of the logging configuration file. For instance, `classpath:logback.xml` for Logback.logging.exception-conversion-word=%wEx # Conversion word used when logging exceptions.logging.file= # Log file name (for instance, `myapp.log`). Names can be an exact location or relative to the current directory.logging.file.max-history=0 # Maximum of archive log files to keep. Only supported with the default logback setup.logging.file.max-size=10MB # Maximum log file size. Only supported with the default logback setup.logging.group.*= # Log groups to quickly change multiple loggers at the same time. For instance, `logging.level.db=org.hibernate,org.springframework.jdbc`.logging.level.*= # Log levels severity mapping. For instance, `logging.level.org.springframework=DEBUG`.logging.path= # Location of the log file. For instance, `/var/log`.logging.pattern.console= # Appender pattern for output to the console. Supported only with the default Logback setup.logging.pattern.dateformat=yyyy-MM-dd HH:mm:ss.SSS # Appender pattern for log date format. Supported only with the default Logback setup.logging.pattern.file= # Appender pattern for output to a file. Supported only with the default Logback setup.logging.pattern.level=%5p # Appender pattern for log level. Supported only with the default Logback setup.logging.register-shutdown-hook=false # Register a shutdown hook for the logging system when it is initialized. 注： 日志配置属性在应用程序生命周期的早期初始化。因此，通过 @PropertySource 注释加载的属性文件中找不到日志记录属性。 日志配置属性独立于实际的日志记录基础结构。因此，spring Boot 不管理特定的配置密钥（例如 Logback 的 logback.configurationFile）。 源码 完整示例：源码 分别展示如何在 Spring Boot 中使用 log4j, log4j2, logback 记录日志。 更多内容 引申 细说 Java 主流日志工具库 Spring Boot 教程 引用 Spring Boot 官方文档之 boot-features-logging]]></content>
  </entry>
  <entry>
    <title><![CDATA[堆]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Falgorithm%2Fdata-structure%2Fheap%2F</url>
    <content type="text"><![CDATA[堆 堆是一种特殊的基于树的满足某些特性的数据结构，整个堆中的所有父子节点的键值都会满足相同的排序条件。堆更准确地可以分为最大堆与最小堆，在最大堆中，父节点的键值永远大于或者等于子节点的值，并且整个堆中的最大值存储于根节点；而最小堆中，父节点的键值永远小于或者等于其子节点的键值，并且整个堆中的最小值存储于根节点。 时间复杂度: 访问最大值 / 最小值: O(1) 插入: O(log(n)) 移除最大值 / 最小值: O(log(n))]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBootTutorialWebUIEazyUI]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Fweb%2Fsbe-web-ui-eazyui%2F</url>
    <content type="text"><![CDATA[SpringBootTutorialWebUIEazyUI EazyUI 是一个简单的用户界面组件的集合。由于 EazyUI 已经封装好大部分 UI 基本功能，能帮用户减少大量的 js 和 css 代码。所以，EazyUI 非常适合用于开发简单的系统或原型系统。 本文示例使用技术点： Spring Boot：主要使用了 spring-boot-starter-web、spring-boot-starter-data-jpa EazyUI：按需加载，并没有引入所有的 EazyUI 特性 数据库：为了测试方便，使用 H2 简介 什么是 EasyUI？ Spring Boot 整合 EazyUI 配置 引入 eazyui 实战 引入 maven 依赖 使用 JPA 使用 Web 使用 EazyUI 完整示例 引用和引申 简介 什么是 EasyUI？ easyui 是基于 jQuery、Angular.、Vue 和 React 的用户界面组件的集合。 easyui 提供了构建现代交互式 javascript 应用程序的基本功能。 使用 easyui，您不需要编写许多 javascript 代码，通常通过编写一些 HTML 标记来定义用户界面。 完整的 HTML5 网页框架。 使用 easyui 开发你的产品时可以大量节省你的时间和规模。 easyui 使用非常简单但功能非常强大。 Spring Boot 整合 EazyUI 配置 application.properties 修改： spring.mvc.view.prefix = /views/spring.mvc.view.suffix = .html 引入 eazyui EazyUI 下载地址：http://www.jeasyui.cn/download.html 在 src/main/resources/static 目录下引入 eazyui。 然后在 html 中引用： &lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset="UTF-8" /&gt; &lt;link rel="stylesheet" type="text/css" href="../lib/easyui/themes/bootstrap/easyui.css" /&gt; &lt;link rel="stylesheet" type="text/css" href="../lib/easyui/themes/icon.css" /&gt; &lt;link rel="stylesheet" type="text/css" href="../lib/easyui/themes/color.css" /&gt; &lt;script type="text/javascript" src="../lib/easyui/jquery.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="../lib/easyui/jquery.easyui.min.js" &gt;&lt;/script&gt; &lt;script type="text/javascript" src="../lib/easyui/locale/easyui-lang-zh_CN.js" &gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;!-- 省略 --&gt; &lt;/body&gt;&lt;/html&gt; 引入 eazyui 后，需要使用哪种组件，可以查看相关文档或 API，十分简单，此处不一一赘述。 实战 引入 maven 依赖 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-collections&lt;/groupId&gt; &lt;artifactId&gt;commons-collections&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 使用 JPA 为了使用 JPA 技术访问数据，我们需要定义 Entity 和 Repository 定义一个 Entity： @Entitypublic class User &#123; @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; private String firstName; private String lastName; private String phone; private String email; protected User() &#123;&#125; public User(String firstName, String lastName, String phone, String email) &#123; this.firstName = firstName; this.lastName = lastName; this.phone = phone; this.email = email; &#125; // 略 getter/setter&#125; 定义一个 Repository： public interface UserRepository extends CrudRepository&lt;User, Long&gt; &#123; List&lt;User&gt; findByLastName(String lastName);&#125; 使用 Web 首页 Controller，将 web 请求定向到指定页面（下面的例子定向到 index.html） @Controllerpublic class IndexController &#123; @RequestMapping(value = &#123;"", "/", "index"&#125;) public String index() &#123; return "index"; &#125;&#125; 此外，需要定义一个 Controller，提供后台的 API 接口 @Controllerpublic class UserController &#123; @Autowired private UserRepository customerRepository; @RequestMapping(value = "/user", method = RequestMethod.GET) public String user() &#123; return "user"; &#125; @ResponseBody @RequestMapping(value = "/user/list") public ResponseDTO&lt;User&gt; list() &#123; Iterable&lt;User&gt; all = customerRepository.findAll(); List&lt;User&gt; list = IteratorUtils.toList(all.iterator()); return new ResponseDTO&lt;&gt;(true, list.size(), list); &#125; @ResponseBody @RequestMapping(value = "/user/add") public ResponseDTO&lt;User&gt; add(User user) &#123; User result = customerRepository.save(user); List&lt;User&gt; list = new ArrayList&lt;&gt;(); list.add(result); return new ResponseDTO&lt;&gt;(true, 1, list); &#125; @ResponseBody @RequestMapping(value = "/user/save") public ResponseDTO&lt;User&gt; save(@RequestParam("id") Long id, User user) &#123; user.setId(id); customerRepository.save(user); List&lt;User&gt; list = new ArrayList&lt;&gt;(); list.add(user); return new ResponseDTO&lt;&gt;(true, 1, list); &#125; @ResponseBody @RequestMapping(value = "/user/delete") public ResponseDTO delete(@RequestParam("id") Long id) &#123; customerRepository.deleteById(id); return new ResponseDTO&lt;&gt;(true, null, null); &#125;&#125; 使用 EazyUI 接下来，我们要使用前面定义的后台接口，仅需要在 EazyUI API 中指定 url 即可。 请留意下面示例中的 url 字段，和实际接口是一一对应的。 &lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Complex Layout - jQuery EasyUI Demo&lt;/title&gt; &lt;meta charset="UTF-8" /&gt; &lt;link rel="stylesheet" type="text/css" href="../lib/easyui/themes/bootstrap/easyui.css" /&gt; &lt;link rel="stylesheet" type="text/css" href="../lib/easyui/themes/icon.css" /&gt; &lt;link rel="stylesheet" type="text/css" href="../lib/easyui/themes/color.css" /&gt; &lt;script type="text/javascript" src="../lib/easyui/jquery.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="../lib/easyui/jquery.easyui.min.js" &gt;&lt;/script&gt; &lt;script type="text/javascript" src="../lib/easyui/locale/easyui-lang-zh_CN.js" &gt;&lt;/script&gt; &lt;style type="text/css"&gt; body &#123; font-family: microsoft yahei; &#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div style="width:100%"&gt; &lt;h2&gt;基本的 CRUD 应用&lt;/h2&gt; &lt;p&gt;数据来源于后台系统&lt;/p&gt; &lt;table id="dg" title="Custom List" class="easyui-datagrid" url="/user/list" toolbar="#toolbar" pagination="true" rownumbers="true" fitColumns="true" singleSelect="true" &gt; &lt;thead&gt; &lt;tr&gt; &lt;th field="id" width="50"&gt;ID&lt;/th&gt; &lt;th field="firstName" width="50"&gt;First Name&lt;/th&gt; &lt;th field="lastName" width="50"&gt;Last Name&lt;/th&gt; &lt;th field="phone" width="50"&gt;Phone&lt;/th&gt; &lt;th field="email" width="50"&gt;Email&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;/table&gt; &lt;div id="toolbar"&gt; &lt;a href="javascript:void(0)" class="easyui-linkbutton" iconCls="icon-add" plain="true" onclick="newUser()" &gt;添加&lt;/a &gt; &lt;a href="javascript:void(0)" class="easyui-linkbutton" iconCls="icon-edit" plain="true" onclick="editUser()" &gt;修改&lt;/a &gt; &lt;a href="javascript:void(0)" class="easyui-linkbutton" iconCls="icon-remove" plain="true" onclick="destroyUser()" &gt;删除&lt;/a &gt; &lt;/div&gt; &lt;div id="dlg" class="easyui-dialog" style="width:400px" data-options="closed:true,modal:true,border:'thin',buttons:'#dlg-buttons'" &gt; &lt;form id="fm" method="post" novalidate style="margin:0;padding:20px 50px" &gt; &lt;h3&gt;User Information&lt;/h3&gt; &lt;div style="margin-bottom:10px"&gt; &lt;input name="firstName" class="easyui-textbox" required="true" label="First Name:" style="width:100%" /&gt; &lt;/div&gt; &lt;div style="margin-bottom:10px"&gt; &lt;input name="lastName" class="easyui-textbox" required="true" label="Last Name:" style="width:100%" /&gt; &lt;/div&gt; &lt;div style="margin-bottom:10px"&gt; &lt;input name="phone" class="easyui-textbox" required="true" label="Phone:" style="width:100%" /&gt; &lt;/div&gt; &lt;div style="margin-bottom:10px"&gt; &lt;input name="email" class="easyui-textbox" required="true" validType="email" label="Email:" style="width:100%" /&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;div id="dlg-buttons"&gt; &lt;a href="javascript:void(0)" class="easyui-linkbutton c6" iconCls="icon-ok" onclick="saveUser()" style="width:90px" &gt;Save&lt;/a &gt; &lt;a href="javascript:void(0)" class="easyui-linkbutton" iconCls="icon-cancel" onclick="javascript:$('#dlg').dialog('close')" style="width:90px" &gt;Cancel&lt;/a &gt; &lt;/div&gt; &lt;/div&gt; &lt;script type="text/javascript"&gt; var url; function newUser() &#123; $('#dlg') .dialog('open') .dialog('center') .dialog('setTitle', 'New User'); $('#fm').form('clear'); url = '/user/add'; &#125; function editUser() &#123; var row = $('#dg').datagrid('getSelected'); if (row) &#123; $('#dlg') .dialog('open') .dialog('center') .dialog('setTitle', 'Edit User'); $('#fm').form('load', row); url = '/user/save'; &#125; &#125; function saveUser() &#123; $('#fm').form('submit', &#123; url: url, onSubmit: function() &#123; return $(this).form('validate'); &#125;, success: function(result) &#123; var result = eval('(' + result + ')'); if (result.errorMsg) &#123; $.messager.show(&#123; title: 'Error', msg: result.errorMsg &#125;); &#125; else &#123; $('#dlg').dialog('close'); // close the dialog $('#dg').datagrid('reload'); // reload the user data &#125; &#125; &#125;); &#125; function destroyUser() &#123; var row = $('#dg').datagrid('getSelected'); if (row) &#123; $.messager.confirm( 'Confirm', 'Are you sure you want to destroy this user?', function(r) &#123; if (r) &#123; $.post( '/user/delete', &#123; id: row.id &#125;, function(result) &#123; if (result.success) &#123; $('#dg').datagrid('reload'); // reload the user data &#125; else &#123; $.messager.show(&#123; // show error message title: 'Error', msg: result.errorMsg &#125;); &#125; &#125;, 'json' ); &#125; &#125; ); &#125; &#125; &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 完整示例 请参考 源码 运行方式： mvn clean package -DskipTests=truejava -jar target/ 在浏览器中访问：http://localhost:8080/ 引用和引申 EasyUI 官网 EazyUI 中文网]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringBootTutorialWebJson]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Fweb%2Fsbe-web-json%2F</url>
    <content type="text"><![CDATA[SpringBootTutorialWebJson 简介 Spring Boot 支持的 Json 库 Spring Web 中的序列化、反序列化 指定类的 Json 序列化、反序列化 @JsonTest Spring Boot 中的 json 配置 Jackson 配置 GSON 配置 Spring Boot 中使用 Fastjson 示例源码 更多内容 简介 Spring Boot 支持的 Json 库 Spring Boot 支持三种 Json 库： Gson Jackson JSON-B Jackson 是 Spring Boot 官方推荐的默认库。 Spring Boot 提供了 Jackson 的自动配置，Jackson 是 spring-boot-starter-json 的一部分。当 Jackson 在类路径上时，会自动配置 ObjectMapper bean。 Spring Boot 提供了 Gson 的自动配置。当 Gson 在 classpath 上时，会自动配置 Gson bean。提供了几个 spring.gson.* 配置属性来自定义配置。为了获得更多控制，可以使用一个或多个 GsonBuilderCustomizer bean。 Spring Boot 提供了 JSON-B 的自动配置。当 JSON-B API 在 classpath 上时，将自动配置 Jsonb bean。首选的 JSON-B 实现是 Apache Johnzon，它提供了依赖关系管理。 Spring Web 中的序列化、反序列化 以下注解都是 spring-web 中提供的支持。 @ResponseBody @Responsebody 注解用于将 Controller 的方法返回的对象，通过适当的 HttpMessageConverter 转换为指定格式后，写入到 HTTP Response 对象的 body 数据区。一般在异步获取数据时使用。通常是在使用 @RequestMapping 后，返回值通常解析为跳转路径，加上 @Responsebody 后返回结果不会被解析为跳转路径，而是直接写入 HTTP 响应正文中。 示例： @ResponseBody@RequestMapping(name = "/getInfo", method = RequestMethod.GET)public InfoDTO getInfo() &#123; return new InfoDTO();&#125; @RequestBody @RequestBody 注解用于读取 HTTP Request 请求的 body 部分数据，使用系统默认配置的 HttpMessageConverter 进行解析，然后把相应的数据绑定到要返回的对象上；再把 HttpMessageConverter 返回的对象数据绑定到 controller 中方法的参数上。 request 的 body 部分的数据编码格式由 header 部分的 Content-Type 指定。 示例： @RequestMapping(name = "/postInfo", method = RequestMethod.POST)public void postInfo(@RequestBody InfoDTO infoDTO) &#123; // ...&#125; @RestController Spring 4 以前： 如果需要返回到指定页面，则需要用 @Controller 配合视图解析器 InternalResourceViewResolver 。 如果需要返回 JSON，XML 或自定义 mediaType 内容到页面，则需要在对应的方法上加上 @ResponseBody 注解。 Spring 4 以后，新增了 @RestController 注解： 它相当于 @Controller + @RequestBody 。 如果使用 @RestController 注解 Controller，则 Controller 中的方法无法返回 jsp 页面，或者 html，配置的视图解析器 InternalResourceViewResolver 将不起作用，直接返回内容。 指定类的 Json 序列化、反序列化 如果使用 Jackson 序列化和反序列化 JSON 数据，您可能需要编写自己的 JsonSerializer 和 JsonDeserializer 类。自定义序列化程序通常通过模块向 Jackson 注册，但 Spring Boot 提供了另一种 @JsonComponent 注释，可以更容易地直接注册 Spring Beans。 您可以直接在 JsonSerializer 或 JsonDeserializer 实现上使用 @JsonComponent 注释。您还可以在包含序列化程序/反序列化程序作为内部类的类上使用它，如以下示例所示： import java.io.*;import com.fasterxml.jackson.core.*;import com.fasterxml.jackson.databind.*;import org.springframework.boot.jackson.*;@JsonComponentpublic class Example &#123; public static class Serializer extends JsonSerializer&lt;SomeObject&gt; &#123; // ... &#125; public static class Deserializer extends JsonDeserializer&lt;SomeObject&gt; &#123; // ... &#125;&#125; ApplicationContext 中的所有 @JsonComponent bean 都会自动注册到 Jackson。因为 @JsonComponent 是使用 @Component 进行元注释的，所以通常的组件扫描规则适用。 Spring Boot 还提供了 JsonObjectSerializer 和 JsonObjectDeserializer 基类，它们在序列化对象时提供了标准 Jackson 版本的有用替代方法。有关详细信息，请参阅 Javadoc 中的 JsonObjectSerializer 和 JsonObjectDeserializer。 @JsonTest 使用 @JsonTest 可以很方便的在 Spring Boot 中测试序列化、反序列化。 使用 @JsonTest 相当于使用以下自动配置： org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration org.springframework.boot.autoconfigure.gson.GsonAutoConfiguration org.springframework.boot.autoconfigure.jackson.JacksonAutoConfiguration org.springframework.boot.autoconfigure.jsonb.JsonbAutoConfiguration org.springframework.boot.test.autoconfigure.json.JsonTestersAutoConfiguration @JsonTest 使用示例： 想试试完整示例，可以参考：源码 @JsonTest@RunWith(SpringRunner.class)public class SimpleJsonTest &#123; private final Logger log = LoggerFactory.getLogger(this.getClass()); @Autowired private JacksonTester&lt;InfoDTO&gt; json; @Test public void testSerialize() throws Exception &#123; SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); InfoDTO infoDTO = new InfoDTO("JSON测试应用", "1.0.0", sdf.parse("2019-01-01 12:00:00")); JsonContent&lt;InfoDTO&gt; jsonContent = json.write(infoDTO); log.info("json content: &#123;&#125;", jsonContent.getJson()); // 或者使用基于JSON path的校验 assertThat(jsonContent).hasJsonPathStringValue("@.appName"); assertThat(jsonContent).extractingJsonPathStringValue("@.appName").isEqualTo("JSON测试应用"); assertThat(jsonContent).hasJsonPathStringValue("@.version"); assertThat(jsonContent).extractingJsonPathStringValue("@.version").isEqualTo("1.0.0"); assertThat(jsonContent).hasJsonPathStringValue("@.date"); assertThat(jsonContent).extractingJsonPathStringValue("@.date").isEqualTo("2019-01-01 12:00:00"); &#125; @Test public void testDeserialize() throws Exception &#123; String content = "&#123;\"appName\":\"JSON测试应用\",\"version\":\"1.0.0\",\"date\":\"2019-01-01\"&#125;"; InfoDTO actual = json.parseObject(content); assertThat(actual.getAppName()).isEqualTo("JSON测试应用"); assertThat(actual.getVersion()).isEqualTo("1.0.0"); &#125;&#125; Spring Boot 中的 json 配置 Jackson 配置 当 Spring Boot 的 json 库为 jackson 时，可以使用以下配置属性（对应 JacksonProperties 类）： spring.jackson.date-format= # Date format string or a fully-qualified date format class name. For instance, `yyyy-MM-dd HH:mm:ss`.spring.jackson.default-property-inclusion= # Controls the inclusion of properties during serialization. Configured with one of the values in Jackson's JsonInclude.Include enumeration.spring.jackson.deserialization.*= # Jackson on/off features that affect the way Java objects are deserialized.spring.jackson.generator.*= # Jackson on/off features for generators.spring.jackson.joda-date-time-format= # Joda date time format string. If not configured, "date-format" is used as a fallback if it is configured with a format string.spring.jackson.locale= # Locale used for formatting.spring.jackson.mapper.*= # Jackson general purpose on/off features.spring.jackson.parser.*= # Jackson on/off features for parsers.spring.jackson.property-naming-strategy= # One of the constants on Jackson's PropertyNamingStrategy. Can also be a fully-qualified class name of a PropertyNamingStrategy subclass.spring.jackson.serialization.*= # Jackson on/off features that affect the way Java objects are serialized.spring.jackson.time-zone= # Time zone used when formatting dates. For instance, "America/Los_Angeles" or "GMT+10".spring.jackson.visibility.*= # Jackson visibility thresholds that can be used to limit which methods (and fields) are auto-detected. GSON 配置 当 Spring Boot 的 json 库为 gson 时，可以使用以下配置属性（对应 GsonProperties 类）： spring.gson.date-format= # Format to use when serializing Date objects.spring.gson.disable-html-escaping= # Whether to disable the escaping of HTML characters such as '&lt;', '&gt;', etc.spring.gson.disable-inner-class-serialization= # Whether to exclude inner classes during serialization.spring.gson.enable-complex-map-key-serialization= # Whether to enable serialization of complex map keys (i.e. non-primitives).spring.gson.exclude-fields-without-expose-annotation= # Whether to exclude all fields from consideration for serialization or deserialization that do not have the "Expose" annotation.spring.gson.field-naming-policy= # Naming policy that should be applied to an object's field during serialization and deserialization.spring.gson.generate-non-executable-json= # Whether to generate non executable JSON by prefixing the output with some special text.spring.gson.lenient= # Whether to be lenient about parsing JSON that doesn't conform to RFC 4627.spring.gson.long-serialization-policy= # Serialization policy for Long and long types.spring.gson.pretty-printing= # Whether to output serialized JSON that fits in a page for pretty printing.spring.gson.serialize-nulls= # Whether to serialize null fields. Spring Boot 中使用 Fastjson 国内很多的 Java 程序员更喜欢使用阿里的 fastjson 作为 json lib。那么，如何在 Spring Boot 中将其替换默认的 jackson 库呢？ 你需要做如下处理： （1）引入 fastjson jar 包： &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.54&lt;/version&gt;&lt;/dependency&gt; （2）实现 WebMvcConfigurer 接口，自定义 configureMessageConverters 接口。如下所示： @Configurationpublic class WebMvcConfig implements WebMvcConfigurer &#123; private final Logger log = LoggerFactory.getLogger(this.getClass()); /** * 自定义消息转换器 * @param converters */ @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; // 清除默认 Json 转换器 converters.removeIf(converter -&gt; converter instanceof MappingJackson2HttpMessageConverter); // 配置 FastJson FastJsonConfig config = new FastJsonConfig(); config.setSerializerFeatures(SerializerFeature.QuoteFieldNames, SerializerFeature.WriteEnumUsingToString, SerializerFeature.WriteMapNullValue, SerializerFeature.WriteDateUseDateFormat, SerializerFeature.DisableCircularReferenceDetect); // 添加 FastJsonHttpMessageConverter FastJsonHttpMessageConverter fastJsonHttpMessageConverter = new FastJsonHttpMessageConverter(); fastJsonHttpMessageConverter.setFastJsonConfig(config); List&lt;MediaType&gt; fastMediaTypes = new ArrayList&lt;&gt;(); fastMediaTypes.add(MediaType.APPLICATION_JSON_UTF8); fastJsonHttpMessageConverter.setSupportedMediaTypes(fastMediaTypes); converters.add(fastJsonHttpMessageConverter); // 添加 StringHttpMessageConverter，解决中文乱码问题 StringHttpMessageConverter stringHttpMessageConverter = new StringHttpMessageConverter(Charset.forName("UTF-8")); converters.add(stringHttpMessageConverter); &#125; // ...&#125; 示例源码 完整示例：源码 更多内容 引申 Spring Boot 教程 引用 Spring Boot 官方文档之 boot-features-json]]></content>
  </entry>
  <entry>
    <title><![CDATA[Cassandra]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fnosql%2FCassandra%2F</url>
    <content type="text"><![CDATA[Cassandra Apache Cassandra 是一个高度可扩展的分区行存储。行被组织成具有所需主键的表。 最新版本：v4.0 Quick Start 简介 更多内容 Quick Start 安装 先决条件 JDK8+ Python 2.7 简介 Apache Cassandra 是一套开源分布式 Key-Value 存储系统。它最初由 Facebook 开发，用于储存特别大的数据。 特性 主要特性 分布式 基于 column 的结构化 高伸展性 Cassandra 的主要特点就是它不是一个数据库，而是由一堆数据库节点共同构成的一个分布式网络服务，对 Cassandra 的一个写操作，会被复制到其他节点上去，对 Cassandra 的读操作，也会被路由到某个节点上面去读取。对于一个 Cassandra 群集来说，扩展性能 是比较简单的事情，只管在群集里面添加节点就可以了。 突出特性 模式灵活 - 使用 Cassandra，像文档存储，不必提前解决记录中的字段。你可以在系统运行时随意的添加或移除字段。这是一个惊人的效率提升，特别是在大型部署上。 真正的可扩展性 - Cassandra 是纯粹意义上的水平扩展。为给集群添加更多容量，可以指向另一台电脑。你不必重启任何进程，改变应用查询，或手动迁移任何数据。 多数据中心识别 - 你可以调整你的节点布局来避免某一个数据中心起火，一个备用的数据中心将至少有每条记录的完全复制。 范围查询 - 如果你不喜欢全部的键值查询，则可以设置键的范围来查询。 列表数据结构 - 在混合模式可以将超级列添加到 5 维。对于每个用户的索引，这是非常方便的。 分布式写操作 - 有可以在任何地方任何时间集中读或写任何数据。并且不会有任何单点失败。 更多内容 Cassandra 官网 Cassandra Github]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux 命令行]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fcli%2FREADME%2F</url>
    <content type="text"><![CDATA[Linux 命令行 📝 知识点 根据应用场景，将常见 Linux 命令分门别类的一一介绍。 如果想快速学习，推荐参考这篇文章：命令行的艺术（转载） 查看 Linux 命令帮助信息 - 关键词：help, whatis, info, which, whereis, man Linux 文件目录管理 - 关键词：cd, ls, pwd, mkdir, rmdir, tree, touch, ln, rename, stat, file, chmod, chown, locate, find, cp, mv, rm Linux 文件内容查看命令 - 关键词：cat, head, tail, more, less, sed, vi, grep Linux 文件压缩和解压 - 关键词：tar, gzip, zip, unzip Linux 用户管理 - 关键词：groupadd, groupdel, groupmod, useradd, userdel, usermod, passwd, su, sudo Linux 系统管理 - 关键词：reboot, exit, shutdown, date, mount, umount, ps, kill, systemctl, service, crontab Linux 网络管理 - 关键词：关键词：curl, wget, telnet, ip, hostname, ifconfig, route, ssh, ssh-keygen, firewalld, iptables, host, nslookup, nc/netcat, ping, traceroute, netstat Linux 硬件管理 - 关键词：df, du, top, free, iotop Linux 软件管理 - 关键词：rpm, yum, apt-get]]></content>
  </entry>
  <entry>
    <title><![CDATA[目录规范]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fmethod%2Fdir-style%2F</url>
    <content type="text"><![CDATA[目录规范 1. 目录结构规范 2. 文件管理软件 2.1. Clover 2.2. Everything 2.3. Wox 2.4. Q-dir 1. 目录结构规范 作为程序员，想必每个人都会有大量的资料、数据。按照条理清晰的目录结构去分类化存储，十分有助于管理文件。 以下是我个人整理的目录结构： .├── Codes #代码目录│ ├── Other #第三方代码目录│ ├── My #个人代码目录│ └── Work #工作代码目录├── Data #数据目录├── Downloads #下载文件目录├── Docs #文档目录│ ├── Books #电子书目录│ ├── My #个人文档目录│ └── Work #工作文档目录├── Movies #视频目录├── Music #音乐目录├── Pictures #图片目录├── Public #共享目录├── Temp #临时文件目录└── Tools #工具软件目录 └── Packages #安装包目录 注：如果您使用的操作系统是 Mac 这种可以为目录或文件添加 tag 的操作系统，那么您可以根据自己的喜好更细致化的管理。 2. 文件管理软件 选用便利的文件管理软件，可以让你的文件管理如虎添翼。这里推荐几款经典的文件管理工具。 2.1. Clover Clover 是 Windows Explorer 资源管理器的一个扩展，为其增加类似谷歌 Chrome 浏览器的多标签页功能。 2.2. Everything Everything 可以立即在 windows 系统中找到制定名称的文件和文件夹。 2.3. Wox Wox 是一款简单易用的 Windows 启动器。可以把它视为 windows 版的 Alfred。 2.4. Q-dir Q-dir 是轻量的文件管理器,特点鲜明,各种布局视图切换灵活,默认四个小窗口组成一个大窗口,操作快捷。]]></content>
      <categories>
        <category>method</category>
      </categories>
      <tags>
        <tag>method</tag>
        <tag>doc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PostgreSQL 快速指南]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Fpostgresql%2F</url>
    <content type="text"><![CDATA[PostgreSQL 快速指南 PostgreSQL 是一个关系型数据库（RDBM）。 关键词：Database, RDBM, psql 安装 添加新用户和新数据库 登录数据库 控制台命令 数据库操作 备份和恢复 参考资料 安装 本文仅以运行在 Centos 环境下举例。 进入官方下载页面，根据操作系统选择合适版本。 官方下载页面要求用户选择相应版本，然后动态的给出安装提示，如下图所示： 前 3 步要求用户选择，后 4 步是根据选择动态提示的安装步骤 （1）选择 PostgreSQL 版本 （2）选择平台 （3）选择架构 （4）安装 PostgreSQL 的 rpm 仓库（为了识别下载源） yum install https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm （5）安装客户端 yum install postgresql10 （6）安装服务端（可选的） yum install postgresql10-server （7）设置开机启动（可选的） /usr/pgsql-10/bin/postgresql-10-setup initdbsystemctl enable postgresql-10systemctl start postgresql-10 添加新用户和新数据库 初次安装后，默认生成一个名为 postgres 的数据库和一个名为 postgres 的数据库用户。这里需要注意的是，同时还生成了一个名为 postgres 的 Linux 系统用户。 首先，新建一个 Linux 新用户，可以取你想要的名字，这里为 dbuser。 sudo adduser dbuser 使用 psql 命令登录 PostgreSQL 控制台： sudo -u postgres psql 这时相当于系统用户 postgres 以同名数据库用户的身份，登录数据库，这是不用输入密码的。如果一切正常，系统提示符会变为&quot;postgres=#&quot;，表示这时已经进入了数据库控制台。以下的命令都在控制台内完成。 （1）使用 \password 命令，为 postgres 用户设置一个密码。 postgres=# \password postgres （2）创建数据库用户 dbuser（刚才创建的是 Linux 系统用户），并设置密码。 CREATE USER dbuser WITH PASSWORD 'password'; （3）创建用户数据库，这里为 exampledb，并指定所有者为 dbuser。 CREATE DATABASE exampledb OWNER dbuser; （4）将 exampledb 数据库的所有权限都赋予 dbuser，否则 dbuser 只能登录控制台，没有任何数据库操作权限。 GRANT ALL PRIVILEGES ON DATABASE exampledb to dbuser; （5）使用\q 命令退出控制台（也可以直接按 ctrl+D）。 登录数据库 添加新用户和新数据库以后，就要以新用户的名义登录数据库，这时使用的是 psql 命令。 psql -U dbuser -d exampledb -h 127.0.0.1 -p 5432 上面命令的参数含义如下：-U 指定用户，-d 指定数据库，-h 指定服务器，-p 指定端口。 输入上面命令以后，系统会提示输入 dbuser 用户的密码。输入正确，就可以登录控制台了。 psql 命令存在简写形式。如果当前 Linux 系统用户，同时也是 PostgreSQL 用户，则可以省略用户名（-U 参数的部分）。举例来说，我的 Linux 系统用户名为 ruanyf，且 PostgreSQL 数据库存在同名用户，则我以 ruanyf 身份登录 Linux 系统后，可以直接使用下面的命令登录数据库，且不需要密码。 psql exampledb 此时，如果 PostgreSQL 内部还存在与当前系统用户同名的数据库，则连数据库名都可以省略。比如，假定存在一个叫做 ruanyf 的数据库，则直接键入 psql 就可以登录该数据库。 psql 另外，如果要恢复外部数据，可以使用下面的命令。 psql exampledb &lt; exampledb.sql 控制台命令 除了前面已经用到的 \password 命令（设置密码）和 \q 命令（退出）以外，控制台还提供一系列其他命令。 \password 设置密码\q 退出\h 查看SQL命令的解释，比如\h select\? 查看psql命令列表\l 列出所有数据库\c [database_name] 连接其他数据库\d 列出当前数据库的所有表格\d [table_name] 列出某一张表格的结构\x 对数据做展开操作\du 列出所有用户 数据库操作 基本的数据库操作，就是使用一般的 SQL 语言。 # 创建新表CREATE TABLE user_tbl(name VARCHAR(20), signup_date DATE);# 插入数据INSERT INTO user_tbl(name, signup_date) VALUES('张三', '2013-12-22');# 选择记录SELECT * FROM user_tbl;# 更新数据UPDATE user_tbl set name = '李四' WHERE name = '张三';# 删除记录DELETE FROM user_tbl WHERE name = '李四' ;# 添加栏位ALTER TABLE user_tbl ADD email VARCHAR(40);# 更新结构ALTER TABLE user_tbl ALTER COLUMN signup_date SET NOT NULL;# 更名栏位ALTER TABLE user_tbl RENAME COLUMN signup_date TO signup;# 删除栏位ALTER TABLE user_tbl DROP COLUMN email;# 表格更名ALTER TABLE user_tbl RENAME TO backup_tbl;# 删除表格DROP TABLE IF EXISTS backup_tbl; 备份和恢复 $ pg_dump --format=t -d db_name -U user_name -h 127.0.0.1 -O -W &gt; dump.sql$ psql -h 127.0.0.1 -U user_name db_name &lt; dump.sql 参考资料 https://www.postgresql.org/download/ http://www.ruanyifeng.com/blog/2013/12/getting_started_with_postgresql.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringBootTutorialCoreProperty]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Fcore%2Fsbe-core-property%2F</url>
    <content type="text"><![CDATA[SpringBootTutorialCoreProperty 加载 property 顺序 随机属性 命令行属性 Application 属性文件 Profile 特定属性 属性中的占位符 YAML 属性 访问属性 多 profile 配置 YAML 的缺点 属性前缀 属性松散绑定规则 属性转换 时间单位转换 数据大小转换 校验属性 源码 更多内容 加载 property 顺序 Spring Boot 加载 property 顺序如下： Devtools 全局配置 (当 devtools 被激活 \~/.spring-boot-devtools.properties). 测试环境中的 @TestPropertySource 注解配置 测试环境中的属性 properties：@SpringBootTest 和 测试注解. 命令行参数 SPRING_APPLICATION_JSON 属性 ServletConfig 初始化参数 ServletContext 初始化参数 JNDI attributes from 通过 java:comp/env 配置的 JNDI 属性 Java 系统属性 (System.getProperties()) 操作系统环境比那里 RandomValuePropertySource 加载 random.* 形式的属性 jar 包外的 application-{profile}.properties 或 application-{profile}.yml 配置 jar 包内的 application-{profile}.properties 或 application-{profile}.yml 配置 jar 包外的 application.properties 或 application.yml 配置 jar 包内的 application.properties 或 application.yml 配置 @PropertySource 绑定的配置 默认属性 (通过 SpringApplication.setDefaultProperties 指定) 随机属性 RandomValuePropertySource 类用于配置随机值。 示例： my.secret=$&#123;random.value&#125;my.number=$&#123;random.int&#125;my.bignumber=$&#123;random.long&#125;my.uuid=$&#123;random.uuid&#125;my.number.less.than.ten=$&#123;random.int(10)&#125;my.number.in.range=$&#123;random.int[1024,65536]&#125; 命令行属性 默认情况下， SpringApplication 会获取 -- 参数（例如 --server.port=9000 ），并将这个 property 添加到 Spring 的 Environment 中。 如果不想加载命令行属性，可以通过 SpringApplication.setAddCommandLineProperties(false) 禁用。 Application 属性文件 SpringApplication 会自动加载以下路径下的 application.properties 配置文件，将其中的属性读到 Spring 的 Environment 中。 当前目录的 /config 子目录 当前目录 classpath 路径下的 /config package classpath 根路径 注： 以上列表的配置文件会根据顺序，后序的配置会覆盖前序的配置。 你可以选择 YAML(yml) 配置文件替换 properties 配置文件。 如果不喜欢 application.properties 作为配置文件名，可以使用 spring.config.name 环境变量替换： $ java -jar myproject.jar --spring.config.name=myproject 可以使用 spring.config.location 环境变量指定配置文件路径： $ java -jar myproject.jar --spring.config.location=classpath:/default.properties,classpath:/override.properties Profile 特定属性 如果定义 application-{profile}.properties 形式的配置文件，将被视为 profile 环境下的特定配置。 可以通过 spring.profiles.active 参数来激活 profile，如果没有激活的 profile,默认会加载 application-default.properties 中的配置。 属性中的占位符 application.properties 中的值会被 Environment 过滤，所以，可以引用之前定义的属性。 app.name=MyAppapp.description=$&#123;app.name&#125; is a Spring Boot application 注：你可以使用此技术来创建 Spring Boot 属性变量。请参考： Section 77.4, “Use ‘Short’ Command Line Arguments YAML 属性 Spring Framework provides two convenient classes that can be used to load YAML documents. The YamlPropertiesFactoryBean loads YAML as Properties and the YamlMapFactoryBean loads YAML as a Map. Spring 框架有两个类支持加载 YAML 文件。 YamlPropertiesFactoryBean 将 YAML 文件的配置加载为 Properties 。 YamlMapFactoryBean 将 YAML 文件的配置加载为 Map 。 示例 1 environments: dev: url: http://dev.example.com name: Developer Setup prod: url: http://another.example.com name: My Cool App 等价于： environments.dev.url=http://dev.example.comenvironments.dev.name=Developer Setupenvironments.prod.url=http://another.example.comenvironments.prod.name=My Cool App YAML 支持列表形式，等价于 property 中的 [index] ： my:servers: - dev.example.com - another.example.com 等价于 my.servers[0]=dev.example.commy.servers[1]=another.example.com 访问属性 YamlPropertySourceLoader 类会将 YAML 配置转化为 Spring Environment 类中的 PropertySource 。然后，你可以如同 properties 文件中的属性一样，使用 @Value 注解来访问 YAML 中配置的属性。 多 profile 配置 server: address: 192.168.1.100---spring: profiles: developmentserver: address: 127.0.0.1---spring: profiles: production &amp; eu-centralserver: address: 192.168.1.120 YAML 的缺点 注：YAML 注解中的属性不能通过 @PropertySource 注解来访问。所以，如果你的项目中使用了一些自定义属性文件，建议不要用 YAML。 属性前缀 package com.example;import java.net.InetAddress;import java.util.ArrayList;import java.util.Collections;import java.util.List;import org.springframework.boot.context.properties.ConfigurationProperties;@ConfigurationProperties(prefix="acme")public class AcmeProperties &#123; private boolean enabled; private InetAddress remoteAddress; private final Security security = new Security(); public boolean isEnabled() &#123; ... &#125; public void setEnabled(boolean enabled) &#123; ... &#125; public InetAddress getRemoteAddress() &#123; ... &#125; public void setRemoteAddress(InetAddress remoteAddress) &#123; ... &#125; public Security getSecurity() &#123; ... &#125; public static class Security &#123; private String username; private String password; private List&lt;String&gt; roles = new ArrayList&lt;&gt;(Collections.singleton("USER")); public String getUsername() &#123; ... &#125; public void setUsername(String username) &#123; ... &#125; public String getPassword() &#123; ... &#125; public void setPassword(String password) &#123; ... &#125; public List&lt;String&gt; getRoles() &#123; ... &#125; public void setRoles(List&lt;String&gt; roles) &#123; ... &#125; &#125;&#125; 相当于支持配置以下属性： acme.enabled acme.remote-address acme.security.username acme.security.password acme.security.roles 然后，你需要使用 @EnableConfigurationProperties 注解将属性类注入配置类中。 @Configuration@EnableConfigurationProperties(AcmeProperties.class)public class MyConfiguration &#123;&#125; 属性松散绑定规则 Spring Boot 属性名绑定比较松散。 以下属性 key 都是等价的： Property Note acme.my-project.person.first-name - 分隔 acme.myProject.person.firstName 驼峰命名 acme.my_project.person.first_name _ 分隔 ACME_MYPROJECT_PERSON_FIRSTNAME 大写字母 属性转换 如果需要类型转换，你可以提供一个 ConversionService bean (一个名叫 conversionService 的 bean) 或自定义属性配置 (一个 CustomEditorConfigurer bean) 或自定义的 Converters (被 @ConfigurationPropertiesBinding 注解修饰的 bena)。 时间单位转换 Spring 使用 java.time.Duration 类代表时间大小，以下场景适用： 除非指定 @DurationUnit ，否则一个 long 代表的时间为毫秒。 ISO-8601 标准格式（ java.time.Duration 的实现就是参照此标准） 你也可以使用以下支持的单位： ns - 纳秒 us - 微秒 ms - 毫秒 s - 秒 m - 分 h - 时 d - 天 示例： @ConfigurationProperties("app.system")public class AppSystemProperties &#123; @DurationUnit(ChronoUnit.SECONDS) private Duration sessionTimeout = Duration.ofSeconds(30); private Duration readTimeout = Duration.ofMillis(1000); public Duration getSessionTimeout() &#123; return this.sessionTimeout; &#125; public void setSessionTimeout(Duration sessionTimeout) &#123; this.sessionTimeout = sessionTimeout; &#125; public Duration getReadTimeout() &#123; return this.readTimeout; &#125; public void setReadTimeout(Duration readTimeout) &#123; this.readTimeout = readTimeout; &#125;&#125; 数据大小转换 Spring 使用 DataSize 类代表数据大小，以下场景适用： long 值（默认视为 byte） 你也可以使用以下支持的单位： B KB MB GB TB 示例： @ConfigurationProperties("app.io")public class AppIoProperties &#123; @DataSizeUnit(DataUnit.MEGABYTES) private DataSize bufferSize = DataSize.ofMegabytes(2); private DataSize sizeThreshold = DataSize.ofBytes(512); public DataSize getBufferSize() &#123; return this.bufferSize; &#125; public void setBufferSize(DataSize bufferSize) &#123; this.bufferSize = bufferSize; &#125; public DataSize getSizeThreshold() &#123; return this.sizeThreshold; &#125; public void setSizeThreshold(DataSize sizeThreshold) &#123; this.sizeThreshold = sizeThreshold; &#125;&#125; 校验属性 @ConfigurationProperties(prefix="acme")@Validatedpublic class AcmeProperties &#123; @NotNull private InetAddress remoteAddress; @Valid private final Security security = new Security(); // ... getters and setters public static class Security &#123; @NotEmpty public String username; // ... getters and setters &#125;&#125; 你也可以自定义一个名为 configurationPropertiesValidator 的校验器 Bean。获取这个 @Bean 的方法必须声明为 static。 源码 完整示例：源码 使用方法： mvn clean packagecd targetjava -jar sbe-core-property.jar 更多内容 引申 Spring Boot 教程 参考 Spring Boot 官方文档之 boot-features-external-config]]></content>
  </entry>
  <entry>
    <title><![CDATA[项目规范]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fmethod%2Fproject-style%2F</url>
    <content type="text"><![CDATA[项目规范 软件项目开发规范。 项目结构 目录 文件 命名规则 目录名 文件名 Java 日志规范 参考资料 项目结构 以下为项目根目录下的文件和目录的组织结构： 目录 codes - 代码目录。 configurations - 配置目录。一般存放项目相关的配置文件。如 maven 的 settings.xml，nginx 的 nginx.conf 等。 demos - 示例目录。 docs - 文档目录。 libs - 第三方库文件。 scripts - 脚本目录。一般存放用于启动、构建项目的可执行脚本文件。 packages - 打包文件目录。Java 项目中可能是 jar、war 等；前端项目中可能是 zip、rar 等；电子书项目中可能是 pdf 等。 文件 .gitignore - git 忽略规则。 .gitattributes - git 属性规则。 .editorconfig - 编辑器书写规则。 README.md - 项目说明文件。 LICENSE - 开源协议。如果项目是开源文件，需要添加。 命名规则 目录名 目录名必须使用半角字符，不得使用全角字符。这也意味着，中文不能用于文件名。 目录名建议只使用小写字母，不使用大写字母。 不佳： Test正确： test 目录名可以使用数字，但不应该是首字符。 不佳： 1-demo正确： demo1 目录名包含多个单词时，单词之间建议使用半角的连词线（-）分隔。 不佳： common_demo正确： common-demo 文件名 文档的文件名不得含有空格。 文件名必须使用半角字符，不得使用全角字符。这也意味着，中文不能用于文件名。 错误： 名词解释.md正确： glossary.md 文件名建议只使用小写字母，不使用大写字母。 错误：TroubleShooting.md正确：troubleshooting.md 为了醒目，某些说明文件的文件名，可以使用大写字母，比如README、LICENSE。 一些约定俗成的习惯可以保持传统写法，如：Java 的文件名一般使用驼峰命名法，且首字母大写；配置文件 applicationContext.xml ；React 中的 JSX 组件文件名一般使用驼峰命名法，且首字母大写等。 文件名包含多个单词时，单词之间建议使用半角的连词线（-）分隔。 不佳：advanced_usage.md正确：advanced-usage.md Java 日志规范 这里基于阿里巴巴 Java 开发手册日志规约章节，结合自己的开发经验做了一些增删和调整。 【强制】应用中不可直接使用日志系统（Log4j、Logback）中的 API，而应依赖使用日志框架 SLF4J 中的 API，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。 import org.slf4j.Logger;import org.slf4j.LoggerFactory;private static final Logger logger = LoggerFactory.getLogger(Abc.class); 【强制】日志文件推荐至少保存 30 天，因为有些异常具备以“周”为频次发生的特点。 【强制】应用中的扩展日志（如打点、临时监控、访问日志等）命名方式：appName_logType_logName.log。logType:日志类型，推荐分类有 stats/desc/monitor/visit 等；logName:日志描述。这种命名的好处：通过文件名就可知道日志文件属于什么应用，什么类型，什么目的，也有利于归类查找。 正例：mppserver 应用中单独监控时区转换异常，如：mppserver_monitor_timeZoneConvert.log 说明：推荐对日志进行分类，如将错误日志和业务日志分开存放，便于开发人员查看，也便于通过日志对系统进行及时监控。 【强制】对 trace/debug/info 级别的日志输出，必须使用条件输出形式或者使用占位符的方式。 说明：logger.debug(&quot;Processing trade with id: &quot; + id + &quot; and symbol: &quot; + symbol); 如果日志级别是 warn，上述日志不会打印，但是会执行字符串拼接操作，如果 symbol 是对象，会执行 toString()方法，浪费了系统资源，执行了上述操作，最终日志却没有打印。 正例：（条件） if (logger.isDebugEnabled()) &#123;logger.debug("Processing trade with id: " + id + " and symbol: " + symbol);&#125; 正例：（占位符） logger.debug("Processing trade with id: &#123;&#125; and symbol : &#123;&#125; ", id, symbol); 【强制】避免重复打印日志，浪费磁盘空间。务必在 log4j.xml 或 logback.xml 中设置 additivity=false。 正例： &lt;logger name="com.taobao.dubbo.config" additivity="false"&gt; 【强制】异常信息应该包括两类信息：案发现场信息和异常堆栈信息。如果不处理，那么通过关键字 throws 往上抛出。 正例：logger.error(各类参数或者对象 toString + “_” + e.getMessage(), e); 【强制】日志格式遵循如下格式： 打印出的日志信息如： 2018-03-29 15:06:57.277 [javalib] [main] [TRACE] i.g.dunwu.javalib.log.LogbackDemo#main - 这是一条 trace 日志记录2018-03-29 15:06:57.282 [javalib] [main] [DEBUG] i.g.dunwu.javalib.log.LogbackDemo#main - 这是一条 debug 日志记录2018-03-29 15:06:57.282 [javalib] [main] [INFO] i.g.dunwu.javalib.log.LogbackDemo#main - 这是一条 info 日志记录2018-03-29 15:06:57.282 [javalib] [main] [WARN] i.g.dunwu.javalib.log.LogbackDemo#main - 这是一条 warn 日志记录2018-03-29 15:06:57.282 [javalib] [main] [ERROR] i.g.dunwu.javalib.log.LogbackDemo#main - 这是一条 error 日志记录 【参考】slf4j 支持的日志级别，按照级别从低到高，分别为：trace &lt; debug &lt; info &lt; warn &lt; error。 建议只使用 debug &lt; info &lt; warn &lt; error 四个级别。 error 日志级别只记录系统逻辑出错、异常等重要的错误信息。如非必要，请不要在此场景打出 error 级别。 warn 日志级别记录用户输入参数错误的情况，避免用户投诉时，无所适从。 info 日志级别记录业务逻辑中一些重要步骤信息。 debug 日志级别记录一些用于调试的信息。 【参考】有一些第三方框架或库的日志对于排查问题具有一定的帮助，如 Spring、Dubbo、Mybatis 等。这些框架所使用的日志库未必和本项目一样，为了避免出现日志无法输出的问题，请引入对应的桥接 jar 包。 参考资料 阿里巴巴 Java 开发手册日志规约章节]]></content>
      <categories>
        <category>method</category>
      </categories>
      <tags>
        <tag>method</tag>
        <tag>project</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringApplication]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Fcore%2FSpringApplication%2F</url>
    <content type="text"><![CDATA[SpringApplication 启动成功 启动失败 定制 SpringApplication Build API 应用事件和监听器 Web 环境变量 访问应用参数 使用 ApplicationRunner 或 CommandLineRunner 退出应用 管理功能 更多内容 SpringApplication 类提供了 run() 方法作为 Spring Boot 工程的启动方法。 public static void main(String[] args) &#123; SpringApplication.run(MySpringConfiguration.class, args);&#125; 启动成功 启动后，你会看到如下控制台信息： . ____ _ __ _ _ /\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: v2.1.1.RELEASE2013-07-31 00:08:16.117 INFO 56603 --- [ main] o.s.b.s.app.SampleApplication : Starting SampleApplication v0.1.0 on mycomputer with PID 56603 (/apps/myapp.jar started by pwebb)2013-07-31 00:08:16.166 INFO 56603 --- [ main] ationConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@6e5a8246: startup date [Wed Jul 31 00:08:16 PDT 2013]; root of context hierarchy2014-03-04 13:09:54.912 INFO 41370 --- [ main] .t.TomcatServletWebServerFactory : Server initialized with port: 80802014-03-04 13:09:56.501 INFO 41370 --- [ main] o.s.b.s.app.SampleApplication : Started SampleApplication in 2.992 seconds (JVM running for 3.658) 启动失败 启动失败，你会看到如下信息： ***************************APPLICATION FAILED TO START***************************Description:Embedded servlet container failed to start. Port 8080 was already in use.Action:Identify and stop the process that's listening on port 8080 or configure this application to listen on another port. Spring Boot 通过 FailureAnalyzers 类来打印错误信息。事实上，FailureAnalyzers 维护所有 FailureAnalyzer 接口实现类，统一管理错误分析信息。如果想自定义 FailureAnalyzer 实现，可以参考： howto-failure-analyzer。 如果没有合适的 FailureAnalyzer 处理异常，也可以通过设置 debug = true 属性来开启 debug 模式。开启 debug 模式相当于开启 org.springframework.boot.autoconfigure.logging.ConditionEvaluationReportLoggingListener 类的 debug 日志打印。 你也可以通过启动参数方式来开启 debug 模式： $ java -jar myproject-0.0.1-SNAPSHOT.jar --debug 定制 SpringApplication 如果 SpringApplication 不能满足项目需要，你也可以自定制一个实例。 示例： public static void main(String[] args) &#123; SpringApplication app = new SpringApplication(MySpringConfiguration.class); app.setBannerMode(Banner.Mode.OFF); app.run(args);&#125; SpringApplication 常常和 @Configuration 搭配使用，如下所示： @Configuration@EnableAutoConfigurationpublic class MyApplication &#123; // ... Bean definitionsjava public static void main(String[] args) throws Exception &#123; SpringApplication.run(MyApplication.class, args); &#125;&#125; SpringApplication 也可以通过 application.properties 读取应用配置属性。支持的配置属性可以参考：SpringApplication javadoc Build API ApplicationContext 支持 build 式 API，如下所示： new SpringApplicationBuilder() .sources(Parent.class) .child(Application.class) .bannerMode(Banner.Mode.OFF) .run(args); 应用事件和监听器 为了利用 Spring 框架事件（例如 ContextRefreshedEvent），SpringApplication 会发送一些应用事件。 注：一些事件实际上是在创建 ApplicationContext 之前就触发，所以对于这样的事件，无法以 Bean 的方式注册监听器。你可以通过 SpringApplication.addListeners(…) 或 SpringApplicationBuilder.listeners(…) 来注册监听器。如果希望自动注册这些监听器，可以添加一个 META-INF/spring.factories 文件，并使用 org.springframework.context.ApplicationListener key 引用你的监听器。如下所示： &gt; org.springframework.context.ApplicationListener=com.example.project.MyListener&gt; SpringApplication 事件触发顺序如下： ApplicationStartingEvent - 在启动 run 方法时触发，优先于除了注册 listeners 和 initializers 以外的任何处理。 ApplicationEnvironmentPreparedEvent - 在上下文中使用 Environment 时触发，这个处理在创建上下文之前。 ApplicationPreparedEvent - 在启动刷新之前，加载 bean 之后触发。 ApplicationStartedEvent - 在上下文刷新后，任何应用命令行 runner 被调用前触发。 ApplicationReadyEvent - 在任意应用命令行 runner 被调用后触发。这意味着应用已经准备好相应服务请求。 ApplicationFailedEvent - 启动时出现异常则触发。 除了SpringApplication 事件，可能你会希望注册自定义监听器。那么，你只需要实现并注入 ApplicationContextAware 接口。 Web 环境变量 SpringApplication 会根据你的配置尝试创建正确类型的 ApplicationContext。 创建的算法如下： 如果 Spring MVC 存在，会使用 AnnotationConfigServletWebServerApplicationContext 。 如果 Spring MVC 不存在，Spring WebFlux 存在，会使用 AnnotationConfigReactiveWebServerApplicationContext 。 否则，使用 AnnotationConfigApplicationContext 。 此外，也可以通过调用 setApplicationContextClass(…) 来主动控制 ApplicationContext 类型。 注：如果是在 Junit 单元测试中使用 SpringApplication ，通常会调用 setWebApplicationType(WebApplicationType.NONE) 访问应用参数 如果需要访问通过 SpringApplication.run(…) 传入的参数，可以注入 org.springframework.boot.ApplicationArguments Bean。ApplicationArguments 接口支持 String[] 型参数，同时支持 option 和 non-option 参数。如下所示： import org.springframework.boot.*;import org.springframework.beans.factory.annotation.*;import org.springframework.stereotype.*;@Componentpublic class MyBean &#123; @Autowired public MyBean(ApplicationArguments args) &#123; boolean debug = args.containsOption("debug"); List&lt;String&gt; files = args.getNonOptionArgs(); // if run with "--debug logfile.txt" debug=true, files=["logfile.txt"] &#125;&#125; Spring Boot 还在 Spring Environment 中注册了一个 CommandLinePropertySource。它可以允许你通过 @Value 注解注入一个应用参数。 使用 ApplicationRunner 或 CommandLineRunner 如果需要在 SpringApplication 启动后立即运行一次指定的代码，你可以实现 ApplicationRunner 或 CommandLineRunner 接口。这两个接口以相同方式工作，并且都提供 run 方法，这个方法会在 SpringApplication.run(…) 方法完成前调用。 CommandLineRunner 接口提供对应用程序参数的访问，作为简单的字符串数组；而 ApplicationRunner 使用前面讨论的 ApplicationArguments 接口。 import org.springframework.boot.*;import org.springframework.stereotype.*;@Componentpublic class MyBean implements CommandLineRunner &#123; public void run(String... args) &#123; // Do something... &#125;&#125; 如果有多个 CommandLineRunner 或 ApplicationRunner 被定义，则必须指定顺序，你可以实现 org.springframework.core.Ordered 接口，或使用 org.springframework.core.annotation.Order 注解。 退出应用 每个 SpringApplication 会向 JVM 注册一个 shutdown 钩子，以确保 ApplicationContext 可以优雅退出。可以使用所有 Spring 标准生命周期回调函数（例如 DisposableBean 接口或 @PreDestroy 注解）。 此外，如果希望在调用 SpringApplication.exit() 时返回一个指定的退出码，可以实现 org.springframework.boot.ExitCodeGenerator 接口。这个退出码会被作为一个状态码被传递给 System.exit() ，如下所示： @SpringBootApplicationpublic class ExitCodeApplication &#123; @Bean public ExitCodeGenerator exitCodeGenerator() &#123; return () -&gt; 42; &#125; public static void main(String[] args) &#123; System.exit(SpringApplication .exit(SpringApplication.run(ExitCodeApplication.class, args))); &#125;&#125; ExitCodeGenerator 接口也可以被异常类实现。当一个异常发生，Spring Boot 会根据实现的 getExitCode() 方法返回退出码。 管理功能 可以通过 spring.application.admin.enabled 属性开启应用的管理功能。这会在平台 MBeanServer 上暴露SpringApplicationAdminMXBean。您可以使用此功能远程管理 Spring Boot 应用程序。 注：如果想知道应用使用的 HTTP 端口是什么，可以通过 local.server.port 获取。 更多内容 引申 Spring Boot 教程 参考 Spring Boot 官方文档之 SpringApplication]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 镜像]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fbasics%2Fdocker-image%2F</url>
    <content type="text"><![CDATA[Docker 镜像 获取镜像 运行 列出镜像 镜像体积 虚悬镜像 中间层镜像 列出部分镜像 以特定格式显示 删除本地镜像 用 ID、镜像名、摘要删除镜像 Untagged 和 Deleted 用 docker image ls 命令来配合 CentOS/RHEL 的用户需要注意的事项 使用 Dockerfile 定制镜像 构建镜像 镜像构建上下文（Context） 其它 docker build 的用法 获取镜像 之前提到过，Docker Hub 上有大量的高质量的镜像可以用，这里我们就说一下怎么获取这些镜像。 从 Docker 镜像仓库获取镜像的命令是 docker pull。其命令格式为： docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] 具体的选项可以通过 docker pull --help 命令看到，这里我们说一下镜像名称的格式。 Docker 镜像仓库地址：地址的格式一般是 &lt;域名/IP&gt;[:端口号]。默认地址是 Docker Hub。 仓库名：如之前所说，这里的仓库名是两段式名称，即 &lt;用户名&gt;/&lt;软件名&gt;。对于 Docker Hub，如果不给出用户名，则默认为 library，也就是官方镜像。 比如： $ docker pull ubuntu:18.0418.04: Pulling from library/ubuntubf5d46315322: Pull complete9f13e0ac480c: Pull completee8988b5b3097: Pull complete40af181810e7: Pull completee6f7c7e5c03e: Pull completeDigest: sha256:147913621d9cdea08853f6ba9116c2e27a3ceffecf3b492983ae97c3d643fbbeStatus: Downloaded newer image for ubuntu:18.04 上面的命令中没有给出 Docker 镜像仓库地址，因此将会从 Docker Hub 获取镜像。而镜像名称是 ubuntu:18.04，因此将会获取官方镜像 library/ubuntu 仓库中标签为 18.04 的镜像。 从下载过程中可以看到我们之前提及的分层存储的概念，镜像是由多层存储所构成。下载也是一层层的去下载，并非单一文件。下载过程中给出了每一层的 ID 的前 12 位。并且下载结束后，给出该镜像完整的 sha256 的摘要，以确保下载一致性。 在使用上面命令的时候，你可能会发现，你所看到的层 ID 以及 sha256 的摘要和这里的不一样。这是因为官方镜像是一直在维护的，有任何新的 bug，或者版本更新，都会进行修复再以原来的标签发布，这样可以确保任何使用这个标签的用户可以获得更安全、更稳定的镜像。 如果从 Docker Hub 下载镜像非常缓慢，可以参照 镜像加速器 一节配置加速器。 运行 有了镜像后，我们就能够以这个镜像为基础启动并运行一个容器。以上面的 ubuntu:18.04 为例，如果我们打算启动里面的 bash 并且进行交互式操作的话，可以执行下面的命令。 $ docker run -it --rm \ ubuntu:18.04 \ bashroot@e7009c6ce357:/# cat /etc/os-releaseNAME="Ubuntu"VERSION="18.04.1 LTS (Bionic Beaver)"ID=ubuntuID_LIKE=debianPRETTY_NAME="Ubuntu 18.04.1 LTS"VERSION_ID="18.04"HOME_URL="https://www.ubuntu.com/"SUPPORT_URL="https://help.ubuntu.com/"BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"VERSION_CODENAME=bionicUBUNTU_CODENAME=bionic docker run 就是运行容器的命令，具体格式我们会在 容器 一节进行详细讲解，我们这里简要的说明一下上面用到的参数。 -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 --rm 可以避免浪费空间。 ubuntu:18.04：这是指用 ubuntu:18.04 镜像为基础来启动容器。 bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 bash。 进入容器后，我们可以在 Shell 下操作，执行任何所需的命令。这里，我们执行了 cat /etc/os-release，这是 Linux 常用的查看当前系统版本的命令，从返回的结果可以看到容器内是 Ubuntu 18.04.1 LTS 系统。 最后我们通过 exit 退出了这个容器。 列出镜像 要想列出已经下载下来的镜像，可以使用 docker image ls 命令。 $ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEredis latest 5f515359c7f8 5 days ago 183 MBnginx latest 05a60462f8ba 5 days ago 181 MBmongo 3.2 fe9198c04d62 5 days ago 342 MB&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MBubuntu 18.04 f753707788c5 4 weeks ago 127 MBubuntu latest f753707788c5 4 weeks ago 127 MBubuntu 14.04 1e0c3dd64ccd 4 weeks ago 188 MB 列表包含了 仓库名、标签、镜像 ID、创建时间 以及 所占用的空间。 其中仓库名、标签在之前的基础概念章节已经介绍过了。镜像 ID 则是镜像的唯一标识，一个镜像可以对应多个标签。因此，在上面的例子中，我们可以看到 ubuntu:18.04 和 ubuntu:latest 拥有相同的 ID，因为它们对应的是同一个镜像。 镜像体积 如果仔细观察，会注意到，这里标识的所占用空间和在 Docker Hub 上看到的镜像大小不同。比如，ubuntu:18.04 镜像大小，在这里是 127 MB，但是在 Docker Hub 显示的却是 50 MB。这是因为 Docker Hub 中显示的体积是压缩后的体积。在镜像下载和上传过程中镜像是保持着压缩状态的，因此 Docker Hub 所显示的大小是网络传输中更关心的流量大小。而 docker image ls 显示的是镜像下载到本地后，展开的大小，准确说，是展开后的各层所占空间的总和，因为镜像到本地后，查看空间的时候，更关心的是本地磁盘空间占用的大小。 另外一个需要注意的问题是，docker image ls 列表中的镜像体积总和并非是所有镜像实际硬盘消耗。由于 Docker 镜像是多层存储结构，并且可以继承、复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 Docker 使用 Union FS，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多。 你可以通过以下命令来便捷的查看镜像、容器、数据卷所占用的空间。 $ docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 24 0 1.992GB 1.992GB (100%)Containers 1 0 62.82MB 62.82MB (100%)Local Volumes 9 0 652.2MB 652.2MB (100%)Build Cache 0B 0B 虚悬镜像 上面的镜像列表中，还可以看到一个特殊的镜像，这个镜像既没有仓库名，也没有标签，均为 &lt;none&gt;。： &lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MB 这个镜像原本是有镜像名和标签的，原来为 mongo:3.2，随着官方镜像维护，发布了新版本后，重新 docker pull mongo:3.2 时，mongo:3.2 这个镜像名被转移到了新下载的镜像身上，而旧的镜像上的这个名称则被取消，从而成为了 &lt;none&gt;。除了 docker pull 可能导致这种情况，docker build 也同样可以导致这种现象。由于新旧镜像同名，旧镜像名称被取消，从而出现仓库名、标签均为 &lt;none&gt; 的镜像。这类无标签镜像也被称为 虚悬镜像(dangling image) ，可以用下面的命令专门显示这类镜像： $ docker image ls -f dangling=trueREPOSITORY TAG IMAGE ID CREATED SIZE&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MB 一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。 $ docker image prune 中间层镜像 为了加速镜像构建、重复利用资源，Docker 会利用 中间层镜像。所以在使用一段时间后，可能会看到一些依赖的中间层镜像。默认的 docker image ls 列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，需要加 -a 参数。 $ docker image ls -a 这样会看到很多无标签的镜像，与之前的虚悬镜像不同，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。这些无标签镜像不应该删除，否则会导致上层镜像因为依赖丢失而出错。实际上，这些镜像也没必要删除，因为之前说过，相同的层只会存一遍，而这些镜像是别的镜像的依赖，因此并不会因为它们被列出来而多存了一份，无论如何你也会需要它们。只要删除那些依赖它们的镜像后，这些依赖的中间层镜像也会被连带删除。 列出部分镜像 不加任何参数的情况下，docker image ls 会列出所有顶级镜像，但是有时候我们只希望列出部分镜像。docker image ls 有好几个参数可以帮助做到这个事情。 根据仓库名列出镜像 $ docker image ls ubuntuREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 18.04 f753707788c5 4 weeks ago 127 MBubuntu latest f753707788c5 4 weeks ago 127 MBubuntu 14.04 1e0c3dd64ccd 4 weeks ago 188 MB 列出特定的某个镜像，也就是说指定仓库名和标签 $ docker image ls ubuntu:18.04REPOSITORY TAG IMAGE ID CREATED SIZEubuntu 18.04 f753707788c5 4 weeks ago 127 MB 除此以外，docker image ls 还支持强大的过滤器参数 --filter，或者简写 -f。之前我们已经看到了使用过滤器来列出虚悬镜像的用法，它还有更多的用法。比如，我们希望看到在 mongo:3.2 之后建立的镜像，可以用下面的命令： $ docker image ls -f since=mongo:3.2REPOSITORY TAG IMAGE ID CREATED SIZEredis latest 5f515359c7f8 5 days ago 183 MBnginx latest 05a60462f8ba 5 days ago 181 MB 想查看某个位置之前的镜像也可以，只需要把 since 换成 before 即可。 此外，如果镜像构建时，定义了 LABEL，还可以通过 LABEL 来过滤。 $ docker image ls -f label=com.example.version=0.1... 以特定格式显示 默认情况下，docker image ls 会输出一个完整的表格，但是我们并非所有时候都会需要这些内容。比如，刚才删除虚悬镜像的时候，我们需要利用 docker image ls 把所有的虚悬镜像的 ID 列出来，然后才可以交给 docker image rm 命令作为参数来删除指定的这些镜像，这个时候就用到了 -q 参数。 $ docker image ls -q5f515359c7f805a60462f8bafe9198c04d6200285df0df87f753707788c5f753707788c51e0c3dd64ccd --filter 配合 -q 产生出指定范围的 ID 列表，然后送给另一个 docker 命令作为参数，从而针对这组实体成批的进行某种操作的做法在 Docker 命令行使用过程中非常常见，不仅仅是镜像，将来我们会在各个命令中看到这类搭配以完成很强大的功能。因此每次在文档看到过滤器后，可以多注意一下它们的用法。 另外一些时候，我们可能只是对表格的结构不满意，希望自己组织列；或者不希望有标题，这样方便其它程序解析结果等，这就用到了 Go 的模板语法。 比如，下面的命令会直接列出镜像结果，并且只包含镜像ID和仓库名： $ docker image ls --format "&#123;&#123;.ID&#125;&#125;: &#123;&#123;.Repository&#125;&#125;"5f515359c7f8: redis05a60462f8ba: nginxfe9198c04d62: mongo00285df0df87: &lt;none&gt;f753707788c5: ubuntuf753707788c5: ubuntu1e0c3dd64ccd: ubuntu 或者打算以表格等距显示，并且有标题行，和默认一样，不过自己定义列： $ docker image ls --format "table &#123;&#123;.ID&#125;&#125;\t&#123;&#123;.Repository&#125;&#125;\t&#123;&#123;.Tag&#125;&#125;"IMAGE ID REPOSITORY TAG5f515359c7f8 redis latest05a60462f8ba nginx latestfe9198c04d62 mongo 3.200285df0df87 &lt;none&gt; &lt;none&gt;f753707788c5 ubuntu 18.04f753707788c5 ubuntu latest1e0c3dd64ccd ubuntu 14.04 删除本地镜像 如果要删除本地的镜像，可以使用 docker image rm 命令，其格式为： $ docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...] 用 ID、镜像名、摘要删除镜像 其中，&lt;镜像&gt; 可以是 镜像短 ID、镜像长 ID、镜像名 或者 镜像摘要。 比如我们有这么一些镜像： $ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEcentos latest 0584b3d2cf6d 3 weeks ago 196.5 MBredis alpine 501ad78535f0 3 weeks ago 21.03 MBdocker latest cf693ec9b5c7 3 weeks ago 105.1 MBnginx latest e43d811ce2f4 5 weeks ago 181.5 MB 我们可以用镜像的完整 ID，也称为 长 ID，来删除镜像。使用脚本的时候可能会用长 ID，但是人工输入就太累了，所以更多的时候是用 短 ID 来删除镜像。docker image ls 默认列出的就已经是短 ID 了，一般取前3个字符以上，只要足够区分于别的镜像就可以了。 比如这里，如果我们要删除 redis:alpine 镜像，可以执行： $ docker image rm 501Untagged: redis:alpineUntagged: redis@sha256:f1ed3708f538b537eb9c2a7dd50dc90a706f7debd7e1196c9264edeea521a86dDeleted: sha256:501ad78535f015d88872e13fa87a828425117e3d28075d0c117932b05bf189b7Deleted: sha256:96167737e29ca8e9d74982ef2a0dda76ed7b430da55e321c071f0dbff8c2899bDeleted: sha256:32770d1dcf835f192cafd6b9263b7b597a1778a403a109e2cc2ee866f74adf23Deleted: sha256:127227698ad74a5846ff5153475e03439d96d4b1c7f2a449c7a826ef74a2d2faDeleted: sha256:1333ecc582459bac54e1437335c0816bc17634e131ea0cc48daa27d32c75eab3Deleted: sha256:4fc455b921edf9c4aea207c51ab39b10b06540c8b4825ba57b3feed1668fa7c7 我们也可以用镜像名，也就是 &lt;仓库名&gt;:&lt;标签&gt;，来删除镜像。 $ docker image rm centosUntagged: centos:latestUntagged: centos@sha256:b2f9d1c0ff5f87a4743104d099a3d561002ac500db1b9bfa02a783a46e0d366cDeleted: sha256:0584b3d2cf6d235ee310cf14b54667d889887b838d3f3d3033acd70fc3c48b8aDeleted: sha256:97ca462ad9eeae25941546209454496e1d66749d53dfa2ee32bf1faabd239d38 当然，更精确的是使用 镜像摘要 删除镜像。 $ docker image ls --digestsREPOSITORY TAG DIGEST IMAGE ID CREATED SIZEnode slim sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228 6e0c4c8e3913 3 weeks ago 214 MB$ docker image rm node@sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228Untagged: node@sha256:b4f0e0bdeb578043c1ea6862f0d40cc4afe32a4a582f3be235a3b164422be228 Untagged 和 Deleted 如果观察上面这几个命令的运行输出信息的话，你会注意到删除行为分为两类，一类是 Untagged，另一类是 Deleted。我们之前介绍过，镜像的唯一标识是其 ID 和摘要，而一个镜像可以有多个标签。 因此当我们使用上面命令删除镜像的时候，实际上是在要求删除某个标签的镜像。所以首先需要做的是将满足我们要求的所有镜像标签都取消，这就是我们看到的 Untagged 的信息。因为一个镜像可以对应多个标签，因此当我们删除了所指定的标签后，可能还有别的标签指向了这个镜像，如果是这种情况，那么 Delete 行为就不会发生。所以并非所有的 docker image rm都会产生删除镜像的行为，有可能仅仅是取消了某个标签而已。 当该镜像所有的标签都被取消了，该镜像很可能会失去了存在的意义，因此会触发删除行为。镜像是多层存储结构，因此在删除的时候也是从上层向基础层方向依次进行判断删除。镜像的多层结构让镜像复用变动非常容易，因此很有可能某个其它镜像正依赖于当前镜像的某一层。这种情况，依旧不会触发删除该层的行为。直到没有任何层依赖当前层时，才会真实的删除当前层。这就是为什么，有时候会奇怪，为什么明明没有别的标签指向这个镜像，但是它还是存在的原因，也是为什么有时候会发现所删除的层数和自己 docker pull 看到的层数不一样的源。 除了镜像依赖以外，还需要注意的是容器对镜像的依赖。如果有用这个镜像启动的容器存在（即使容器没有运行），那么同样不可以删除这个镜像。之前讲过，容器是以镜像为基础，再加一层容器存储层，组成这样的多层存储结构去运行的。因此该镜像如果被这个容器所依赖的，那么删除必然会导致故障。如果这些容器是不需要的，应该先将它们删除，然后再来删除镜像。 用 docker image ls 命令来配合 像其它可以承接多个实体的命令一样，可以使用 docker image ls -q 来配合使用 docker image rm，这样可以成批的删除希望删除的镜像。我们在“镜像列表”章节介绍过很多过滤镜像列表的方式都可以拿过来使用。 比如，我们需要删除所有仓库名为 redis 的镜像： $ docker image rm $(docker image ls -q redis) 或者删除所有在 mongo:3.2 之前的镜像： $ docker image rm $(docker image ls -q -f before=mongo:3.2) 充分利用你的想象力和 Linux 命令行的强大，你可以完成很多非常赞的功能。 CentOS/RHEL 的用户需要注意的事项 在 Ubuntu/Debian 上有 UnionFS 可以使用，如 aufs 或者 overlay2，而 CentOS 和 RHEL 的内核中没有相关驱动。因此对于这类系统，一般使用 devicemapper 驱动利用 LVM 的一些机制来模拟分层存储。这样的做法除了性能比较差外，稳定性一般也不好，而且配置相对复杂。Docker 安装在 CentOS/RHEL 上后，会默认选择 devicemapper，但是为了简化配置，其 devicemapper是跑在一个稀疏文件模拟的块设备上，也被称为 loop-lvm。这样的选择是因为不需要额外配置就可以运行 Docker，这是自动配置唯一能做到的事情。但是 loop-lvm 的做法非常不好，其稳定性、性能更差，无论是日志还是 docker info 中都会看到警告信息。官方文档有明确的文章讲解了如何配置块设备给 devicemapper 驱动做存储层的做法，这类做法也被称为配置 direct-lvm。 除了前面说到的问题外，devicemapper + loop-lvm 还有一个缺陷，因为它是稀疏文件，所以它会不断增长。用户在使用过程中会注意到 /var/lib/docker/devicemapper/devicemapper/data 不断增长，而且无法控制。很多人会希望删除镜像或者可以解决这个问题，结果发现效果并不明显。原因就是这个稀疏文件的空间释放后基本不进行垃圾回收的问题。因此往往会出现即使删除了文件内容，空间却无法回收，随着使用这个稀疏文件一直在不断增长。 所以对于 CentOS/RHEL 的用户来说，在没有办法使用 UnionFS 的情况下，一定要配置 direct-lvm 给 devicemapper，无论是为了性能、稳定性还是空间利用率。 或许有人注意到了 CentOS 7 中存在被 backports 回来的 overlay 驱动，不过 CentOS 里的这个驱动达不到生产环境使用的稳定程度，所以不推荐使用。 使用 Dockerfile 定制镜像 从刚才的 docker commit 的学习中，我们可以了解到，镜像的定制实际上就是定制每一层所添加的配置、文件。如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 还以之前定制 nginx 镜像为例，这次我们使用 Dockerfile 来定制。 在一个空白目录中，建立一个文本文件，并命名为 Dockerfile： $ mkdir mynginx$ cd mynginx$ touch Dockerfile 其内容为： FROM nginxRUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html 这个 Dockerfile 很简单，一共就两行。涉及到了两条指令，FROM 和 RUN。 构建镜像 好了，让我们再回到之前定制的 nginx 镜像的 Dockerfile 来。现在我们明白了这个 Dockerfile 的内容，那么让我们来构建这个镜像吧。 在 Dockerfile 文件所在目录执行： $ docker build -t nginx:v3 .Sending build context to Docker daemon 2.048 kBStep 1 : FROM nginx ---&gt; e43d811ce2f4Step 2 : RUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html ---&gt; Running in 9cdc27646c7b ---&gt; 44aa4490ce2cRemoving intermediate container 9cdc27646c7bSuccessfully built 44aa4490ce2c 从命令的输出结果中，我们可以清晰的看到镜像的构建过程。在 Step 2 中，如同我们之前所说的那样，RUN 指令启动了一个容器 9cdc27646c7b，执行了所要求的命令，并最后提交了这一层 44aa4490ce2c，随后删除了所用到的这个容器 9cdc27646c7b。 这里我们使用了 docker build 命令进行镜像构建。其格式为： docker build [选项] &lt;上下文路径/URL/-&gt; 在这里我们指定了最终镜像的名称 -t nginx:v3，构建成功后，我们可以像之前运行 nginx:v2 那样来运行这个镜像，其结果会和 nginx:v2 一样。 镜像构建上下文（Context） 如果注意，会看到 docker build 命令最后有一个 .。. 表示当前目录，而 Dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定 Dockerfile 所在路径，这么理解其实是不准确的。如果对应上面的命令格式，你可能会发现，这是在指定上下文路径。那么什么是上下文呢？ 首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种 C/S 设计，让我们操作远程服务器的 Docker 引擎变得轻而易举。 当我们进行镜像构建的时候，并非所有定制都会通过 RUN 指令完成，经常会需要将一些本地文件复制进镜像，比如通过 COPY 指令、ADD 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是 Docker 引擎中构建的。那么在这种客户端/服务端的架构中，如何才能让服务端获得本地文件呢？ 这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。 如果在 Dockerfile 中这么写： COPY ./package.json /app/ 这并不是要复制执行 docker build 命令所在的目录下的 package.json，也不是复制 Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的 package.json。 因此，COPY 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么 COPY ../package.json /app 或者 COPY /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，Docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。 现在就可以理解刚才的命令 docker build -t nginx:v3 . 中的这个 .，实际上是在指定上下文的目录，docker build 命令会将该目录下的内容打包交给 Docker 引擎以帮助构建镜像。 如果观察 docker build 输出，我们其实已经看到了这个发送上下文的过程： $ docker build -t nginx:v3 .Sending build context to Docker daemon 2.048 kB... 理解构建上下文对于镜像构建是很重要的，避免犯一些不应该的错误。比如有些初学者在发现 COPY /opt/xxxx /app 不工作后，于是干脆将 Dockerfile 放到了硬盘根目录去构建，结果发现 docker build 执行后，在发送一个几十 GB 的东西，极为缓慢而且很容易构建失败。那是因为这种做法是在让 docker build打包整个硬盘，这显然是使用错误。 一般来说，应该会将 Dockerfile 置于一个空目录下，或者项目根目录下。如果该目录下没有所需文件，那么应该把所需文件复制一份过来。如果目录下有些东西确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore 一样的语法写一个 .dockerignore，该文件是用于剔除不需要作为上下文传递给 Docker 引擎的。 那么为什么会有人误以为 . 是指定 Dockerfile 所在目录呢？这是因为在默认情况下，如果不额外指定 Dockerfile 的话，会将上下文目录下的名为 Dockerfile 的文件作为 Dockerfile。 这只是默认行为，实际上 Dockerfile 的文件名并不要求必须为 Dockerfile，而且并不要求必须位于上下文目录中，比如可以用 -f ../Dockerfile.php 参数指定某个文件作为 Dockerfile。 当然，一般大家习惯性的会使用默认的文件名 Dockerfile，以及会将其置于镜像构建上下文目录中。 其它 docker build 的用法 直接用 Git repo 进行构建 或许你已经注意到了，docker build 还支持从 URL 构建，比如可以直接从 Git repo 中构建： $ docker build https://github.com/twang2218/gitlab-ce-zh.git#:8.14docker build https://github.com/twang2218/gitlab-ce-zh.git\#:8.14Sending build context to Docker daemon 2.048 kBStep 1 : FROM gitlab/gitlab-ce:8.14.0-ce.08.14.0-ce.0: Pulling from gitlab/gitlab-ceaed15891ba52: Already exists773ae8583d14: Already exists... 这行命令指定了构建所需的 Git repo，并且指定默认的 master 分支，构建目录为 /8.14/，然后 Docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。 用给定的 tar 压缩包构建 $ docker build http://server/context.tar.gz 如果所给出的 URL 不是个 Git repo，而是个 tar 压缩包，那么 Docker 引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。 从标准输入中读取 Dockerfile 进行构建 docker build - &lt; Dockerfile 或 cat Dockerfile | docker build - 如果标准输入传入的是文本文件，则将其视为 Dockerfile，并开始构建。这种形式由于直接从标准输入中读取 Dockerfile 的内容，它没有上下文，因此不可以像其他方法那样可以将本地文件 COPY 进镜像之类的事情。 从标准输入中读取上下文压缩包进行构建 $ docker build - &lt; context.tar.gz 如果发现标准输入的文件格式是 gzip、bzip2 以及 xz 的话，将会使其为上下文压缩包，直接将其展开，将里面视为上下文，并开始构建。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Base - Html, Css, Javascript]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fbase%2FREADME%2F</url>
    <content type="text"><![CDATA[Base - Html, Css, Javascript 前端 web 技术的基石：html + css + js HTML 定义了网页的内容。 CSS 定义了网页的样式。 JavaScript 定义了网页的行为。 📖 本章内容 建议学习路线：html -&gt; css -&gt; js Html Css Javascript 📚 拓展阅读 Css 教程 JavaScript 教程 🚪 传送门 👉 前端技术指南]]></content>
  </entry>
  <entry>
    <title><![CDATA[Angular 教程]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fmvc%2Fangular%2F</url>
    <content type="text"><![CDATA[Angular 教程 📓 本文已归档到：「blog」 Angular 是一个用 HTML 和 TypeScript 构建客户端应用的平台与框架。 入门 Angular CLI 入门示例 架构 组件与模块 表单 Observable 和 RxJS 引导启动 NgModule 依赖注入 HttpClient 路由和导航 动画 参考资料 入门 Angular CLI Angular 提供了 Angular CLI，帮助用户创建项目、创建应用和库代码，并执行多种开发任务，比如测试、打包和发布。 Angular CLI，非常适合 Angular 初学者入门。 安装 Angular CLI： npm install -g @angular/cli 创建项目： ng new my-first-projectcd my-first-project 创建组件： # 语法ng generate &lt;schematic&gt; [options]ng g &lt;schematic&gt; [options]# 示例ng g c new-component # 创建组件ng g module new-module # 创建模块ng g pipe new-pipe # 创建模块ng g service new-service # 创建模块... ng generate 扩展阅读：https://angular.cn/cli/generate 启动服务： ng serve --openng s --open # 简写版 构建项目： ng build my-app -c production Angular CLI 扩展阅读： 快速上手 - angular-cli 命令参考手册 angular-cli Github 入门示例 扩展阅读： Angular 官方的英雄教程 - 通过一个小项目，一步步引导读者去使用、了解 Angular 基础特性。 架构 Angular 是一个用 HTML 和 TypeScript 构建客户端应用的平台与框架。 Angular 的基本构造块是 NgModule，它为组件提供了编译的上下文环境。 NgModule 会把相关的代码收集到一些功能集中。Angular 应用就是由一组 NgModule 定义出的。 应用至少会有一个用于引导应用的根模块，通常还会有很多特性模块。 组件定义视图。视图是一组可见的屏幕元素，Angular 可以根据你的程序逻辑和数据来选择和修改它们。 每个应用都至少有一个根组件。 组件使用服务。服务会提供那些与视图不直接相关的功能。服务提供商可以作为依赖被注入到组件中， 这能让你的代码更加模块化、可复用，而且高效。 组件和模板共同定义了 Angular 的视图。 组件类上的装饰器为其添加了元数据，其中包括指向相关模板的指针。 组件模板中的指令和绑定标记会根据程序数据和程序逻辑修改这些视图。 依赖注入器会为组件提供一些服务，比如路由器服务就能让你定义如何在视图之间导航。 组件与模块 表单 Observable 和 RxJS 引导启动 NgModule 依赖注入 HttpClient 路由和导航 动画 参考资料 官方 Angular 官网 Angular 中文网 Angular Github AngularJS Github]]></content>
  </entry>
  <entry>
    <title><![CDATA[Javascript 教程]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fbase%2Fjs%2F</url>
    <content type="text"><![CDATA[Javascript 教程 简介 用法 输出 变量 数据类型 操作符 语句 控制语句 if…else if…else 语句 switch 语句 for 循环 while 循环 break 和 continue 函数 注释 更多内容 简介 JavaScript 是一种脚本，一门编程语言，它可以在网页上实现复杂的功能，网页展现给你的不再是简单的静态信息，实时的内容更新，交互式的地图，2D/3D 动画，滚动播放的视频，等等。 用法 HTML 中的脚本必须位于 &lt;script&gt; 与 &lt;/script&gt; 标签之间。 脚本可被放置在 HTML 页面的 &lt;body&gt; 和 &lt;head&gt; 部分中。 （1）如需在 HTML 页面中插入 JavaScript，请使用 &lt;script&gt; 标签。 &lt;script&gt;alert("我的第一个 JavaScript");&lt;/script&gt; （2）如需使用外部文件，请在 &lt;script&gt; 标签的 src 属性中设置该 .js 文件： &lt;!DOCTYPE html&gt;&lt;html&gt; &lt;body&gt; &lt;script src="myScript.js"&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 输出 JavaScript 没有任何打印或者输出的函数。 JavaScript 可以通过不同的方式来输出数据： 使用 window.alert() 弹出警告框。 使用 document.write() 方法将内容写到 HTML 文档中。 使用 innerHTML 写入到 HTML 元素。 使用 console.log() 写入到浏览器的控制台。 变量 JavaScript 使用关键字 var 来定义变量， 使用等号 = 来为变量赋值。 var x, length;x = 5;length = 6; 变量命名要求： 变量必须以字母开头 变量也能以 $ 和 _ 符号开头（不过我们不推荐这么做） 变量名称对大小写敏感（y 和 Y 是不同的变量） 数据类型 JavaScript 有多种数据类型：字符串（String）、数字(Number)、布尔(Boolean)、数组(Array)、对象(Object)、空（Null）、未定义（Undefined）。 var length = 16; // Number 通过数字字面量赋值var points = x * 10; // Number 通过表达式字面量赋值var lastName = "Johnson"; // String 通过字符串字面量赋值var cars = ["Saab", "Volvo", "BMW"]; // Array 通过数组字面量赋值var person = &#123; firstName: "John", lastName: "Doe" &#125;; // Object 通过对象字面量赋值 操作符 JavaScript 支持的操作符： 赋值，算术和位运算符 条件，比较及逻辑运算符 语句 语句是用分号 ; 分隔。 x = 5 + 6;y = x * 10; 控制语句 分支： if…else if…else 语句 if (condition1) &#123; // 当条件 1 为 true 时执行的代码&#125; else if (condition2) &#123; // 当条件 2 为 true 时执行的代码&#125; else &#123; // 当条件 1 和 条件 2 都不为 true 时执行的代码&#125; switch 语句 switch (n) &#123; case 1: // 执行代码块 1 break; case 2: // 执行代码块 2 break; default: // n 与 case 1 和 case 2 不同时执行的代码 break;&#125; for 循环 for (语句 1; 语句 2; 语句 3) &#123; // 被执行的代码块&#125; while 循环 while (条件) &#123; // 需要执行的代码&#125; break 和 continue break 语句可用于跳出循环。 continue 语句中断循环中的迭代，如果出现了指定的条件，然后继续循环中的下一个迭代。 函数 函数就是包裹在花括号中的代码块，前面使用了关键词 function： function myFunction(a, b) &#123; return a * b; // 返回 a 乘于 b 的结果&#125; 注释 单行注释以 // 开头。 多行注释以 /* 开始，以 */ 结尾。 JavaScript 不会执行注释。 可以添加注释来对 JavaScript 进行解释，或者提高代码的可读性。 /* 多行注释*/// 单行注释 更多内容 📚 拓展阅读 Html Css Javascript 📦 本文归档在 我的前端技术教程系列：frontend-tutorial 书籍 You Don’t Know JS 教程 W3Cschool JavaScript 教程 MDN JavaScript 教程 规范 Airbnb JavaScript Style Guide - JavaScript 编程规范 JavaScript Standard Style - 自带 linter &amp; 代码自动修正 更多资源 awesome-javascript]]></content>
  </entry>
  <entry>
    <title><![CDATA[ngular、React、Vue]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fmvc%2Freact-vue-angular%2F</url>
    <content type="text"><![CDATA[ngular、React、Vue 框架成熟度 React，Angular 和 Vue 的比较 Typescript，ES6 与 ES5 JSX 还是 HTML 框架和库 状态管理和数据绑定 其他的编程概念 灵活性与精简到微服务 体积和性能 测试 通用与原生 app 学习曲线 总结 如何选择 React、Vue、Angular？ 更多内容 框架成熟度 活跃度： Github Star 数：React &gt; Vue &gt; Angular 贡献者数：React &gt; Angular &gt; Vue React，Angular 和 Vue 的比较 React 和 Vue 都擅长处理组件：小型的无状态的函数接收输入和返回元素作为输出。 Typescript，ES6 与 ES5 React 专注于使用 Javascript ES6； Vue 使用 Javascript ES5 或 ES6； Angular 依赖于 TypeScript。 注：与整个 JavaScript 语言相比，TypeScript 的用户群仍然很小。TypeScript 存在消失的可能性。 JSX 还是 HTML JSX 是一个类似 HTML 语法的可选预处理器，并随后在 JavaScript 中进行编译。JSX 对于开发来说是一个很大的优势，因为代码写在同一个地方，可以在代码完成和编译时更好地检查工作成果。当你在 JSX 中输入错误时，React 将不会编译，并打印输出错误的行号。 JSX 意味着 React 中的所有内容都是 Javascript – 用于 JSX 模板和逻辑。 React 使用 JSX； Angular 2 继续把 ‘JS’ 放到 HTML 中；Angular 模板使用特殊的 Angular 语法来增强 HTML； Vue 具有“单个文件组件”。这似乎是对于关注分离的权衡 - 模板，脚本和样式在一个文件中，但在三个不同的有序部分中。 注：把 html 放到 js 是好的选择，因为 js 比 html 更强大。 框架和库 Angular 是一个框架而不是一个库，因为它提供了关于如何构建应用程序的强有力的约束，并且还提供了更多开箱即用的功能。 React 和 Vue 是很灵活的。他们的库可以和各种包搭配。 状态管理和数据绑定 React 经常与 Redux 在一起使用。Redux 以三个基本原则来自述： 单一数据源（Single source of truth） State 是只读的（State is read-only） 使用纯函数执行修改（Changes are made with pure functions） 整个应用程序的状态存储在单个 store 的状态树中。这有助于调试应用程序，一些功能更容易实现。状态是只读的，只能通过 action 来改变，以避免竞争条件（这也有助于调试）。 React 和 Angular 之间的巨大差异是 单向与双向绑定。 Vue 支持单向绑定和双向绑定（默认为单向绑定）。 其他的编程概念 Angular 包含依赖注入（dependency injection），即一个对象将依赖项（服务）提供给另一个对象（客户端）的模式。这导致更多的灵活性和更干净的代码。 模型 - 视图 - 控制器模式（MVC）将项目分为三个部分：模型，视图和控制器。Angular（MVC 模式的框架）有开箱即用的 MVC 特性。React 只有 V —— 你需要自己解决 M 和 C。 灵活性与精简到微服务 React 和 Vue 通过只选择真正需要的东西，你可以更好地控制应用程序的大小。它们提供了更灵活的方式去把一个老应用的一部分从单页应用（SPA）转移到微服务。Angular 最适合单页应用（SPA），因为它可能太臃肿而不能用于微服务。 React 可以让你将应用程序的一小部分替换成更好用的 JS 库，而不是期待你的框架能够创新。小巧，可组合的单一用途工具的理念永远不会过时。 总结一下：Vue 有着很好的性能和高深的内存分配技巧。如果比较快慢的话，这些框架都非常接近（比如 Inferno）。请记住，性能基准只能作为考虑的附注，而不是作为判断标准。 体积和性能 Angular 框架非常臃肿。gzip 文件大小为 143k，而 Vue 为 23K，React 为 43k。 为了提高性能，React 和 Vue 都使用了虚拟 DOM（Virtual DOM）。 测试 Facebook 使用 Jest 来测试其 React 代码。 Angular 2 中使用 Jasmine 作为测试框架。 Vue 缺乏测试指导 通用与原生 app React 和 Angular 都支持原生开发。Angular 拥有用于原生应用的 NativeScript（由 Telerik 支持）和用于混合开发的 Ionic 框架。借助 React，你可以试试 react-native-renderer 来构建跨平台的 iOS 和 Android 应用程序，或者用 react-native 开发原生 app。许多 app（包括 Facebook；查看更多的展示）都是用 react-native 构建的。 学习曲线 Angular 的学习曲线确实很陡。 对于 React，你可能需要针对第三方库进行大量重大决策。 Vue 学习起来很容易。 总结：如果你是一个没有经验的 Javascript 开发人员 - 或者如果你在过去十年中主要使用 jQuery，那么你应该考虑使用 Vue。转向 React 时，思维方式的转换更为明显。Vue 看起来更像是简单的 Javascript，同时也引入了一些新的概念：组件，事件驱动模型和单向数据流。这同样是很小的部分。 在调试方面，React 和 Vue 的黑魔法更少是一个加分项。找出 bug 更容易，因为需要看的地方少了，堆栈跟踪的时候，自己的代码和那些库之间有更明显的区别。 总结 如何选择 React、Vue、Angular？ 我应该选什么？如果你在 Google 工作：Angular 如果你喜欢 TypeScript：Angular（或 React） 如果你喜欢面向对象编程（OOP）: Angular 如果你需要指导手册，架构和帮助：Angular 如果你在 Facebook 工作：React 如果你喜欢灵活性：React 如果你喜欢大型的技术生态系统：React 如果你喜欢在几十个软件包中进行选择：React 如果你喜欢 JS 和“一切都是 Javascript 的方法”：React 如果你喜欢真正干净的代码：Vue 如果你想要最平缓的学习曲线：Vue 如果你想要最轻量级的框架：Vue 如果你想在一个文件中分离关注点：Vue 如果你一个人工作，或者有一个小团队：Vue（或 React） 如果你的应用程序往往变得非常大：Angular（或 React） 如果你想用 react-native 构建一个应用程序：React 如果你想在圈子中有很多的开发者：Angular 或 React 如果你与设计师合作，并需要干净的 HTML 文件：Angular or Vue 如果你喜欢 Vue 但是害怕有限的技术生态系统：React 如果你不能决定，先学习 React，然后 Vue，然后 Angular。 更多内容 [译] 2017 年比较 Angular、React、Vue 三剑客]]></content>
  </entry>
  <entry>
    <title><![CDATA[Vue 教程]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fmvc%2Fvue%2F</url>
    <content type="text"><![CDATA[Vue 教程 简介 vue 生命周期 更多内容 简介 Vue (读音 /vjuː/，类似于 view) 是一套用于构建用户界面的渐进式框架。与其它大型框架不同的是，Vue 被设计为可以自底向上逐层应用。Vue 的核心库只关注视图层，不仅易于上手，还便于与第三方库或既有项目整合。 Vue.js 的核心是一个允许采用简洁的模板语法来声明式地将数据渲染进 DOM 的系统： html &lt;div id="app"&gt;&#123;&#123; message &#125;&#125;&lt;/div&gt; Javascript var app = new Vue(&#123; el: '#app', data: &#123; message: 'Hello Vue!' &#125;&#125;); vue 生命周期 更多内容 官方 vue Github vue 官方文档 vue-router Github - vue 官方路由框架 vuex Github - vue 官方集中式状态管理框架 vue-cli Github - vue 官方开发工具 vue-devtools Github - vue 官方 debug 工具 更多资源 Awesome Vue]]></content>
  </entry>
  <entry>
    <title><![CDATA[TypeScript]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fes6%2Ftypescript%2F</url>
    <content type="text"><![CDATA[TypeScript TypeScript 是 JavaScript 类型的超集，它可以编译成纯 JavaScript。 TypeScript 可以在任何浏览器、任何计算机和任何操作系统上运行，并且是开源的。 关键词： typescript, tsc 入门 （1）安装 TypeScript npm install -g typescript （2）编写 TypeScript 代码 创建 greeter.ts 文件，内容如下： class Student &#123; fullName: string; constructor(public firstName, public middleInitial, public lastName) &#123; this.fullName = firstName + " " + middleInitial + " " + lastName; &#125;&#125;interface Person &#123; firstName: string; lastName: string;&#125;function greeter(person : Person) &#123; return "Hello, " + person.firstName + " " + person.lastName;&#125;let user = new Student("Jane", "M.", "User");document.body.innerHTML = greeter(user); （3）使用 tsc 编译代码 tsc greeter.ts 输出结果为一个 greeter.js 文件，它包含了和输入文件中相同的 JavsScript 代码。 （4）在 html 中引用编译后的 js 文件 新建 greeter.html 文件，内容如下： &lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt;&lt;title&gt;TypeScript Greeter&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;script src="greeter.js"&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 在浏览器里打开 greeter.html 运行这个应用。 更多内容 📦 本文归档在 我的前端技术教程系列：frontend-tutorial 官方 TypeScript 官网 TypeScript 中文网 TypeScript Github TypeScript 样例 教程 TypeScript 入门教程]]></content>
  </entry>
  <entry>
    <title><![CDATA[Yarn 入门]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fnodejs%2Fyarn%2F</url>
    <content type="text"><![CDATA[Yarn 入门 Yarn 是快速、可靠、安全的 js 包管理器。 关键词： nodejs, 包管理, yarn, yarn.lock 简介 安装 Yarn 工作流 Yarn 常用命令 初始化新项目 添加依赖包 更新依赖包 删除依赖包 安装依赖项 创建一个新项目 配置文件 更多内容 简介 Yarn 是快速、可靠、安全的 js 包管理器。 快速 - Yarn 会缓存它下载的每个包，所以无需重复下载。它还能并行化操作以最大化资源利用率，安装速度之快前所未有。 安全 - Yarn 会在每个安装包被执行前校验其完整性。 可靠 - Yarn 使用格式详尽而又简洁的 lockfile 文件和确定性算法来安装依赖，能够保证在一个系统上的运行的安装过程也会以同样的方式运行在其他系统上。 安装 先决条件：已安装 Nodejs。 执行命令：npm i -g yarn 虽然还有其他安装方式，但并不推荐。 Yarn 工作流 Yarn 工作流： 创建一个新项目 增加／更新／删除依赖 安装／重装你的依赖 引入版本控制系统（例如 git） 持续集成 Yarn 常用命令 每个命令都会更新 package.json 和 yarn.lock 文件。 初始化新项目 yarn init 添加依赖包 在使用一个包之前，你需要执行以下命令将其加入依赖项列表： yarn add [package] [package]会被加入到package.json文件中的依赖列表，同时yarn.lock也会被更新。 &#123; "name": "my-package", "dependencies": &#123;+ "package-1": "^1.0.0" &#125; &#125; 你可以用以下参数添加其它类型的依赖： yarn add --dev 添加到 devDependencies yarn add --peer 添加到 peerDependencies yarn add --optional 添加到 optionalDependencies 通过指定依赖版本和标签，你可以安装一个特定版本的包： yarn add [package]@[version]yarn add [package]@[tag] [version] 或 [tag] 会被添加到 package.json，并在安装依赖时被解析。 例如： yarn add package-1@1.2.3yarn add package-2@^1.0.0yarn add package-3@beta&#123; "dependencies": &#123; "package-1": "1.2.3", "package-2": "^1.0.0", "package-3": "beta" &#125;&#125; 将依赖项添加到不同依赖项类别 分别添加到 devDependencies、peerDependencies 和 optionalDependencies： yarn add [package] --devyarn add [package] --peeryarn add [package] --optional 更新依赖包 yarn upgrade [package]yarn upgrade [package]@[version]yarn upgrade [package]@[tag] 这会更新package.json和yarn.lock 文件。 &#123; "name": "my-package", "dependencies": &#123;- "package-1": "^1.0.0"+ "package-1": "^2.0.0" &#125; &#125; 删除依赖包 yarn remove [package] 这会更新package.json和yarn.lock 文件。 安装依赖项 yarn install 是用于安装一个项目的所有依赖。 Yarn 会从 package.json 中读取依赖，并将依赖信息存储到 yarn.lock 中。 如果你正在开发一个包，通常你会在以下情况之后进行依赖安装： 你刚检出需要这些依赖项的项目代码。 项目的另一个开发者添加了新的依赖，你需要用到。 有很多参数可以控制依赖安装的过程，包括： 安装所有依赖 - yarn 或 yarn install 安装一个包的单一版本 - yarn install --flat 强制重新下载所有包 - yarn install --force 只安装生产环境依赖 - yarn install --production 参考：yarn install 的 完整参数列表。 创建一个新项目 不论是已经有了现成的代码仓库（目录），还是正着手启动一个全新项目，你都可以使用同样的方法引入 Yarn。 在命令行终端里，跳转到准备引入 Yarn 的目录（通常是一个项目的根目录），执行以下命令： yarn init 这将打开一个用于创建 Yarn 项目的交互式表单，其中包含以下问题： name (your-project):version (1.0.0):description:entry point (index.js):git repository:author:license (MIT): 你既可以回答这些问题，也可以直接敲回车键（enter/return）使用默认配置或者留空。 配置文件 为了别人能使用你的包，以下文件必须被提交进版本控制系统： package.json - 包含包的所有依赖信息； yarn.lock - 记录每一个依赖项的确切版本信息； 包实现功能的实际项目代码。 请参阅Yarn Example Package项目，查看一个可用的 Yarn 包所需的最少文件配置。 现在应该创建了一个和下面文件内容类似的 package.json： &#123; "name": "my-new-project", "version": "1.0.0", "description": "My New Project description.", "main": "index.js", "repository": &#123; "url": "https://example.com/your-username/my-new-project", "type": "git" &#125;, "author": "Your Name &lt;you@example.com&gt;", "license": "MIT"&#125; 执行yarn init之后，除了以上文件被创建之外，没有任何副作用。你可以随意编辑此文件。 package.json文件里存储了项目的有关信息。 包括项目名称、维护者信息、代码托管地址，以及最重要的：项目依赖。 更多内容 📚 拓展阅读 Node.js Npm Yarn 📦 本文归档在 我的前端技术教程系列：frontend-tutorial Yarn Github Yarn 官方文档]]></content>
  </entry>
  <entry>
    <title><![CDATA[JavaScript QA 工具总结]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fqa%2Fqa-summary%2F</url>
    <content type="text"><![CDATA[JavaScript QA 工具总结 JavaScript QA 工具总结 [JSLint](#jslinthttpsgithubcomdouglascrockfordjslint) [JSHint](#jshinthttpsgithubcomjshintjshint) [ESLint ✔️](#eslinthttpsgithubcomeslinteslint-heavy_check_mark) [Prettier ✔️](#prettierhttpsgithubcomprettierprettier-heavy_check_mark) [EditorConfig ✔️](#editorconfighttpseditorconfigorg-heavy_check_mark) [Standard ✔️](#standardhttpsgithubcomferossstandard-heavy_check_mark) [TSlint](#tslinthttpsgithubcompalantirtslint) 更多内容 JSLint JavaScript 静态检查工具。 优点 可以直接使用 缺点 没有配置文件，规则不能修改 不支持自定义规则 没有文档记录规则 很难弄清楚哪个规则引起的错误 JSHint JavaScript 静态检查工具。 优点 大多是参数可以配置 支持配置文件，在大项目中容易使用 已经支持需要类库，像 jQuery、QUnit、NodeJS、Mocha 等 支持基本的 ES6 缺点 难于知道哪个规则产生错误 存在两类选项：强制选项和松散选项。使得配置有些混乱 不支持自定义规则 ESLint ✔️ JavaScript 静态检查工具。 优点 灵活：任何规则都可以开启闭合，以及有些规则有些额外配置 很容易拓展和有需要可用插件 容易理解产出 包含了在其他检查器中不可用的规则，使得 ESLint 在错误检查上更有用 支持 ES6，唯一支持 JSX 的工具 支持自定义报告 缺点 速度慢，但不是主要问题 Prettier ✔️ 可定制的代码格式化工具。 支持格式： JavaScript · TypeScript · Flow · JSX · JSON CSS · SCSS · Less HTML · Vue · Angular GraphQL · Markdown · YAML Intellij 和 Vscode 都可以安装 Prettier 插件来使用。 Install · Options · CLI · API EditorConfig ✔️ 可以在编辑器或 IDE 中维护代码格式。 EditorConfig 应用示例 Standard ✔️ JavaScript 静态检查工具。 优点 无须配置。 自动代码格式化。 只需运行 standard --fix 提前发现风格及程序问题。 TSlint TypeScript 静态检查工具。 更多内容 📦 本文归档在 我的前端技术教程系列：frontend-tutorial]]></content>
  </entry>
  <entry>
    <title><![CDATA[Webpack 入门]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fwebpack%2Fwebpack%2F</url>
    <content type="text"><![CDATA[Webpack 入门 Webpack 是一个模组打包工具（module bundler）。其主要目的是将 JavaScript 文件捆绑在浏览器中，但它也能够转换，捆绑或打包任何资源文件。 Webpack 可以按需加载应用程序的组件。使得 Javascript 应用可以高度复用。 当前版本：4.x 关键词： webpack, loader 安装 本地安装 全局安装 创建一个 bundle 文件 webpack.config.js 单入口(Entry) 多入口(Entry) 更多内容 安装 本地安装 $ npm install --save-dev webpack$ npm install --save-dev webpack@&lt;version&gt; 如果你在项目中使用了 npm ，npm 首先会在本地模块中寻找 webpack。这是一个实用的小技巧。 "scripts": &#123; "start": "webpack --config mywebpack.config.js"&#125; 上面是 npm 的标准配置，也是我们推荐的实践。 当你在本地安装 webpack 后，你能够从 node_modules/.bin/webpack 访问它的 bin 版本。 全局安装 $ npm install --global webpack webpack 命令现在可以全局执行了。 创建一个 bundle 文件 创建一个 app/index.js 文件。 document.write("&lt;h1&gt;Hello World&lt;/h1&gt;"); 创建一个 index.html 文件。 &lt;html&gt;&lt;body&gt;&lt;script type="text/javascript" src="./dist/bundle.js"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 执行命令 $ webpack ./app/index.js ./dist/bundle.js 这条命令的第一个参数为输入文件，第二个参数为输出文件。 会在目录下生成一个 dist/bundle.js 文件，它已打包所需的所有代码的输出文件。 在浏览器中打开 index.html 文件。 🔦 示例： (DEMO00) webpack.config.js webpack.config.js 为 webpack 默认的配置文件，当执行 webpack 命令时，webpack 会在当前目录下自动搜索 webpack.config.js 文件。 单入口(Entry) 基于 (DEMO00) 的代码，新建一个 webpack.config.js 文件，内容如下： const path = require("path");module.exports = &#123; // 这里应用程序开始执行 // webpack 开始打包 entry: "./app/index.js", // webpack 如何输出结果的相关选项 output: &#123; // 所有输出文件的目标路径 // 必须是绝对路径（使用 Node.js 的 path 模块） path: path.resolve(__dirname, "dist"), // 「入口分块(entry chunk)」的文件名模板（出口分块？） filename: "bundle.js" &#125;&#125;; 执行命令 $ webpack 在浏览器中打开 index.html 文件。 🔦 示例： (DEMO01) 多入口(Entry) 如果有多个入口文件怎么办？很简单，我们来看一个示例： 新建 app/about.js 文件 document.write("&lt;h2&gt;ABOUT&lt;/h2&gt;"); 新建 app/home.js 文件 document.write("&lt;h1&gt;HOME&lt;/h1&gt;"); 新建 index.html 文件 &lt;html&gt;&lt;body&gt;&lt;script src="dist/home.js"&gt;&lt;/script&gt;&lt;script src="dist/about.js"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 新建 webpack.config.js 文件 const path = require("path");module.exports = &#123; // 这里应用程序开始执行 // webpack 开始打包 // 本例中 entry 为多入口 entry: &#123; home: "./app/home.js", about: "./app/about.js" &#125;, // webpack 如何输出结果的相关选项 output: &#123; // 所有输出文件的目标路径 // 必须是绝对路径（使用 Node.js 的 path 模块） path: path.resolve(__dirname, "dist"), // 「入口分块(entry chunk)」的文件名模板（出口分块？） // filename: "bundle.js", // 用于多个入口点(entry point)（出口点？） filename: "[name].js" // 用于多个入口点(entry point)（出口点？） // filename: "[chunkhash].js", // 用于长效缓存 // filename: "[name].[chunkhash].js", // 用于长效缓存 &#125;&#125;; 执行命令 $ webpack 在浏览器中打开 index.html 文件。 🔦 示例： (DEMO02) 更多内容 📚 拓展阅读 Webpack 入门 Webpack 概念 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 📦 本文归档在 我的前端技术教程系列：frontend-tutorial 官方资料 Webpack 官网 Webpack 中文网 webpack Github webpack-dev-server Github webpack-dev-server 官方文档 入门资料 webpack-demos webpack-howto webpack-handbook 教程 如何学习 Webpack Webpack 概念 Webpack 入门 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 文章 JavaScript 模块化七日谈 前端模块化开发那点历史 更多资源 awesome-webpack-cn]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mysql 原理]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Fmysql%2Fmysql-theory%2F</url>
    <content type="text"><![CDATA[Mysql 原理 关键词：存储引擎,数据类型,事务,MVCC,索引,执行计划,主从复制 1. 存储引擎 1.1. InnoDB 1.2. MyISAM 1.3. 选择存储引擎 2. 数据类型 2.1. 整型 2.2. 浮点数 2.3. 字符串 2.4. 时间和日期 3. 事务 3.1. 事务隔离级别 3.2. 死锁 4. MVCC 5. 索引 5.1. 索引的优点和缺点 5.2. 索引类型 5.3. 索引数据结构 5.4. 索引原则 6. 查询性能优化 6.1. 使用 Explain 进行分析 6.2. 优化数据访问 6.3. 重构查询方式 7. 复制 7.1. 主从复制 7.2. 读写分离 8. 参考资料 1. 存储引擎 在文件系统中，Mysql 将每个数据库（也可以成为 schema）保存为数据目录下的一个子目录。创建表示，Mysql 会在数据库子目录下创建一个和表同名的 .frm 文件保存表的定义。因为 Mysql 使用文件系统的目录和文件来保存数据库和表的定义，大小写敏感性和具体平台密切相关。Windows 中大小写不敏感；类 Unix 中大小写敏感。不同的存储引擎保存数据和索引的方式是不同的，但表的定义则是在 Mysql 服务层统一处理的。 1.1. InnoDB InnoDB 是 MySQL 默认的事务型存储引擎，只有在需要 InnoDB 不支持的特性时，才考虑使用其它存储引擎。 InnoDB 实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ 间隙锁（next-key locking）防止幻影读。 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。 内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。 支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。 1.2. MyISAM MyISAM 设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用 MyISAM。 MyISAM 提供了大量的特性，包括压缩表、空间数据索引等。 不支持事务。 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。 可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。 如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。 1.3. 选择存储引擎 Mysql 内置的存储引擎 InnoDB - Mysql 的默认事务型存储引擎。性能不错且支持自动崩溃恢复。 MyISAM - Mysql 5.1 版本前的默认存储引擎。特性丰富但不支持事务，也没有崩溃恢复功能。 CSV - 可以将 CSV 文件作为 Mysql 的表来处理，但这种表不支持索引。 Memory - 适合快速访问数据，且数据不会被修改，重启丢失也没有关系。 NDB - 用于 Mysql 集群场景。 如何选择合适的存储引擎？ 大多数情况下，InnoDB 都是正确的选择，除非需要用到 InnoDB 不具备的特性。 如果应用需要选择 InnoDB 以外的存储引擎，可以考虑以下因素： 事务：如果需要支持事务，InnoDB 是首选。如果不需要支持事务，且主要是 SELECT 和 INSERT 操作，MyISAM 是不错的选择。 并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。所以，InnoDB 并发性能更高。 外键：InnoDB 支持外键。 备份：InnoDB 支持在线热备份。 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。 其它特性：MyISAM 支持压缩表和空间数据索引。 转换表的存储引擎 下面的语句可以将 mytable 表的引擎修改为 InnoDB ALTER TABLE mytable ENGINE = InnoDB 2. 数据类型 2.1. 整型 TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。 INT(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。 2.2. 浮点数 FLOAT 和 DOUBLE 为浮点类型，DECIMAL 为高精度小数类型。CPU 原生支持浮点运算，但是不支持 DECIMAl 类型的计算，因此 DECIMAL 的计算比浮点类型需要更高的代价。 FLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。 2.3. 字符串 主要有 CHAR 和 VARCHAR 两种类型，一种是定长的，一种是变长的。 VARCHAR 这种变长类型能够节省空间，因为只需要存储必要的内容。但是在执行 UPDATE 时可能会使行变得比原来长，当超出一个页所能容纳的大小时，就要执行额外的操作。MyISAM 会将行拆成不同的片段存储，而 InnoDB 则需要分裂页来使行放进页内。 VARCHAR 会保留字符串末尾的空格，而 CHAR 会删除。 2.4. 时间和日期 MySQL 提供了两种相似的日期时间类型：DATATIME 和 TIMESTAMP。 DATATIME 能够保存从 1001 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。 它与时区无关。 默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATATIME 值，例如“2008-01-16 22:37:08”，这是 ANSI 标准定义的日期和时间表示方法。 TIMESTAMP 和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年 到 2038 年。 它和时区有关，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。 MySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。 默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。 应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。 3. 事务 事务指的是满足 ACID 特性的一组操作。 Mysql 中，使用 START TRANSACTION 语句开始一个事务；使用 COMMIT 语句提交所有的修改；使用 ROLLBACK 语句撤销所有的修改。 Mysql 不是所有的存储引擎都实现了事务处理。支持事务的存储引擎有：InnoDB 和 NDB Cluster。 用户可以根据业务是否需要事务处理（事务处理可以保证数据安全，但会增加系统开销），选择合适的存储引擎。 Mysql 默认采用自动提交（AUTOCOMMIT）模式。 3.1. 事务隔离级别 InnoDB 支持 SQL 标准的四种隔离级别，默认的级别是可重复读。并且，通过间隙锁（next-key locking）策略防止幻读的出现。 3.2. 死锁 在 Mysql 中，锁的行为和顺序与存储引擎相关。 InnoDB 中解决死锁问题的方法是：将持有最少行级排他锁的事务进行回滚。 4. MVCC InnoDB 的 MVCC，是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的过期时间。当然，存储的并不是实际的时间值，而是系统版本号。每开始一个新事务，系统版本号就会自动递增。事务开始时的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。 下面是在可重复读隔离级别下，MVCC 的具体操作： SELECT 当开始新一个事务时，该事务的版本号肯定会大于当前所有数据行快照的创建版本号，理解这一点很关键。 多个事务必须读取到同一个数据行的快照，并且这个快照是距离现在最近的一个有效快照。但是也有例外，如果有一个事务正在修改该数据行，那么它可以读取事务本身所做的修改，而不用和其它事务的读取结果一致。 把没有对一个数据行做修改的事务称为 T，T 所要读取的数据行快照的创建版本号必须小于 T 的版本号，因为如果大于或者等于 T 的版本号，那么表示该数据行快照是其它事务的最新修改，因此不能去读取它。 除了上面的要求，T 所要读取的数据行快照的删除版本号必须大于 T 的版本号，因为如果小于等于 T 的版本号，那么表示该数据行快照是已经被删除的，不应该去读取它。 INSERT 将当前系统版本号作为数据行快照的创建版本号。 DELETE 将当前系统版本号作为数据行快照的删除版本号。 UPDATE 将当前系统版本号作为更新后的数据行快照的创建版本号，同时将当前系统版本号作为更新前的数据行快照的删除版本号。可以理解为先执行 DELETE 后执行 INSERT。 5. 索引 索引能够轻易将查询性能提升几个数量级。 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效。对于中到大型的表，索引就非常有效。但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。 索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。 5.1. 索引的优点和缺点 优点： 大大减少了服务器需要扫描的数据行数。 帮助服务器避免进行排序和创建临时表（B+Tree 索引是有序的，可以用来做 ORDER BY 和 GROUP BY 操作）； 将随机 I/O 变为顺序 I/O（B+Tree 索引是有序的，也就将相邻的数据都存储在一起）。 缺点： 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立组合索引那么需要的空间就会更大。 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 5.2. 索引类型 MySQL 目前主要有以下几种索引类型： 普通索引 普通索引：最基本的索引，没有任何限制。 CREATE TABLE `table` ( ... INDEX index_name (title(length))) 唯一索引 唯一索引：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。 CREATE TABLE `table` ( ... UNIQUE indexName (title(length))) 主键索引 主键索引：一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。一般是在建表的时候同时创建主键索引。 CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT, ... PRIMARY KEY (`id`)) 组合索引 组合索引：多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合。 CREATE TABLE `table` ( ... INDEX index_name (title(length), title(length), ...)) 全文索引 全文索引：主要用来查找文本中的关键字，而不是直接与索引中的值相比较。fulltext 索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的 WHERE 语句的参数匹配。fulltext 索引配合 match against 操作使用，而不是一般的 WHERE 语句加 LIKE。它可以在 CREATE TABLE，ALTER TABLE ，CREATE INDEX 使用，不过目前只有 char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用 CREATE INDEX 创建 fulltext 索引，要比先为一张表建立 fulltext 然后再将数据写入的速度快很多。 CREATE TABLE `table` ( `content` text CHARACTER NULL, ... FULLTEXT (content)) 5.3. 索引数据结构 B+Tree 索引 B+Tree 索引是大多数 MySQL 存储引擎的默认索引类型。 因为不再需要进行全表扫描，只需要对树进行搜索即可，因此查找速度快很多。除了用于查找，还可以用于排序和分组。 可以指定多个列作为索引列，多个索引列共同组成键。 B+Tree 索引适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。 如果不是按照索引列的顺序进行查找，则无法使用索引。 InnoDB 的 B+Tree 索引分为主索引和辅助索引。 主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。 B Tree 原理 B-Tree 定义一条数据记录为一个二元组 [key, data]，B-Tree 是满足下列条件的数据结构： 所有叶节点具有相同的深度，也就是说 B-Tree 是平衡的； 一个节点中的 key 从左到右非递减排列； 如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。 查找算法：首先在根节点进行二分查找，如果找到则返回对应节点的 data，否则在相应区间的指针指向的节点递归进行查找。 由于插入删除新的数据记录会破坏 B-Tree 的性质，因此在插入删除时，需要对树进行一个分裂、合并、旋转等操作以保持 B-Tree 性质。 B+Tree 与 B-Tree 相比，B+Tree 有以下不同点： 每个节点的指针上限为 2d 而不是 2d+1（d 为节点的出度）； 内节点不存储 data，只存储 key； 叶子节点不存储指针。 顺序访问指针的 B+Tree 一般在数据库系统或文件系统中使用的 B+Tree 结构都在经典 B+Tree 基础上进行了优化，在叶子节点增加了顺序访问指针，做这个优化的目的是为了提高区间访问的性能。 优势 红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B Tree 作为索引结构，主要有以下两个原因： （一）更少的检索次数 平衡树检索数据的时间复杂度等于树高 h，而树高大致为 O(h)=O(logdN)，其中 d 为每个节点的出度。 红黑树的出度为 2，而 B Tree 的出度一般都非常大。红黑树的树高 h 很明显比 B Tree 大非常多，因此检索的次数也就更多。 B+Tree 相比于 B-Tree 更适合外存索引，因为 B+Tree 内节点去掉了 data 域，因此可以拥有更大的出度，检索效率会更高。 （二）利用计算机预读特性 为了减少磁盘 I/O，磁盘往往不是严格按需读取，而是每次都会预读。这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的旋转时间，因此速度会非常快。 操作系统一般将内存和磁盘分割成固态大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点，并且可以利用预读特性，相邻的节点也能够被预先载入。 更多内容请参考：MySQL 索引背后的数据结构及算法原理 哈希索引 InnoDB 引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 哈希索引能以 O(1) 时间进行查找，但是失去了有序性，它具有以下限制： 无法用于排序与分组； 只支持精确查找，无法用于部分查找和范围查找； 全文索引 MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。查找条件使用 MATCH AGAINST，而不是普通的 WHERE。 全文索引一般使用倒排索引实现，它记录着关键词到其所在文档的映射。 InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。 空间数据索引（R-Tree） MyISAM 存储引擎支持空间数据索引，可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。 必须使用 GIS 相关的函数来维护数据。 5.4. 索引原则 最左前缀匹配原则 mysql 会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配。 例如：a = 1 and b = 2 and c &gt; 3 and d = 4，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d 的顺序可以任意调整。 让选择性最强的索引列放在前面，索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。 例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。 SELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity,COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity,COUNT(*)FROM payment; staff_id_selectivity: 0.0001customer_id_selectivity: 0.0373 COUNT(*): 16049 = 和 in 可以乱序 比如 a = 1 and b = 2 and c = 3 建立（a,b,c）索引可以任意顺序，mysql 的查询优化器会帮你优化成索引可以识别的形式。 索引列不能参与计算 在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引。 例如下面的查询不能使用 actor_id 列的索引： SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5; 尽量的扩展索引，不要新建索引 比如表中已经有 a 的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 多列索引 在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。 SELECT film_id, actor_ id FROM sakila.film_actorWhERE actor_id = 1 AND film_id = 1; 前缀索引 对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。 对于前缀长度的选取需要根据索引选择性来确定。 覆盖索引 索引包含所有需要查询的字段的值。 具有以下优点： 因为索引条目通常远小于数据行的大小，所以若只读取索引，能大大减少数据访问量。 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。 6. 查询性能优化 6.1. 使用 Explain 进行分析 Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。 比较重要的字段有： select_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数 更多内容请参考：MySQL 性能优化神器 Explain 使用分析 6.2. 优化数据访问 减少请求的数据量 （一）只返回必要的列 最好不要使用 SELECT * 语句。 （二）只返回必要的行 使用 WHERE 语句进行查询过滤，有时候也需要使用 LIMIT 语句来限制返回的数据。 （三）缓存重复查询的数据 使用缓存可以避免在数据库中进行查询，特别要查询的数据经常被重复查询，缓存可以带来的查询性能提升将会是非常明显的。 减少服务器端扫描的行数 最有效的方式是使用索引来覆盖查询。 6.3. 重构查询方式 切分大查询 一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。 DELEFT FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH); rows_affected = 0do &#123; rows_affected = do_query( "DELETE FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000")&#125; while rows_affected &gt; 0 分解大连接查询 将一个大连接查询（JOIN）分解成对每一个表进行一次单表查询，然后将结果在应用程序中进行关联，这样做的好处有： 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。 减少锁竞争； 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可扩展。 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。 SELECT * FROM tagJOIN tag_post ON tag_post.tag_id=tag.idJOIN post ON tag_post.post_id=post.idWHERE tag.tag='mysql'; SELECT * FROM tag WHERE tag='mysql';SELECT * FROM tag_post WHERE tag_id=1234;SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904); 7. 复制 7.1. 主从复制 主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。 binlog 线程 ：负责将主服务器上的数据更改写入二进制文件（binlog）中。 I/O 线程 ：负责从主服务器上读取二进制日志文件，并写入从服务器的中继日志中。 SQL 线程 ：负责读取中继日志并重放其中的 SQL 语句。 7.2. 读写分离 主服务器用来处理写操作以及实时性要求比较高的读操作，而从服务器用来处理读操作。 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。 MySQL 读写分离能提高性能的原因在于： 主从服务器负责各自的读和写，极大程度缓解了锁的争用； 从服务器可以配置 MyISAM 引擎，提升查询性能以及节约系统开销； 增加冗余，提高可用性。 8. 参考资料 BaronScbwartz, PeterZaitsev, VadimTkacbenko 等. 高性能 MySQL[M]. 电子工业出版社, 2013. 姜承尧. MySQL 技术内幕: InnoDB 存储引擎 [M]. 机械工业出版社, 2011. 20+ 条 MySQL 性能优化的最佳经验 How to create unique row ID in sharded databases? SQL Azure Federation – Introduction]]></content>
  </entry>
  <entry>
    <title><![CDATA[计算机网络之物理层]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fcommunication%2Fnetwork-physical%2F</url>
    <content type="text"><![CDATA[计算机网络之物理层 摘要 物理层（Physical Layer） - 物理层只接收和发送一串比特(bit)流，不考虑信息的意义和信息结构。 数据单元：比特流。 典型设备：光纤、同轴电缆、双绞线、中继器和集线器。 通信系统模型 通信方式 通信信号 调制解调 基本带通调制方法 通信媒介 信道复用 通信系统模型 通信系统模型分为三大部分：源系统（包括信源和发送器）、传输系统、目的系统（包括信宿接收器）。 重要概念： 信源 - 也叫源点。产生各类信息的实体。 信道 - 通信的通道，是信号传输的媒介。 信宿 - 传输信息的归宿。 码元 - 在数字通信中常常用时间间隔相同的符号来表示一个二进制数字，这样的时间间隔内的信号称为(二进制）码元。 通信方式 有三种通信方式： 单工通信：单向传输 半双工通信：双向交替传输 全双工通信：双向同时传输 通信信号 通信的目的是传送消息。如语音、文字、图像、视频都是消息。数据时传送消息的实体。信号是数据的电气或电磁的表现。 模拟信号和数字信号 模拟信号 - 模拟信号是连续的信号。 数字信号 - 数字信号是离散的信号。 调制解调 重要概念： 基带信号 - 来自信源的信号叫做基带信号。 调制 - 将各种数字基带信号转换成适于信道传输的数字调制信号(已调信号或频带信号)。简单来说：调制即，数字 -&gt; 模拟。 解调 - 在接收端将收到的数字频带信号还原成数字基带信号。简单来说：解调即，模拟 -&gt; 数字。 📌 提示：我们上网时所用到的调制解调器（俗称“猫”），指的就是转换数字和模拟信号的机器。 信号要在信道上传输就要经过调制。 调制分为：基带调制和带通调制 基本带通调制方法 如果你收听过广播，一定经常听到 AM、FM 这两个关键词，这是什么意思呢？答案如下： 调幅（AM） - 即载波的振幅随基带数字信号而变化。 调频（FM） - 即载波的频率随基带数字信号而变化。 调相（PM） - 即载波的初始相位随基带数字信号而变化。 📌 提示：我们收听广播时，为了接收不同广播台的信号，就要调整 AM 或 FM，指的就是这里的调制方法。 通信媒介 通信媒介分为两大类： 导引型 - 双绞线、电缆、光纤 非导引型 - 无线、红外线、大气、激光 信道复用 信道复用就是将用于传输信道的总带宽划分成若干个子频带（或称子信道），每一个子信道传输一路信号。 频分复用 时分复用 波分复用 码分复用]]></content>
      <categories>
        <category>communication</category>
      </categories>
      <tags>
        <tag>communication</tag>
        <tag>network</tag>
        <tag>physical</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gitlab 快速教程]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Fgitlab%2Fgitlab-quickstart%2F</url>
    <content type="text"><![CDATA[Gitlab 快速教程 准备 Git - 如果不熟悉 Git ，可以先阅读：Git 教程 创建你的 SSH key 使用 Gitlab 的第一步是生成你自己的 SSH 密钥对（Github 也类似）。 登录 Gitlab 打开 Profile settings. 跳转到 SSH keys tab 页 黏贴你的 SSH 公钥内容到 Key 文本框 为了便于识别，你可以为其命名 点击 Add key 将 SSH 公钥添加到 GitLab 创建项目 输入项目信息，点击 Create project 按钮，在 Gitlab 创建项目。 克隆项目到本地 可以选择 SSH 或 HTTPS 方式克隆项目到本地（推荐 SSH） 拷贝项目地址，然后在本地执行 git clone &lt;url&gt; 创建 Issue 依次点击 Project’s Dashboard &gt; Issues &gt; New Issue 可以新建 Issue 在项目中直接添加 issue 在未关闭 issue 中，点击 New Issue 添加 issue 通过项目面板添加 issue 通过 issue 面板添加 issue 更多内容 引申 操作系统、运维部署总结系列 引用 官网]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux 典型运维应用]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Flinux%E5%85%B8%E5%9E%8B%E8%BF%90%E7%BB%B4%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Linux 典型运维应用 💡 如果没有特殊说明，本文的案例都是针对 Centos 发行版本。 网络操作 无法访问外网域名 （1）在 hosts 中添加本机实际 IP 和本机实际域名的映射 echo "192.168.0.1 hostname" &gt;&gt; /etc/hosts 如果不知道本机域名，使用 hostname 命令查一下；如果不知道本机实际 IP，使用 ifconfig 查一下。 （2）配置信赖的 DNS 服务器 执行 vi /etc/resolv.conf ，添加以下内容： nameserver 114.114.114.114nameserver 8.8.8.8 114.114.114.114 是国内老牌 DNS 8.8.8.8 是 Google DNS 👉 参考：公共 DNS 哪家强 （3）测试一下能否 ping 通 www.baidu.com 开启、关闭防火墙 # 开启防火墙 22 端口iptables -I INPUT -p tcp --dport 22 -j accept# 彻底关闭防火墙sudo systemctl status firewalld.servicesudo systemctl stop firewalld.servicesudo systemctl disable firewalld.service 👉 参考：https://www.cnblogs.com/moxiaoan/p/5683743.html 系统维护 使用 NTP 进行时间同步 （1）先安装时钟同步工具 ntp yum -y install ntp ntp 的配置文件路径为： /etc/ntp.conf （2）启动 NTP 服务 systemctl start ntpd.service （3）放开防火墙 123 端口 NTP 服务的端口是 123,使用的是 udp 协议，所以 NTP 服务器的防火墙必须对外开放 udp 123 这个端口。 /sbin/iptables -A INPUT -p UDP -i eth0 -s 192.168.0.0/24 --dport 123 -j ACCEPT （4）执行时间同步 /usr/sbin/ntpdate ntp.sjtu.edu.cn ntp.sjtu.edu.cn 是上海交通大学 ntp 服务器。 （5）自动定时同步时间 执行如下命令，就可以在每天凌晨 3 点同步系统时间： echo "* 3 * * * /usr/sbin/ntpdate ntp.sjtu.edu.cn" &gt;&gt; /etc/crontabsystemctl restart crond.service 👉 参考：https://www.cnblogs.com/quchunhui/p/7658853.html 自动化脚本 Linux 开机自启动脚本 （1）在 /etc/rc.local 文件中添加命令 如果不想将脚本粘来粘去，或创建链接，可以在 /etc/rc.local 文件中添加启动命令 先修改好脚本，使其所有模块都能在任意目录启动时正常执行; 再在 /etc/rc.local 的末尾添加一行以绝对路径启动脚本的行; 例： 执行 vim /etc/rc.local 命令，输入以下内容： #!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don't# want to do the full Sys V style init stuff.touch /var/lock/subsys/local/opt/pjt_test/test.pl （2）在 /etc/rc.d/init.d 目录下添加自启动脚本 Linux 在 /etc/rc.d/init.d 下有很多的文件，每个文件都是可以看到内容的，其实都是一些 shell 脚本或者可执行二进制文件。 Linux 开机的时候，会加载运行 /etc/rc.d/init.d 目录下的程序，因此我们可以把想要自动运行的脚本放到这个目录下即可。系统服务的启动就是通过这种方式实现的。 （3）运行级别设置 简单的说，运行级就是操作系统当前正在运行的功能级别。 不同的运行级定义如下:# 0 - 停机（千万不能把initdefault 设置为0 ）# 1 - 单用户模式 进入方法#init s = init 1# 2 - 多用户，没有 NFS# 3 - 完全多用户模式(标准的运行级)# 4 - 没有用到# 5 - X11 多用户图形模式（xwindow)# 6 - 重新启动 （千万不要把initdefault 设置为6 ） 这些级别在 /etc/inittab 文件里指定，这个文件是 init 程序寻找的主要文件，最先运行的服务是放在/etc/rc.d 目录下的文件。 在 /etc 目录下面有这么几个目录值得注意：rcS.d rc0.d rc1.d … rc6.d (0，1… 6 代表启动级别 0 代表停止，1 代表单用户模式，2-5 代表多用户模式，6 代表重启) 它们的作用就相当于 redhat 下的 rc.d ，你可以把脚本放到 rcS.d，然后修改文件名，给它一个启动序号，如: S88mysql 不过，最好的办法是放到相应的启动级别下面。具体作法: （1）先把脚本 mysql 放到 /etc/init.d 目录下 （2）查看当前系统的启动级别 $ runlevelN 3 （3）设定启动级别 # 98 为启动序号# 2 是系统的运行级别，可自己调整，注意不要忘了结尾的句点$ update-rc.d mysql start 98 2 . 现在我们到 /etc/rc2.d 下，就多了一个 S98mysql 这样的符号链接。 （4）重启系统，验证设置是否有效。 （5）移除符号链接 当你需要移除这个符号连接时，方法有三种： 直接到 /etc/rc2.d 下删掉相应的链接，当然不是最好的方法； 推荐做法：update-rc.d -f s10 remove 如果 update-rc.d 命令你不熟悉，还可以试试看 rcconf 这个命令，也很方便。 👉 参考： https://blog.csdn.net/linuxshine/article/details/50717272 https://www.cnblogs.com/ssooking/p/6094740.html 定时执行脚本 （1）安装crontab （2）开启crontab服务 开机自动启动 crond 服务：chkconfig crond on 或者，按以下命令手动启动： # 启动服务systemctl start crond.service# 停止服务systemctl stop crond.service# 重启服务systemctl restart crond.service# 重新载入配置systemctl reload crond.service# 查看状态systemctl status crond.service （3）设置需要执行的脚本 有两种方法： 在命令行输入：crontab -e 然后添加相应的任务，存盘退出。 直接编辑 /etc/crontab 文件，即 vi /etc/crontab，添加相应的任务。 示例： SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed# 每天早上3点时钟同步* 3 * * * /usr/sbin/ntpdate ntp.sjtu.edu.cn # 每两个小时以root身份执行 /home/hello.sh 脚本0 */2 * * * root /home/hello.sh 👉 参考：https://blog.csdn.net/z_yong_cool/article/details/79288397 配置 设置 Linux 启动模式 停机(记得不要把 initdefault 配置为 0，因为这样会使 Linux 不能启动) 单用户模式，就像 Win9X 下的安全模式 多用户，但是没有 NFS 完全多用户模式，准则的运行级 通常不用，在一些特殊情况下可以用它来做一些事情 X11，即进到 X-Window 系统 重新启动 (记得不要把 initdefault 配置为 6，因为这样会使 Linux 不断地重新启动) 设置方法： $ sed -i 's/id:5:initdefault:/id:3:initdefault:/' /etc/inittab]]></content>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 安装部署]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[ZooKeeper 安装部署 环境要求：JDK6+ 下载解压 ZooKeeper 创建配置文件 启动 ZooKeeper 服务器 启动 CLI 停止 ZooKeeper 服务器 在安装 ZooKeeper 之前，请确保你的系统是在以下任一操作系统上运行： 任意 Linux OS - 支持开发和部署。适合演示应用程序。 Windows OS - 仅支持开发。 Mac OS - 仅支持开发。 安装步骤如下： 下载解压 ZooKeeper 进入官方下载地址：http://zookeeper.apache.org/releases.html#download ，选择合适版本。 解压到本地： $ tar -zxf zookeeper-3.4.6.tar.gz$ cd zookeeper-3.4.6 创建配置文件 你必须创建 conf/zoo.cfg 文件，否则启动时会提示你没有此文件。 初次尝试，不妨直接使用 Kafka 提供的模板配置文件 conf/zoo_sample.cfg： $ cp conf/zoo_sample.cfg conf/zoo.cfg 启动 ZooKeeper 服务器 执行以下命令 $ bin/zkServer.sh start 执行此命令后，你将收到以下响应 $ JMX enabled by default$ Using config: /Users/../zookeeper-3.4.6/bin/../conf/zoo.cfg$ Starting zookeeper ... STARTED 启动 CLI 键入以下命令 $ bin/zkCli.sh 键入上述命令后，将连接到 ZooKeeper 服务器，你应该得到以下响应。 Connecting to localhost:2181................................................Welcome to ZooKeeper!................................WATCHER::WatchedEvent state:SyncConnected type: None path:null[zk: localhost:2181(CONNECTED) 0] 停止 ZooKeeper 服务器 连接服务器并执行所有操作后，可以使用以下命令停止 zookeeper 服务器。 $ bin/zkServer.sh stop 本节安装内容参考：Zookeeper 安装 更多内容 引申 操作系统、运维部署总结系列]]></content>
  </entry>
  <entry>
    <title><![CDATA[Tomcat 安装]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Ftomcat%2F</url>
    <content type="text"><![CDATA[Tomcat 安装 安装 启动 脚本 安装 安装步骤如下： （1）下载并解压到本地 进入官网下载地址：https://tomcat.apache.org/download-80.cgi ，选择合适的版本下载。 我选择的是最新稳定版本 8.5.28：http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.28/bin/apache-tomcat-8.5.28.tar.gz 我个人喜欢存放在：/opt/tomcat wget -O /opt/tomcat/apache-tomcat-8.5.28.tar.gz http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.28/bin/apache-tomcat-8.5.28.tar.gzcd /opt/tomcattar zxvf apache-tomcat-8.5.28.tar.gz 启动 启动 tomcat 服务 cd /opt/tomcat/apache-tomcat-8.5.28/bin./catalina.sh start 停止 tomcat 服务 cd /opt/tomcat/apache-tomcat-8.5.28/bin./catalina.sh stop 脚本 | 安装脚本 | 更多内容 引申 操作系统、运维部署总结系列]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 简介]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fdocker-introduction%2F</url>
    <content type="text"><![CDATA[Docker 简介 Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。 概述 Docker 是什么 Docker 是开发，传输和运行应用程序的开放平台。 Docker 使您能够将应用程序与基础架构分开，以便快速交付软件。 借助 Docker，您可以像管理应用程序一样管理基础架构。通过利用 Docker 的方法快速进行运输，测试和部署代码，您可以显著缩短编写代码和在生产环境中运行代码之间的耗时。 Docker 平台 Docker 提供了被称为容器的松散隔离环境，在环境中可以打包和运行应用程序。隔离和安全性允许您在给定主机上同时运行多个容器。容器是轻量级的，因为它们不需要管理程序的额外负载，而是直接在主机的内核中运行。这意味着您可以在给定的硬件组合上运行更多容器，而不是使用虚拟机。你甚至可以在实际上是虚拟机的主机中运行 Docker 容器！ Docker 提供工具和平台来管理容器的生命周期： 使用容器开发您的应用程序及其支持组件。 容器成为分发和测试你的应用程序的单元。 准备好后，将您的应用程序部署到生产环境中，作为容器或协调服务。无论您的生产环境是本地数据中心，云提供商还是两者的混合，这都是一样的。 Docker 引擎 Docker 引擎是一个 C/S 架构的应用，它有这些主要的组件： 服务器是一个长期运行的程序，被称为守护进程。 REST API 指定程序可用于与守护进程进行通信并指示其执行操作的接口。 命令行客户端。 CLI 使用 Docker REST API 通过脚本或直接 CLI 命令来控制 Docker 守护进程或与其进行交互。许多其他 Docker 应用程序使用底层的 API 和 CLI。 守护进程创建并管理 Docker 对象，如镜像，容器，网络和卷。 传统虚拟机和 Docker 概念 Docker 包括三个基本概念 镜像（Image） 容器（Container） 仓库（Repository） 镜像 我们都知道，操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统。比如官方镜像 ubuntu:18.04 就包含了完整的一套 Ubuntu 18.04 最小系统的 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 分层存储 因为镜像包含操作系统完整的 root 文件系统，其体积往往是庞大的，因此在 Docker 设计时，就充分利用 Union FS 的技术，将其设计为分层存储的架构。所以严格来说，镜像并非是像一个 ISO 那样的打包文件，镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 容器 镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性，很多人初学 Docker 时常常会混淆容器和虚拟机。 前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。 数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。 仓库 镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。 一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt; 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。 以 Ubuntu 镜像 为例，ubuntu 是仓库的名字，其内包含有不同的版本标签，如，16.04, 18.04。我们可以通过 ubuntu:14.04，或者 ubuntu:18.04 来具体指定所需哪个版本的镜像。如果忽略了标签，比如 ubuntu，那将视为 ubuntu:latest。 仓库名经常以 两段式路径 形式出现，比如 jwilder/nginx-proxy，前者往往意味着 Docker Registry 多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体 Docker Registry 的软件或服务。 引用和引申 https://yeasy.gitbooks.io/docker_practice/content/basic_concept/repository.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 入门]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fdocker-quickstart%2F</url>
    <content type="text"><![CDATA[Docker 入门 Hello World 示例 Hello World 示例 （1）拉取镜像 docker image pull library/hello-world docker image pull 是抓取 image 文件的命令。library/hello-world 是 image 文件在仓库里面的位置，其中 library 是 image 文件所在的组，hello-world 是 image 文件的名字。 由于 Docker 官方提供的 image 文件，都放在library组里面，所以它的是默认组，可以省略。因此，上面的命令可以写成下面这样。 docker image pull hello-world （2）查看镜像 ~ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest 4ab4c602aa5e 3 months ago 1.84kB （3）运行镜像 docker container run hello-world docker container run 命令会从 image 文件，生成一个正在运行的容器实例。 注意，docker container run 命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的 docker image pull 命令并不是必需的步骤。 如果运行成功，你会在屏幕上读到下面的输出。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 教程]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fdocker%2F</url>
    <content type="text"><![CDATA[Docker 教程 简介 Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。 Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。 入门篇 基础篇 安装 Docker 分为 CE 和 EE 两大版本。 CE 即社区版（免费，支持周期 7 个月）。Docker CE 分为 stable, test, 和 nightly 三个更新频道。每六个月发布一个 stable 版本。 EE 即企业版，强调安全，付费使用，支持周期 24 个月。 Docker CE 可以安装在 Linux 、Windows 10 (PC) 和 MAC 上。 参考： 官方安装指南 Docker 中文教程安装指南 Docker 镜像 Docker 容器 Dockerfile FROM(指定基础镜像) RUN(执行命令) COPY(复制文件) ADD(更高级的复制文件) CMD(容器启动命令) ENTRYPOINT(入口点) ENV(设置环境变量) ARG(构建参数) VOLUME(定义匿名卷) EXPOSE(暴露端口) WORKDIR(指定工作目录) USER(指定当前用户) HEALTHCHECK(健康检查) ONBUILD(为他人作嫁衣裳) 进阶篇 设计 实战篇 常见问题 附录 命令 资源 术语 技巧 版本 反馈]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux 教程]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2FREADME%2F</url>
    <content type="text"><![CDATA[Linux 教程 💡 指南 学习之前，先看一下入门三问： 什么是 Linux？ Linux 是一套免费使用和自由传播的类 Unix 操作系统，是一个基于 POSIX 和 UNIX 的多用户、多任务、支持多线程和多 CPU 的操作系统。它能运行主要的 UNIX 工具软件、应用程序和网络协议。它支持 32 位和 64 位硬件。Linux 继承了 Unix 以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 为什么学习 Linux？ Linux 常用于网站服务器或嵌入式应用。世界上大部分网站都部署在 Linux 服务器上，作为一名 web 开发人员， 如何学习 Linux？ 📝 知识点 Linux 命令 根据应用场景，将常见 Linux 命令分门别类的一一介绍。 如果想快速学习，推荐参考这篇文章：命令行的艺术（转载） 查看 Linux 命令帮助信息 - 关键词：help, whatis, info, which, whereis, man Linux 文件目录管理 - 关键词：cd, ls, pwd, mkdir, rmdir, tree, touch, ln, rename, stat, file, chmod, chown, locate, find, cp, mv, rm Linux 文件内容查看命令 - 关键词：cat, head, tail, more, less, sed, vi, grep Linux 文件压缩和解压 - 关键词：tar, gzip, zip, unzip Linux 用户管理 - 关键词：groupadd, groupdel, groupmod, useradd, userdel, usermod, passwd, su, sudo Linux 系统管理 - 关键词：reboot, exit, shutdown, date, mount, umount, ps, kill, systemctl, service, crontab Linux 网络管理 - 关键词：关键词：curl, wget, telnet, ip, hostname, ifconfig, route, ssh, ssh-keygen, firewalld, iptables, host, nslookup, nc/netcat, ping, traceroute, netstat Linux 硬件管理 - 关键词：df, du, top, free, iotop Linux 软件管理 - 关键词：rpm, yum, apt-get 工具 Git Vim Linux 运维 💡 说明 这里总结了多种常用研发软件的安装、配置、使用指南。并提供基本安装、运行的脚本。 环境部署工具 ：适合开发、运维人员，在 CentOS 机器上安装常用命令工具或开发软件。 Scripts：安装配置脚本，按照说明安装使用即可。 Docs: 安装配置文档，说明安装的方法以及一些注意事项。 Tutorial: 教程文档。 Linux 服务器运维 Linux 典型运维应用 samba 使用详解 应用、服务、工具运维和调优 研发环境 JDK | Scripts | Docs | Maven | Scripts | Docs | Nginx | Scripts | Docs | Nodejs | Scripts | Docs | Tomcat | Scripts | Docs | Zookeeper | Scripts | Docs | 研发工具 Nexus - Maven 私服。 | Docs | Jenkins - 持续集成和持续交付平台。 | Scripts | Docs | Elastic - 常被称为 ELK ，是 Java 世界最流行的分布式日志解决方案 。 ELK 是 Elastic 公司旗下三款产品 ElasticSearch 、Logstash 、Kibana 的首字母组合。 | Docs | 版本控制 Gitlab - Git 代码管理平台 Svn - Svn 是 Subversion 的简称，是一个开放源代码的版本控制系统，它采用了分支管理系统。 | Docs | 消息中间件 Kafka - 应该是 Java 世界最流行的消息中间件了吧。 | Scripts | Docs | RocketMQ - 阿里巴巴开源的消息中间件。 | Scripts | Docs | 数据库 Mysql - 关系型数据库 | Docs | PostgreSQL - 关系型数据库 | Docs | Mongodb - Nosql | Scripts | Docs | Redis - Nosql | Scripts | Docs | 📚 学习资源 Linux 资源汇总 awesome-linux - Linux 资源汇总 awesome-linux-software - Linux 软件汇总 Linux 教程 鸟哥的私房菜 - 久负盛名的 Linux 教程 菜鸟教程-Linux - 入门级 Linux 教程 Linux 工具快速教程 Linux 帮助手册 命令行的艺术 - Linux 命令 cheat sheet Linux 命令大全 - Linux 命令在线帮助手册 linux-command - Linux 命令在线帮助手册 linux-tutorial - Linux 环境下各种软件安装部署 🚪 传送门 | 回首頁 |]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fkubernetes%2Fkubernetes%2F</url>
    <content type="text"><![CDATA[Kubernetes Kubernetes 是用于自动部署，扩展和管理 Docker 应用程序的开源系统。简称 K8S 关键词： docker 功能 简介 基础 进阶 命令 引用和引申 功能 基于容器的应用部署、维护和滚动升级 负载均衡和服务发现 跨机器和跨地区的集群调度 自动伸缩 无状态服务和有状态服务 广泛的 Volume 支持 插件机制保证扩展性 简介 Kubernetes 主控组件（Master） 包含三个进程，都运行在集群中的某个节上，通常这个节点被称为 master 节点。这些进程包括：kube-apiserver、kube-controller-manager 和 kube-scheduler。 集群中的每个非 master 节点都运行两个进程： kubelet，和 master 节点进行通信。 kube-proxy，一种网络代理，将 Kubernetes 的网络服务代理到每个节点上。 Kubernetes 对象 Kubernetes 包含若干抽象用来表示系统状态，包括：已部署的容器化应用和负载、与它们相关的网络和磁盘资源以及有关集群正在运行的其他操作的信息。 Pod - kubernetes 对象模型中最小的单元，它代表集群中一个正在运行的进程。 Service Volume Namespace 高级对象 ReplicaSet Deployment StatefulSet DaemonSet Job 基础 进阶 命令 客户端配置 # Setup autocomplete in bash; bash-completion package should be installed firstsource &lt;(kubectl completion bash)# View Kubernetes configkubectl config view# View specific config items by json pathkubectl config view -o jsonpath='&#123;.users[?(@.name == "k8s")].user.password&#125;'# Set credentials for foo.kuberntes.comkubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword 查找资源 # List all services in the namespacekubectl get services# List all pods in all namespaces in wide formatkubectl get pods -o wide --all-namespaces# List all pods in json (or yaml) formatkubectl get pods -o json# Describe resource details (node, pod, svc)kubectl describe nodes my-node# List services sorted by namekubectl get services --sort-by=.metadata.name# List pods sorted by restart countkubectl get pods --sort-by='.status.containerStatuses[0].restartCount'# Rolling update pods for frontend-v1kubectl rolling-update frontend-v1 -f frontend-v2.json# Scale a replicaset named 'foo' to 3kubectl scale --replicas=3 rs/foo# Scale a resource specified in "foo.yaml" to 3kubectl scale --replicas=3 -f foo.yaml# Execute a command in every pod / replicafor i in 0 1; do kubectl exec foo-$i -- sh -c 'echo $(hostname) &gt; /usr/share/nginx/html/index.html'; done 资源管理 # Get documentation for pod or servicekubectl explain pods,svc# Create resource(s) like pods, services or daemonsetskubectl create -f ./my-manifest.yaml# Apply a configuration to a resourcekubectl apply -f ./my-manifest.yaml# Start a single instance of Nginxkubectl run nginx --image=nginx# Create a secret with several keyscat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: password: $(echo "s33msi4" | base64) username: $(echo "jane"| base64)EOF# Delete a resourcekubectl delete -f ./my-manifest.yaml 监控和日志 # Deploy Heapster from Github repositorykubectl create -f deploy/kube-config/standalone/# Show metrics for nodeskubectl top node# Show metrics for podskubectl top pod# Show metrics for a given pod and its containerskubectl top pod pod_name --containers# Dump pod logs (stdout)kubectl logs pod_name# Stream pod container logs (stdout, multi-container case)kubectl logs -f pod_name -c my-container 引用和引申 官方 Github 官网 教程 Kubernetes 中文指南 文章 https://github.com/LeCoupa/awesome-cheatsheets/blob/master/tools/kubernetes.sh 更多资源 awesome-kubernetes]]></content>
  </entry>
  <entry>
    <title><![CDATA[Svn 安装、配置、使用指南]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Fsvn%2F</url>
    <content type="text"><![CDATA[Svn 安装、配置、使用指南 Svn 是 Subversion 的简称，是一个开放源代码的版本控制系统，它采用了分支管理系统。 1. 安装配置 1.1. 安装 svn 1.2. 创建 svn 仓库 1.3. 配置 svnserve.conf 1.4. 配置 passwd 1.5. 配置 authz 1.6. 启动关闭 svn 1.7. 开机自启动 svn 方法 1.8. svn 客户端访问 2. 更多内容 1. 安装配置 1.1. 安装 svn $ yum install -y subversion 1.2. 创建 svn 仓库 $ mkdir -p /share/svn$ svnadmin create /share/svn$ ls /share/svnconf db format hooks locks README.txt 在 conf 目录下有三个重要的配置文件 authz - 是权限控制文件 passwd - 是帐号密码文件 svnserve.conf - 是 SVN 服务配置文件 1.3. 配置 svnserve.conf $ vim /share/svn/conf/svnserve.conf 打开下面的 5 个注释 anon-access = read #匿名用户可读auth-access = write #授权用户可写password-db = passwd #使用哪个文件作为账号文件authz-db = authz #使用哪个文件作为权限文件realm = /share/svn # 认证空间名，版本库所在目录 1.4. 配置 passwd $ vim /share/svn/conf/passwd 添加内容如下： [users]user1 = 123456user2 = 123456user3 = 123456 1.5. 配置 authz $ vim /share/svn/conf/authz 添加内容如下： [/]user1 = rwuser2 = rwuser3 = rw*= 1.6. 启动关闭 svn $ svnserve -d -r /share/svn # 启动 svn$ killall svnserve # 关闭 svn 1.7. 开机自启动 svn 方法 安装好 svn 服务后，默认是没有随系统启动自动启动的，而一般我们有要求 svn 服务稳定持续的提供服务。所以，有必要配置开机自启动 svn 服务。 Centos7 以前 编辑 /etc/rc.d/rc.local 文件： $ vi /etc/rc.d/rc.local 输入以下内容： # 开机自动启动 svn，默认端口是 3690$ /usr/bin/svnserve -d -r /share/svn --listen-port 3690 注意： 我们在用终端操作的时候，可以直接使用以下命令启动 SVN：svnserve -d -r /share/svn，但是在 /etc/rc.d/rc.local 文件中必须写上完整的路径！ 如果不知道 svnserve 命令安装在哪儿，可以使用 whereis svnserve 查找。 Centos7 CentOS 7 中的 /etc/rc.d/rc.local 是没有执行权限的，系统建议创建 systemd service 启动服务。 找到 svn 的 service 配置文件 /etc/sysconfig/svnserve 编辑配置文件 $ vi /etc/sysconfig/svnserve 将 OPTIONS=&quot;-r /var/svn&quot; 改为 svn 版本库存放的目录，:wq 保存退出。 执行 systemctl enable svnserve.service 重启服务器后，执行 ps -ef | grep svn 应该可以看到 svn 服务的进程已经启动。 1.8. svn 客户端访问 进入 svn 官方下载地址，选择合适的版本，下载并安装。 新建一个目录，然后打开鼠标右键菜单，选择 SVN Checkout。 在新的窗口，输入地址 svn://&lt;你的 IP&gt; 即可，不出意外输入用户名和密码就能连接成功了（这里的用户、密码必须在 passwd 配置文件的清单中）。默认端口 3690，如果你修改了端口，那么要记得加上端口号。如下图所示： 2. 更多内容 引申 操作系统、运维部署总结系列 引用 https://www.cnblogs.com/liuxianan/p/linux_install_svn_server.html https://blog.csdn.net/testcs_dn/article/details/45395645 https://www.cnblogs.com/moxiaoan/p/5683743.html https://blog.csdn.net/realghost/article/details/52396648]]></content>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 安装部署]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Frocketmq%2F</url>
    <content type="text"><![CDATA[RocketMQ 安装部署 环境要求 下载解压 启动 Name Server 启动 Broker 收发消息 关闭服务器 FAQ connect to &lt;172.17.0.1:10909&gt; failed 参考资料 环境要求 推荐 64 位操作系统：Linux/Unix/Mac 64bit JDK 1.8+ Maven 3.2.x Git 下载解压 进入官方下载地址：https://rocketmq.apache.org/dowloading/releases/，选择合适版本 建议选择 binary 版本。 解压到本地： &gt; unzip rocketmq-all-4.2.0-source-release.zip&gt; cd rocketmq-all-4.2.0/ 启动 Name Server &gt; nohup sh bin/mqnamesrv &amp;&gt; tail -f ~/logs/rocketmqlogs/namesrv.logThe Name Server boot success... 启动 Broker &gt; nohup sh bin/mqbroker -n localhost:9876 -c conf/broker.conf &amp;&gt; tail -f ~/logs/rocketmqlogs/broker.logThe broker[%s, 172.30.30.233:10911] boot success... 收发消息 执行收发消息操作之前，不许告诉客户端命名服务器的位置。在 RocketMQ 中有多种方法来实现这个目的。这里，我们使用最简单的方法——设置环境变量 NAMESRV_ADDR ： &gt; export NAMESRV_ADDR=localhost:9876&gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.ProducerSendResult [sendStatus=SEND_OK, msgId= ...&gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.ConsumerConsumeMessageThread_%d Receive New Messages: [MessageExt... 关闭服务器 &gt; sh bin/mqshutdown brokerThe mqbroker(36695) is running...Send shutdown request to mqbroker(36695) OK&gt; sh bin/mqshutdown namesrvThe mqnamesrv(36664) is running...Send shutdown request to mqnamesrv(36664) OK FAQ connect to &lt;172.17.0.1:10909&gt; failed 启动后，生产者客户端连接 RocketMQ 时报错： org.apache.rocketmq.remoting.exception.RemotingConnectException: connect to &lt;172.17.0.1:10909&gt; failed at org.apache.rocketmq.remoting.netty.NettyRemotingClient.invokeSync(NettyRemotingClient.java:357) at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessageSync(MQClientAPIImpl.java:343) at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessage(MQClientAPIImpl.java:327) at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessage(MQClientAPIImpl.java:290) at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendKernelImpl(DefaultMQProducerImpl.java:688) at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendSelectImpl(DefaultMQProducerImpl.java:901) at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.send(DefaultMQProducerImpl.java:878) at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.send(DefaultMQProducerImpl.java:873) at org.apache.rocketmq.client.producer.DefaultMQProducer.send(DefaultMQProducer.java:369) at com.emrubik.uc.mdm.sync.utils.MdmInit.sendMessage(MdmInit.java:62) at com.emrubik.uc.mdm.sync.utils.MdmInit.main(MdmInit.java:2149) 原因：RocketMQ 部署在虚拟机上，内网 ip 为 10.10.30.63，该虚拟机一个 docker0 网卡，ip 为 172.17.0.1。RocketMQ broker 启动时默认使用了 docker0 网卡，生产者客户端无法连接 172.17.0.1，造成以上问题。 解决方案 （1）干掉 docker0 网卡或修改网卡名称 （2）停掉 broker，修改 broker 配置文件，重启 broker。 修改 conf/broker.conf，增加两行来指定启动 broker 的 IP： namesrvAddr = 10.10.30.63:9876brokerIP1 = 10.10.30.63 启动时需要指定配置文件 nohup sh bin/mqbroker -n localhost:9876 -c conf/broker.conf &amp; 更多内容 引申 操作系统、运维部署总结系列 引用 RocketMQ 官方文档 RocketMQ 搭建及刨坑]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 安装 Nginx]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fpractice%2Fdocker-install-nginx%2F</url>
    <content type="text"><![CDATA[Docker 安装 Nginx 实测环境：Centos 查看可用镜像 执行 docker search nginx 命令查看可用镜像： # docker search nginxINDEX NAME DESCRIPTION STARS OFFICIAL AUTOMATEDdocker.io docker.io/nginx Official build of Nginx. 8272 [OK] docker.io docker.io/jwilder/nginx-proxy Automated Nginx reverse proxy for docker c... 1300 [OK]docker.io docker.io/richarvey/nginx-php-fpm Container running Nginx + PHP-FPM capable ... 540 [OK]docker.io docker.io/jrcs/letsencrypt-nginx-proxy-companion LetsEncrypt container to use with nginx as... 336 [OK]... 选择下载镜像 执行 docker pull nginx 命令下载镜像 运行镜像 docker run -p 80:80 --name mynginx -d nginx]]></content>
  </entry>
  <entry>
    <title><![CDATA[Nodejs 安装]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Fnodejs%2F</url>
    <content type="text"><![CDATA[Nodejs 安装 安装方法 先安装 nvm 安装 Nodejs 脚本 安装方法 先安装 nvm 推荐安装 nvm(Node Version Manager) ，来管理 node.js 版本。 安装步骤如下： （1）执行安装脚本 rm -rf ~/.nvmcurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.8/install.sh | bash. ~/.nvm/nvm.sh （2）检验是否安装成功 执行 nvm --version 命令。 注意：如果出现 nvm: command not found ，关闭终端，然后再打开终端试试。 安装 Nodejs 安装步骤如下： （1）使用 nvm 安装 nodejs 指定版本 执行以下命令： nvm install 8.9.4nvm use 8.9.4 （2）检验是否安装成功 执行 node --version 命令。 注意：如果出现 node: command not found ，关闭终端，然后再打开终端试试。 脚本 | 安装脚本 | 更多内容 引申 操作系统、运维部署总结系列]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kafka 安装部署]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Fkafka%2F</url>
    <content type="text"><![CDATA[Kafka 安装部署 环境要求： JDK8 ZooKeeper 下载解压 启动服务器 停止服务器 创建主题 生产者生产消息 消费者消费消息 集群部署 下载解压 进入官方下载地址：http://kafka.apache.org/downloads，选择合适版本。 解压到本地： &gt; tar -xzf kafka_2.11-1.1.0.tgz&gt; cd kafka_2.11-1.1.0 现在您已经在您的机器上下载了最新版本的 Kafka。 启动服务器 由于 Kafka 依赖于 ZooKeeper，所以运行前需要先启动 ZooKeeper &gt; bin/zookeeper-server-start.sh config/zookeeper.properties[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)... 然后，启动 Kafka &gt; bin/kafka-server-start.sh config/server.properties[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)... 停止服务器 执行所有操作后，可以使用以下命令停止服务器 $ bin/kafka-server-stop.sh config/server.properties 创建主题 创建一个名为 test 的 Topic，这个 Topic 只有一个分区以及一个备份： &gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 生产者生产消息 运行生产者，然后可以在控制台中输入一些消息，这些消息会发送到服务器： &gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testThis is a messageThis is another message 消费者消费消息 启动消费者，然后获得服务器中 Topic 下的消息： &gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginningThis is a messageThis is another message 集群部署 复制配置为多份（Windows 使用 copy 命令代理）： &gt; cp config/server.properties config/server-1.properties&gt; cp config/server.properties config/server-2.properties 修改配置： config/server-1.properties: broker.id=1 listeners=PLAINTEXT://:9093 log.dir=/tmp/kafka-logs-1config/server-2.properties: broker.id=2 listeners=PLAINTEXT://:9094 log.dir=/tmp/kafka-logs-2 其中，broker.id 这个参数必须是唯一的。 端口故意配置的不一致，是为了可以在一台机器启动多个应用节点。 根据这两份配置启动三个服务器节点： &gt; bin/kafka-server-start.sh config/server.properties &amp;...&gt; bin/kafka-server-start.sh config/server-1.properties &amp;...&gt; bin/kafka-server-start.sh config/server-2.properties &amp;... 创建一个新的 Topic 使用 三个备份： &gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic 查看主题： &gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topicTopic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 1 Replicas: 1,2,0 Isr: 1,2,0 leader - 负责指定分区的所有读取和写入的节点。每个节点将成为随机选择的分区部分的领导者。 replicas - 是复制此分区日志的节点列表，无论它们是否为领导者，或者即使它们当前处于活动状态。 isr - 是“同步”复制品的集合。这是副本列表的子集，该列表当前处于活跃状态并且已经被领导者捕获。 更多内容 引申 操作系统、运维部署总结系列]]></content>
  </entry>
  <entry>
    <title><![CDATA[Jenkins 安装]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Fjenkins%2F</url>
    <content type="text"><![CDATA[Jenkins 安装 环境要求 JDK：JDK7+，官网推荐是 JDK 8 安装 启动 脚本 FAQ 登录密码 忘记密码 卡在 check 页面 卡在 getting startted 页面 参考资料 安装 安装步骤如下： （1）下载并解压到本地 进入官网下载地址：https://jenkins.io/download/ ，选择合适的版本下载。 我选择的是最新稳定 war 版本 2.89.4：http://mirrors.jenkins.io/war-stable/latest/jenkins.war 我个人喜欢存放在：/opt/software/jenkins mkdir -p /opt/software/jenkinswget -O /opt/software/jenkins/jenkins.war http://mirrors.jenkins.io/war-stable/latest/jenkins.war 启动 如果你和我一样，选择 war 版本，那么你可以将 war 移到 Tomcat 的 webapps 目录下，通过 Tomcat 来启动。 当然，也可以通过 java -jar 方式来启动。 启动 jenkins 服务 cd /opt/software/jenkinsnohup java -jar jenkins.war --httpPort=8080 &gt;&gt; nohup.out 2&gt;&amp;1 &amp; 脚本 | 安装脚本 | FAQ 登录密码 如果不知道初始登录密码，可以通过以下方式查看： 执行命令 cat /root/.jenkins/secrets/initialAdminPassword，打印出来的即是初始登录密码。 忘记密码 1.执行 vim /root/.jenkins/config.xml ，删除以下内容 &lt;useSecurity&gt;true&lt;/useSecurity&gt;&lt;authorizationStrategy class="hudson.security.FullControlOnceLoggedInAuthorizationStrategy"&gt; &lt;denyAnonymousReadAccess&gt;true&lt;/denyAnonymousReadAccess&gt;&lt;/authorizationStrategy&gt;&lt;securityRealm class="hudson.security.HudsonPrivateSecurityRealm"&gt; &lt;disableSignup&gt;true&lt;/disableSignup&gt; &lt;enableCaptcha&gt;false&lt;/enableCaptcha&gt;&lt;/securityRealm&gt; 2.重启 Jenkins 服务； 3.进入首页&gt;“系统管理”&gt;“Configure Global Security”； 4.勾选“启用安全”； 5.点选“Jenkins 专有用户数据库”，并点击“保存”； 6.重新点击首页&gt;“系统管理”,发现此时出现“管理用户”； 7.点击进入展示“用户列表”； 8.点击右侧进入修改密码页面，修改后即可重新登录。 卡在 check 页面 现象：输入密码后，卡在 check 页面 原因：jenkins 在安装插件前总是尝试连接 www.google.com，来判断网络是否连通。谷歌的网站在大陆是连不上的，所以会出现这个问题。 解决方案：执行vim /root/.jenkins/updates/default.json，将 connectionCheckUrl 后的 www.google.com 改为 www.baidu.com 。然后重启即可。 或者直接执行命令： sed -i 's/www.google.com/www.baidu.com/g' /root/.jenkins/updates/default.json 卡在 getting startted 页面 现象：卡在 getting startted 页面 原因：jenkins 默认的插件下载服务器地址在国外，如果不翻墙下载不了。 解决方案：执行vim /root/.jenkins/hudson.model.UpdateCenter.xml，将 &lt;url&gt; 改为 http://mirror.xmission.com/jenkins/updates/update-center.json 。然后重启即可。 或者直接执行命令： sed -i '/^&lt;url&gt;/s/.*/&lt;url&gt;http:\/\/mirror.xmission.com\/jenkins\/updates\/update-center.json&lt;\/url&gt;/g' /root/.jenkins/hudson.model.UpdateCenter.xml 更多内容 引申 操作系统、运维部署总结系列 引用 https://jenkins.io/doc/pipeline/tour/getting-started/ https://www.cnblogs.com/austinspark-jessylu/p/6894944.html http://blog.csdn.net/jlminghui/article/details/54952148]]></content>
  </entry>
  <entry>
    <title><![CDATA[Gitlab 安装]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Flinux%2Fops%2Fservice%2Fgitlab%2Fgitlab-install%2F</url>
    <content type="text"><![CDATA[Gitlab 安装 环境： OS: CentOS7 安装 gitlab 常规安装 gitlab Docker 安装 gitlab 安装 gitlab-ci-multi-runner 常规安装 gitlab-ci-multi-runner Docker 安装 gitlab-ci-multi-runner 自签名证书 创建证书 gitlab 配置 更多内容 安装 gitlab 常规安装 gitlab 进入官方下载地址：https://about.gitlab.com/install/ ，如下图，选择合适的版本。 以 CentOS7 为例： 安装和配置必要依赖 在系统防火墙中启用 HTTP 和 SSH sudo yum install -y curl policycoreutils-python openssh-serversudo systemctl enable sshdsudo systemctl start sshdsudo firewall-cmd --permanent --add-service=httpsudo systemctl reload firewalld 安装 Postfix ，使得 Gitlab 可以发送通知邮件 sudo yum install postfixsudo systemctl enable postfixsudo systemctl start postfix 添加 Gitlab yum 仓库并安装包 添加 Gitlab yum 仓库 curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash 通过 yum 安装 gitlab-ce sudo EXTERNAL_URL="http://gitlab.example.com" yum install -y gitlab-ce 安装完成后，即可通过默认的 root 账户进行登录。更多细节可以参考：documentation for detailed instructions on installing and configuration Docker 安装 gitlab 拉取镜像 docker pull docker.io/gitlab/gitlab-ce 启动 docker run -d \ --hostname gitlab.zp.io \ --publish 8443:443 --publish 80:80 --publish 2222:22 \ --name gitlab \ --restart always \ --volume $GITLAB_HOME/config:/etc/gitlab \ --volume $GITLAB_HOME/logs:/var/log/gitlab \ --volume $GITLAB_HOME/data:/var/opt/gitlab \ gitlab/gitlab-ce 安装 gitlab-ci-multi-runner 参考：https://docs.gitlab.com/runner/install/ 常规安装 gitlab-ci-multi-runner 下载 sudo wget -O /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-amd64 配置执行权限 sudo chmod +x /usr/local/bin/gitlab-runner 如果想使用 Docker，安装 Docker（可选的） curl -sSL https://get.docker.com/ | sh 创建 CI 用户 sudo useradd --comment 'GitLab Runner' --create-home gitlab-runner --shell /bin/bash 安装并启动服务 sudo gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runnersudo gitlab-runner start 注册 Runner （1）执行命令： sudo gitlab-runner register （2）输入 Gitlab URL 和 令牌 URL 和令牌信息在 Gitlab 的 Runner 管理页面获取： Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com )https://gitlab.comPlease enter the gitlab-ci token for this runnerxxx （3）输入 Runner 的描述 Please enter the gitlab-ci description for this runner[hostame] my-runner （4）输入 Runner 相关的标签 Please enter the gitlab-ci tags for this runner (comma separated):my-tag,another-tag （5）输入 Runner 执行器 Please enter the executor: ssh, docker+machine, docker-ssh+machine, kubernetes, docker, parallels, virtualbox, docker-ssh, shell:docker 如果想选择 Docker 作为执行器，你需要指定默认镜像（ .gitlab-ci.yml 中没有此配置） Please enter the Docker image (eg. ruby:2.1):alpine:latest Docker 安装 gitlab-ci-multi-runner 拉取镜像 docker pull docker.io/gitlab/gitlab-runner 启动 docker run -d --name gitlab-runner --restart always \ -v /srv/gitlab-runner/config:/etc/gitlab-runner \ -v /var/run/docker.sock:/var/run/docker.sock \ gitlab/gitlab-runner:latest 自签名证书 首先，创建认证目录 sudo mkdir -p /etc/gitlab/sslsudo chmod 700 /etc/gitlab/ssl 创建证书 创建 Private Key sudo openssl genrsa -des3 -out /etc/gitlab/ssl/gitlab.domain.com.key 2048 会提示输入密码，请记住 生成 Certificate Request sudo openssl req -new -key /etc/gitlab/ssl/gitlab.domain.com.key -out /etc/gitlab/ssl/gitlab.domain.com.csr 根据提示，输入信息 Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:JSLocality Name (eg, city) [Default City]:NJOrganization Name (eg, company) [Default Company Ltd]:xxxxxOrganizational Unit Name (eg, section) []:Common Name (eg, your name or your server's hostname) []:gitlab.xxxx.ioEmail Address []:Please enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []: 移除 Private Key 中的密码短语 sudo cp -v /etc/gitlab/ssl/gitlab.domain.com.&#123;key,original&#125;sudo openssl rsa -in /etc/gitlab/ssl/gitlab.domain.com.original -out /etc/gitlab/ssl/gitlab.domain.com.keysudo rm -v /etc/gitlab/ssl/gitlab.domain.com.original 创建证书 sudo openssl x509 -req -days 1460 -in /etc/gitlab/ssl/gitlab.domain.com.csr -signkey /etc/gitlab/ssl/gitlab.domain.com.key -out /etc/gitlab/ssl/gitlab.domain.com.crt 移除证书请求文件 sudo rm -v /etc/gitlab/ssl/gitlab.domain.com.csr 设置文件权限 sudo chmod 600 /etc/gitlab/ssl/gitlab.domain.com.* gitlab 配置 sudo vim /etc/gitlab/gitlab.rbexternal_url 'https://gitlab.domain.com' gitlab 网站 https： nginx['redirect_http_to_https'] = true gitlab ci 网站 https： ci_nginx['redirect_http_to_https'] = true 复制证书到 gitlab 目录： sudo cp /etc/gitlab/ssl/gitlab.domain.com.crt /etc/gitlab/trusted-certs/ gitlab 重新配置+更新： sudo gitlab-ctl reconfiguresudo gitlab-ctl restart 更多内容 引申 操作系统、运维部署总结系列]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 安装 MySQL]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fpractice%2Fdocker-install-mysql%2F</url>
    <content type="text"><![CDATA[Docker 安装 MySQL 实测环境：Centos 查看可下载镜像 # docker search mysqlINDEX NAME DESCRIPTION STARS OFFICIAL AUTOMATEDdocker.io docker.io/mysql MySQL is a widely used, open-source relati... 5757 [OK] docker.io docker.io/mariadb MariaDB is a community-developed fork of M... 1863 [OK] docker.io docker.io/mysql/mysql-server Optimized MySQL Server Docker images. Crea... 397 [OK]... 选择下载官方镜像 比如，我想下载最新版本，则执行如下命令： docker pull mysql 使用镜像 docker run -p 3306:3306 --name mysql -v /opt/docker_v/mysql/conf:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -d mysql 资源 https://hub.docker.com/_/mysql/]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 服务]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fbasics%2Fdocker-services%2F</url>
    <content type="text"><![CDATA[Docker 服务 关于服务 在分布式应用程序中，应用程序的不同部分被称为“服务”。例如，如果您想象一个视频共享网站，它可能包含用于将应用程序数据存储在数据库中的服务，用户上传文件后在后台传输的服务，前端应用服务等等。 服务实际上只是“生产环境中的容器”。一个服务只运行一个镜像，但它需要制定镜像的运行方式 - 应该使用哪个端口，容器应该运行多少副本，以便服务具有所需的容量，以及等等。缩放服务会更改运行该软件的容器实例的数量，从而为流程中的服务分配更多计算资源。 幸运的是，使用 Docker 平台定义，运行和扩展服务非常简单 - 只需编写一个 docker-compose.yml 文件即可。 docker-compose.yml 文件 它是一个YAML文件，它定义了Docker容器在生产中的行为方式。 version: "3"services: web: # replace username/repo:tag with your name and image details image: username/repo:tag deploy: replicas: 5 resources: limits: cpus: "0.1" memory: 50M restart_policy: condition: on-failure ports: - "80:80" networks: - webnetnetworks: webnet:]]></content>
  </entry>
  <entry>
    <title><![CDATA[Vue CLI]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fmvc%2Fvue-cli%2F</url>
    <content type="text"><![CDATA[Vue CLI 🛠 Vue.js 开发的标准工具 version: 3.x 简介 Vue CLI 是一个基于 Vue.js 进行快速开发的完整系统，提供： 通过 @vue/cli 搭建交互式的项目脚手架。 通过 @vue/cli + @vue/cli-service-global 快速开始零配置原型开发。 一个运行时依赖 ( @vue/cli-service )，该依赖： 可升级； 基于 webpack 构建，并带有合理的默认配置； 可以通过项目内的配置文件进行配置； 可以通过插件进行扩展。 一个丰富的官方插件集合，集成了前端生态中最好的工具。 一套完全图形化的创建和管理 Vue.js 项目的用户界面。 入门 安装： npm install -g @vue/cli# ORyarn global add @vue/cli 创建一个项目： vue create my-project# ORvue ui 确认安装成功： vue --version 基础 vue serve 你可以使用 vue serve 和 vue build 命令对单个 *.vue 文件进行快速原型开发，不过这需要先额外安装一个全局的扩展： npm install -g @vue/cli-service-global vue serve 的缺点就是它需要安装全局依赖，这使得它在不同机器上的一致性不能得到保证。因此这只适用于快速原型开发。 Usage: serve [options] [entry]在开发环境模式下零配置为 .js 或 .vue 文件启动一个服务器Options: -o, --open 打开浏览器 -c, --copy 将本地 URL 复制到剪切板 -h, --help 输出用法信息 你所需要的仅仅是一个 App.vue 文件： &lt;template&gt; &lt;h1&gt;Hello!&lt;/h1&gt;&lt;/template&gt; 然后在这个 App.vue 文件所在的目录下运行： vue serve vue serve 使用了和 vue create 创建的项目相同的默认设置 (webpack、Babel、PostCSS 和 ESLint)。它会在当前目录自动推导入口文件——入口可以是 main.js、index.js、App.vue 或 app.vue 中的一个。你也可以显式地指定入口文件： vue serve MyComponent.vue 如果需要，你还可以提供一个 index.html、package.json、安装并使用本地依赖、甚至通过相应的配置文件配置 Babel、PostCSS 和 ESLint。 vue build Usage: build [options] [entry]在生产环境模式下零配置构建一个 .js 或 .vue 文件Options: -t, --target &lt;target&gt; 构建目标 (app | lib | wc | wc-async, 默认值：app) -n, --name &lt;name&gt; 库的名字或 Web Components 组件的名字 (默认值：入口文件名) -d, --dest &lt;dir&gt; 输出目录 (默认值：dist) -h, --help 输出用法信息 你也可以使用 vue build 将目标文件构建成一个生产环境的包并用来部署： vue build MyComponent.vue vue build 也提供了将组件构建成为一个库或一个 Web Components 组件的能力。查阅构建目标了解更多。 vue create 运行以下命令来创建一个新项目： vue create hello-world 你会被提示选取一个 preset。你可以选默认的包含了基本的 Babel + ESLint 设置的 preset，也可以选“手动选择特性”来选取需要的特性。 vue create 命令有一些可选项，你可以通过运行以下命令进行探索： vue create --help用法：create [options] &lt;app-name&gt;创建一个由 `vue-cli-service` 提供支持的新项目选项： -p, --preset &lt;presetName&gt; 忽略提示符并使用已保存的或远程的预设选项 -d, --default 忽略提示符并使用默认预设选项 -i, --inlinePreset &lt;json&gt; 忽略提示符并使用内联的 JSON 字符串预设选项 -m, --packageManager &lt;command&gt; 在安装依赖时使用指定的 npm 客户端 -r, --registry &lt;url&gt; 在安装依赖时使用指定的 npm registry -g, --git [message] 强制 / 跳过 git 初始化，并可选的指定初始化提交信息 -n, --no-git 跳过 git 初始化 -f, --force 覆写目标目录可能存在的配置 -c, --clone 使用 git clone 获取远程预设选项 -x, --proxy 使用指定的代理创建项目 -b, --bare 创建项目时省略默认组件中的新手指导信息 -h, --help 输出使用帮助信息 vue ui 可以通过 vue ui 命令以图形化界面创建和管理项目： vue ui 上述命令会打开一个浏览器窗口，并以图形化界面将你引导至项目创建的流程。 插件 Vue CLI 使用了一套基于插件的架构。 插件可以修改 webpack 的内部配置，也可以向 vue-cli-service 注入命令。在项目创建的过程中，绝大部分列出的特性都是通过插件来实现的。 在现有的项目中安装插件 每个 CLI 插件都会包含一个 (用来创建文件的) 生成器和一个 (用来调整 webpack 核心配置和注入命令的) 运行时插件。当你使用 vue create 来创建一个新项目的时候，有些插件会根据你选择的特性被预安装好。如果你想在一个已经被创建好的项目中安装一个插件，可以使用 vue add 命令： vue add @vue/eslint 项目本地的插件 如果你需要在项目里直接访问插件 API 而不需要创建一个完整的插件，你可以在 package.json 文件中使用 vuePlugins.service 选项： &#123; "vuePlugins": &#123; "service": ["my-commands.js"] &#125;&#125; 每个文件都需要暴露一个函数，接受插件 API 作为第一个参数。关于插件 API 的更多信息可以查阅插件开发指南。 你也可以通过 vuePlugins.ui 选项添加像 UI 插件一样工作的文件： &#123; "vuePlugins": &#123; "ui": ["my-ui.js"] &#125;&#125; 更多信息请阅读 UI 插件 API。 Preset 一个 Vue CLI preset 是一个包含创建新项目所需预定义选项和插件的 JSON 对象，让用户无需在命令提示中选择它们。 在 vue create 过程中保存的 preset 会被放在你的 home 目录下的一个配置文件中 (\~/.vuerc)。你可以通过直接编辑这个文件来调整、添加、删除保存好的 preset。 这里有一个 preset 的示例： &#123; "useConfigFiles": true, "router": true, "vuex": true, "cssPreprocessor": "sass", "plugins": &#123; "@vue/cli-plugin-babel": &#123;&#125;, "@vue/cli-plugin-eslint": &#123; "config": "airbnb", "lintOn": ["save", "commit"] &#125; &#125;&#125; Preset 插件的版本管理 你可以显式地指定用到的插件的版本： &#123; "plugins": &#123; "@vue/cli-plugin-eslint": &#123; "version": "^3.0.0", // ... 该插件的其它选项 &#125; &#125;&#125; 注意对于官方插件来说这不是必须的——当被忽略时，CLI 会自动使用 registry 中最新的版本。不过我们推荐为 preset 列出的所有第三方插件提供显式的版本范围。 允许插件的命令提示 每个插件在项目创建的过程中都可以注入它自己的命令提示，不过当你使用了一个 preset，这些命令提示就会被跳过，因为 Vue CLI 假设所有的插件选项都已经在 preset 中声明过了。 在有些情况下你可能希望 preset 只声明需要的插件，同时让用户通过插件注入的命令提示来保留一些灵活性。 对于这种场景你可以在插件选项中指定 &quot;prompts&quot;: true 来允许注入命令提示： &#123; "plugins": &#123; "@vue/cli-plugin-eslint": &#123; // 让用户选取他们自己的 ESLint config "prompts": true &#125; &#125;&#125; 远程 Preset 你可以通过发布 git repo 将一个 preset 分享给其他开发者。这个 repo 应该包含以下文件： preset.json: 包含 preset 数据的主要文件（必需）。 generator.js: 一个可以注入或是修改项目中文件的 Generator。 prompts.js 一个可以通过命令行对话为 generator 收集选项的 prompts 文件。 发布 repo 后，你就可以在创建项目的时候通过 --preset 选项使用这个远程的 preset 了： # 从 GitHub repo 使用 presetvue create --preset username/repo my-project GitLab 和 BitBucket 也是支持的。如果要从私有 repo 获取，请确保使用 --clone 选项： vue create --preset gitlab:username/repo --clone my-projectvue create --preset bitbucket:username/repo --clone my-project 加载文件系统中的 Preset 当开发一个远程 preset 的时候，你必须不厌其烦的向远程 repo 发出 push 进行反复测试。为了简化这个流程，你也可以直接在本地测试 preset。如果 --preset 选项的值是一个相对或绝对文件路径，或是以 .json 结尾，则 Vue CLI 会加载本地的 preset： # ./my-preset 应当是一个包含 preset.json 的文件夹vue create --preset ./my-preset my-project# 或者，直接使用当前工作目录下的 json 文件：vue create --preset my-preset.json my-project CLI 服务 在一个 Vue CLI 项目中，@vue/cli-service 安装了一个名为 vue-cli-service 的命令。你可以在 npm scripts 中以 vue-cli-service、或者从终端中以 ./node_modules/.bin/vue-cli-service 访问这个命令。 这是你使用默认 preset 的项目的 package.json： &#123; "scripts": &#123; "serve": "vue-cli-service serve", "build": "vue-cli-service build" &#125;&#125; 你可以通过 npm 或 Yarn 调用这些 script： npm run serve# ORyarn serve 开发 浏览器兼容性 browserslist 你会发现有 package.json 文件里的 browserslist 字段 (或一个单独的 .browserslistrc 文件)，指定了项目的目标浏览器的范围。这个值会被 @babel/preset-env 和 Autoprefixer 用来确定需要转译的 JavaScript 特性和需要添加的 CSS 浏览器前缀。 Polyfill 一个默认的 Vue CLI 项目会使用 @vue/babel-preset-app，它通过 @babel/preset-env 和 browserslist 配置来决定项目需要的 polyfill。 默认情况下，它会把 useBuiltIns: 'usage' 传递给 @babel/preset-env，这样它会根据源代码中出现的语言特性自动检测需要的 polyfill。这确保了最终包里 polyfill 数量的最小化。然而，这也意味着如果其中一个依赖需要特殊的 polyfill，默认情况下 Babel 无法将其检测出来。 如果有依赖需要 polyfill，你有几种选择： 如果该依赖基于一个目标环境不支持的 ES 版本撰写: 将其添加到 vue.config.js 中的 transpileDependencies 选项。这会为该依赖同时开启语法语法转换和根据使用情况检测 polyfill。 如果该依赖交付了 ES5 代码并显式地列出了需要的 polyfill: 你可以使用 @vue/babel-preset-app 的 polyfills 选项预包含所需要的 polyfill。注意 es6.promise 将被默认包含，因为现在的库依赖 Promise 是非常普遍的。 // babel.config.jsmodule.exports = &#123; presets: [ ['@vue/app', &#123; polyfills: [ 'es6.promise', 'es6.symbol' ] &#125;] ]&#125; 提示 我们推荐以这种方式添加 polyfill 而不是在源代码中直接导入它们，因为如果这里列出的 polyfill 在 browserslist 的目标中不需要，则它会被自动排除。 如果该依赖交付 ES5 代码，但使用了 ES6+ 特性且没有显式地列出需要的 polyfill (例如 Vuetify)：请使用 useBuiltIns: ‘entry’ 然后在入口文件添加 import ‘@babel/polyfill’。这会根据 browserslist 目标导入所有 polyfill，这样你就不用再担心依赖的 polyfill 问题了，但是因为包含了一些没有用到的 polyfill 所以最终的包大小可能会增加。 HTML 和静态资源 HTML public/index.html 文件是一个会被 html-webpack-plugin 处理的模板。在构建过程中，资源链接会被自动注入。另外，Vue CLI 也会自动注入 resource hint (preload/prefetch、manifest 和图标链接 (当用到 PWA 插件时) 以及构建过程中处理的 JavaScript 和 CSS 文件的资源链接。 插值 因为 index 文件被用作模板，所以你可以使用 lodash template 语法插入内容： &lt;%= VALUE %&gt; 用来做不转义插值； &lt;%- VALUE %&gt; 用来做 HTML 转义插值； &lt;% expression %&gt; 用来描述 JavaScript 流程控制。 除了被 html-webpack-plugin 暴露的默认值之外，所有客户端环境变量也可以直接使用。例如，BASE_URL 的用法： &lt;link rel="icon" href="&lt;%= BASE_URL %&gt;favicon.ico"&gt; 更多内容可以查阅： baseUrl Preload CSS Webpack 环境变量和模式 构建目标 部署 配置 配置细节参考：https://cli.vuejs.org/zh/config/]]></content>
  </entry>
  <entry>
    <title><![CDATA[Webpack 资源管理]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fwebpack%2Fasset-management%2F</url>
    <content type="text"><![CDATA[Webpack 资源管理 📌 提示： 版本问题 本文基于 webpack 2.x 版本。webpack 2.x 相比 webpack 1.x 有重大改变。所以，如果你的项目中已使用了 webpack 1.x ，本教程的示例将不适用，请慎重。 如果铁了心要升级 webpack ，请参考 webpack 官方文档 - 从 v1 迁移到 v2 阅读建议 阅读本文前，建议先阅读 Webpack 概念 。 webpack 的优势 webpack 最重要的功能就是资源管理。 JavaScript 世界已有好几个有名的资源管理工具，webpack 有什么独到之处呢？ 在 webpack 出现之前，前端开发人员会使用 grunt 和 gulp 等工具来处理这些 web 资源，如样式文件（例如 .css, .less, .sass），图片（例如 .png, .jpg, .svg），字体（例如 .woff, .woff2, .eot）和数据（例如 .json, .xml, .csv）等，并将它们从 /src 文件夹移动到 /dist 或 /build 目录中。 而 webpack 从 entry(入口) 开始，访问应用程序，并**动态打包(dynamically bundle)**所有依赖项。这是极好的创举，因为现在每个模块都可以明确表述它自身的依赖，这可以避免打包未使用的模块。 Loader Loader(加载器) 用于对模块的源代码进行转换。 使用加载器一般遵循几步： 安装加载器 配置 Loader 引用资源文件 安装加载器 根据需要加载的资源文件，选择下载对应的加载器。 $ npm install --save-dev css-loader 更多 webpack 可用 Loader 请查看：webpack loaders 配置 Loader ​⚠️ 注意： webpack 2.x 版本的 Loader 配置和 webpack 1.x 版本差别很大。 Loader 在 webpack.config.js 文件的 module 属性中配置。 资源类型对应单一加载器 module: &#123; rules: [ &#123;test: /\.css$/, loader: 'css-loader'&#125; ]&#125;, 资源类型对应多个加载器 module: &#123; rules: [ &#123; test: /\.css$/, use: ["style-loader", "css-loader"] &#125; ]&#125;, 加载器含配置选项 module: &#123; rules: [ &#123; test: /\.css$/, use: [ &#123; loader: 'style-loader' &#125;, &#123; loader: 'css-loader', options: &#123; modules: true &#125; &#125; ] &#125; ]&#125;, 引用资源文件 完成上两步后，就可以在 JavaScript 中使用 import ，require 关键字引用相应类型资源文件。 import "./index.css";require("./index.css"); Plugin Plugin(插件) 用于解决 Loader 无法解决的问题，它是 Loader 的辅助。 由于 plugin 可以携带参数/选项，你必须在 wepback 配置中，向 plugins 属性传入 new 实例。 安装插件 webpack 自身包含了一些常用插件，你可以通过 webpack 来引用。除此之外的插件，使用前需要安装 $ npm install --save-dev html-webpack-plugin OpenBrowserPlugin 更多 webpack Plugins 可以查看： webpack plugins 配置 Plugin const webpack = require("webpack");const HtmlWebpackPlugin = require("html-webpack-plugin");const OpenBrowserPlugin = require("open-browser-webpack-plugin");module.exports = &#123; // 附加插件列表 plugins: [ // 压缩 js 插件 new webpack.optimize.UglifyJsPlugin(&#123; compress: &#123; warnings: false &#125; &#125;), // 用于简化 HTML 文件（index.html）的创建，提供访问 bundle 的服务。 new HtmlWebpackPlugin(&#123; title: "frontend-tutorial", template: "./index.html" &#125;), // 自动打开浏览器 new OpenBrowserPlugin(&#123; url: "http://localhost:8080" &#125;) ]&#125;; 加载资源专题 加载 React 很多浏览器并不识别 React 语法，为了让浏览器支持，你需要使用 babel-loader 解释器来转义 React 语法为普通的 Javascript 语法。 ⚠️ 注意： 官方推荐 babel-loader 和 webpack 的对应版本 webpack 1.x | babel-loader &lt;= 6.x webpack 2.x | babel-loader &gt;= 7.x （推荐）（^6.2.10 也可以运行，但会有不赞成的警告(deprecation warnings)） 首先，安装需要使用的库： $ npm install --save-dev babel-loader babel-preset-es2015 babel-preset-react babel-preset-xxx 表示你希望转义的语法。 webpack.config.js 中的模块配置如下： // 关于模块配置module: &#123; // 模块规则（配置 loader、解析器等选项） rules: [ // 这里是匹配条件，每个选项都接收一个正则表达式或字符串 // test 和 include 具有相同的作用，都是必须匹配选项 // exclude 是必不匹配选项（优先于 test 和 include） // 最佳实践： // - 只在 test 和 文件名匹配 中使用正则表达式 // - 在 include 和 exclude 中使用绝对路径数组 // - 尽量避免 exclude，更倾向于使用 include &#123; // 语义解释器，将 js/jsx 文件中的 es2015/react 语法自动转为浏览器可识别的 Javascript 语法 test: /\.jsx?$/, include: path.resolve(__dirname, "app"), // 应该应用的 loader，它相对上下文解析 // 为了更清晰，`-loader` 后缀在 webpack 2 中不再是可选的 // 查看 webpack 1 升级指南。 loader: "babel-loader", // loader 的可选项 options: &#123; presets: ["es2015", "react"] &#125;, &#125;, ]&#125;, ​🔦 示例 DEMO04： (DEMO / SOURCE) 加载 CSS 为了从 JavaScript 模块中 import 一个 CSS 文件，你只需要在 module 中安装并添加 style-loader 和 css-loader 。 $ npm install --save-dev style-loader css-loader webpack.config.js module.exports = &#123; //... module: &#123; rules: [ &#123; test: /\.css$/, use: ["css-loader", "style-loader"] &#125; ] &#125; //...&#125;; 好了，此时你就可以在代码中通过 import './style.css' 的方式引入 CSS 文件。 其余，加载 less，sass 等样式文件也是大同小异，不一一细说。 🔦 示例 DEMO06： (DEMO / SOURCE) 加载图片 如何打包、加载图片呢？你可以使用 file-loader 来指定要加载的图片。 $ npm install --save-dev file-loader webpack.config.js module.exports = &#123; //... module: &#123; rules: [ &#123; test: /\.(png|svg|jpg|gif)$/, use: ["file-loader"] &#125; ] &#125; //...&#125;; 然后，你可以通过 import imgBig from './lion.png' 的方式引入图片。例： import React from "react";import imgBig from "./lion.png";class Welcome extends React.PureComponent &#123; render() &#123; return ( &lt;div&gt; &lt;h1&gt;Hello, &#123;this.props.name&#125;&lt;/h1&gt; &lt;img src=&#123;imgBig&#125; /&gt; &lt;/div&gt; ); &#125;&#125;export default Welcome; 压缩图片 这还不算完，日常开发中，经常会遇到有些图片文件过大的问题，这会影响你的 app 的加载速度。webpack 提供了压缩图片的方法帮你解决图片大的问题。 首先，你需要安装 image-webpack-loader $ npm i --save-dev image-webpack-loader 接下来，修改 webpack.config.js &#123; // 图片加载 + 图片压缩 test: /\.(png|svg|jpg|gif)$/, loaders: [ "file-loader", &#123; loader: "image-webpack-loader", query: &#123; progressive: true, pngquant: &#123; quality: "65-90", speed: 4 &#125; &#125; &#125; ]&#125; 🔦 示例 DEMO07： (DEMO / SOURCE) 加载字体 那么，像字体这样的其他资源如何处理呢？file-loader 和 url-loader 可以接收并加载任何文件，然后将其输出到构建目录。这就是说，我们可以将它们用于任何类型的文件，包括字体： webpack.config.js module.exports = &#123; //... module: &#123; rules: [ &#123; test: /\.(woff|woff2|eot|ttf|svg)$/, use: ["file-loader"] &#125; ] &#125; //...&#125;; 一切就绪后，你可以在 css 文件中这样引入字体： @font-face &#123; font-family: "MyDiyFont"; src: url("./font/iconfont.eot"); /* IE9*/ src: url("./font/iconfont.eot?#iefix") format("embedded-opentype"), /* IE6-IE8 */ url("./font/iconfont.woff") format("woff"), /* chrome、firefox */ url("./font/iconfont.ttf") format("truetype"), /* chrome、firefox、opera、Safari, Android, iOS 4.2+*/ url("./font/iconfont.svg#iconfont") format("svg"); /* iOS 4.1- */&#125;h1 &#123; font-family: "MyDiyFont"; font-size: 24px;&#125;p &#123; font-family: "MyDiyFont"; font-size: 18px;&#125; 然后，相对路径，会被替换为构建目录中的完整路径/文件名。 🔦 示例 DEMO08： (DEMO / SOURCE) 更多内容 📚 拓展阅读 Webpack 入门 Webpack 概念 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 📦 本文归档在 我的前端技术教程系列：frontend-tutorial 官方资料 Webpack 官网 Webpack 中文网 webpack Github webpack-dev-server Github webpack-dev-server 官方文档 入门资料 webpack-demos webpack-howto webpack-handbook 教程 如何学习 Webpack Webpack 概念 Webpack 入门 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 文章 JavaScript 模块化七日谈 前端模块化开发那点历史 更多资源 awesome-webpack-cn]]></content>
  </entry>
  <entry>
    <title><![CDATA[Webpack 概念]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fwebpack%2Fconcept%2F</url>
    <content type="text"><![CDATA[Webpack 概念 webpack 是一个现代的 JavaScript 应用程序的模块打包器(module bundler)。当 webpack 处理应用程序时，它会递归地构建一个依赖关系图表(dependency graph)，其中包含应用程序需要的每个模块，然后将所有这些模块打包成少量的 bundle - 通常只有一个，由浏览器加载。 学习 webpack，需要先了解几个核心概念，下面会一一道来。 模块化(module) 在模块化编程中，开发者将程序分解相对独立的代码块，并称之为模块。 每个模块具有比完整程序更小的接触面，使得校验、调试、测试轻而易举。 精心编写的模块提供了可靠的抽象和封装界限，使得应用程序中每个模块都具有条理清楚的设计和明确的目的。 Node.js 从最一开始就支持模块化编程。然而，在 web，模块化的支持正缓慢到来。在 web 存在多种支持 JavaScript 模块化的工具，这些工具各有优势和限制。webpack 基于从这些系统获得的经验教训，并将模块的概念应用于项目中的任何文件。 什么是 webpack 模块 对比 Node.js 模块，webpack 模块能够以各种方式表达它们的依赖关系，几个例子如下： ES2015 import 语句 CommonJS require() 语句 AMD define 和 require 语句 css/sass/less 文件中的 @import 语句。 样式(url(...))或 HTML 文件(``)中的图片链接(image url) webpack 1 需要特定的 loader 来转换 ES 2015 import，然而 webpack 2 天然支持。 支持的模块类型 webpack 通过 loader 可以支持各种语言和预处理器编写模块。loader 描述了 webpack 如何处理 非 JavaScript(non-JavaScript) 模块，并且在bundle中引入这些依赖。 webpack 社区已经为各种流行语言和语言处理器构建了 loader，包括： CoffeeScript TypeScript ESNext (Babel) Sass Less Stylus 总的来说，webpack 提供了可定制的、强大和丰富的 API，允许任何技术栈使用 webpack，保持了在你的开发、测试和生成流程中无侵入性(non-opinionated)。 配置文件 - webpack.config.js webpack 是高度可配置的，如何模块化打包、加载都可以基于配置文件定制。 webpack 的默认配置文件是 webpack.config.js。 因为 webpack 配置是标准的 Node.js CommonJS 模块，你可以使用如下特性： 通过 require(...) 导入其他文件 通过 require(...) 使用 npm 的工具函数 使用 JavaScript 控制流表达式，例如 ?: 操作符 对常用值使用常量或变量 编写并执行函数来生成部分配置 依赖图表(Dependency Graph) 任何时候，一个文件依赖于另一个文件，webpack 就把此视为文件之间有依赖关系。这使得 webpack 可以接收非代码资源(non-code asset)（例如图像或 web 字体），并且可以把它们作为依赖提供给你的应用程序。 webpack 从命令行或配置文件中定义的一个模块列表开始，处理你的应用程序。 从这些入口起点开始，webpack 递归地构建一个依赖图表，这个依赖图表包含着应用程序所需的每个模块，然后将所有这些模块打包为少量的 bundle- 通常只有一个 - 可由浏览器加载。 对于 HTTP/1.1 客户端，由 webpack 打包你的应用程序会尤其强大，因为在浏览器发起一个新请求时，它能够减少应用程序必须等待的时间。对于 HTTP/2，你还可以使用代码拆分(Code Splitting)以及通过 webpack 打包来实现最佳优化。 入口(entry) webpack 将创建所有应用程序的依赖关系图表(dependency graph)。图表的起点被称之为入口起点(entry point)。入口起点告诉 webpack 从哪里开始，并遵循着依赖关系图表知道要打包什么。可以将您应用程序的入口起点认为是**根上下文(contextual root)**或 app 第一个启动文件。 在 webpack 中，我们使用 webpack 配置对象(webpack configuration object) 中的 entry 属性来定义入口。 例： module.exports = &#123; entry: "./path/to/my/entry/file.js"&#125;; 输出(output) 将所有的资源(assets)归拢在一起后，还需要告诉 webpack 在哪里打包应用程序。webpack 的 output 属性描述了如何处理归拢在一起的代码(bundled code)。 例： const path = require("path");module.exports = &#123; entry: "./path/to/my/entry/file.js", output: &#123; path: path.resolve(__dirname, "dist"), filename: "my-first-webpack.bundle.js" &#125;&#125;; 加载(loader) webpack 的目标是，让 webpack 聚焦于项目中的所有资源(asset)，而浏览器不需要关注考虑这些（这并不意味着资源(asset)都必须打包在一起）。webpack 把每个文件(.css, .html, .scss, .jpg, etc.) 都作为模块处理。然而 webpack 只理解 JavaScript。 webpack loader 会将这些文件转换为模块，而转换后的文件会被添加到依赖图表中。 在更高层面，webpack 的配置有两个目标。 识别出(identify)应该被对应的 loader 进行转换(transform)的那些文件 由于进行过文件转换，所以能够将被转换的文件添加到依赖图表（并且最终添加到 bundle 中）(use 属性) 例： const path = require("path");const config = &#123; entry: "./path/to/my/entry/file.js", output: &#123; path: path.resolve(__dirname, "dist"), filename: "my-first-webpack.bundle.js" &#125;, module: &#123; rules: [&#123; test: /\.(js|jsx)$/, use: "babel-loader" &#125;] &#125;&#125;;module.exports = config; 插件(plugins) 由于 loader 仅在每个文件的基础上执行转换，而 插件(plugins) 最常用于（但不限于）在打包模块的“compilation”和“chunk”生命周期执行操作和自定义功能（查看更多）。webpack 的插件系统极其强大和可定制化。 想要使用一个插件，你只需要 require() 它，然后把它添加到 plugins 数组中。多数插件可以通过选项(option)自定义。你也可以在一个配置文件中因为不同目的而多次使用同一个插件，你需要使用 new 创建实例来调用它。 例： const HtmlWebpackPlugin = require("html-webpack-plugin"); //installed via npmconst webpack = require("webpack"); //to access built-in pluginsconst path = require("path");const config = &#123; entry: "./path/to/my/entry/file.js", output: &#123; path: path.resolve(__dirname, "dist"), filename: "my-first-webpack.bundle.js" &#125;, module: &#123; rules: [&#123; test: /\.(js|jsx)$/, use: "babel-loader" &#125;] &#125;, plugins: [ new webpack.optimize.UglifyJsPlugin(), new HtmlWebpackPlugin(&#123; template: "./src/index.html" &#125;) ]&#125;;module.exports = config; 热替换(Hot Module Replacement) 模块热替换功能会在应用程序运行过程中替换、添加或删除模块，而无需重新加载页面。这使得你可以在独立模块变更后，无需刷新整个页面，就可以更新这些模块，极大地加速了开发时间。 这一切是如何运行的？ 站在 App 的角度 app 代码要求 HMR runtime 检查更新。 HMR runtime （异步）下载更新，然后通知 app 代码更新可用。 app 代码要求 HMR runtime 应用更新。 HMR runtime （异步）应用更新。 你可以设置 HMR，使此进程自动触发更新，或者你可以选择要求在用户交互后进行更新。 站在编译器(webpack) 的角度 除了普通资源，编译器(compiler)需要发出 “update”，以允许更新之前的版本到新的版本。“update” 由两部分组成： 待更新 manifest (JSON) 一个或多个待更新 chunk (JavaScript) manifest 包括新的编译 hash 和所有的待更新 chunk 目录。 每个待更新 chunk 包括用于与所有被更新模块相对应 chunk 的代码（或一个 flag 用于表明模块要被移除）。 编译器确保模块 ID 和 chunk ID 在这些构建之间保持一致。通常将这些 ID 存储在内存中（例如，当使用 webpack-dev-server 时），但是也可能将它们存储在一个 JSON 文件中。 站在模块的角度 HMR 是可选功能，只会影响包含 HMR 代码的模块。举个例子，通过 style-loader 为 style 样式追加补丁。 为了运行追加补丁，style-loader 实现了 HMR 接口；当它通过 HMR 接收到更新，它会使用新的样式替换旧的样式。 类似的，当在一个模块中实现了 HMR 接口，你可以描述出当模块被更新后发生了什么。然而在多数情况下，不需要强制在每个模块中写入 HMR 代码。如果一个模块没有 HMR 处理函数，更新就会冒泡。这意味着一个简单的处理函数能够对整个模块树(complete module tree)进行处理。如果在这个模块树中，一个单独的模块被更新，那么整个模块树都会被重新加载（只会重新加载，不会迁移）。 站在 HMR Runtime 的角度 (Technical) 对于模块系统的 runtime，附加的代码被发送到 parents 和 children 跟踪模块。 在管理方面，runtime 支持两个方法 check 和 apply。 check 发送 HTTP 请求来更新 manifest。如果请求失败，说明没有可用更新。如果请求成功，待更新 chunk 会和当前加载过的 chunk 进行比较。对每个加载过的 chunk，会下载相对应的待更新 chunk。当所有待更新 chunk 完成下载，就会准备切换到 ready 状态。 apply 方法将所有被更新模块标记为无效。对于每个无效模块，都需要在模块中有一个更新处理函数，或者在它的父级模块们中有更新处理函数。否则，无效标记冒泡，并将父级也标记为无效。每个冒泡继续直到到达应用程序入口起点，或者到达带有更新处理函数的模块（以最先到达为准）。如果它从入口起点开始冒泡，则此过程失败。 之后，所有无效模块都被（通过 dispose 处理函数）处理和解除加载。然后更新当前 hash，并且调用所有 “accept” 处理函数。runtime 切换回闲置状态，一切照常继续。 产生的文件 (Technical) 左侧表示初始编译器通过。右侧表示更新了模块 4 和 9 。 它能够用于？ 你可以在开发过程中将 HMR 作为 LiveReload 的替代。webpack-dev-server 支持热模式，在试图重新加载整个页面之前，热模式会尝试使用 HMR 来更新。查看如何实现在 React 项目中使用 HMR 为例。 一些 loader 已经生成可热更新的模块。例如，style-loader 能够置换出页面的样式表。对于这样的模块，你不需要做任何特殊处理。 webpack 的强大之处在于它的可定制化，取决于特定项目需求，这里有许多配置 HMR 的方式。 更多内容 📚 拓展阅读 Webpack 入门 Webpack 概念 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 📦 本文归档在 我的前端技术教程系列：frontend-tutorial 官方资料 Webpack 官网 Webpack 中文网 webpack Github webpack-dev-server Github webpack-dev-server 官方文档 入门资料 webpack-demos webpack-howto webpack-handbook 教程 如何学习 Webpack Webpack 概念 Webpack 入门 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 文章 JavaScript 模块化七日谈 前端模块化开发那点历史 更多资源 awesome-webpack-cn]]></content>
  </entry>
  <entry>
    <title><![CDATA[Webpack 开发工具]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fwebpack%2Fdevelopment%2F</url>
    <content type="text"><![CDATA[Webpack 开发工具 ​⚠️ 注意： 永远不要在生产环境中使用这些工具，永远不要。 devtool 当 JavaScript 异常抛出时，你常会想知道这个错误发生在哪个文件的哪一行。然而因为 webpack 将文件输出为一个或多个 bundle，所以 追踪这一错误会很不方便。 Source maps 试图解决这一问题。它有很多选择，各有优劣： devtool build rebuild production quality eval +++ +++ no generated code cheap-eval-source-map + ++ no transformed code (lines only) cheap-source-map + o yes transformed code (lines only) cheap-module-eval-source-map o ++ no original source (lines only) cheap-module-source-map o - yes original source (lines only) eval-source-map – + no original source source-map – – yes original source nosources-source-map – – yes without source content + 表示较快，- 表示较慢，o 表示时间相同 对于开发环境，通常希望更快速的 Source Map，需要添加到 bundle 中以增加体积为代价，但是对于生产环境，则希望更精准的 Source Map，需要从 bundle 中分离并独立存在。 个人建议：开发环境使用 cheap-module-eval-source-map ；开发环境使用 cheap-module-source-map 。 使用方式非常简单，在 webpack.config.js 中配置如下： module.exports = &#123; // 通过在浏览器调试工具(browser devtools)中添加元信息(meta info)增强调试 // devtool: "eval", // 没有模块映射，而是命名模块。以牺牲细节达到最快。 // devtool: "source-map", // 牺牲了构建速度的 `source-map' 是最详细的 // devtool: "inline-source-map", // 嵌入到源文件中 // devtool: "eval-source-map", // 将 SourceMap 嵌入到每个模块中 // devtool: "hidden-source-map", // SourceMap 不在源文件中引用 // devtool: "cheap-source-map", // 没有模块映射(module mappings)的 SourceMap 低级变体(cheap-variant) // devtool: "cheap-module-source-map", // 有模块映射(module mappings)的 SourceMap 低级变体 devtool: "cheap-module-eval-source-map"&#125;; webpack-dev-server webpack-dev-server 可以提供了一个服务器和实时重载（live reloading） 功能。 在开始前，确定你有一个 index.html 文件指向你的 bundle。假设 output.filename 是 bunlde.js。 &lt;html&gt;&lt;body&gt;&lt;script type="text/javascript" src="./dist/bundle.js"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 首先从 npm 安装 webpack-dev-server： $ npm install --save-dev webpack-dev-server 安装完成之后，你应该可以使用 webpack-dev-server 了，方式如下： $ webpack-dev-server --open 上述命令应该自动在浏览器中打开 http://localhost:8080。 ​📌 提示： 本教程中的 示例代码 除了 demo00 ，都可以使用 webpack-dev-server 命令启动服务。 在你的文件中做一点更改并且保存。你应该可以在控制台中看到正在编译。编译完成之后，页面应该会刷新。如果控制台中什么都没发生，你可能需要调整下 watchOptions。 默认情况下 webpack 会使用inline mode（内联模式）。这种模式在你的 bundle 中注入客户端（用来 live reloading 和展示构建错误）。Inline 模式下，你会在你的 DevTools 控制台中看到构建错误。 webpack-dev-server 可以做很多事情，比如转发请求到你的后端服务器。 webpack-dev-server 支持很多 cli 参数，来手动配置服务的选项。 但是，个人建议，一种更好的做法是在 webpack.config.js 文件中通过配置 devServer 属性来配置 webpack-dev-server 。 更多配置项参考：官方文档 - 开发中 Server(DevServer) ​🔦 示例 DEMO12： (DEMO / SOURCE) 在本示例中，devServer 配置如下： &gt; devServer: &#123;&gt; contentBase: [path.join(__dirname, "dist")],&gt; compress: true,&gt; port: 9000, // 启动端口号&gt; inline: true,&gt; &#125;&gt; 执行 webpack-dev-server 后，会启动一个端口为 9000 的本地服务。 热模块替换（Hot Module Replacement） ​📌​ 提示： 模块热替换功能一般用于开发环境。 现在你有了实时重载功能，你甚至可以更进一步：Hot Module Replacement（热模块替换）。这是一个接口，使得你可以替换模块而不需要刷新页面。不用每次修改都重新启动服务，这可以极大地提高开发效率。 那么，如何配置 webpack 来实现热替换呢？ 请按以下步骤一步步来： 首先，安装依赖 react-hot-loader（确保使用这个包的 next 版本） $ npm install --save babel-loader react-hot-loader@next 配置 entry 你需要定义几个用于热替换的入口路径 entry: &#123; main: [ // App 入口 "./app/index", // 开启 React 代码的模块热替换(HMR) 'react-hot-loader/patch', // 为 webpack-dev-server 的环境打包代码 // 然后连接到指定服务器域名与端口 'webpack-dev-server/client?http://localhost:9000', // 为热替换(HMR)打包好代码 // only- 意味着只有成功更新运行代码才会执行热替换(HMR) 'webpack/hot/only-dev-server', ],&#125;, 配置 output publicPath 对于热替换(HMR)是必须的，让 webpack 知道在哪里载入热更新的模块(chunk) output: &#123; ... ... // 对于热替换(HMR)是必须的，让 webpack 知道在哪里载入热更新的模块(chunk) publicPath: "/"&#125;, 配置 module 需要使用 ES2015 模块来使 HMR 正常工作。为此，在我们的 es2015 preset 设置中，将 module 选项设置为 false。 并且，在此要引入 react-hot-loader/babel 开启 React 代码的模块热替换(HMR) module: &#123; rules: [ &#123; // 语义解释器，将 js/jsx 文件中的 es2015/react 语法自动转为浏览器可识别的 Javascript 语法 test: /\.jsx?$/, include: path.resolve(__dirname, "app"), exclude: /node_modules/, // 应该应用的 loader，它相对上下文解析 // 为了更清晰，`-loader` 后缀在 webpack 2 中不再是可选的 // 查看 webpack 1 升级指南。 loader: "babel-loader", // loader 的可选项 options: &#123; presets: [ // webpack 现在已经支持原生的 import 语句了, 并且将其运用在 tree-shaking 特性上 [ "es2015", &#123; "modules": false &#125; ], "react" // 转译 React 组件为 JavaScript 代码 ], plugins: [ "react-hot-loader/babel" // 开启 React 代码的模块热替换(HMR) ] &#125;, &#125;, ] &#125;, 配置 devServer 此处，也需要引入 publicPath ，且和上文 output 的 publicPath 值保持一致。 hot 属性需要置为 true，表示开启服务器的模块热替换。 devServer: &#123; contentBase: [path.join(__dirname, "dist")], compress: true, port: 9000, // 启动端口号 hot: true, // 启用 webpack 的模块热替换特性 inline: true, publicPath: "/", // 和上文 output 的“publicPath”值保持一致&#125; 配置 plugins 最后，需要开启 webpack 自带的 HotModuleReplacementPlugin 和 NamedModulesPlugin 插件，启动热替换功能。 plugins: [ // 开启全局的模块热替换(HMR) new webpack.HotModuleReplacementPlugin(), // 当模块热替换(HMR)时在浏览器控制台输出对用户更友好的模块名字信息 new webpack.NamedModulesPlugin(),], ​🔦 示例 DEMO13： (SOURCE) 在示例中，启动服务后，打开浏览器，访问 http://localhost:9000/ 。 按快捷键 F12 打开浏览器调试窗口，可以看到类似提示信息 &gt; [HMR] Waiting for update signal from WDS...&gt; [HMR] Waiting for update signal from WDS...&gt; [WDS] Hot Module Replacement enabled.&gt; 这表示热替换功能已启动。 修改 app/index.jsx 文件，来看看热替换的效果： 修改前： &gt; ReactDOM.render(&gt; &lt;Welcome name="Zhang Peng" /&gt;,&gt; document.getElementById("root")&gt; );&gt; 修改后： &gt; ReactDOM.render(&lt;Welcome name="guest" /&gt;, document.getElementById("root"));&gt; 此时，应该看到页面内容会替换为你修改的内容。 更多内容 📚 拓展阅读 Webpack 入门 Webpack 概念 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 📦 本文归档在 我的前端技术教程系列：frontend-tutorial 官方资料 Webpack 官网 Webpack 中文网 webpack Github webpack-dev-server Github webpack-dev-server 官方文档 入门资料 webpack-demos webpack-howto webpack-handbook 教程 如何学习 Webpack Webpack 概念 Webpack 入门 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 文章 JavaScript 模块化七日谈 前端模块化开发那点历史 更多资源 awesome-webpack-cn]]></content>
  </entry>
  <entry>
    <title><![CDATA[Webpack 代码分离]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fwebpack%2Fcode-splitting%2F</url>
    <content type="text"><![CDATA[Webpack 代码分离 📌 提示： 版本问题 本文基于 webpack 2.x 版本。webpack 2.x 相比 webpack 1.x 有重大改变。所以，如果你的项目中已使用了 webpack 1.x ，本教程的示例将不适用，请慎重。 如果铁了心要升级 webpack ，请参考 webpack 官方文档 - 从 v1 迁移到 v2 阅读建议 阅读本文前，建议先阅读 Webpack 概念 。 代码分离是 webpack 中最引人注目的特性之一。 你可以把你的代码分离到不同的 bundle 中，然后你就可以去按需加载这些文件。 总的来说， webpack 分离可以分为两类： 资源分离 代码分离 资源分离(Resource Splitting) 对第三方库(vendor) 和 CSS 进行代码分离，这些方式有助于实现缓存和并行加载。 分离 CSS(CSS Splitting) 你可能也想将你的样式代码分离到单独的 bundle 中，以此使其独立于应用程序逻辑。这加强了样式的可缓存性，并且使得浏览器能够并行加载应用程序代码中的样式文件，避免 FOUC 问题 (无样式内容造成的闪烁)。 安装 ExtractTextWebpackPlugin 如下 $ npm install --save-dev extract-text-webpack-plugin webpack.config.js 中需要按下面进行配置： const ExtractTextPlugin = require("extract-text-webpack-plugin");module.exports = &#123; // 关于模块配置 module: &#123; // 模块规则（配置 loader、解析器等选项） rules: [ &#123; // css 加载 test: /\.css$/, use: ExtractTextPlugin.extract(&#123; use: "css-loader" &#125;) &#125; ] &#125;, // 附加插件列表 plugins: [ // 将样式文件独立打包 new ExtractTextPlugin("styles.css") ]&#125;; ​🔦 示例 DEMO09： (DEMO / SOURCE) 分离第三方库(Vendor Code Splitting) 一个典型的应用程序，由于框架/功能性需求，会依赖于许多第三方库的代码。不同于应用程序代码，这些第三方库代码不会频繁修改。 如果我们将这些库(library)中的代码，保留在与应用程序代码相独立的 bundle 中，我们就可以利用浏览器缓存机制，把这些文件长时间地缓存在用户机器上。 为了完成这个目标，不管应用程序代码如何变化，vendor 文件名中的 hash 部分必须保持不变。学习如何使用 CommonsChunkPlugin 分离 vendor/library 代码。 webpack.config.js const webpack = require("webpack");module.exports = &#123; // 这里应用程序开始执行 // webpack 开始打包 // 本例中 entry 为多入口 entry: &#123; main: "./app/index", vendor: "react" &#125;, // webpack 如何输出结果的相关选项 output: &#123; // 所有输出文件的目标路径 // 必须是绝对路径（使用 Node.js 的 path 模块） path: path.resolve(__dirname, "dist"), // 「入口分块(entry chunk)」的文件名模板（出口分块？） // filename: "bundle.min.js", // filename: "[name].js", // 用于多个入口点(entry point)（出口点？） // filename: "[chunkhash].js", // 用于长效缓存 filename: "[name].[chunkhash:8].js" // 用于长效缓存 &#125;, // 附加插件列表 plugins: [ new webpack.optimize.CommonsChunkPlugin(&#123; name: "vendor" // 指定公共 bundle 的名字。 &#125;) ]&#125;; 在上面的配置中， 在 entry 属性中，将 react 指定为一个独立的入口 vendor； 然后，在 output 属性中，将 filename 指定为 [name].[chunkhash:8].js，这表示输出文件的文件名样式。 最后在 plugins 列表中引入 CommonsChunkPlugin 插件，用来指定 bundle 。 执行 webpack 命令后，webpack 会生成 2 个 bundle 文件，形式如： main.bef8f974.jsvendor.2f1bd4c8.js ​🔦 示例 DEMO10： (DEMO / SOURCE) 代码按需分离(On Demand Code Splitting) 虽然前面几类资源分离，需要用户预先在配置中指定分离模块，但也可以在应用程序代码中创建动态分离模块。 这可以用于更细粒度的代码块，例如，根据我们的应用程序路由，或根据用户行为预测。这可以使用户按照实际需要加载非必要资源。 前一节，我们了解了 webpack 可以将资源拆分为 bundle。接下来，我们要学习如何异步加载。例如，这允许首先提供最低限度的引导 bundle，并在稍后再异步地加载其他功能。 webpack 支持两种相似的技术实现此目的：使用 import() (推荐，ECMAScript 提案) 和 require.ensure() (遗留，webpack 特定)。本文只介绍官方推荐的 import() 方式。 ES2015 loader 规范定义了 import() 作为一种在运行时(runtime)动态载入 ES2015 模块的方法。 webpack 把 import() 作为一个分离点(split-point)，并把引入的模块作为一个单独的 chunk。 import() 将模块名字作为参数并返回一个 Promoise 对象，即 import(name) -&gt; Promise 配合 Babel 使用 如果你想要在 Babel 中使用 import，但是由于 import() 还是属于 Stage 3 的特性，所以你需要安装/添加 syntax-dynamic-import 插件来避免 parser 报错。在草案正式成为规范后，就不再需要这个插件了。 例： 我们来定义一个 Clock 组件，动态引入 moment 库。 首先，安装 moment 库。 $ npm install --save-dev moment JavaScript 代码： class Clock extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123; date: new Date().toLocaleDateString() &#125;; this.click = this.click.bind(this); &#125; click() &#123; // 动态引入import() import("moment") .then(moment =&gt; moment().format("MMMM Do YYYY, h:mm:ss a")) .then(str =&gt; this.setState(&#123; date: str &#125;)) .catch(err =&gt; console.log("Failed to load moment", err)); &#125; render() &#123; return ( &lt;div&gt; &lt;h2&gt;It is &#123;this.state.date&#125;.&lt;/h2&gt; &lt;p onClick=&#123;this.click&#125;&gt;Click here to changing the time.&lt;/p&gt; &lt;/div&gt; ); &#125;&#125; webpack.config.js // 关于模块配置module: &#123; // 模块规则（配置 loader、解析器等选项） rules: [ &#123; // 语义解释器，将 js/jsx 文件中的 es2015/react 语法自动转为浏览器可识别的 Javascript 语法 test: /\.jsx?$/, include: path.resolve(__dirname, "app"), // 应该应用的 loader，它相对上下文解析 // 为了更清晰，`-loader` 后缀在 webpack 2 中不再是可选的 // 查看 webpack 1 升级指南。 loader: "babel-loader", // loader 的可选项 options: &#123; presets: ["es2015", "react"], plugins: ['syntax-dynamic-import'] &#125;, &#125;, ]&#125;, ​🔦 示例 DEMO11： (DEMO / SOURCE) 更多内容 📚 拓展阅读 Webpack 入门 Webpack 概念 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 📦 本文归档在 我的前端技术教程系列：frontend-tutorial 官方资料 Webpack 官网 Webpack 中文网 webpack Github webpack-dev-server Github webpack-dev-server 官方文档 入门资料 webpack-demos webpack-howto webpack-handbook 教程 如何学习 Webpack Webpack 概念 Webpack 入门 Webpack 资源管理 Webpack 代码分离 Webpack 开发工具 文章 JavaScript 模块化七日谈 前端模块化开发那点历史 更多资源 awesome-webpack-cn]]></content>
  </entry>
  <entry>
    <title><![CDATA[Freemark]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavalib%2Ffreemark%2F</url>
    <content type="text"><![CDATA[Freemark FreeMarker 是一款 模板引擎： 即一种基于模板和要改变的数据， 并用来生成输出文本(HTML 网页，电子邮件，配置文件，源代码等)的通用工具。 它不是面向最终用户的，而是一个 Java 类库，是一款程序员可以嵌入他们所开发产品的组件。 1. 简介 1.1. 什么是 Freemark？ 2. 入门 2.1. 创建 Configuration 实例 2.2. 创建数据模型 2.3. 获取模板 2.4. 合并模板和数据模型 2.5. 完整示例 3. 基础 模板 4. 进阶 5. 引用和引申 1. 简介 1.1. 什么是 Freemark？ FreeMarker 是一款 模板引擎： 即一种基于模板和要改变的数据， 并用来生成输出文本(HTML 网页，电子邮件，配置文件，源代码等)的通用工具。 它不是面向最终用户的，而是一个 Java 类库，是一款程序员可以嵌入他们所开发产品的组件。 简言之：模板 + 数据 = 输出 2. 入门 2.1. 创建 Configuration 实例 首先，你应该创建一个 freemarker.template.Configuration 实例， 然后调整它的设置。Configuration 实例是存储 FreeMarker 应用级设置的核心部分。同时，它也处理创建和缓存预解析模板(比如 Template 对象)的工作。 // Create your Configuration instance, and specify if up to what FreeMarker// version (here 2.3.22) do you want to apply the fixes that are not 100%// backward-compatible. See the Configuration JavaDoc for details.Configuration cfg = new Configuration(Configuration.VERSION_2_3_22);// Specify the source where the template files come from. Here I set a// plain directory for it, but non-file-system sources are possible too:cfg.setDirectoryForTemplateLoading(new File("/where/you/store/templates"));// Set the preferred charset template files are stored in. UTF-8 is// a good choice in most applications:cfg.setDefaultEncoding("UTF-8");// Sets how errors will appear.// During web page *development* TemplateExceptionHandler.HTML_DEBUG_HANDLER is better.cfg.setTemplateExceptionHandler(TemplateExceptionHandler.RETHROW_HANDLER); 注：不需要重复创建 Configuration 实例；它的代价很高，尤其是会丢失缓存。Configuration 实例就是应用级别的单例。 2.2. 创建数据模型 在简单的示例中你可以使用 java.lang 和 java.util 包中的类， 还有用户自定义的 Java Bean 来构建数据对象： 使用 java.lang.String 来构建字符串。 使用 java.lang.Number 来派生数字类型。 使用 java.lang.Boolean 来构建布尔值。 使用 java.util.List 或 Java 数组来构建序列。 使用 java.util.Map 来构建哈希表。 使用自定义的 bean 类来构建哈希表，bean 中的项和 bean 的属性对应。比如， product 的 price 属性 (getProperty())可以通过 product.price 获取。(bean 的 action 也可以通过这种方式拿到； 要了解更多可以参看 这里) 示例： // Create the root hashMap&lt;String, Object&gt; root = new HashMap&lt;&gt;();// Put string ``user'' into the rootroot.put("user", "Big Joe");// Create the hash for ``latestProduct''Map&lt;String, Object&gt; latest = new HashMap&lt;&gt;();// and put it into the rootroot.put("latestProduct", latest);// put ``url'' and ``name'' into latestlatest.put("url", "products/greenmouse.html");latest.put("name", "green mouse"); 2.3. 获取模板 模板代表了 freemarker.template.Template 实例。典型的做法是从 Configuration 实例中获取一个 Template 实例。无论什么时候你需要一个模板实例， 都可以使用它的 getTemplate 方法来获取。在 之前 设置的目录中的 test.ftl 文件中存储 示例模板，那么就可以这样来做： Template temp = cfg.getTemplate("test.ftl"); 当调用这个方法的时候，将会创建一个 test.ftl 的 Template 实例，通过读取 */where/you/store/templates/*test.ftl 文件，之后解析(编译)它。Template 实例以解析后的形式存储模板， 而不是以源文件的文本形式。 Configuration 缓存 Template 实例，当再次获得 test.ftl 的时候，它可能再读取和解析模板文件了， 而只是返回第一次的 Template实例。 2.4. 合并模板和数据模型 我们已经知道，数据模型+模板=输出，我们有了一个数据模型 (root) 和一个模板 (temp)， 为了得到输出就需要合并它们。这是由模板的 process 方法完成的。它用数据模型 root 和 Writer 对象作为参数，然后向 Writer 对象写入产生的内容。 为简单起见，这里我们只做标准的输出： Writer out = new OutputStreamWriter(System.out);temp.process(root, out); 这会向你的终端输出你在模板开发指南部分的 第一个示例 中看到的内容。 Java I/O 相关注意事项：基于 out 对象，必须保证 out.close() 最后被调用。当 out 对象被打开并将模板的输出写入文件时，这是很电影的做法。其它时候， 比如典型的 Web 应用程序，那就 不能 关闭 out 对象。FreeMarker 会在模板执行成功后 (也可以在 Configuration 中禁用) 调用 out.flush()，所以不必为此担心。 请注意，一旦获得了 Template 实例， 就能将它和不同的数据模型进行不限次数 (Template实例是无状态的)的合并。此外， 当 Template实例创建之后 test.ftl 文件才能访问，而不是在调用处理方法时。 2.5. 完整示例 源码 3. 基础 数值 注意观察每个数据模型的例子你也许能发现：被&quot;(root)&quot;所标识的内容就是哈希表类型的值。 当编写如 user 这样的代码时，那就意味着要把&quot;user&quot;变量存储在哈希表的根上。 就像编写 root.user一样，这里但并没有名&quot;root&quot;为的变量， 那么这就起不到任何作用了。 某些人也许会被这种数据模型的例子所困惑，也就是说，根哈希表包含更多的哈希表或序列 (lotteryNumbers and cargo)。其它就没有更特殊的内容了。 哈希表包含其他变量，那些变量包含其它值，这些数值可以是字符串，数字等变量， 当然也可以是哈希表或序列变量。最初我们解释过的，就像字符串和数字， 序列或哈希表也是一种值的表示形式。 类型 Freemark 支持的类型有： 标量： 字符串 数字 布尔值 日期/时间 (日期，时间或日期时间) 容器： 哈希表 序列 集合 子程序： 方法和函数 用户自定义指令 其它/很少使用： 结点 模板 总体结构 模板(FTL 编程)是由如下部分混合而成的： 文本：文本会照着原样来输出。 插值：这部分的输出会被计算的值来替换。插值由 ${ and } 所分隔。 FTL 标签：FTL 标签和 HTML 标签很相似，但是它们却是给 FreeMarker 的指示， 而且不会打印在输出内容中。 注释：注释和 HTML 的注释也很相似，但它们是由 &lt;#-- 和 --&gt;来分隔的。注释会被 FreeMarker 直接忽略， 更不会在输出内容中显示。 ⚠️ 注意： FTL 是区分大小写的。 插值 仅仅可以在 文本 中使用。 FTL 标签 不可以在其他 FTL 标签 和 插值 中使用。 注释 可以放在 FTL 标签 和 插值 中。 指令 使用 FTL 标签来调用 指令。 FTL 标签分为两种： 开始标签： &lt;#*directivename* *parameters*&gt; 结束标签： &lt;/#*directivename*&gt; 除了标签以 # 开头外，其他都和 HTML，XML 的语法很相似。 如果标签没有嵌套内容(在开始标签和结束标签之间的内容)，那么可以只使用开始标签。 例如 &lt;#if *something*&gt;*...*&lt;/#if&gt;， 而 FreeMarker 知道 &lt;#include *something*&gt; 中的 include 指令没有可嵌套的内容。 *parameters* 的格式由 *directivename*来决定。 事实上，指令有两种类型： 预定义指令 和 用户自定义指令。 对于用户自定义的指令使用 @ 来代替 #。 ⚠️ 注意： FreeMarker 仅仅关心 FTL 标签的嵌套而不关心 HTML 标签的嵌套。 它只会把 HTML 看做是文本，不会来解释 HTML。 如果你尝试使用一个不存在的指令(比如，输错了指令的名称)， FreeMarker 就会拒绝执行模板，同时抛出错误信息。 FreeMarker 会忽略 FTL 标签中多余的 空白标记。 表达式 以下为快速浏览清单，如果需要了解更多细节，请参考这里。 直接指定值 字符串： &quot;Foo&quot; 或者 'Foo' 或者 &quot;It's \&quot;quoted\&quot;&quot; 或者 'It\'s &quot;quoted&quot;' 或者 r&quot;C:\raw\string&quot; 数字： 123.45 布尔值： true， false 序列： [&quot;foo&quot;, &quot;bar&quot;, 123.45]； 值域： 0..9, 0..&lt;10 (或 0..!10), 0.. 哈希表： {&quot;name&quot;:&quot;green mouse&quot;, &quot;price&quot;:150} 检索变量 顶层变量： user 从哈希表中检索数据： user.name， user[&quot;name&quot;] 从序列中检索数据： products[5] 特殊变量： .main 字符串操作 插值(或连接)： &quot;Hello ${user}!&quot; (或 &quot;Hello &quot; + user + &quot;!&quot;) 获取一个字符： name[0] 字符串切分： 包含结尾： name[0..4]，不包含结尾： name[0..&lt;5]，基于长度(宽容处理)： name[0..*5]，去除开头：name[5..] 序列操作 连接： users + [&quot;guest&quot;] 序列切分：包含结尾： products[20..29]， 不包含结尾： products[20..&lt;30]，基于长度(宽容处理)：products[20..*10]，去除开头： products[20..] 哈希表操作 连接： passwords + { &quot;joe&quot;: &quot;secret42&quot; } 算术运算： (x * 1.5 + 10) / 2 - y % 100 比较运算： x == y， x != y， x &lt; y， x &gt; y， x &gt;= y， x &lt;= y， x lt y， x lte y， x gt y， x gte y， 等等。。。。。。 逻辑操作： !registered &amp;&amp; (firstVisit || fromEurope) 内建函数： name?upper_case, path?ensure_starts_with('/') 方法调用： repeat(&quot;What&quot;, 3) 处理不存在的值 默认值： name!&quot;unknown&quot; 或者 (user.name)!&quot;unknown&quot; 或者 name! 或者 (user.name)! 检测不存在的值： name?? 或者 (user.name)?? 赋值操作： =, +=, -=, *=, /=, %=, ++, -- 插值 插值的使用格式是： ${*expression*}，这里的 *expression* 可以是所有种类的表达式(比如 ${100 + x})。 插值是用来给 *表达式* 插入具体值然后转换为文本(字符串)。插值仅仅可以在两种位置使用：在 文本 区 (比如 &lt;h1&gt;Hello ${name}!&lt;/h1&gt;) 和 字符串表达式 (比如 &lt;#include &quot;/footer/${company}.html&quot;&gt;)中。 表达式的结果必须是字符串，数字或者日期/时间/日期-时间值， 因为(默认是这样)仅仅这些值可以被插值自动转换为字符串。其它类型的值 (比如布尔值，序列)必须 “手动地” 转换成字符串(后续会有一些建议)， 否则就会发生错误，中止模板执行。 注意：插值 仅仅 在 文本区 (比如 &lt;h1&gt;Hello ${name}!&lt;/h1&gt;) 和 字符串 中起作用。 ✔️ &lt;#include &quot;/footer/${company}.html&quot;&gt; ✔️ &lt;#if big&gt;...&lt;/#if&gt; ❌ &lt;#if ${big}&gt;...&lt;/#if&gt; ❌ &lt;#if &quot;${big}&quot;&gt;...&lt;/#if&gt; 4. 进阶 5. 引用和引申 http://freemarker.foofun.cn/]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>template</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBootTutorialDataElasticsearch]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Fdata%2Fsbe-data-elasticsearch%2F</url>
    <content type="text"><![CDATA[SpringBootTutorialDataElasticsearch 简介 通过 REST 客户端连接 Elasticsearch 通过 Jest 连接 Elasticsearch 通过 Spring Data 访问 Elasticsearch Elasticsearch Repositories 源码 更多内容 简介 Elasticsearch 是一个开源的、分布式的搜索和分析引擎。 通过 REST 客户端连接 Elasticsearch 如果在 classpath 路径下存在 org.elasticsearch.client:elasticsearch-rest-client jar 包，Spring Boot 会自动配置并注册一个 RestClient Bean，它的默认访问路径为：localhost:9200。 你可以使用如下方式进行定制： spring.elasticsearch.rest.uris=http://search.example.com:9200spring.elasticsearch.rest.username=userspring.elasticsearch.rest.password=secret 您还可以注册实现任意数量的 RestClientBuilderCustomizer bean，以进行更高级的定制。要完全控制注册，请定义 RestClient bean。 如果 classpath 路径有 org.elasticsearch.client：elasticsearch-rest-high-level-client jar 包，Spring Boot 将自动配置一个 RestHighLevelClient，它包装任何现有的 RestClient bean，重用其 HTTP 配置。 通过 Jest 连接 Elasticsearch 如果 classpath 上有 Jest，你可以注入一个自动配置的 JestClient，默认情况下是 localhost:9200。您可以进一步调整客户端的配置方式，如以下示例所示： spring.elasticsearch.jest.uris=http://search.example.com:9200spring.elasticsearch.jest.read-timeout=10000spring.elasticsearch.jest.username=userspring.elasticsearch.jest.password=secret 您还可以注册实现任意数量的 HttpClientConfigBuilderCustomizer bean，以进行更高级的定制。以下示例调整为其他 HTTP 设置： static class HttpSettingsCustomizer implements HttpClientConfigBuilderCustomizer &#123; @Override public void customize(HttpClientConfig.Builder builder) &#123; builder.maxTotalConnection(100).defaultMaxTotalConnectionPerRoute(5); &#125;&#125; 要完全控制注册，请定义 JestClient bean。 通过 Spring Data 访问 Elasticsearch 要连接到 Elasticsearch，您必须提供一个或多个集群节点的地址。可以通过将 spring.data.elasticsearch.cluster-nodes 属性设置为以逗号分隔的 host:port 列表来指定地址。使用此配置，可以像任何其他 Spring bean 一样注入 ElasticsearchTemplate 或 TransportClient，如以下示例所示： spring.data.elasticsearch.cluster-nodes=localhost:9300@Componentpublic class MyBean &#123; private final ElasticsearchTemplate template; public MyBean(ElasticsearchTemplate template) &#123; this.template = template; &#125; // ...&#125; 如果你添加了自定义的 ElasticsearchTemplate 或 TransportClient @Bean ，就会替换默认的配置。 Elasticsearch Repositories Spring Data 包含对 Elasticsearch 的 repository 支持。基本原则是根据方法名称自动为您构建查询。 事实上，Spring Data JPA 和 Spring Data Elasticsearch 共享相同的通用基础架构。 源码 完整示例：源码 使用方法： mvn clean packagecd targetjava -jar sbe-data-elasticsearch.jar 更多内容 引申 Spring Boot 教程 参考 Spring Boot 官方文档之 boot-features-elasticsearch Spring Data Elasticsearch Github Spring Data Elasticsearch 官方文档]]></content>
  </entry>
  <entry>
    <title><![CDATA[计算机网络之应用层]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fcommunication%2Fnetwork-application%2F</url>
    <content type="text"><![CDATA[计算机网络之应用层 HTTP DNS FTP DHCP TELNET 电子邮件协议 1. SMTP 2. POP3 3. IMAP 常用端口 Web 页面请求过程 1. DHCP 配置主机信息 2. ARP 解析 MAC 地址 3. DNS 解析域名 4. HTTP 请求页面 HTTP 超文本传输协议（英语：HyperText Transfer Protocol，缩写：HTTP）是一种用于分布式、协作式和超媒体信息系统的应用层协议。HTTP 是万维网的数据通信的基础。 设计 HTTP 最初的目的是为了提供一种发布和接收 HTML 页面的方法。通过 HTTP 或者 HTTPS 协议请求的资源由统一资源标识符（Uniform Resource Identifiers，URI）来标识。 安全套接字层超文本传输协议 HTTPS 为了数据传输的安全，HTTPS 在 HTTP 的基础上加入了 SSL 协议，SSL 依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。 👉 扩展阅读：HTTP DNS 域名服务器 DNS 是一个分布式数据库，提供了主机名和 IP 地址之间相互转换的服务。这里的分布式数据库是指，每个站点只保留它自己的那部分数据。 域名具有层次结构，从上到下依次为：根域名、顶级域名、二级域名。 DNS 可以使用 UDP 或者 TCP 进行传输，使用的端口号都为 53。大多数情况下 DNS 使用 UDP 进行传输，这就要求域名解析器和域名服务器都必须自己处理超时和重传来保证可靠性。在两种情况下会使用 TCP 进行传输： 如果返回的响应超过的 512 字节（UDP 最大只支持 512 字节的数据）。 区域传送（区域传送是主域名服务器向辅助域名服务器传送变化的那部分数据）。 👉 扩展阅读：DNS FTP 文件传送协议 FTP 使用 TCP 进行连接，它需要两个连接来传送一个文件： 控制连接：服务器打开端口号 21 等待客户端的连接，客户端主动建立连接后，使用这个连接将客户端的命令传送给服务器，并传回服务器的应答。 数据连接：用来传送一个文件数据。 根据数据连接是否是服务器端主动建立，FTP 有主动和被动两种模式： 主动模式：服务器端主动建立数据连接，其中服务器端的端口号为 20，客户端的端口号随机，但是必须大于 1024，因为 0~1023 是熟知端口号。 被动模式：客户端主动建立数据连接，其中客户端的端口号由客户端自己指定，服务器端的端口号随机。 主动模式要求客户端开放端口号给服务器端，需要去配置客户端的防火墙。被动模式只需要服务器端开放端口号即可，无需客户端配置防火墙。但是被动模式会导致服务器端的安全性减弱，因为开放了过多的端口号。 FTPS 是一种对常用的文件传输协议（FTP）添加传输层安全（TLS）和安全套接层（SSL）加密协议支持的扩展协议。 DHCP 动态主机配置协议 DHCP (Dynamic Host Configuration Protocol) 提供了即插即用的连网方式，用户不再需要去手动配置 IP 地址等信息。 DHCP 配置的内容不仅是 IP 地址，还包括子网掩码、网关 IP 地址。 DHCP 工作过程如下： 客户端发送 Discover 报文，该报文的目的地址为 255.255.255.255:67，源地址为 0.0.0.0:68，被放入 UDP 中，该报文被广播到同一个子网的所有主机上。如果客户端和 DHCP 服务器不在同一个子网，就需要使用中继代理。 DHCP 服务器收到 Discover 报文之后，发送 Offer 报文给客户端，该报文包含了客户端所需要的信息。因为客户端可能收到多个 DHCP 服务器提供的信息，因此客户端需要进行选择。 如果客户端选择了某个 DHCP 服务器提供的信息，那么就发送 Request 报文给该 DHCP 服务器。 DHCP 服务器发送 Ack 报文，表示客户端此时可以使用提供给它的信息。 TELNET 远程登录协议 TELNET 用于登录到远程主机上，并且远程主机上的输出也会返回。 TELNET 可以适应许多计算机和操作系统的差异，例如不同操作系统系统的换行符定义。 电子邮件协议 一个电子邮件系统由三部分组成：用户代理、邮件服务器以及邮件协议。 邮件协议包含发送协议和读取协议，发送协议常用 SMTP，读取协议常用 POP3 和 IMAP。 1. SMTP SMTP 只能发送 ASCII 码，而互联网邮件扩充 MIME 可以发送二进制文件。MIME 并没有改动或者取代 SMTP，而是增加邮件主体的结构，定义了非 ASCII 码的编码规则。 2. POP3 POP3 的特点是只要用户从服务器上读取了邮件，就把该邮件删除。 3. IMAP IMAP 协议中客户端和服务器上的邮件保持同步，如果不手动删除邮件，那么服务器上的邮件也不会被删除。IMAP 这种做法可以让用户随时随地去访问服务器上的邮件。 常用端口 应用 应用层协议 端口号 传输层协议 备注 域名解析 DNS 53 UDP/TCP 长度超过 512 字节时使用 TCP 动态主机配置协议 DHCP 67/68 UDP 简单网络管理协议 SNMP 161/162 UDP 文件传送协议 FTP 20/21 TCP 控制连接 21，数据连接 20 远程终端协议 TELNET 23 TCP 超文本传送协议 HTTP 80 TCP 简单邮件传送协议 SMTP 25 TCP 邮件读取协议 POP3 110 TCP 网际报文存取协议 IMAP 143 TCP Web 页面请求过程 1. DHCP 配置主机信息 假设主机最开始没有 IP 地址以及其它信息，那么就需要先使用 DHCP 来获取。 主机生成一个 DHCP 请求报文，并将这个报文放入具有目的端口 67 和源端口 68 的 UDP 报文段中。 该报文段则被放入在一个具有广播 IP 目的地址(255.255.255.255) 和源 IP 地址（0.0.0.0）的 IP 数据报中。 该数据报则被放置在 MAC 帧中，该帧具有目的地址 FF:FF:FF:FF:FF:FF，将广播到与交换机连接的所有设备。 连接在交换机的 DHCP 服务器收到广播帧之后，不断地向上分解得到 IP 数据报、UDP 报文段、DHCP 请求报文，之后生成 DHCP ACK 报文，该报文包含以下信息：IP 地址、DNS 服务器的 IP 地址、默认网关路由器的 IP 地址和子网掩码。该报文被放入 UDP 报文段中，UDP 报文段有被放入 IP 数据报中，最后放入 MAC 帧中。 该帧的目的地址是请求主机的 MAC 地址，因为交换机具有自学习能力，之前主机发送了广播帧之后就记录了 MAC 地址到其转发接口的交换表项，因此现在交换机就可以直接知道应该向哪个接口发送该帧。 主机收到该帧后，不断分解得到 DHCP 报文。之后就配置它的 IP 地址、子网掩码和 DNS 服务器的 IP 地址，并在其 IP 转发表中安装默认网关。 2. ARP 解析 MAC 地址 主机通过浏览器生成一个 TCP 套接字，套接字向 HTTP 服务器发送 HTTP 请求。为了生成该套接字，主机需要知道网站的域名对应的 IP 地址。 主机生成一个 DNS 查询报文，该报文具有 53 号端口，因为 DNS 服务器的端口号是 53。 该 DNS 查询报文被放入目的地址为 DNS 服务器 IP 地址的 IP 数据报中。 该 IP 数据报被放入一个以太网帧中，该帧将发送到网关路由器。 DHCP 过程只知道网关路由器的 IP 地址，为了获取网关路由器的 MAC 地址，需要使用 ARP 协议。 主机生成一个包含目的地址为网关路由器 IP 地址的 ARP 查询报文，将该 ARP 查询报文放入一个具有广播目的地址（FF:FF:FF:FF:FF:FF）的以太网帧中，并向交换机发送该以太网帧，交换机将该帧转发给所有的连接设备，包括网关路由器。 网关路由器接收到该帧后，不断向上分解得到 ARP 报文，发现其中的 IP 地址与其接口的 IP 地址匹配，因此就发送一个 ARP 回答报文，包含了它的 MAC 地址，发回给主机。 3. DNS 解析域名 知道了网关路由器的 MAC 地址之后，就可以继续 DNS 的解析过程了。 网关路由器接收到包含 DNS 查询报文的以太网帧后，抽取出 IP 数据报，并根据转发表决定该 IP 数据报应该转发的路由器。 因为路由器具有内部网关协议（RIP、OSPF）和外部网关协议（BGP）这两种路由选择协议，因此路由表中已经配置了网关路由器到达 DNS 服务器的路由表项。 到达 DNS 服务器之后，DNS 服务器抽取出 DNS 查询报文，并在 DNS 数据库中查找待解析的域名。 找到 DNS 记录之后，发送 DNS 回答报文，将该回答报文放入 UDP 报文段中，然后放入 IP 数据报中，通过路由器反向转发回网关路由器，并经过以太网交换机到达主机。 4. HTTP 请求页面 有了 HTTP 服务器的 IP 地址之后，主机就能够生成 TCP 套接字，该套接字将用于向 Web 服务器发送 HTTP GET 报文。 在生成 TCP 套接字之前，必须先与 HTTP 服务器进行三次握手来建立连接。生成一个具有目的端口 80 的 TCP SYN 报文段，并向 HTTP 服务器发送该报文段。 HTTP 服务器收到该报文段之后，生成 TCP SYN ACK 报文段，发回给主机。 连接建立之后，浏览器生成 HTTP GET 报文，并交付给 HTTP 服务器。 HTTP 服务器从 TCP 套接字读取 HTTP GET 报文，生成一个 HTTP 响应报文，将 Web 页面内容放入报文主体中，发回给主机。 浏览器收到 HTTP 响应报文后，抽取出 Web 页面内容，之后进行渲染，显示 Web 页面。]]></content>
      <categories>
        <category>communication</category>
      </categories>
      <tags>
        <tag>communication</tag>
        <tag>network</tag>
        <tag>application</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBootTutorialDataJdbc]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Fdata%2Fsbe-data-jdbc%2F</url>
    <content type="text"><![CDATA[SpringBootTutorialDataJdbc 简介 API execute update query 实战 配置数据源 注入 JdbcTemplate 完整示例 更多内容 简介 Spring Data 包含对 JDBC 的存储库支持，并将自动为 CrudRepository 上的方法生成 SQL。对于更高级的查询，提供了 @Query 注解。 当 classpath 上存在必要的依赖项时，Spring Boot 将自动配置 Spring Data 的 JDBC 存储库。它们可以通过 spring-boot-starter-data-jdbc 的单一依赖项添加到项目中。如有必要，可以通过将 @EnableJdbcRepositories 批注或 JdbcConfiguration 子类添加到应用程序来控制 Spring Data JDBC 的配置。 更多 Spring Data JDBC 细节，可以参考 Spring Data JDBC 官方文档。 API spring-boot-starter-data-jdbc 引入了 spring-jdbc ，其 JDBC 特性就是基于 spring-jdbc。 而 spring-jdbc 最核心的 API 无疑就是 JdbcTemplate，可以说所有的 JDBC 数据访问，几乎都是围绕着这个类去工作的。 Spring 对数据库的操作在 Jdbc 层面做了深层次的封装，利用依赖注入，把数据源配置装配到 JdbcTemplate 中，再由 JdbcTemplate 负责具体的数据访问。 JdbcTemplate 主要提供以下几类方法： execute 方法：可以用于执行任何 SQL 语句，一般用于执行 DDL 语句； update 方法及 batchUpdate 方法：update 方法用于执行新增、修改、删除等语句；batchUpdate 方法用于执行批处理相关语句； query 方法及 queryForXXX 方法：用于执行查询相关语句； call 方法：用于执行存储过程、函数相关语句。 为了方便演示，以下增删改查操作都围绕一个名为 user 的表（该表的主键 id 是自增序列）进行，该表的数据实体如下： public class User &#123; private Integer id; private String name; private Integer age; // 省略 getter/setter&#125; 数据实体只要是一个纯粹的 Java Bean 即可，无需任何注解修饰。 execute 使用 execute 执行 DDL 语句，创建一个名为 test 的数据库，并在此数据库下新建一个名为 user 的表。 public void recreateTable() &#123; jdbcTemplate.execute("DROP DATABASE IF EXISTS test"); jdbcTemplate.execute("CREATE DATABASE test"); jdbcTemplate.execute("USE test"); jdbcTemplate.execute("DROP TABLE if EXISTS user"); jdbcTemplate.execute("DROP TABLE if EXISTS user"); // @formatter:off StringBuilder sb = new StringBuilder(); sb.append("CREATE TABLE user (id int (10) unsigned NOT NULL AUTO_INCREMENT,\n") .append("name varchar (64) NOT NULL DEFAULT '',\n") .append("age tinyint (3) NOT NULL DEFAULT 0,\n") .append("PRIMARY KEY (ID));\n"); // @formatter:on jdbcTemplate.execute(sb.toString());&#125; update 新增数据 public void insert(String name, Integer age) &#123; jdbcTemplate.update("INSERT INTO user(name, age) VALUES(?, ?)", name, age);&#125; 删除数据 public void delete(String name) &#123; jdbcTemplate.update("DELETE FROM user WHERE name = ?", name);&#125; 修改数据 public void update(User user) &#123; jdbcTemplate.update("UPDATE USER SET name=?, age=? WHERE id=?", user.getName(), user.getAge(), user.getId());&#125; 批处理 public void batchInsert(List&lt;User&gt; users) &#123; String sql = "INSERT INTO user(name, age) VALUES(?, ?)"; List&lt;Object[]&gt; params = new ArrayList&lt;&gt;(); users.forEach(item -&gt; &#123; params.add(new Object[] &#123;item.getName(), item.getAge()&#125;); &#125;); jdbcTemplate.batchUpdate(sql, params);&#125; query 查单个对象 public User queryByName(String name) &#123; try &#123; return jdbcTemplate .queryForObject("SELECT * FROM user WHERE name = ?", new BeanPropertyRowMapper&lt;&gt;(User.class), name); &#125; catch (EmptyResultDataAccessException e) &#123; return null; &#125;&#125; 查多个对象 public List&lt;User&gt; list() &#123; return jdbcTemplate.query("select * from USER", new BeanPropertyRowMapper(User.class));&#125; 获取某个记录某列或者 count、avg、sum 等函数返回唯一值 public Integer count() &#123; try &#123; return jdbcTemplate.queryForObject("SELECT COUNT(*) FROM user", Integer.class); &#125; catch (EmptyResultDataAccessException e) &#123; return null; &#125;&#125; 实战 配置数据源 在 src/main/resource 目录下添加 application.properties 配置文件，内容如下： spring.datasource.url = jdbc:mysql://localhost:3306/test?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=falsespring.datasource.username = rootspring.datasource.password = rootspring.datasource.driver-class-name = com.mysql.cj.jdbc.Driver 需要根据实际情况，替换 url、username、password。 注入 JdbcTemplate @Servicepublic class UserDAOImpl implements UserDAO &#123; private JdbcTemplate jdbcTemplate; @Autowired public UserDAOImpl(JdbcTemplate jdbcTemplate) &#123; this.jdbcTemplate = jdbcTemplate; &#125; // ...&#125; 完整示例 请参考：源码 使用方法： 运行应用或单元测试例前，请先执行 codes/data/sbe-data-jdbc/sql/ddl.sql 更多内容 引申 Spring Boot 教程 参考 Spring Boot 官方文档之 boot-features-data-jdbc]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringBootTutorialCoreRetry]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Fcore%2Fsbe-core-retry%2F</url>
    <content type="text"><![CDATA[SpringBootTutorialCoreRetry 源码 更多内容 简介 开启 Retry 使用 @EnableRetry 注解开启 Retry 功能。 import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.retry.annotation.EnableRetry;@EnableRetry@SpringBootApplicationpublic class SbeCoreRetryApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SbeCoreRetryApplication.class, args); &#125;&#125; @Retryable 使用 @Retryable 注解修饰方法 源码 完整示例：源码 更多内容 引申 Spring Boot 教程 参考 Bean Definition Profiles boot-features-profiles]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringBootTutorialCoreProfile]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Fcore%2Fsbe-core-profile%2F</url>
    <content type="text"><![CDATA[SpringBootTutorialCoreProfile 一个应用为了在不同的环境下工作，常常会有不同的配置，代码逻辑处理。Spring Boot 对此提供了简便的支持。 关键词： @Profile、spring.profiles.active 区分环境的配置 properties 配置 yml 配置 区分环境的代码 修饰类 修饰注解 修饰方法 激活 profile 插件激活 profile main 方法激活 profile jar 激活 profile 在 Java 代码中激活 profile 源码 更多内容 区分环境的配置 properties 配置 假设，一个应用的工作环境有：dev、test、prod 那么，我们可以添加 4 个配置文件： applcation.properties - 公共配置 application-dev.properties - 开发环境配置 application-test.properties - 测试环境配置 application-prod.properties - 生产环境配置 在 applcation.properties 文件中可以通过以下配置来激活 profile： spring.profiles.active = test yml 配置 与 properties 文件类似，我们也可以添加 4 个配置文件： applcation.yml - 公共配置 application-dev.yml - 开发环境配置 application-test.yml - 测试环境配置 application-prod.yml - 生产环境配置 在 applcation.yml 文件中可以通过以下配置来激活 profile： spring: profiles: active: prod 此外，yml 文件也可以在一个文件中完成所有 profile 的配置： # 激活 prodspring: profiles: active: prod# 也可以同时激活多个 profile# spring.profiles.active: prod,proddb,prodlog---# dev 配置spring: profiles: dev# 略去配置---spring: profiles: test# 略去配置---spring.profiles: prodspring.profiles.include: - proddb - prodlog---spring: profiles: proddb# 略去配置---spring: profiles: prodlog# 略去配置 注意：不同 profile 之间通过 --- 分割 区分环境的代码 使用 @Profile 注解可以指定类或方法在特定的 Profile 环境生效。 修饰类 @Configuration@Profile("production")public class JndiDataConfig &#123; @Bean(destroyMethod="") public DataSource dataSource() throws Exception &#123; Context ctx = new InitialContext(); return (DataSource) ctx.lookup("java:comp/env/jdbc/datasource"); &#125;&#125; 修饰注解 @Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Profile("production")public @interface Production &#123;&#125; 修饰方法 @Configurationpublic class AppConfig &#123; @Bean("dataSource") @Profile("development") public DataSource standaloneDataSource() &#123; return new EmbeddedDatabaseBuilder() .setType(EmbeddedDatabaseType.HSQL) .addScript("classpath:com/bank/config/sql/schema.sql") .addScript("classpath:com/bank/config/sql/test-data.sql") .build(); &#125; @Bean("dataSource") @Profile("production") public DataSource jndiDataSource() throws Exception &#123; Context ctx = new InitialContext(); return (DataSource) ctx.lookup("java:comp/env/jdbc/datasource"); &#125;&#125; 激活 profile 插件激活 profile spring-boot:run -Drun.profiles=prod main 方法激活 profile --spring.profiles.active=prod jar 激活 profile java -jar -Dspring.profiles.active=prod *.jar 在 Java 代码中激活 profile 直接指定环境变量来激活 profile： System.setProperty("spring.profiles.active", "test"); 在 Spring 容器中激活 profile： AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext();ctx.getEnvironment().setActiveProfiles("development");ctx.register(SomeConfig.class, StandaloneDataConfig.class, JndiDataConfig.class);ctx.refresh(); 源码 完整示例：源码 使用方法： mvn clean packagecd targetjava -jar -Dspring.profiles.active=prod sbe-core-profile.jar 更多内容 引申 Spring Boot 教程 参考 Bean Definition Profiles boot-features-profiles]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringBootTutorialCoreBanner]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Fcore%2Fsbe-core-banner%2F</url>
    <content type="text"><![CDATA[SpringBootTutorialCoreBanner 简介 变量 配置 编程 源码 更多内容 Spring Boot 启动时默认会显示以下 logo： . ____ _ __ _ _ /\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.1.1.RELEASE) 实际上，Spring Boot 支持自定义 logo 的功能。 让我们来看看如何实现的。 简介 只要你在 resources 目录下放置名为 banner.txt、banner.gif 、banner.jpg 或 banner.png 的文件，Spring Boot 会自动加载，将其作为启动时打印的 logo。 对于文本文件，Spring Boot 会将其直接输出。 对于图像文件（ banner.gif 、banner.jpg 或 banner.png ），Spring Boot 会将图像转为 ASCII 字符，然后输出。 变量 banner.txt 文件中还可以使用变量来设置字体、颜色、版本号。 变量 描述 ${application.version} MANIFEST.MF 中定义的版本。如：1.0 ${application.formatted-version} MANIFEST.MF 中定义的版本，并添加一个 v 前缀。如：v1.0 ${spring-boot.version} Spring Boot 版本。如：2.1.1.RELEASE. ${spring-boot.formatted-version} Spring Boot 版本，并添加一个 v 前缀。如：v2.1.1.RELEASE ${Ansi.NAME} (or ${AnsiColor.NAME}, ${AnsiBackground.NAME}, ${AnsiStyle.NAME}) ANSI 颜色、字体。更多细节，参考：AnsiPropertySource。 ${application.title} MANIFEST.MF 中定义的应用名。 示例： 在 Spring Boot 项目中的 resources 目录下添加一个名为 banner.txt 的文件，内容如下： $&#123;AnsiColor.BRIGHT_YELLOW&#125;$&#123;AnsiStyle.BOLD&#125; ________ ___ ___ ________ ___ __ ___ ___|\ ___ \|\ \|\ \|\ ___ \|\ \ |\ \|\ \|\ \\ \ \_|\ \ \ \\\ \ \ \\ \ \ \ \ \ \ \ \ \\\ \ \ \ \ \\ \ \ \\\ \ \ \\ \ \ \ \ __\ \ \ \ \\\ \ \ \ \_\\ \ \ \\\ \ \ \\ \ \ \ \|\__\_\ \ \ \\\ \ \ \_______\ \_______\ \__\\ \__\ \____________\ \_______\ \|_______|\|_______|\|__| \|__|\|____________|\|_______|$&#123;AnsiBackground.WHITE&#125;$&#123;AnsiColor.RED&#125;$&#123;AnsiStyle.UNDERLINE&#125;:: Spring Boot :: (v$&#123;spring-boot.version&#125;):: Spring Boot Tutorial :: (v1.0.0) 注：${} 设置字体颜色的变量之间不能换行或空格分隔，否则会导致除最后一个变量外，都不生效。 启动应用后，控制台将打印如下 logo： 推荐两个生成字符画的网站，可以将生成的字符串放入这个banner.txt 文件： http://www.network-science.de/ascii/ http://patorjk.com/software/taag/ 配置 application.properties 中与 Banner 相关的配置： # banner 模式。有三种模式：console/log/off# console 打印到控制台（通过 System.out）# log - 打印到日志中# off - 关闭打印spring.main.banner-mode = off# banner 文件编码spring.banner.charset = UTF-8# banner 文本文件路径spring.banner.location = classpath:banner.txt# banner 图像文件路径（可以选择 png,jpg,gif 文件）spring.banner.image.location = classpath:banner.gifused).# 图像 banner 的宽度（字符数）spring.banner.image.width = 76# 图像 banner 的高度（字符数）spring.banner.image.height =# 图像 banner 的左边界（字符数）spring.banner.image.margin = 2# 是否将图像转为黑色控制台主题spring.banner.image.invert = false 当然，你也可以在 YAML 文件中配置，例如： spring: main: banner-mode: off 编程 默认，Spring Boot 会注册一个 SpringBootBanner 的单例 Bean，用来负责打印 Banner。 如果想完全个人定制 Banner，可以这么做：先实现 org.springframework.boot.Banner#printBanner 接口来自己定制 Banner。在将这个 Banner 通过 SpringApplication.setBanner(…) 方法注入 Spring Boot。 源码 完整示例：源码 使用方法： mvn clean packagecd targetjava -jar sbe-core-banner.jar 更多内容 引申 Spring Boot 教程 参考 Spring Boot 官方文档之 Customizing the Banner]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 仓库]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fbasics%2Fdocker-repository%2F</url>
    <content type="text"><![CDATA[Docker 仓库 仓库（Repository）是集中存放镜像的地方。 一个容易混淆的概念是注册服务器（Registry）。实际上注册服务器是管理仓库的具体服务器，每个服务器上可以有多个仓库，而每个仓库下面有多个镜像。从这方面来说，仓库可以被认为是一个具体的项目或目录。例如对于仓库地址 dl.dockerpool.com/ubuntu 来说，dl.dockerpool.com 是注册服务器地址，ubuntu 是仓库名。 Docker Hub 目前 Docker 官方维护了一个公共仓库 Docker Hub，其中已经包括了数量超过 15,000 的镜像。大部分需求都可以通过在 Docker Hub 中直接下载镜像来实现。 注册 你可以在 https://cloud.docker.com 免费注册一个 Docker 账号。 登录 可以通过执行 docker login 命令交互式的输入用户名及密码来完成在命令行界面登录 Docker Hub。 你可以通过 docker logout 退出登录。 拉取镜像 你可以通过 docker search 命令来查找官方仓库中的镜像，并利用 docker pull 命令来将它下载到本地。 例如以 centos 为关键词进行搜索： $ docker search centosNAME DESCRIPTION STARS OFFICIAL AUTOMATEDcentos The official build of CentOS. 465 [OK]tianon/centos CentOS 5 and 6, created using rinse instea... 28blalor/centos Bare-bones base CentOS 6.5 image 6 [OK]saltstack/centos-6-minimal 6 [OK]tutum/centos-6.4 DEPRECATED. Use tutum/centos:6.4 instead. ... 5 [OK] 可以看到返回了很多包含关键字的镜像，其中包括镜像名字、描述、收藏数（表示该镜像的受关注程度）、是否官方创建、是否自动创建。 官方的镜像说明是官方项目组创建和维护的，automated 资源允许用户验证镜像的来源和内容。 根据是否是官方提供，可将镜像资源分为两类。 一种是类似 centos 这样的镜像，被称为基础镜像或根镜像。这些基础镜像由 Docker 公司创建、验证、支持、提供。这样的镜像往往使用单个单词作为名字。 还有一种类型，比如 tianon/centos 镜像，它是由 Docker 的用户创建并维护的，往往带有用户名称前缀。可以通过前缀 username/ 来指定使用某个用户提供的镜像，比如 tianon 用户。 另外，在查找的时候通过 --filter=stars=N 参数可以指定仅显示收藏数量为 N 以上的镜像。 下载官方 centos 镜像到本地。 $ docker pull centosPulling repository centos0b443ba03958: Download complete539c0211cd76: Download complete511136ea3c5a: Download complete7064731afe90: Download complete 推送镜像 用户也可以在登录后通过 docker push 命令来将自己的镜像推送到 Docker Hub。 以下命令中的 username 请替换为你的 Docker 账号用户名。 $ docker tag ubuntu:18.04 username/ubuntu:18.04$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu 18.04 275d79972a86 6 days ago 94.6MBusername/ubuntu 18.04 275d79972a86 6 days ago 94.6MB$ docker push username/ubuntu:18.04$ docker search usernameNAME DESCRIPTION STARS OFFICIAL AUTOMATEDusername/ubuntu 自动创建 自动创建（Automated Builds）功能对于需要经常升级镜像内程序来说，十分方便。 有时候，用户创建了镜像，安装了某个软件，如果软件发布新版本则需要手动更新镜像。 而自动创建允许用户通过 Docker Hub 指定跟踪一个目标网站（目前支持 GitHub 或 BitBucket）上的项目，一旦项目发生新的提交或者创建新的标签（tag），Docker Hub 会自动构建镜像并推送到 Docker Hub 中。 要配置自动创建，包括如下的步骤： 创建并登录 Docker Hub，以及目标网站； 在目标网站中连接帐户到 Docker Hub； 在 Docker Hub 中 配置一个自动创建； 选取一个目标网站中的项目（需要含 Dockerfile）和分支； 指定 Dockerfile 的位置，并提交创建。 之后，可以在 Docker Hub 的 自动创建页面 中跟踪每次创建的状态。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 之 Hello World]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fbasics%2Fdocker-helloworld%2F</url>
    <content type="text"><![CDATA[Docker 之 Hello World 前提 确保你的环境上已经成功安装 Docker。 Hello World 实例 使用 docker version 命令确保你的环境已成功安装 Docker。 # docker versionClient: Version: 1.13.1 API version: 1.26 Package version: &lt;unknown&gt; Go version: go1.8.3 Git commit: 774336d/1.13.1 Built: Wed Mar 7 17:06:16 2018 OS/Arch: linux/amd64Server: Version: 1.13.1 API version: 1.26 (minimum version 1.12) Package version: &lt;unknown&gt; Go version: go1.8.3 Git commit: 774336d/1.13.1 Built: Wed Mar 7 17:06:16 2018 OS/Arch: linux/amd64 Experimental: false 使用 docker run 命令运行 Hello World 镜像。 docker run hello-worldUnable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-worldca4f61b1923c: Pull completeDigest: sha256:ca0eeb6fb05351dfc8759c20733c91def84cb8007aa89a5bf606bc8b315b9fc7Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.... 使用 docker image ls命令查看镜像 docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/maven latest 76c9ab5df55b 7 days ago 737 MBdocker.io/python 2.7-slim 5541369755c4 13 days ago 139 MBdocker.io/hello-world latest f2a91732366c 4 months ago 1.85 kBdocker.io/java 8-jre e44d62cf8862 14 months ago 311 MBdocker.io/training/webapp latest 6fae60ef3446 2 years ago 349 MB 使用 docker container ls --all 命令查看容器 如果查看正在运行的容器，不需要添加 --all 参数。 docker container ls --allCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESa661d957c6fa hello-world "/hello" 2 minutes ago Exited (0) 2 minutes ago mystifying_swartz3098f24a1064 docker.io/hello-world "/hello" 6 minutes ago Exited (0) 6 minutes ago sad_yonath4c98c4f18a39 hello-world "/hello" 8 minutes ago Exited (0) 8 minutes ago admiring_banach]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 资源]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fappendix%2Fdocker-resources%2F</url>
    <content type="text"><![CDATA[Docker 资源 Docker 官方资源 Docker 官网：http://www.docker.com Docker Github：https://github.com/moby/moby Docker 官方文档：https://docs.docker.com/ Docker windows 入门：https://docs.docker.com/windows/ Docker Linux 入门：https://docs.docker.com/linux/ Docker mac 入门：https://docs.docker.com/mac/ Docker 用户指引：https://docs.docker.com/engine/userguide/ Docker 官方博客：http://blog.docker.com/ Docker Hub: https://hub.docker.com/ Docker 开源： https://www.docker.com/open-source 资源整理 Awesome Docker： https://github.com/veggiemonk/awesome-docker Docker 中文资源 Docker 中文网站：https://www.docker-cn.com/ Docker 安装手册：https://docs.docker-cn.com/engine/installation/ Docker 国内镜像 网易加速器：http://hub-mirror.c.163.com 官方中国加速器：https://registry.docker-cn.com ustc 的镜像：https://docker.mirrors.ustc.edu.cn daocloud：https://www.daocloud.io/mirror#accelerator-doc（注册后使用）]]></content>
  </entry>
  <entry>
    <title><![CDATA[Html 快速入门]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fbase%2Fhtml%2F</url>
    <content type="text"><![CDATA[Html 快速入门 超文本标记语言（英语：HyperText Markup Language，简称：HTML）是一种用于创建网页的标准标记语言。 关键词： 标签, 元素, 属性 简介 什么是 HTML？ HTML 文档 = 网页 HTML 结构 HTML 标签 HTML 元素 HTML 属性 Quickstart 基础 样式 链接、锚点、图片 列表 表单 表格 区块 框架 Html 基础 标题 段落 链接 图像 水平线 换行 注释 FAQ 中文编码 html 和 htm HTML 忽略空格和换行 更多内容 简介 什么是 HTML？ HTML 是用来描述网页的一种语言。 HTML 指的是超文本标记语言: Hypertext Markup Language HTML 不是一种编程语言，而是一种标记语言 标记语言是一套标记标签 (markup tag) HTML 使用标记标签来描述网页 HTML 文档包含了 HTML标签及文本内容 HTML 文档也叫做 web 页面 HTML 文档 = 网页 HTML 文档描述网页 HTML 文档包含 HTML 标签和纯文本 HTML 文档也被称为网页 Web 浏览器的作用是读取 HTML 文档，并以网页的形式显示出它们。浏览器不会显示 HTML 标签，而是使用标签来解释页面的内容。 HTML 结构 文档类型声明 &lt;!doctype html&gt; 声明为 HTML5 文档。 &lt;!DOCTYPE&gt; 声明有助于浏览器中正确显示网页。 网络上有很多不同的文件，如果能够正确声明 HTML 的版本，浏览器就能正确显示网页内容。 doctype 声明是不区分大小写的，以下方式均可： &lt;!DOCTYPE html&gt;&lt;!DOCTYPE HTML&gt;&lt;!doctype html&gt;&lt;!Doctype Html&gt; 通用声明 HTML5 &lt;!DOCTYPE html&gt; HTML 4.01 &lt;!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd"&gt; XHTML 1.0 &lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt; html 元素 &lt;html&gt; 元素是 HTML 页面的根元素。 head 元素 &lt;head&gt; 元素包含了文档的元（meta）数据。 body 元素 &lt;body&gt; 元素包含了可见的页面内容。 &lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;页面标题&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;这是一个标题&lt;/h1&gt;&lt;p&gt;这是一个段落。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; HTML 标签 HTML 标记标签通常被称为 HTML 标签 (HTML tag)。 HTML 标签是由尖括号包围的关键词，比如 &lt;html&gt; HTML 标签通常是成对出现的，比如 &lt;b&gt; 和 &lt;/b&gt; 标签对中的第一个标签是开始标签，第二个标签是结束标签 开始和结束标签也被称为开放标签和闭合标签 &lt;标签&gt;内容&lt;/标签&gt; HTML 元素 “HTML 标签” 和 “HTML 元素” 通常都是描述同样的意思. 但是严格来讲, 一个 HTML 元素包含了开始标签与结束标签，如下实例: HTML 元素: &lt;p&gt;这是一个段落。&lt;/p&gt; HTML 属性 HTML 元素可以设置属性 属性可以在元素中添加附加信息 属性一般描述于开始标签 属性总是以名称/值对的形式出现，比如：name=“value”。 属性和属性值对大小写不敏感。 不过，万维网联盟在其 HTML 4 推荐标准中推荐小写的属性/属性值。 而新版本的 (X)HTML 要求使用小写属性。 HTML 属性常用引用属性值 属性值应该始终被包括在引号内。 双引号是最常用的，不过使用单引号也没有问题。 提示: 在某些个别的情况下，比如属性值本身就含有双引号，那么您必须使用单引号，例如：name='John &quot;ShotGun&quot; Nelson' 常用属性 下面列出了适用于大多数 HTML 元素的属性： 属性 描述 class 为 html 元素定义一个或多个类名（classname）(类名从样式文件引入) id 定义元素的唯一 id style 规定元素的行内样式（inline style） title 描述了元素的额外信息 (作为工具条使用) Quickstart 基础 &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;这是文章标题&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;!-- 这是注释 --&gt;&lt;h1&gt;这是标题 1&lt;/h1&gt;&lt;h2&gt;这是标题 2&lt;/h2&gt;&lt;h3&gt;这是标题 3&lt;/h3&gt;&lt;h4&gt;这是标题 4&lt;/h4&gt;&lt;h5&gt;这是标题 5&lt;/h5&gt;&lt;h6&gt;这是标题 6&lt;/h6&gt;&lt;p&gt;这是段落&lt;/p&gt;&lt;br&gt; &lt;!-- 这是换行 --&gt;&lt;hr&gt; &lt;!-- 这是分割线 --&gt;&lt;/body&gt;&lt;/html&gt; 样式 &lt;b&gt;粗体文本&lt;/b&gt;&lt;i&gt;斜体文本&lt;/i&gt;&lt;u&gt;下划线文本&lt;/u&gt;&lt;em&gt;定义着重文字&lt;/em&gt;&lt;br /&gt;&lt;strong&gt;定义加重语气&lt;/strong&gt;&lt;br /&gt;&lt;ins&gt;定义插入字&lt;/ins&gt;&lt;br /&gt;&lt;sub&gt;定义删除字&lt;/sub&gt;&lt;br /&gt;&lt;sub&gt;上标&lt;/sub&gt;&lt;sup&gt;下标&lt;/sup&gt;&lt;!-- 计算机样式 --&gt;&lt;pre&gt;预格式文本&lt;/pre&gt;&lt;code&gt;一段电脑代码&lt;/code&gt;&lt;dfn&gt;定义项目&lt;/dfn&gt;&lt;kbd&gt;键盘输入&lt;/kbd&gt;&lt;samp&gt;计算机样本&lt;/samp&gt;&lt;var&gt;变量&lt;/var&gt;&lt;!-- 特殊含义的样式 --&gt;&lt;address&gt; Written by &lt;a href="mailto:webmaster@example.com"&gt;Jon Doe&lt;/a&gt;.&lt;br&gt; Visit us at:&lt;br&gt; Example.com&lt;br&gt; Box 564, Disneyland&lt;br&gt; USA&lt;/address&gt;&lt;!-- 该段落文字从左到右显示 --&gt;&lt;bdo dir="rtl"&gt;该段落文字从右到左显示&lt;/bdo&gt;&lt;!-- 长的引用语 --&gt;&lt;blockquote cite="http://www.worldwildlife.org/who/index.html"&gt; For 50 years, WWF has been protecting the future of nature. The world's leading conservation organization, WWF works in 100 countries and is supported by 1.2 million members in the United States and close to 5 million globally.&lt;/blockquote&gt;&lt;!-- 短的引用语 --&gt;WWF's goal is to: &lt;q&gt;Build a future where people live in harmony with nature.&lt;/q&gt;&lt;!-- 定义引用、引证 --&gt;&lt;p&gt;&lt;cite&gt;The Scream&lt;/cite&gt; by Edward Munch. Painted in 1893.&lt;/p&gt;&lt;dfn&gt;定义项目&lt;/dfn&gt; 链接、锚点、图片 &lt;a href="http://www.example.com/"&gt;This is a Link&lt;/a&gt;&lt;a href="http://www.example.com/"&gt;&lt;img src="URL" alt="Alternate Text"&gt;&lt;/a&gt;&lt;a href="mailto:webmaster@example.com"&gt;Send e-mail&lt;/a&gt;A named anchor:&lt;a name="tips"&gt;Useful Tips Section&lt;/a&gt;&lt;a href="#tips"&gt;Jump to the Useful Tips Section&lt;/a&gt; 列表 &lt;!-- 无序列表 --&gt;&lt;ul&gt;&lt;li&gt;First item&lt;/li&gt;&lt;li&gt;Next item&lt;/li&gt;&lt;/ul&gt;&lt;!-- 有序列表 --&gt;&lt;ol&gt;&lt;li&gt;First item&lt;/li&gt;&lt;li&gt;Next item&lt;/li&gt;&lt;/ol&gt;&lt;!-- 自定义列表 --&gt;&lt;dl&gt;&lt;dt&gt;First term&lt;/dt&gt;&lt;dd&gt;Definition&lt;/dd&gt;&lt;dt&gt;Next term&lt;/dt&gt;&lt;dd&gt;Definition&lt;/dd&gt;&lt;/dl&gt; 表单 &lt;form action="http://www.example.com/test.asp" method="post/get"&gt;&lt;input type="text" name="lastname"value="Nixon" size="30" maxlength="50"&gt;&lt;input type="password"&gt;&lt;input type="checkbox" checked="checked"&gt;&lt;input type="radio" checked="checked"&gt;&lt;input type="submit"&gt;&lt;input type="reset"&gt;&lt;input type="hidden"&gt;&lt;select&gt;&lt;option&gt;Apples&lt;option selected&gt;Bananas&lt;option&gt;Cherries&lt;/select&gt;&lt;textarea name="Comment" rows="60" cols="20"&gt;&lt;/textarea&gt;&lt;/form&gt; 表格 &lt;table border="1"&gt; &lt;caption&gt;Monthly Savings&lt;/caption&gt; &lt;colgroup&gt; &lt;col span="1" style="background-color:#dcdcdc"&gt; &lt;col style="background-color:#00bfff"&gt; &lt;/colgroup&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Month&lt;/th&gt; &lt;th&gt;Savings&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tfoot&gt; &lt;tr&gt; &lt;td&gt;Sum&lt;/td&gt; &lt;td&gt;$180&lt;/td&gt; &lt;/tr&gt; &lt;/tfoot&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;January&lt;/td&gt; &lt;td&gt;$100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;February&lt;/td&gt; &lt;td&gt;$80&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; 区块 &lt;div style="color:#0000FF"&gt; &lt;h3&gt;这是一个在 div 元素中的标题。&lt;/h3&gt; &lt;p&gt;这是一个在 div 元素中的文本。&lt;/p&gt;&lt;/div&gt;&lt;p&gt;&lt;span style="color:red"&gt;some text.&lt;/span&gt;some other text.&lt;/p&gt; 框架 &lt;iframe src="http://www.runoob.com"&gt; &lt;p&gt;您的浏览器不支持 iframe 标签。&lt;/p&gt;&lt;/iframe&gt; Html 基础 标题 标题很重要 请确保标题标签只用于标题。不要仅仅是为了产生粗体或大号的文本而使用标题。 搜索引擎使用标题为您的网页的结构和内容编制索引。 因为用户可以通过标题来快速浏览您的网页，所以用标题来呈现文档结构是很重要的。 HTML 标题（Heading）是通过 &lt;h1&gt; - &lt;h6&gt; 标签来定义的。 &lt;h1&gt; 定义最大的标题。&lt;h6&gt; 定义最小的标题。 默认情况下，HTML 会自动地在块级元素前后添加一个额外的空行，比如段落、标题元素前后。 &lt;h1&gt;这是标题 1&lt;/h1&gt;&lt;h2&gt;这是标题 2&lt;/h2&gt;&lt;h3&gt;这是标题 3&lt;/h3&gt;&lt;h4&gt;这是标题 4&lt;/h4&gt;&lt;h5&gt;这是标题 5&lt;/h5&gt;&lt;h6&gt;这是标题 6&lt;/h6&gt; 段落 可以把 HTML 文档分割为若干段落。 HTML 段落是通过 &lt;p&gt; 来定义的。 浏览器会自动地在段落的前后添加空行。（&lt;p&gt; 是块级元素） &lt;p&gt;这是一个段落。&lt;/p&gt;&lt;p&gt;这是另外一个段落。&lt;/p&gt; 使用空的段落标记 &lt;p&gt;&lt;/p&gt; 去插入一个空行是个坏习惯。用 &lt;br /&gt; 标签代替它！ 链接 HTML 使用超级链接与网络上的另一个文档相连。 几乎可以在所有的网页中找到链接。点击链接可以从一张页面跳转到另一张页面。 HTML 使用标签 &lt;a&gt; 来设置超文本链接。 超链接可以是一个字，一个词，或者一组词，也可以是一幅图像，您可以点击这些内容来跳转到新的文档或者当前文档中的某个部分。 当您把鼠标指针移动到网页中的某个链接上时，箭头会变为一只小手。 我们通过使用 &lt;a&gt; 标签在 HTML 中创建链接。有两种使用 &lt;a&gt; 标签的方式： 通过使用 href 属性 - 创建指向另一个文档的链接 通过使用 name 属性 - 创建文档内的书签 &lt;a href="http://www.baidu.com"&gt;这是一个链接&lt;/a&gt; herf 属性 href 属性规定链接的目标。 链接的 HTML 代码很简单。它类似这样： &lt;a href="url"&gt;Link text&lt;/a&gt; 开始标签和结束标签之间的文字被作为超级链接来显示。 示例 &lt;a href="http://www.example.com/"&gt;Visit&lt;/a&gt; 提示：“链接文本” 不必一定是文本。图片或其他 HTML 元素都可以成为链接。 name 属性 name 属性规定锚（anchor）的名称。 您可以使用 name 属性创建 HTML 页面中的书签。书签不会以任何特殊方式显示，它对读者是不可见的。 当使用命名锚（named anchors）时，我们可以创建直接跳至该命名锚（比如页面中某个小节）的链接，这样使用者就无需不停地滚动页面来寻找他们需要的信息了。 命名锚的语法： &lt;a name="label"&gt;锚（显示在页面上的文本）&lt;/a&gt; 提示：锚的名称可以是任何你喜欢的名字。 提示：您可以使用 id 属性来替代 name 属性，命名锚同样有效。 示例 首先，我们在 HTML 文档中对锚进行命名（创建一个书签）： &lt;a name="tips"&gt;基本的注意事项 - 有用的提示&lt;/a&gt; 然后，我们在同一个文档中创建指向该锚的链接： &lt;a href="#tips"&gt;有用的提示&lt;/a&gt; 您也可以在其他页面中创建指向该锚的链接： &lt;a href="http://www.example.com/html/html-links.html#tips"&gt;有用的提示&lt;/a&gt; 在上面的代码中，我们将 # 符号和锚名称添加到 URL 的末端，就可以直接链接到 tips 这个命名锚了。 target 属性 使用 target 属性，你可以定义被链接的文档在何处显示。 下面的这行会在新窗口打开文档： &lt;a href="http://www.examplel.com/" target="_blank"&gt;Visit&lt;/a&gt; 图像 HTML 图像是通过标签 &lt;img&gt; 来定义的。 &lt;img&gt; 是空标签，意思是说，它只包含属性，并且没有闭合标签。 &lt;img src="/assets/images/html.png" width="600" height="800" /&gt; src 属性 要在页面上显示图像，你需要使用 src 属性。src 指 “source”。源属性的值是图像的 URL 地址。 定义图像的语法是： &lt;img src="url" /&gt; URL 指存储图像的位置。如果名为 “boat.gif” 的图像位于 www.example.com 的 images 目录中，那么其 URL 为 http://www.example.com/images/boat.gif。 浏览器将图像显示在文档中图像标签出现的地方。如果你将图像标签置于两个段落之间，那么浏览器会首先显示第一个段落，然后显示图片，最后显示第二段。 alt 属性 alt 属性用来为图像定义一串预备的可替换的文本。替换文本属性的值是用户定义的。 &lt;img src="boat.gif" alt="Big Boat"&gt; 在浏览器无法载入图像时，替换文本属性告诉读者她们失去的信息。此时，浏览器将显示这个替代性的文本而不是图像。为页面上的图像都加上替换文本属性是个好习惯，这样有助于更好的显示信息，并且对于那些使用纯文本浏览器的人来说是非常有用的。 水平线 &lt;hr /&gt; 标签在 HTML 页面中创建水平线。 水平线可用于分隔内容。 &lt;p&gt;这是一个段落。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;这是一个段落。&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;这是一个段落。&lt;/p&gt; 换行 如果您希望在不产生一个新段落的情况下进行换行（新行），请使用 &lt;br /&gt; 标签： &lt;p&gt;这个&lt;br /&gt;段落&lt;br /&gt;演示了分行的效果&lt;/p&gt; &lt;br /&gt; 元素是一个空的 HTML 元素。由于关闭标签没有任何意义，因此它没有结束标签。 选择 &lt;br&gt; 还是 &lt;br /&gt; ? 您也许发现 &lt;br&gt; 与 &lt;br /&gt; 很相似。 在 XHTML、XML 以及未来的 HTML 版本中，不允许使用没有结束标签（闭合标签）的 HTML 元素。 即使 &lt;br&gt; 在所有浏览器中的显示都没有问题，使用 &lt;br /&gt; 也是更长远的保障。 HTML 忽略空格和换行 对于 HTML，您无法通过在 HTML 代码中添加额外的空格或换行来改变输出的效果。 当显示页面时，浏览器会移除源代码中多余的空格和空行。所有连续的空格或空行都会被算作一个空格。需要注意的是，HTML 代码中的所有连续的空行（换行）也被显示为一个空格。 &lt;html&gt;&lt;body&gt;&lt;h1&gt;春晓&lt;/h1&gt;&lt;p&gt; 春眠不觉晓， 处处闻啼鸟。 夜来风雨声， 花落知多少。&lt;/p&gt;&lt;p&gt;注意，浏览器忽略了源代码中的排版（省略了多余的空格和换行）。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 说明：HTML 的输出结果并不会按照源代码中那样去排版内容。 注释 可以将注释插入 HTML 代码中，这样可以提高其可读性，使代码更易被人理解。浏览器会忽略注释，也不会显示它们。 &lt;!-- 这是一个注释 --&gt; 条件注释 您也许会在 HTML 中偶尔发现条件注释： &lt;!--[if IE 8]&gt; .... some HTML here ....&lt;![endif]--&gt; 条件注释定义只有 Internet Explorer 执行的 HTML 标签。 FAQ 中文编码 目前在大部分浏览器中，直接输出中文会出现中文乱码的情况，这时候我们就需要在头部将字符声明为 UTF-8。 添加 &lt;meta charset=&quot;UTF-8&quot;&gt; 元素。 &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;title&gt;页面标题&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;我的第一个标题&lt;/h1&gt;&lt;p&gt;我的第一个段落。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; html 和 htm .html 和 .htm 都是 html 文档的后缀名，二者没有区别。 HTML 忽略空格和换行 对于 HTML，您无法通过在 HTML 代码中添加额外的空格或换行来改变输出的效果。 当显示页面时，浏览器会移除源代码中多余的空格和空行。所有连续的空格或空行都会被算作一个空格。需要注意的是，HTML 代码中的所有连续的空行（换行）也被显示为一个空格。 &lt;html&gt;&lt;body&gt;&lt;h1&gt;春晓&lt;/h1&gt;&lt;p&gt; 春眠不觉晓， 处处闻啼鸟。 夜来风雨声， 花落知多少。&lt;/p&gt;&lt;p&gt;注意，浏览器忽略了源代码中的排版（省略了多余的空格和换行）。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 说明：HTML 的输出结果并不会按照源代码中那样去排版内容。 更多内容 📚 拓展阅读 Html Css Javascript 📦 本文归档在 我的前端技术教程系列：frontend-tutorial mozilla html 教程 W3school html 教程]]></content>
  </entry>
  <entry>
    <title><![CDATA[Babel 教程]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fes6%2Fbabel%2F</url>
    <content type="text"><![CDATA[Babel 教程 Babel 是一个通用的多用途 JavaScript 编译器。 ⚠ 注意： Babel 可以与很多构建工具（如 Browserify、Grunt、Gulp 等）进行集成。由于本教程选择 Webpack ，所以只讲解与 Webpack 的集成。想了解如何与其它工具集成，请参考：官方文档 - installation 关键词： babel-cli, .babelrc, preset, polyfill 1. 简介 1.1. Babel 能做什么？ 1.2. Babel 不能做什么？ 2. 安装 Babel 2.1. babel-cli 2.2. babel-node 2.3. babel-register 2.4. babel-core 2.5. 与 webpack 集成 3. 配置 Babel 3.1. .babelrc 3.2. 在其它工具中配置 4. 执行 Babel 生成的代码 4.1. babel-polyfill 4.2. babel-runtime 5. 更多内容 1. 简介 1.1. Babel 能做什么？ Babel 通过语法转换来支持最新版本的 JavaScript （ES6），而不用等待浏览器的支持。 Babel 可以转换 React 的 JSX 语法和删除类型注释。 Babel 是由插件构建的。因此，你可以根据自己的需要订制。 支持 source map ，所以您可以轻松调试您编译的代码。 1.2. Babel 不能做什么？ Babel 只转换语法（如箭头函数），不支持新的全局变量。但是，您可以使用 babel-polyfill 来辅助支持。 2. 安装 Babel 2.1. babel-cli babel-cli 是 Babel 的命令行工具。 安装 # 本地安装$ npm install --save-dev babel-cli# 全局安装$ npm install --global babel-cli 用法 # 将编译后的结果直接输出至终端$ babel example.js# 将结果写入到指定的文件$ babel example.js --out-file compiled.js$ babel example.js -o compiled.js# 将一个目录整个编译成一个新的目录$ babel src --out-dir lib$ babel src -d lib 与 package.json 集成 ​📌 提示： 建议使用本地安装方式安装 babel-cli 。 原因在与： 在同一台机器上的不同项目或许会依赖不同版本的 Babel 并允许你有选择的更新。 这意味着你对工作环境没有隐式依赖，这让你的项目有很好的可移植性并且易于安装。 本地安装 babel-cli ，直接使用 babel 命令将无法识别。你可以选在在 package.json 文件的 scripts 属性中定义命令。npm 会自动找到本地安装的库。 &#123; "scripts": &#123; "build": "babel src -d lib" &#125;, "devDependencies": &#123; "babel-cli": "^6.0.0" &#125;&#125; 现在可以在终端里执行命令： $ npm run build ​🔦 示例 DEMO01： (SOURCE) 说明： 示例的上一级目录 codes/chapter04/babel 已经配好了配置文件。 在 codes/chapter04/babel 路径下执行命令： &gt; $ npm install&gt; $ npm run demo01&gt; 会生成一个 dist/demo01 目录，其中就是被转码后的文件。 2.2. babel-node babel-cli 工具自带一个 babel-node 命令，提供一个支持 ES6 的 REPL 环境。它支持 Node 的 REPL 环境的所有功能，而且可以直接运行 ES6 代码。 它不用单独安装，而是随 babel-cli 一起安装。然后，执行 babel-node 就进入 PEPL 环境。 然后用 babel-node 来替代 node 运行所有的代码： $ babel-node&gt; (x =&gt; x * 2)(1)2 babel-node 命令可以直接运行 ES6 脚本: $ babel-node example.js 如果用 npm 的话只需要这样做： &#123; "scripts": &#123; "script-name": "babel-node script.js" &#125;, "devDependencies": &#123; "babel-cli": "^6.0.0" &#125;&#125; 然后，执行命令： $ npm run babel-node 2.3. babel-register 下一个常用的运行 Babel 的方法是通过 babel-register。这种方法只需要引入文件就可以运行 Babel，或许能更好地融入你的项目设置。 ​⚠️ 注意： 这种方法并不适合正式产品环境使用。 直接部署用此方式编译的代码不是好的做法。 在部署之前预先编译会更好。 不过用在构建脚本或是其他本地运行的脚本中是非常合适的。 安装 $ npm install --save-dev babel-register 使用 创建 index.js 文件： console.log('Hello world!'); 这是，使用 node index.js 来运行它是不会使用 Babel 来编译的。所以我们需要设置 babel-register。. 创建 register.js 文件： require('babel-register');require('./index.js'); 这样做可以把 Babel 注册到 Node 的模块系统中并开始编译其中 require 的所有文件。 执行命令 现在我们可以使用 register.js 来代替 node index.js 来运行了。 $ node register.js 需要注意的是：你不能在你要编译的文件内同时注册 Babel，因为 node 会在 Babel 编译它之前就将它执行了。 ​🔦 示例 DEMO02： (SOURCE) 说明： 示例的上一级目录 codes/chapter04/babel 已经配好了配置文件。 在 codes/chapter04/babel 路径下执行命令： &gt; $ npm install&gt; $ npm run demo02&gt; 控制台会打印如下内容： &gt; &gt; node demo02/register.js&gt;&gt; Hello world!&gt; 2.4. babel-core 如果你需要以编程的方式来使用 Babel，可以使用 babel-core 这个包。 安装 $ npm install babel-core 使用 在代码中引入 babel-core var babel = require('babel-core'); 编译 API # 如果是字符串形式的 JavaScript 代码，可以使用 transform 编译babel.transform("code();", options);// =&gt; &#123; code, map, ast &#125;# 如果是文件的话，异步编译使用 transformFilebabel.transformFile("filename.js", options, function(err, result) &#123; result; // =&gt; &#123; code, map, ast &#125;&#125;);# 如果是文件的话，同步编译使用 transformFileSyncbabel.transformFileSync("filename.js", options);// =&gt; &#123; code, map, ast &#125;# 要是已经有一个 Babel AST（抽象语法树）了就可以直接从 AST 进行转换babel.transformFromAst(ast, code, options);// =&gt; &#123; code, map, ast &#125; 2.5. 与 webpack 集成 ​📌 提示： 本教程由于选择的编译工具为 webpack ，所以这里只介绍与 webpack 的集成。 实际上，Babel 还可以与其它许多工具集成，更多内容参考：官方文档 - setup 安装 $ npm install --save-dev babel-loader babel-core **配置 ** 在 Chapter03 的 Webpack 资源管理 一文中，介绍过使用 babel-loader 来处理 React 语法。 在 webpack.config.js 配置如下： // 关于模块配置module: &#123; // 模块规则（配置 loader、解析器等选项） rules: [ &#123; // 语义解释器，将 js/jsx 文件中的 es2015/react 语法自动转为浏览器可识别的 Javascript 语法 test: /\.jsx?$/, include: path.resolve(__dirname, "app"), loader: "babel-loader", &#125;, ]&#125;, ​🔦 示例 chapter03-jigsaw： (SOURCE) chapter04-jigsaw： (SOURCE) 说明： chapter04-jigsaw 和 chapter03-jigsaw 相比，多了一个 .babelrc 文件。它其实是将 chapter03-jigsaw 中的 webpack.common.js 文件里的 babel-loader 的配置移入了 .babelrc 文件。 这两个代码目录的执行结果完全相同。 执行方法： &gt; $ npm install&gt; # 开发环境 - 本地启动一个访问地址为 localhost:9000 的 web app&gt; $ npm run dev&gt; # 开发环境 - 生成一个 dist 目录，其中打包了所有资源文件，在浏览器打开 index.html，可以看到和开发环境差不多的展示。&gt; $ npm run prod&gt; # 清除输出目录 dist&gt; $ npm run clean&gt; 3. 配置 Babel ​📌 提示： 由于 Babel 是一个非常灵活的通用编译器，因此默认情况下它反而什么都不做。 你需要通过配置文件，明确地告诉 Babel 应该要做什么。 3.1. .babelrc .babelrc 文件是 Babel 的默认配置文件。 .babelrc 文件的内容形式就是序列化的 JSON。 该文件用来设置转码规则（presets）和插件（plugins），基本格式如下： &#123; "presets": [], "plugins": []&#125; 转码规则(preset) 转码规则可以告诉 Babel 去处理什么语法。 常见的转码规则有： babel-preset-es2015 这是 ES2015（最新版本的 JavaScript 标准，也叫做 ES6）的转码规则。使用它后，Babel 可以将 ES6 语法转码为普通 JavaScript（即 ES5） 语法。 babel-preset-react 这是 React 的转码规则。使用它后，Babel 可以将 React 语法转码为普通 JavaScript 语法。 babel-preset-stage-x 这是 ES7 不同阶段语法提案的转码规则。使用它后，Babel 可以将 ES7 不同阶段语法转码为普通 JavaScript 语法。 ​📌 提示： JavaScript 还有一些提案，正在积极通过 TC39（ECMAScript 标准背后的技术委员会）的流程成为标准的一部分。 这个流程分为 5（0－4）个阶段。 随着提案得到越多的关注就越有可能被标准采纳，于是他们就继续通过各个阶段，最终在阶段 4 被标准正式采纳。以下是 4 个不同阶段的（打包的）预设： babel-preset-stage-0 babel-preset-stage-1 babel-preset-stage-2 babel-preset-stage-3 stage-4 预设是不存在的因为它就是上面的 es2015 预设。 以上每种预设都依赖于紧随的后期阶段预设。例如，babel-preset-stage-1 依赖 babel-preset-stage-2，后者又依赖babel-preset-stage-3。 安装 # es2015（即ES6）语法转码规则$ npm install --save-dev babel-preset-es2015# react 语法转码规则$ npm install --save-dev babel-preset-react# stage 是指 ES7 不同阶段的语法转码规则，选装一个即可$ npm install --save-dev babel-preset-stage-0$ npm install --save-dev babel-preset-stage-1$ npm install --save-dev babel-preset-stage-2$ npm install --save-dev babel-preset-stage-3 使用 安装完后，需要在配置文件 .babelrc 中引入项目中实际需要的预设转码规则，让 Babel 得以知道规则。 形式如下： &#123; "presets": ["es2015", "react", "stage-0"], "plugins": []&#125; 插件(plugins) 插件是 Babel 的核心。 Babel 插件大致分为三类： 转码插件 有很多种插件：将 ES6 / ES2015 转换为 ES5，转换为 ES3，minification，JSX，flow，实验功能等等。 语法插件 这些只是使转码插件能够解析某些功能（转码插件已经包含语法插件，因此这两个功能你都不需要）：babel-plugin-syntax-x 助手 这些主要用于各种插件内部使用：babel-helper-x。 更多插件请在 npm 搜索（真的好多！） 更详细介绍请参考：官方文档 - 插件 3.2. 在其它工具中配置 ​📌​ 提示： 除了在 .babelrc 文件中定义 Babel 配置。实际上，还可以在其他工具中对其进行配置。 在 package.json 中配置 可以在 package.json 文件的 babel 属性中配置 Babel 规则。 配置方法与 .babelrc 文件完全相同。 形式如下： "babel": &#123; "presets": [ "es2015" ] "plugins": []&#125;, 在 webpack.config.js 中配置 可以在 webpack.config.js 文件配置 babel-loader 时，直接在 options 属性中配置 Babel 规则。 形式如下： &#123; test: /\.jsx?$/, include: path.resolve(__dirname, "app"), exclude: /node_modules/, loader: "babel-loader", options: &#123; presets: [ [ "es2015", &#123; "modules": false &#125; ], "react" ], plugins: [ "syntax-dynamic-import", // 动态导入插件 "react-hot-loader/babel" // 开启 React 代码的模块热替换(HMR) ] &#125;,&#125; 4. 执行 Babel 生成的代码 即便你已经用 Babel 编译了你的代码，但这还不算完。 4.1. babel-polyfill Babel 几乎可以编译所有新潮的 JavaScript 语法，但对于 APIs 来说却并非如此。 比方说，下列含有箭头函数的需要编译的代码： function addAll() &#123; return Array.from(arguments).reduce((a, b) =&gt; a + b);&#125; 最终会变成这样： function addAll() &#123; return Array.from(arguments).reduce(function(a, b) &#123; return a + b; &#125;);&#125; 然而，它依然无法随处可用。因为并非所有的 JavaScript 环境都支持 Array.from。 为了解决这个问题，我们使用一种叫做 Polyfill（代码填充，也可译作兼容性补丁） 的技术。 简单地说，polyfill 即是在当前运行环境中用来复制（意指模拟性的复制，而不是拷贝）尚不存在的原生 api 的代码。 能让你提前使用还不可用的 APIs，Array.from 就是一个例子。 Babel 提供了 babel-polyfill 来支持 polyfill 。 安装 $ npm install --save babel-polyfill 使用 然后，只需要在文件顶部导入 babel-polyfill 就可以了： import 'babel-polyfill'; 4.2. babel-runtime babel-runtime 与 polyfill 类似，不同之处在于它不修改全局范围，并且与 babel-plugin-transform-runtime（通常在库/插件代码中）一起使用。 为了实现 ECMAScript 规范的细节，Babel 会使用“助手”方法来保持生成代码的整洁。 由于这些助手方法可能会特别长并且会被添加到每一个文件的顶部，因此你可以把它们统一移动到一个单一的“运行时（runtime）”中去。 通过安装 babel-plugin-transform-runtime 和 babel-runtime 来开始。 $ npm install --save-dev babel-plugin-transform-runtime$ npm install --save babel-runtime 然后更新 .babelrc： &#123; "presets": ["es2015", "react", "stage-0"], "plugins": ["transform-runtime", "transform-es2015-classes"]&#125; 现在，Babel 会把这样的代码： class Foo &#123; method() &#123;&#125;&#125; 编译成： import _classCallCheck from 'babel-runtime/helpers/classCallCheck';import _createClass from 'babel-runtime/helpers/createClass';let Foo = (function() &#123; function Foo() &#123; _classCallCheck(this, Foo); &#125; _createClass(Foo, [ &#123; key: 'method', value: function method() &#123;&#125; &#125; ]); return Foo;&#125;)(); 这样就不需要把 _classCallCheck 和 _createClass 这两个助手方法放进每一个需要的文件里去了。 5. 更多内容 📦 本文归档在 我的前端技术教程系列：frontend-tutorial 官方 Babel 官网 Babel 中文网 Babel Github]]></content>
  </entry>
  <entry>
    <title><![CDATA[Npm 入门]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fnodejs%2Fnpm%2F</url>
    <content type="text"><![CDATA[Npm 入门 Npm 是随同 Nodejs 一起安装的 js 包管理工具。 关键词： nodejs, 包管理, npm, cnpm, package.json, node_modules 简介 安装 Npm 工作流 Npm 常用命令 初始化新项目 安装模块 卸载模块 更新模块 管理配置文件 发布包 执行脚本 查看安装信息 配置文件 package 版本 版本号 npm 扩展 cnpm nrm 更多内容 简介 Npm 是随同 Nodejs 一起安装的 js 包管理工具。 常见的使用场景有以下几种： 允许用户从 Npm 服务器下载别人编写的第三方包到本地使用。 允许用户从 Npm 服务器下载并安装别人编写的命令行程序到本地使用。 允许用户将自己编写的包或命令行程序上传到 Npm 服务器供别人使用。 如果一个项目中存在 package.json 文件，那么用户可以直接使用 npm install 命令自动安装和维护当前项目所需的所有模块。在 package.json 文件中，开发者可以指定每个依赖项的版本范围，这样既可以保证模块自动更新，又不会因为所需模块功能大幅变化导致项目出现问题。开发者也可以选择将模块固定在某个版本之上。 安装 由于新版的 nodejs 已经集成了 npm，所以之前 npm 也一并安装好了。同样可以通过输入 npm -v 来测试是否成功安装。命令如下，出现版本提示表示安装成功: $ npm -v2.3.0 可以通过命令方式升级 npm Linux - sudo npm install npm -g Window - npm install npm -g Npm 工作流 Npm 工作流： 创建一个新项目 增加／更新／删除依赖 安装／重装你的依赖 引入版本控制系统（例如 git） 持续集成 Npm 常用命令 每个命令都会更新 package.json 文件。 Npm 提供了很多命令，例如install和publish，使用npm help可查看所有命令。 使用npm help可查看某条命令的详细帮助，例如npm help install。 在package.json所在目录下使用npm install . -g可先在本地安装当前命令行程序，可用于发布前的本地测试。 使用npm update可以把当前目录下node_modules子目录里边的对应模块更新至最新版本。 使用npm update -g可以把全局安装的对应命令行程序更新至最新版。 使用npm cache clear可以清空 Npm 本地缓存，用于对付使用相同版本号发布新版本代码的人。 使用npm unpublish @可以撤销发布自己发布过的某个版本代码。 Npm 提供了很多命令，例如 install 和 publish，使用 npm help 可查看所有命令。 初始化新项目 npm init 用于初始化项目，它会创建一个名为 package.json 的配置文件。 命令格式 npm init [-f|--force|-y|--yes] 说明 执行命令后，npm 会问你一系列问题，然后在执行命令的目录下创建一个package.json文件。 如果使用 -f / --force 或 -y / --yes ，npm 会使用默认值为你创建 package.json 文件，不再询问任何问题。 创建模块，package.json 文件是必不可少的。我们可以使用 Npm 生成 package.json 文件，生成的文件包含了基本的结果。 $ npm initThis utility will walk you through creating a `package.json` file.It only covers the most common items, and tries to guess sensible defaults.See `npm help json` for definitive documentation on these fieldsand exactly what they do.Use `npm install &lt;pkg&gt; --save` afterwards to install a package andsave it as a dependency in the `package.json` file.Press ^C at any time to quit.name: (node_modules) runoob # 模块名version: (1.0.0)description: Node.js 测试模块(www.runoob.com) # 描述entry point: (index.js)test command: make testgit repository: https://github.com/runoob/runoob.git # Github 地址keywords:author:license: (ISC)About to write to ……/node_modules/package.json: # 生成地址&#123; "name": "runoob", "version": "1.0.0", "description": "Node.js 测试模块(www.runoob.com)", ……&#125;Is this ok? (yes) yes 以上的信息，你需要根据你自己的情况输入。在最后输入 “yes” 后会生成 package.json 文件。 接下来我们可以使用以下命令在 npm 资源库中注册用户（使用邮箱注册）： $ npm adduserUsername: mcmohdPassword:Email: (this IS public) mcmohd@gmail.com 安装模块 npm install 用于安装模块。 命令格式 npm install (with no args, in package dir)npm install &lt;tarball file&gt; # 安装位于文件系统上的包。npm install &lt;tarball url&gt; # 获取 url，然后安装它。为了区分此选项和其他选项，参数必须以“http://”或“https://”开头。npm install &lt;folder&gt; # 安装位于文件系统上某文件夹中的包npm install [&lt;@scope&gt;/]&lt;name&gt; # 安装指定的包的最新版本。npm install [&lt;@scope&gt;/]&lt;name&gt;@&lt;tag&gt; # 安装被 tag 引用的包的版本。如果 tag 不存在于该包的注册表数据中，则失败。npm install [&lt;@scope&gt;/]&lt;name&gt;@&lt;version&gt; # 安装指定的包的版本。如果版本尚未发布到注册表，则失败。npm install [&lt;@scope&gt;/]&lt;name&gt;@&lt;version range&gt; # 安装与指定版本范围相匹配的包版本。 npm install [&lt;@scope&gt;/] [-S|--save|-D|--save-dev|-O|--save-optional] 参数说明： npm install 有 3 个可选参数，用于保存或更新主 package.json 中的包版本： -S, --save - 包将被添加到 dependencies。 -D, --save-dev - 包将被添加到 devDependencies。 -O, --save-optional - 包将被添加到 optionalDependencies。 当使用上述任何选项将依赖保存到 package.json 时，有两个额外的可选标志 - -E, --save-exact - 会在 package.json 文件指定安装模块的确切版本。 -B, --save-bundle - 包也将被添加到bundleDependencies。 全局安装与本地安装 npm 的包安装分为本地安装（local）、全局安装（global）两种，从敲的命令行来看，差别只是有没有-g 而已，比如 npm install express # 本地安装npm install express -g # 全局安装 本地安装 将安装包放在 node_modules 下（运行 npm 命令时所在的目录），如果没有 node_modules 目录，会在当前执行 npm 命令的目录下新建 node_modules 目录。 可以通过 require() 来引入本地安装的包。 示例：我们使用 npm 命令安装常用的 Node.js 的 web 框架模块 express: $ npm install express 安装好之后，express 包就放在了工程目录下的 node_modules 目录中，因此在代码中只需要通过 require('express') 的方式就好，无需指定第三方包路径。 var express = require('express'); 全局安装 将安装包放在 /usr/local 下或者你 node 的安装目录。 可以直接在命令行里使用。 如果你希望具备两者功能，则需要在两个地方安装它或使用 npm link。 接下来我们使用全局方式安装 express $ npm install express -g 安装过程输出如下内容，第一行输出了模块的版本号及安装位置。 express@4.13.3 node_modules/express├── escape-html@1.0.2├── range-parser@1.0.2├── merge-descriptors@1.0.0├── array-flatten@1.1.1├── cookie@0.1.3├── utils-merge@1.0.0├── parseurl@1.3.0├── cookie-signature@1.0.6├── methods@1.1.1├── fresh@0.3.0├── vary@1.0.1├── path-to-regexp@0.1.7├── content-type@1.0.1├── etag@1.7.0├── serve-static@1.10.0├── content-disposition@0.5.0├── depd@1.0.1├── qs@4.0.0├── finalhandler@0.4.0 (unpipe@1.0.0)├── on-finished@2.3.0 (ee-first@1.1.1)├── proxy-addr@1.0.8 (forwarded@0.1.0, ipaddr.js@1.0.1)├── debug@2.2.0 (ms@0.7.1)├── type-is@1.6.8 (media-typer@0.3.0, mime-types@2.1.6)├── accepts@1.2.12 (negotiator@0.5.3, mime-types@2.1.6)└── send@0.13.0 (destroy@1.0.3, statuses@1.2.1, ms@0.7.1, mime@1.3.4, http-errors@1.3.1) 卸载模块 npm uninstall 用于卸载包。 命令格式 npm uninstall [&lt;@scope&gt;/]&lt;pkg&gt;[@&lt;version&gt;]... [-S|--save|-D|--save-dev|-O|--save-optional]aliases: remove, rm, r, un, unlink 说明 在全局模式下（即，在命令中附加-g或--global），它将当前包上下文作为全局包卸载。 npm uninstall 有 3 个可选参数，用于保存或更新主 package.json 中的包版本： -S, --save - 包将被添加到 dependencies。 -D, --save-dev - 包将被添加到 devDependencies。 -O, --save-optional - 包将被添加到 optionalDependencies。 例： npm uninstall saxnpm uninstall sax --savenpm uninstall @myorg/privatepackage --savenpm uninstall node-tap --save-devnpm uninstall dtrace-provider --save-optional 更新模块 npm update 用于更新本地安装的模块。 命令格式 npm update [-g] [&lt;pkg&gt;...]aliases: up, upgrade 注：从 npm@2.6.1 开始，npm update 仅更新顶级包。旧版本的 npm 会递归检查所有的依赖。如果要达到旧版本的行为，请使用npm --depth 9999 update。 管理配置文件 npm config 命令用于管理配置文件。 命令格式 npm config set &lt;key&gt; &lt;value&gt; [-g|--global] # 设置一个配置参数npm config get &lt;key&gt; # 获取一个配置参数npm config delete &lt;key&gt; # 删除一个配置参数npm config list # 打印配置参数列表npm config edit # 直接编辑配置文件npm get &lt;key&gt; # npm config get &lt;key&gt; 的简写。npm set &lt;key&gt; &lt;value&gt; [-g|--global] # npm config set &lt;key&gt; &lt;value&gt; [-g|--global] 的简写 发布包 npm publish 用于发布一个包。 命令格式 npm publish [&lt;tarball&gt;|&lt;folder&gt;] [--tag &lt;tag&gt;] [--access &lt;public|restricted&gt;]Publishes '.' if no argument suppliedSets tag 'latest' if no --tag specified 说明：将包发布到注册表，以便可以按名称安装。如果没有本地 .gitignore 或 .npmignore 文件，则包括软件包目录中的所有文件。如果这两个过滤文件都存在时，某个文件被 .gitignore 忽略，而不被 .npmignore 忽略，则它将被包括。 执行脚本 npm run 用于执行脚本。 如果在 package.json 文件中的 scripts 字段定义了命令，就可以使用 npm run 来执行脚本命令。 示例： 假设 package.json 文件中的 scripts 字段如下定义： "scripts": &#123; "test": "mocha", "lint": "eslint lib bin hot scripts", "prepublish": "npm run test &amp;&amp; npm run lint", "start": "node index.js"&#125; npm run test - 相当于执行 mocha 命令。它会开始执行测试框架 Mocha 。 npm run lint - 相当于执行 eslint lib bin hot scripts 命令。它会开始执行 eslint 检查。 npm run prepublish - 相当于执行 npm run test 和 npm run lint 两条命令。现在你了解如何复合命令了吧。 npm start - 相当于执行 node index.js 。Node.js 启动一个服务的入口脚本。 查看安装信息 你可以使用以下命令来查看所有全局安装的模块： $ npm list -g├─┬ cnpm@4.3.2│ ├── auto-correct@1.0.0│ ├── bagpipe@0.3.5│ ├── colors@1.1.2│ ├─┬ commander@2.9.0│ │ └── graceful-readlink@1.0.1│ ├─┬ cross-spawn@0.2.9│ │ └── lru-cache@2.7.3…… 如果要查看某个模块的版本号，可以使用命令如下： $ npm list gruntprojectName@projectVersion /path/to/project/folder└── grunt@0.4.1 配置文件 使用 npm 来管理的 javascript 项目一般都有一个package.json文件。它定义了这个项目所依赖的各种包，以及项目的配置信息（比如名称、版本、依赖等元数据）。 package.json 中的内容就是 json 形式。 重要字段： name - 包名。 version - 包的版本号。 description - 包的描述。 homepage - 包的官网 url 。 author - 包的作者姓名。 contributors - 包的其他贡献者姓名。 dependencies - 指定项目运行所依赖的模块。 devDependencies - 指定项目开发所依赖的模块。 repository - 包代码存放的地方的类型，可以是 git 或 svn，git 可在 Github 上。 main - main 字段是一个模块 ID，它是一个指向你程序的主要项目。就是说，如果你包的名字叫 express，然后用户安装它，然后 require(“express”)。 keywords - 关键字 bin - 用来指定各个内部命令对应的可执行文件的位置。 scripts - 指定了运行脚本命令的 npm 命令行缩写。 示例：一个完整的 package.json &#123; "name": "reactnotes", "version": "1.0.0", "description": "react 教程", "main": "./index.js", "dependencies": &#123; "react": "^15.4.1", "react-dom": "^15.4.1" &#125;, "devDependencies": &#123; "webpack-dev-server": "^1.16.2" &#125;, "scripts": &#123; "start": "node index.js" &#125;, "repository": &#123; "type": "git", "url": "git+https://github.com/atlantis1024/ReactNotes.git" &#125;, "author": "victor", "license": "Apache-2.0", "bugs": &#123; "url": "https://github.com/atlantis1024/ReactNotes/issues" &#125;, "homepage": "https://github.com/atlantis1024/ReactNotes#readme"&#125; package 版本 上文介绍 package.json 文件中的 dependencies 和 devDependencies 字段，这二者都是 json 数组。它们的每个 json 子对象，key 表示包名，value 表示版本。 npm 允许的版本声明方式十分多样。下面将为你介绍一二。 说明 version - 安装一个确定的版本，遵循“大版本.次要版本.小版本”的格式规定。如：1.0.0。 \~version - 以 \~1.0.0 来举例，表示安装 1.0.x 的最新版本（不低于 1.0.0）。但是大版本号和次要版本号不能变。 ^version - 以 ^1.0.0 来举例，表示安装 1.x.x 的最新版本（不低于 1.0.0），但是大版本号不能变。 1.2.x - 表示安装 1.2.x。 &gt;、&gt;=、&lt;、&lt;= - 可以像数组比较一样，使用比较符来限定版本范围。 version1 - version2 - 相当于 &gt;=version1 &lt;=version2. range1 || range2 - 版本满足 range1 或 range2 两个限定条件中任意一个即可。 tag - 一个指定 tag 对应的版本。 * 或 &quot;&quot; (空字符串)：任意版本。 latest - 最新版本。 http://... 或 file://... - 你可以指定 http 或本地文件路径下的包作为版本。 git... - 参考下面的“直接将 Git Url 作为依赖包版本” user/repo - 参考下面的“直接将 Git Url 作为依赖包版本” 例：下面的版本声明都是有效的 &#123; "dependencies": &#123; "foo": "1.0.0 - 2.9999.9999", "bar": "&gt;=1.0.2 &lt;2.1.2", "baz": "&gt;1.0.2 &lt;=2.3.4", "boo": "2.0.1", "qux": "&lt;1.0.0 || &gt;=2.3.1 &lt;2.4.5 || &gt;=2.5.2 &lt;3.0.0", "asd": "http://asdf.com/asdf.tar.gz", "til": "~1.2", "elf": "~1.2.3", "two": "2.x", "thr": "3.3.x", "lat": "latest", "dyl": "file:../dyl" &#125;&#125; 直接将 Git Url 作为依赖包版本 Git Url 形式可以如下： git://github.com/user/project.git#commit-ishgit+ssh://user@hostname:project.git#commit-ishgit+ssh://user@hostname/project.git#commit-ishgit+http://user@hostname/project/blah.git#commit-ishgit+https://user@hostname/project/blah.git#commit-ish 版本号 使用 Npm 下载和发布代码时都会接触到版本号。Npm 使用语义版本号来管理代码，这里简单介绍一下。 语义版本号分为 X.Y.Z 三位，分别代表主版本号、次版本号和补丁版本号。当代码变更时，版本号按以下原则更新。 如果只是修复 bug，需要更新 Z 位。 如果是新增了功能，但是向下兼容，需要更新 Y 位。 如果有大变动，向下不兼容，需要更新 X 位。 版本号有了这个保证后，在申明第三方包依赖时，除了可依赖于一个固定版本号外，还可依赖于某个范围的版本号。例如&quot;argv&quot;: &quot;0.0.x&quot;表示依赖于 0.0.x 系列的最新版 argv。 Npm 支持的所有版本号范围指定方式可以查看官方文档。 npm 扩展 cnpm 大家都知道国内直接使用 npm 的官方镜像是非常慢的，这里推荐使用淘宝 NPM 镜像。 淘宝 NPM 镜像是一个完整 npmjs.org 镜像，你可以用此代替官方版本(只读)，同步频率目前为 10 分钟 一次以保证尽量与官方服务同步。 你可以使用淘宝定制的 cnpm (gzip 压缩支持) 命令行工具代替默认的 npm: $ npm install -g cnpm --registry=https://registry.npm.taobao.org 这样就可以使用 cnpm 命令来安装模块了： $ cnpm install [name] 更多信息可以查阅：http://npm.taobao.org/。 nrm Nrm 是 NPM 注册服务器管理工具，可以快速切换不同的注册表：npm，cnpm，nj，taobao，或者是你自己的私服。 安装 nrm npm install -g nrm 查看可用服务器 # 查看可用服务器$ nrm ls* npm ----- https://registry.npmjs.org/ cnpm ---- http://r.cnpmjs.org/ taobao -- https://registry.npm.taobao.org/ nj ------ https://registry.nodejitsu.com/ rednpm -- http://registry.mirror.cqupt.edu.cn skimdb -- https://skimdb.npmjs.com/registry# 切换服务器$ nrm use tabao nrm 命令语义 Usage: nrm [options] [command] Commands: ls List all the registries use &lt;registry&gt; Change registry to registry add &lt;registry&gt; &lt;url&gt; [home] Add one custom registry del &lt;registry&gt; Delete one custom registry home &lt;registry&gt; [browser] Open the homepage of registry with optional browser test [registry] Show the response time for one or all registries help Print this help Options: -h, --help output usage information -V, --version output the version number 更多内容 📚 拓展阅读 Node.js Npm Yarn 📦 本文归档在 我的前端技术教程系列：frontend-tutorial Npm 官网 Npm 中文网 Npm Github 淘宝 NPM 镜像 - 代替官方版本，加速下载 awesome-npm - npm 资源 sinopia - 零配置搭建 npm 私服 nrm - npm 服务器地址管理工具 NPM 使用介绍]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring 中使用 JDBC 访问数据]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring%2Fdata%2Fspring-and-jdbc%2F</url>
    <content type="text"><![CDATA[Spring 中使用 JDBC 访问数据 准备 使用 Spring 的 JDBC 功能，你需要在 pom.xml 中引入以下依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.1.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; 配置数据源 使用 JNDI 数据源 如果 Spring 应用部署在支持 JNDI 的WEB服务器上（如WebSphere、JBoss、Tomcat等），就可以使用JNDI获取数据源。 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans"xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:jee="http://www.springframework.org/schema/jee" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.2.xsdhttp://www.springframework.org/schema/jeehttp://www.springframework.org/schema/jee/spring-jee-3.2.xsd"&gt; &lt;!-- 1.使用bean配置jndi数据源 --&gt; &lt;bean id="dataSource" class="org.springframework.jndi.JndiObjectFactoryBean"&gt; &lt;property name="jndiName" value="java:comp/env/jdbc/orclight" /&gt; &lt;/bean&gt; &lt;!-- 2.使用jee标签配置jndi数据源，与1等价，但是需要引入命名空间 --&gt; &lt;jee:jndi-lookup id="dataSource" jndi-name=" java:comp/env/jdbc/orclight" /&gt;&lt;/beans&gt; 使用数据源连接池 Spring 本身并没有提供数据源连接池的实现。 推荐使用 Druid 。 &lt;bean id="dataSource" class="com.alibaba.druid.pool.DruidDataSource" init-method="init" destroy-method="close"&gt; &lt;property name="driverClassName" value="$&#123;jdbc.driver&#125;"/&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt; &lt;!-- 配置初始化大小、最小、最大 --&gt; &lt;property name="initialSize" value="1"/&gt; &lt;property name="minIdle" value="1"/&gt; &lt;property name="maxActive" value="10"/&gt; &lt;!-- 配置获取连接等待超时的时间 --&gt; &lt;property name="maxWait" value="10000"/&gt; &lt;!-- 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 --&gt; &lt;property name="timeBetweenEvictionRunsMillis" value="60000"/&gt; &lt;!-- 配置一个连接在池中最小生存的时间，单位是毫秒 --&gt; &lt;property name="minEvictableIdleTimeMillis" value="300000"/&gt; &lt;property name="testWhileIdle" value="true"/&gt; &lt;!-- 这里建议配置为TRUE，防止取到的连接不可用 --&gt; &lt;property name="testOnBorrow" value="true"/&gt; &lt;property name="testOnReturn" value="false"/&gt; &lt;!-- 打开PSCache，并且指定每个连接上PSCache的大小 --&gt; &lt;property name="poolPreparedStatements" value="true"/&gt; &lt;property name="maxPoolPreparedStatementPerConnectionSize" value="20"/&gt; &lt;!-- 这里配置提交方式，默认就是TRUE，可以不用配置 --&gt; &lt;property name="defaultAutoCommit" value="true"/&gt; &lt;!-- 验证连接有效与否的SQL，不同的数据配置不同 --&gt; &lt;property name="validationQuery" value="select 1 "/&gt; &lt;property name="filters" value="stat"/&gt; &lt;/bean&gt; 基于JDBC驱动的数据源 &lt;bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource"&gt; &lt;property name="driverClassName" value="$&#123;jdbc.driver&#125;"/&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt;&lt;/bean&gt; 使用 JDBC 模板 Spring 将数据访问的样板式代码提取到模板类中。Spring 提供了 3 个模板类： JdbcTemplate：最基本的 Spring JDBC 模板，这个模板支持最简单的 JDBC 数据库访问功能以及简单的索引参数查询。 SimpleJdbcTemplate：改模板类利用 Java 5 的一些特性，如自动装箱、泛型以及可变参数列表来简化 JDBC 模板的使用。 NamedParameterJdbcTemplate：使用该模板类执行查询时，可以将查询值以命名参数的形式绑定到 SQL 中，而不是使用简单的索引参数。 注：Spring 实战推荐使用 SimpleJdbcTemplate。 使用 Spring JDBC 的步骤如下： 创建 DTO 类。DTO 是 Data Transfer Object 的缩写，即数据传输对象。 创建实现 RowMapper 接口的类。它可以看成是数据表实际实体和 DTO 之间的映射关系。 声明数据库读写接口的 DAO 接口。定义 DAO 的好处在于对于数据层上层的业务，调用 DAO 时仅关注对外暴露的读写方法，而不考虑底层的具体持久化方式。这样，便于替换持久化方式。 创建一个 DAO 接口的实现类，使用 Spring 的 JDBC 模板去实现接口。 最后，只需要按照 Spring 的 JDBC 规范配置 xml。定义一个 DAO 接口的实现类的 JavaBean，并将数据源注入进去。 实例 实例可以参考 spring-data-jdbc 中的实例。 执行 io.github.dunwu.spring.data.jdbc 包中的 Test 单元测试，即可看到测试结果。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring 的数据访问策略]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring%2Fdata%2Fdata-access-in-spring%2F</url>
    <content type="text"><![CDATA[Spring 的数据访问策略 Spring 的目标之一，就是允许开发人员在开发过程中能够遵循面向对象（OO）原则中的“针对接口编程”。Spring 对数据访问的支持也不例外。 DAO 是 Data Access Object 的缩写，即数据访问对象。它提供了数据读写到数据库中的一种方式。 Spring 支持 DAO 技术主要目的是：使得切换持久化技术十分方便，如 JDBC、JPA、ORM 等。这样数据访问层仅暴露 DAO 接口，而上层不需要关注具体的持久化技术，达到了解耦的目的。 Spring 的异常体系 Spring JDBC不同于JDBC，提供了比较丰富的数据异常类型。 Spring的数据异常并没有与特定的持久化方式相关联，所以异常对于不同的持久化方式都是一致的。 Spring的数据异常都继承自 DataAccessException，这种异常是非检查型异常。即，不一定非要捕获Spring所抛出的数据访问异常。 数据访问模板化 Spring 在数据访问过程中，采用了模板方法设计模式。 它将数据访问过程分为两块：模板（template）和回调（callback）。模板管理过程中固定的部分，而回调处理自定义的数据访问代码。 下图中，左边属于模板固定部分，右边属于模板回调部分。 对于不同的持久化平台，Spring 提供了多个可选的模板。 使用DAO支持类 基于模板-回调设计，Spring 提供了 DAO 支持类，而将业务本身的 DAO 类作为它的子类。 下图展示了模板类、DAO 支持类以及自定义 DAO 实现之间的关系。 Spring不仅提供了多个数据模板实现类，还为每种模板提供了对应的 DAO 支持类。]]></content>
  </entry>
  <entry>
    <title><![CDATA[JavaMail 使用小结]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavalib%2Fjavamail%2F</url>
    <content type="text"><![CDATA[JavaMail 使用小结 概述 邮件相关的标准 厂商所提供的 JavaMail 服务程序可以有选择地实现某些邮件协议，常见的邮件协议包括： SMTP(Simple Mail Transfer Protocol) ：即简单邮件传输协议，它是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式。 POP3(Post Office Protocol - Version 3) ：即邮局协议版本 3 ，用于接收电子邮件的标准协议。 IMAP(Internet Mail Access Protocol) ：即 Internet 邮件访问协议。是 POP3 的替代协议。 这三种协议都有对应 SSL 加密传输的协议，分别是 **SMTPS **， **POP3S **和 **IMAPS **。 MIME(Multipurpose Internet Mail Extensions) ：即多用途因特网邮件扩展标准。它不是邮件传输协议。但对传输内容的消息、附件及其它的内容定义了格式。 JavaMail 简介 JavaMail 是由 Sun 发布的用来处理 email 的 API 。它并没有包含在 Java SE 中，而是作为 Java EE 的一部分。 mail.jar ：此 JAR 文件包含 JavaMail API 和 Sun 提供的 SMTP 、 IMAP 和 POP3 服务提供程序； activation.jar ：此 JAR 文件包含 JAF API 和 Sun 的实现。 JavaMail 包中用于处理电子邮件的核心类是： Properties 、 Session 、 Message 、 Address 、 Authenticator 、 Transport 、 Store 等。 邮件传输过程 如上图，电子邮件的处理步骤如下： 创建一个 Session 对象。 Session 对象创建一个 Transport 对象 /Store 对象，用来发送 / 保存邮件。 Transport 对象 /Store 对象连接邮件服务器。 Transport 对象 /Store 对象创建一个 Message 对象 ( 也就是邮件内容 ) 。 Transport 对象发送邮件； Store 对象获取邮箱的邮件。 Message 结构 MimeMessage 类：代表整封邮件。 MimeBodyPart 类：代表邮件的一个 MIME 信息。 MimeMultipart 类：代表一个由多个 MIME 信息组合成的组合 MIME 信息。 JavaMail 的核心类 JavaMail 对收发邮件进行了高级的抽象，形成了一些关键的的接口和类，它们构成了程序的基础，下面我们分别来了解一下这些最常见的对象。 java.util.Properties 类（属性对象） java.util.Properties 类代表一组属性集合。 它的每一个键和值都是 String 类型。 由于 JavaMail 需要和邮件服务器进行通信，这就要求程序提供许多诸如服务器地址、端口、用户名、密码等信息， JavaMail 通过 Properties 对象封装这些属性信息。 例： 如下面的代码封装了几个属性信息： Properties prop = new Properties();prop.setProperty("mail.debug", "true");prop.setProperty("mail.host", "[email protected]");prop.setProperty("mail.transport.protocol", "smtp");prop.setProperty("mail.smtp.auth", "true"); 针对不同的的邮件协议， JavaMail 规定了服务提供者必须支持一系列属性， 下表是一些常见属性（属性值都以 String 类型进行设置，属性类型栏仅表示属性是如何被解析的）： 关键词 类型 描述 mail.debug boolean debug 开关。 mail.host String 指定发送、接收邮件的默认邮箱服务器。 mail.store.protocol String 指定接收邮件的协议。 mail.transport.protocol String 指定发送邮件的协议。 mail.debug.auth boolean debug 输出中是否包含认证命令。默认是 false 。 详情请参考官方 API 文档： https://javamail.java.net/nonav/docs/api/ 。 javax.mail.Session 类（会话对象） Session 表示一个邮件会话。 Session 的主要作用包括两个方面： 接收各种配置属性信息：通过 Properties 对象设置的属性信息； 初始化 JavaMail 环境：根据 JavaMail 的配置文件，初始化 JavaMail 环境，以便通过 Session 对象创建其他重要类的实例。 JavaMail 在 Jar 包的 META-INF 目录下，通过以下文件提供了基本配置信息，以便 session 能够根据这个配置文件加载提供者的实现类： javamail.default.providers javamail.default.address.map 例： Properties props = new Properties();props.setProperty("mail.transport.protocol", "smtp");Session session = Session.getInstance(props); javax.mail.Transport 类（邮件传输） 邮件操作只有发送或接收两种处理方式。 JavaMail 将这两种不同操作描述为传输（ javax.mail.Transport ）和存储（ javax.mail.Store ），传输对应邮件的发送，而存储对应邮件的接收。 getTransport - Session 类中的 getTransport **() **有多个重载方法，可以用来创建 Transport 对象。 connect - 如果设置了认证命令—— mail.smtp.auth ，那么使用 Transport 类的 connect 方法连接服务器时，则必须加上用户名和密码。 sendMessage - Transport 类的 sendMessage 方法用来发送邮件消息。 close - Transport 类的 close 方法用来关闭和邮件服务器的连接。 javax.mail.Store 类（邮件存储 ） getStore - Session 类中的 getStore () 有多个重载方法，可以用来创建 Store 对象。 connect - 如果设置了认证命令—— mail.smtp.auth ，那么使用 Store 类的 connect 方法连接服务器时，则必须加上用户名和密码。 getFolder - Store 类的 getFolder 方法可以 获取邮箱内的邮件夹 Folder 对象 close - Store 类的 close 方法用来关闭和邮件服务器的连接。 javax.mail.Message 类（消息对象） javax.mail.Message - 是个抽象类，只能用子类去实例化，多数情况下为 javax.mail.internet.MimeMessage。 MimeMessage - 代表 MIME 类型的电子邮件消息。 要创建一个 Message ，需要将 Session 对象传递给 MimeMessage 构造器： MimeMessage message = new MimeMessage(session); 注意：还存在其它构造器，如用按 RFC822 格式的输入流来创建消息。 setFrom - 设置邮件的发件人 setRecipient - 设置邮件的发送人、抄送人、密送人 三种预定义的地址类型是： Message.RecipientType.TO - 收件人 Message.RecipientType.CC - 抄送人 Message.RecipientType.BCC - 密送人 setSubject - 设置邮件的主题 setContent - 设置邮件内容 setText - 如果邮件内容是纯文本，可以使用此接口设置文本内容。 javax.mail.Address 类（地址） 一旦您创建了 Session 和 Message ，并将内容填入消息后，就可以用 Address 确定信件地址了。和 Message 一样， Address 也是个抽象类。您用的是 javax.mail.internet.InternetAddress 类。 若创建的地址只包含电子邮件地址，只要传递电子邮件地址到构造器就行了。 例： Address address = new InternetAddress("[email protected]"); Authenticator 类（认证者） 与 java.net 类一样， JavaMail API 也可以利用 Authenticator 通过用户名和密码访问受保护的资源。对于 JavaMail API 来说，这些资源就是邮件服务器。Authenticator 在 javax.mail 包中，而且它和 java.net 中同名的类 Authenticator 不同。两者并不共享同一个 Authenticator ，因为 JavaMail API 用于 Java 1.1 ，它没有 java.net 类别。 要使用 Authenticator ，先创建一个抽象类的子类，并从 getPasswordAuthentication() 方法中返回 PasswordAuthentication 实例。创建完成后，您必需向 session 注册 Authenticator 。然后，在需要认证的时候，就会通知 Authenticator 。您可以弹出窗口，也可以从配置文件中（虽然没有加密是不安全的）读取用户名和密码，将它们作为 PasswordAuthentication 对象返回给调用程序。 例： Properties props = new Properties();Authenticator auth = new MyAuthenticator();Session session = Session.getDefaultInstance(props, auth); 实例 发送文本邮件 public static void main(String[] args) throws Exception &#123; Properties prop = new Properties(); prop.setProperty("mail.debug", "true"); prop.setProperty("mail.host", MAIL_SERVER_HOST); prop.setProperty("mail.transport.protocol", "smtp"); prop.setProperty("mail.smtp.auth", "true"); // 1、创建session Session session = Session.getInstance(prop); Transport ts = null; // 2、通过session得到transport对象 ts = session.getTransport(); // 3、连上邮件服务器 ts.connect(MAIL_SERVER_HOST, USER, PASSWORD); // 4、创建邮件 MimeMessage message = new MimeMessage(session); // 邮件消息头 message.setFrom(new InternetAddress(MAIL_FROM)); // 邮件的发件人 message.setRecipient(Message.RecipientType.TO, new InternetAddress(MAIL_TO)); // 邮件的收件人 message.setRecipient(Message.RecipientType.CC, new InternetAddress(MAIL_CC)); // 邮件的抄送人 message.setRecipient(Message.RecipientType.BCC, new InternetAddress(MAIL_BCC)); // 邮件的密送人 message.setSubject("测试文本邮件"); // 邮件的标题 // 邮件消息体 message.setText("天下无双。"); // 5、发送邮件 ts.sendMessage(message, message.getAllRecipients()); ts.close();&#125; 发送 HTML 格式的邮件 public static void main(String[] args) throws Exception &#123; Properties prop = new Properties(); prop.setProperty("mail.debug", "true"); prop.setProperty("mail.host", MAIL_SERVER_HOST); prop.setProperty("mail.transport.protocol", "smtp"); prop.setProperty("mail.smtp.auth", "true"); // 1、创建session Session session = Session.getInstance(prop); Transport ts = null; // 2、通过session得到transport对象 ts = session.getTransport(); // 3、连上邮件服务器 ts.connect(MAIL_SERVER_HOST, USER, PASSWORD); // 4、创建邮件 MimeMessage message = new MimeMessage(session); // 邮件消息头 message.setFrom(new InternetAddress(MAIL_FROM)); // 邮件的发件人 message.setRecipient(Message.RecipientType.TO, new InternetAddress(MAIL_TO)); // 邮件的收件人 message.setRecipient(Message.RecipientType.CC, new InternetAddress(MAIL_CC)); // 邮件的抄送人 message.setRecipient(Message.RecipientType.BCC, new InternetAddress(MAIL_BCC)); // 邮件的密送人 message.setSubject("测试HTML邮件"); // 邮件的标题 String htmlContent = "&lt;h1&gt;Hello&lt;/h1&gt;" + "&lt;p&gt;显示图片&lt;img src='cid:abc.jpg'&gt;1.jpg&lt;/p&gt;"; MimeBodyPart text = new MimeBodyPart(); text.setContent(htmlContent, "text/html;charset=UTF-8"); MimeBodyPart image = new MimeBodyPart(); DataHandler dh = new DataHandler(new FileDataSource("D:\\05_Datas\\图库\\吉他少年背影.png")); image.setDataHandler(dh); image.setContentID("abc.jpg"); // 描述数据关系 MimeMultipart mm = new MimeMultipart(); mm.addBodyPart(text); mm.addBodyPart(image); mm.setSubType("related"); message.setContent(mm); message.saveChanges(); // 5、发送邮件 ts.sendMessage(message, message.getAllRecipients()); ts.close();&#125; 发送带附件的邮件 public static void main(String[] args) throws Exception &#123; Properties prop = new Properties(); prop.setProperty("mail.debug", "true"); prop.setProperty("mail.host", MAIL_SERVER_HOST); prop.setProperty("mail.transport.protocol", "smtp"); prop.setProperty("mail.smtp.auth", "true"); // 1、创建session Session session = Session.getInstance(prop); // 2、通过session得到transport对象 Transport ts = session.getTransport(); // 3、连上邮件服务器 ts.connect(MAIL_SERVER_HOST, USER, PASSWORD); // 4、创建邮件 MimeMessage message = new MimeMessage(session); // 邮件消息头 message.setFrom(new InternetAddress(MAIL_FROM)); // 邮件的发件人 message.setRecipient(Message.RecipientType.TO, new InternetAddress(MAIL_TO)); // 邮件的收件人 message.setRecipient(Message.RecipientType.CC, new InternetAddress(MAIL_CC)); // 邮件的抄送人 message.setRecipient(Message.RecipientType.BCC, new InternetAddress(MAIL_BCC)); // 邮件的密送人 message.setSubject("测试带附件邮件"); // 邮件的标题 MimeBodyPart text = new MimeBodyPart(); text.setContent("邮件中有两个附件。", "text/html;charset=UTF-8"); // 描述数据关系 MimeMultipart mm = new MimeMultipart(); mm.setSubType("related"); mm.addBodyPart(text); String[] files = &#123; "D:\\00_Temp\\temp\\1.jpg", "D:\\00_Temp\\temp\\2.png" &#125;; // 添加邮件附件 for (String filename : files) &#123; MimeBodyPart attachPart = new MimeBodyPart(); attachPart.attachFile(filename); mm.addBodyPart(attachPart); &#125; message.setContent(mm); message.saveChanges(); // 5、发送邮件 ts.sendMessage(message, message.getAllRecipients()); ts.close();&#125; 获取邮箱中的邮件 public static void main(String[] args) throws Exception &#123; // 创建一个有具体连接信息的Properties对象 Properties prop = new Properties(); prop.setProperty("mail.debug", "true"); prop.setProperty("mail.store.protocol", "pop3"); prop.setProperty("mail.pop3.host", MAIL_SERVER_HOST); // 1、创建session Session session = Session.getInstance(prop); // 2、通过session得到Store对象 Store store = session.getStore(); // 3、连上邮件服务器 store.connect(MAIL_SERVER_HOST, USER, PASSWORD); // 4、获得邮箱内的邮件夹 Folder folder = store.getFolder("inbox"); folder.open(Folder.READ_ONLY); // 获得邮件夹Folder内的所有邮件Message对象 Message[] messages = folder.getMessages(); for (int i = 0; i &lt; messages.length; i++) &#123; String subject = messages[i].getSubject(); String from = (messages[i].getFrom()[0]).toString(); System.out.println("第 " + (i + 1) + "封邮件的主题：" + subject); System.out.println("第 " + (i + 1) + "封邮件的发件人地址：" + from); &#125; // 5、关闭 folder.close(false); store.close();&#125; 转发邮件 例：获取指定邮件夹下的第一封邮件并转发 public static void main(String[] args) throws Exception &#123; Properties prop = new Properties(); prop.put("mail.store.protocol", "pop3"); prop.put("mail.pop3.host", MAIL_SERVER_POP3); prop.put("mail.pop3.starttls.enable", "true"); prop.put("mail.smtp.auth", "true"); prop.put("mail.smtp.host", MAIL_SERVER_SMTP); // 1、创建session Session session = Session.getDefaultInstance(prop); // 2、读取邮件夹 Store store = session.getStore("pop3"); store.connect(MAIL_SERVER_POP3, USER, PASSWORD); Folder folder = store.getFolder("inbox"); folder.open(Folder.READ_ONLY); // 获取邮件夹中第1封邮件信息 Message[] messages = folder.getMessages(); if (messages.length &lt;= 0) &#123; return; &#125; Message message = messages[0]; // 打印邮件关键信息 String from = InternetAddress.toString(message.getFrom()); if (from != null) &#123; System.out.println("From: " + from); &#125; String replyTo = InternetAddress.toString(message.getReplyTo()); if (replyTo != null) &#123; System.out.println("Reply-to: " + replyTo); &#125; String to = InternetAddress.toString(message.getRecipients(Message.RecipientType.TO)); if (to != null) &#123; System.out.println("To: " + to); &#125; String subject = message.getSubject(); if (subject != null) &#123; System.out.println("Subject: " + subject); &#125; Date sent = message.getSentDate(); if (sent != null) &#123; System.out.println("Sent: " + sent); &#125; // 设置转发邮件信息头 Message forward = new MimeMessage(session); forward.setFrom(new InternetAddress(MAIL_FROM)); forward.setRecipient(Message.RecipientType.TO, new InternetAddress(MAIL_TO)); forward.setSubject("Fwd: " + message.getSubject()); // 设置转发邮件内容 MimeBodyPart bodyPart = new MimeBodyPart(); bodyPart.setContent(message, "message/rfc822"); Multipart multipart = new MimeMultipart(); multipart.addBodyPart(bodyPart); forward.setContent(multipart); forward.saveChanges(); Transport ts = session.getTransport("smtp"); ts.connect(USER, PASSWORD); ts.sendMessage(forward, forward.getAllRecipients()); folder.close(false); store.close(); ts.close(); System.out.println("message forwarded successfully....");&#125;]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>mail</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[资源]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring%2Fappendix%2Fresources%2F</url>
    <content type="text"><![CDATA[资源 官方资源 https://spring.io/ Spring Framework 官方文档 Spring Framework Github 教程资源 跟我学 Spring3 by zhangkaitao 入门资源 Spring初学快速入门 by 易百 进阶资源 整合资源 Springside4 by Calvin Xiao]]></content>
  </entry>
  <entry>
    <title><![CDATA[JUnit 快速指南]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavalib%2Fjunit%2F</url>
    <content type="text"><![CDATA[JUnit 快速指南 version: junit5 1. 安装 2. JUnit 注解 3. 编写单元测试 3.1. 基本的单元测试类和方法 3.2. 定制测试类和方法的显示名称 3.3. 断言（Assertions） 3.4. 假想（Assumptions） 3.5. 禁用 3.6. 测试条件 3.7. 嵌套测试 3.8. 重复测试 3.9. 参数化测试 4. 引用和引申 1. 安装 在 pom 中添加依赖 &lt;properties&gt; &lt;junit.jupiter.version&gt;5.3.2&lt;/junit.jupiter.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.jupiter.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter-params&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.jupiter.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter-engine&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.jupiter.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 组件间依赖关系： 2. JUnit 注解 Annotation Description @Test Denotes that a method is a test method. Unlike JUnit 4’s @Test annotation, this annotation does not declare any attributes, since test extensions in JUnit Jupiter operate based on their own dedicated annotations. Such methods are inherited unless they are overridden. @ParameterizedTest Denotes that a method is a parameterized test. Such methods are inherited unless they are overridden. @RepeatedTest Denotes that a method is a test template for a repeated test. Such methods are inherited unless they are overridden. @TestFactory Denotes that a method is a test factory for dynamic tests. Such methods are inherited unless they are overridden. @TestInstance Used to configure the test instance lifecycle for the annotated test class. Such annotations are inherited. @TestTemplate Denotes that a method is a template for test cases designed to be invoked multiple times depending on the number of invocation contexts returned by the registered providers. Such methods are inherited unless they are overridden. @DisplayName Declares a custom display name for the test class or test method. Such annotations are not inherited. @BeforeEach Denotes that the annotated method should be executed before each @Test, @RepeatedTest, @ParameterizedTest, or @TestFactory method in the current class; analogous to JUnit 4’s @Before. Such methods are inherited unless they are overridden. @AfterEach Denotes that the annotated method should be executed after each @Test, @RepeatedTest, @ParameterizedTest, or @TestFactory method in the current class; analogous to JUnit 4’s @After. Such methods are inherited unless they are overridden. @BeforeAll Denotes that the annotated method should be executed before all @Test, @RepeatedTest, @ParameterizedTest, and @TestFactory methods in the current class; analogous to JUnit 4’s @BeforeClass. Such methods are inherited (unless they are hidden or overridden) and must be static (unless the “per-class” test instance lifecycle is used). @AfterAll Denotes that the annotated method should be executed after all @Test, @RepeatedTest, @ParameterizedTest, and @TestFactory methods in the current class; analogous to JUnit 4’s @AfterClass. Such methods are inherited (unless they are hidden or overridden) and must be static (unless the “per-class” test instance lifecycle is used). @Nested Denotes that the annotated class is a nested, non-static test class. @BeforeAll and @AfterAllmethods cannot be used directly in a @Nested test class unless the “per-class” test instance lifecycle is used. Such annotations are not inherited. @Tag Used to declare tags for filtering tests, either at the class or method level; analogous to test groups in TestNG or Categories in JUnit 4. Such annotations are inherited at the class level but not at the method level. @Disabled Used to disable a test class or test method; analogous to JUnit 4’s @Ignore. Such annotations are not inherited. @ExtendWith Used to register custom extensions. Such annotations are inherited. 3. 编写单元测试 3.1. 基本的单元测试类和方法 import org.junit.jupiter.api.*;import org.slf4j.Logger;import org.slf4j.LoggerFactory;class Junit5StandardTests &#123; private static final Logger LOGGER = LoggerFactory.getLogger(Junit5StandardTests.class); @BeforeAll static void beforeAll() &#123; LOGGER.info("call beforeAll()"); &#125; @BeforeEach void beforeEach() &#123; LOGGER.info("call beforeEach()"); &#125; @Test void succeedingTest() &#123; LOGGER.info("call succeedingTest()"); &#125; @Test void failingTest() &#123; LOGGER.info("call failingTest()"); // fail("a failing test"); &#125; @Test @Disabled("for demonstration purposes") void skippedTest() &#123; LOGGER.info("call skippedTest()"); // not executed &#125; @AfterEach void afterEach() &#123; LOGGER.info("call afterEach()"); &#125; @AfterAll static void afterAll() &#123; LOGGER.info("call afterAll()"); &#125;&#125; 3.2. 定制测试类和方法的显示名称 支持普通字符、特殊符号、emoji import org.junit.jupiter.api.DisplayName;import org.junit.jupiter.api.Test;@DisplayName("A special test case")class JunitDisplayNameDemo &#123; @Test @DisplayName("Custom test name containing spaces") void testWithDisplayNameContainingSpaces() &#123; &#125; @Test @DisplayName("╯°□°）╯") void testWithDisplayNameContainingSpecialCharacters() &#123; &#125; @Test @DisplayName("😱") void testWithDisplayNameContainingEmoji() &#123; &#125;&#125; 3.3. 断言（Assertions） import org.junit.jupiter.api.BeforeAll;import org.junit.jupiter.api.Test;import static java.time.Duration.ofMillis;import static java.time.Duration.ofMinutes;import static org.junit.jupiter.api.Assertions.*;class AssertionsDemo &#123; private static Person person; @BeforeAll public static void beforeAll() &#123; person = new Person("John", "Doe"); &#125; @Test void standardAssertions() &#123; assertEquals(2, 2); assertEquals(4, 4, "The optional assertion message is now the last parameter."); assertTrue('a' &lt; 'b', () -&gt; "Assertion messages can be lazily evaluated -- " + "to avoid constructing complex messages unnecessarily."); &#125; @Test void groupedAssertions() &#123; // In a grouped assertion all assertions are executed, and any // failures will be reported together. assertAll("person", () -&gt; assertEquals("John", person.getFirstName()), () -&gt; assertEquals("Doe", person.getLastName())); &#125; @Test void dependentAssertions() &#123; // Within a code block, if an assertion fails the // subsequent code in the same block will be skipped. assertAll("properties", () -&gt; &#123; String firstName = person.getFirstName(); assertNotNull(firstName); // Executed only if the previous assertion is valid. assertAll("first name", () -&gt; assertTrue(firstName.startsWith("J")), () -&gt; assertTrue(firstName.endsWith("n"))); &#125;, () -&gt; &#123; // Grouped assertion, so processed independently // of results of first name assertions. String lastName = person.getLastName(); assertNotNull(lastName); // Executed only if the previous assertion is valid. assertAll("last name", () -&gt; assertTrue(lastName.startsWith("D")), () -&gt; assertTrue(lastName.endsWith("e"))); &#125;); &#125; @Test void exceptionTesting() &#123; Throwable exception = assertThrows(IllegalArgumentException.class, () -&gt; &#123; throw new IllegalArgumentException("a message"); &#125;); assertEquals("a message", exception.getMessage()); &#125; @Test void timeoutNotExceeded() &#123; // The following assertion succeeds. assertTimeout(ofMinutes(2), () -&gt; &#123; // Perform task that takes less than 2 minutes. &#125;); &#125; @Test void timeoutNotExceededWithResult() &#123; // The following assertion succeeds, and returns the supplied object. String actualResult = assertTimeout(ofMinutes(2), () -&gt; &#123; return "a result"; &#125;); assertEquals("a result", actualResult); &#125; @Test void timeoutNotExceededWithMethod() &#123; // The following assertion invokes a method reference and returns an object. String actualGreeting = assertTimeout(ofMinutes(2), AssertionsDemo::greeting); assertEquals("Hello, World!", actualGreeting); &#125; @Test void timeoutExceeded() &#123; // The following assertion fails with an error message similar to: // execution exceeded timeout of 10 ms by 91 ms assertTimeout(ofMillis(10), () -&gt; &#123; // Simulate task that takes more than 10 ms. Thread.sleep(100); &#125;); &#125; @Test void timeoutExceededWithPreemptiveTermination() &#123; // The following assertion fails with an error message similar to: // execution timed out after 10 ms assertTimeoutPreemptively(ofMillis(10), () -&gt; &#123; // Simulate task that takes more than 10 ms. Thread.sleep(100); &#125;); &#125; private static String greeting() &#123; return "Hello, World!"; &#125;&#125; 3.4. 假想（Assumptions） import static org.junit.jupiter.api.Assertions.assertEquals;import static org.junit.jupiter.api.Assumptions.assumeTrue;import static org.junit.jupiter.api.Assumptions.assumingThat;import org.junit.jupiter.api.Test;class AssumptionsDemo &#123; @Test void testOnlyOnCiServer() &#123; assumeTrue("CI".equals(System.getenv("ENV"))); // remainder of test &#125; @Test void testOnlyOnDeveloperWorkstation() &#123; assumeTrue("DEV".equals(System.getenv("ENV")), () -&gt; "Aborting test: not on developer workstation"); // remainder of test &#125; @Test void testInAllEnvironments() &#123; assumingThat("CI".equals(System.getenv("ENV")), () -&gt; &#123; // perform these assertions only on the CI server assertEquals(2, 2); &#125;); // perform these assertions in all environments assertEquals("a string", "a string"); &#125;&#125; 3.5. 禁用 禁用单元测试类示例： import org.junit.jupiter.api.Disabled;import org.junit.jupiter.api.Test;@Disabledclass DisabledClassDemo &#123; @Test void testWillBeSkipped() &#123; &#125;&#125; 禁用单元测试方法示例： import org.junit.jupiter.api.Disabled;import org.junit.jupiter.api.Test;class DisabledTestsDemo &#123; @Disabled @Test void testWillBeSkipped() &#123; &#125; @Test void testWillBeExecuted() &#123; &#125;&#125; 3.6. 测试条件 操作系统条件 @Test@EnabledOnOs(MAC)void onlyOnMacOs() &#123; // ...&#125;@TestOnMacvoid testOnMac() &#123; // ...&#125;@Test@EnabledOnOs(&#123; LINUX, MAC &#125;)void onLinuxOrMac() &#123; // ...&#125;@Test@DisabledOnOs(WINDOWS)void notOnWindows() &#123; // ...&#125;@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Test@EnabledOnOs(MAC)@interface TestOnMac &#123;&#125; Java 运行时版本条件 @Test@EnabledOnJre(JAVA_8)void onlyOnJava8() &#123; // ...&#125;@Test@EnabledOnJre(&#123; JAVA_9, JAVA_10 &#125;)void onJava9Or10() &#123; // ...&#125;@Test@DisabledOnJre(JAVA_9)void notOnJava9() &#123; // ...&#125; 系统属性条件 @Test@EnabledIfSystemProperty(named = "os.arch", matches = ".*64.*")void onlyOn64BitArchitectures() &#123; // ...&#125;@Test@DisabledIfSystemProperty(named = "ci-server", matches = "true")void notOnCiServer() &#123; // ...&#125; 3.7. 嵌套测试 import static org.junit.jupiter.api.Assertions.assertEquals;import static org.junit.jupiter.api.Assertions.assertFalse;import static org.junit.jupiter.api.Assertions.assertThrows;import static org.junit.jupiter.api.Assertions.assertTrue;import java.util.EmptyStackException;import java.util.Stack;import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.DisplayName;import org.junit.jupiter.api.Nested;import org.junit.jupiter.api.Test;@DisplayName("A stack")class TestingAStackDemo &#123; Stack&lt;Object&gt; stack; @Test @DisplayName("is instantiated with new Stack()") void isInstantiatedWithNew() &#123; new Stack&lt;&gt;(); &#125; @Nested @DisplayName("when new") class WhenNew &#123; @BeforeEach void createNewStack() &#123; stack = new Stack&lt;&gt;(); &#125; @Test @DisplayName("is empty") void isEmpty() &#123; assertTrue(stack.isEmpty()); &#125; @Test @DisplayName("throws EmptyStackException when popped") void throwsExceptionWhenPopped() &#123; assertThrows(EmptyStackException.class, () -&gt; stack.pop()); &#125; @Test @DisplayName("throws EmptyStackException when peeked") void throwsExceptionWhenPeeked() &#123; assertThrows(EmptyStackException.class, () -&gt; stack.peek()); &#125; @Nested @DisplayName("after pushing an element") class AfterPushing &#123; String anElement = "an element"; @BeforeEach void pushAnElement() &#123; stack.push(anElement); &#125; @Test @DisplayName("it is no longer empty") void isNotEmpty() &#123; assertFalse(stack.isEmpty()); &#125; @Test @DisplayName("returns the element when popped and is empty") void returnElementWhenPopped() &#123; assertEquals(anElement, stack.pop()); assertTrue(stack.isEmpty()); &#125; @Test @DisplayName("returns the element when peeked but remains not empty") void returnElementWhenPeeked() &#123; assertEquals(anElement, stack.peek()); assertFalse(stack.isEmpty()); &#125; &#125; &#125;&#125; 3.8. 重复测试 import static org.junit.jupiter.api.Assertions.assertEquals;import java.util.logging.Logger;import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.DisplayName;import org.junit.jupiter.api.RepeatedTest;import org.junit.jupiter.api.RepetitionInfo;import org.junit.jupiter.api.TestInfo;class RepeatedTestsDemo &#123; private Logger logger = // ... @BeforeEach void beforeEach(TestInfo testInfo, RepetitionInfo repetitionInfo) &#123; int currentRepetition = repetitionInfo.getCurrentRepetition(); int totalRepetitions = repetitionInfo.getTotalRepetitions(); String methodName = testInfo.getTestMethod().get().getName(); logger.info(String.format("About to execute repetition %d of %d for %s", // currentRepetition, totalRepetitions, methodName)); &#125; @RepeatedTest(10) void repeatedTest() &#123; // ... &#125; @RepeatedTest(5) void repeatedTestWithRepetitionInfo(RepetitionInfo repetitionInfo) &#123; assertEquals(5, repetitionInfo.getTotalRepetitions()); &#125; @RepeatedTest(value = 1, name = "&#123;displayName&#125; &#123;currentRepetition&#125;/&#123;totalRepetitions&#125;") @DisplayName("Repeat!") void customDisplayName(TestInfo testInfo) &#123; assertEquals(testInfo.getDisplayName(), "Repeat! 1/1"); &#125; @RepeatedTest(value = 1, name = RepeatedTest.LONG_DISPLAY_NAME) @DisplayName("Details...") void customDisplayNameWithLongPattern(TestInfo testInfo) &#123; assertEquals(testInfo.getDisplayName(), "Details... :: repetition 1 of 1"); &#125; @RepeatedTest(value = 5, name = "Wiederholung &#123;currentRepetition&#125; von &#123;totalRepetitions&#125;") void repeatedTestInGerman() &#123; // ... &#125;&#125; 3.9. 参数化测试 @ParameterizedTest@ValueSource(strings = &#123; "racecar", "radar", "able was I ere I saw elba" &#125;)void palindromes(String candidate) &#123; assertTrue(isPalindrome(candidate));&#125; 4. 引用和引申 Github 官方用户手册 Javadoc 版本声明 官方示例]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mockito 快速指南]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavalib%2Fmockito%2F</url>
    <content type="text"><![CDATA[mockito 快速指南 Mockito 是一个针对 Java 的 mock 框架。 预备知识 使用 mock 对象来进行测试 单元测试的目标和挑战 测试类的分类 Mock 对象的产生 使用 Mockito 生成 Mock 对象 为自己的项目添加 Mockito 依赖 在 Gradle 添加 Mockito 依赖 在 Maven 添加 Mockito 依赖 在 Eclipse IDE 使用 Mockito 以 OSGi 或者 Eclipse 插件形式添加 Mockito 依赖 使用 Mockito API 静态引用 使用 Mockito 创建和配置 mock 对象 配置 mock 验证 mock 对象方法是否被调用 使用 Spy 封装 java 对象 使用 @InjectMocks 在 Mockito 中进行依赖注入 捕捉参数 Mockito 的限制 在 Android 中使用 Mockito 实例：使用 Mockito 写一个 Instrumented Unit Test 创建一个测试的 Android 应用 在 app/build.gradle 文件中添加 Mockito 依赖 创建测试 实例：使用 Mockito 创建一个 mock 对象 目标 创建一个 Twitter API 的例子 模拟 ITweet 的实例 验证方法调用 验证 模拟静态方法 使用 Powermock 来模拟静态方法 用封装的方法代替 Powermock 引用和引申 预备知识 如果需要往下学习，你需要先理解 Junit 框架中的单元测试。 如果你不熟悉 JUnit，请查看下面的教程： http://www.vogella.com/tutorials/JUnit/article.html 使用 mock 对象来进行测试 单元测试的目标和挑战 单元测试的思路是在不涉及依赖关系的情况下测试代码（隔离性），所以测试代码与其他类或者系统的关系应该尽量被消除。一个可行的消除方法是替换掉依赖类（测试替换），也就是说我们可以使用替身来替换掉真正的依赖对象。 测试类的分类 dummy object 做为参数传递给方法但是绝对不会被使用。譬如说，这种测试类内部的方法不会被调用，或者是用来填充某个方法的参数。 Fake 是真正接口或抽象类的实现体，但给对象内部实现很简单。譬如说，它存在内存中而不是真正的数据库中。（译者注：Fake 实现了真正的逻辑，但它的存在只是为了测试，而不适合于用在产品中。） stub 类是依赖类的部分方法实现，而这些方法在你测试类和接口的时候会被用到，也就是说 stub 类在测试中会被实例化。stub 类会回应任何外部测试的调用。stub 类有时候还会记录调用的一些信息。 mock object 是指类或者接口的模拟实现，你可以自定义这个对象中某个方法的输出结果。 测试替代技术能够在测试中模拟测试类以外对象。因此你可以验证测试类是否响应正常。譬如说，你可以验证在 Mock 对象的某一个方法是否被调用。这可以确保隔离了外部依赖的干扰只测试测试类。 我们选择 Mock 对象的原因是因为 Mock 对象只需要少量代码的配置。 Mock 对象的产生 你可以手动创建一个 Mock 对象或者使用 Mock 框架来模拟这些类，Mock 框架允许你在运行时创建 Mock 对象并且定义它的行为。 一个典型的例子是把 Mock 对象模拟成数据的提供者。在正式的生产环境中它会被实现用来连接数据源。但是我们在测试的时候 Mock 对象将会模拟成数据提供者来确保我们的测试环境始终是相同的。 Mock 对象可以被提供来进行测试。因此，我们测试的类应该避免任何外部数据的强依赖。 通过 Mock 对象或者 Mock 框架，我们可以测试代码中期望的行为。譬如说，验证只有某个存在 Mock 对象的方法是否被调用了。 使用 Mockito 生成 Mock 对象 Mockito 是一个流行 mock 框架，可以和 JUnit 结合起来使用。Mockito 允许你创建和配置 mock 对象。使用 Mockito 可以明显的简化对外部依赖的测试类的开发。 一般使用 Mockito 需要执行下面三步 模拟并替换测试代码中外部依赖 执行测试代码 验证测试代码是否被正确的执行 为自己的项目添加 Mockito 依赖 在 Gradle 添加 Mockito 依赖 如果你的项目使用 Gradle 构建，将下面代码加入 Gradle 的构建文件中为自己项目添加 Mockito 依赖 repositories &#123; jcenter() &#125;dependencies &#123; testCompile "org.mockito:mockito-core:2.0.57-beta" &#125; 在 Maven 添加 Mockito 依赖 需要在 Maven 声明依赖，您可以在 http://search.maven.org 网站中搜索 g:“org.mockito”, a:“mockito-core” 来得到具体的声明方式。 在 Eclipse IDE 使用 Mockito Eclipse IDE 支持 Gradle 和 Maven 两种构建工具，所以在 Eclipse IDE 添加依赖取决你使用的是哪一个构建工具。 以 OSGi 或者 Eclipse 插件形式添加 Mockito 依赖 在 Eclipse RCP 应用依赖通常可以在 p2 update 上得到。Orbit 是一个很好的第三方仓库，我们可以在里面寻找能在 Eclipse 上使用的应用和插件。 Orbit 仓库地址：http://download.eclipse.org/tools/orbit/downloads 使用 Mockito API 静态引用 如果在代码中静态引用了org.mockito.Mockito.*;，那你你就可以直接调用静态方法和静态变量而不用创建对象，譬如直接调用 mock() 方法。 使用 Mockito 创建和配置 mock 对象 除了上面所说的使用 mock() 静态方法外，Mockito 还支持通过 @Mock 注解的方式来创建 mock 对象。 如果你使用注解，那么必须要实例化 mock 对象。Mockito 在遇到使用注解的字段的时候，会调用MockitoAnnotations.initMocks(this) 来初始化该 mock 对象。另外也可以通过使用@RunWith(MockitoJUnitRunner.class)来达到相同的效果。 通过下面的例子我们可以了解到使用@Mock 的方法和MockitoRule规则。 import static org.mockito.Mockito.*;public class MockitoTest &#123; @Mock MyDatabase databaseMock; (1) @Rule public MockitoRule mockitoRule = MockitoJUnit.rule(); (2) @Test public void testQuery() &#123; ClassToTest t = new ClassToTest(databaseMock); (3) boolean check = t.query("* from t"); (4) assertTrue(check); (5) verify(databaseMock).query("* from t"); (6) &#125;&#125; 告诉 Mockito 模拟 databaseMock 实例 Mockito 通过 @mock 注解创建 mock 对象 使用已经创建的 mock 初始化这个类 在测试环境下，执行测试类中的代码 使用断言确保调用的方法返回值为 true 验证 query 方法是否被 MyDatabase 的 mock 对象调用 配置 mock 当我们需要配置某个方法的返回值的时候，Mockito 提供了链式的 API 供我们方便的调用 when(….).thenReturn(….)可以被用来定义当条件满足时函数的返回值，如果你需要定义多个返回值，可以多次定义。当你多次调用函数的时候，Mockito 会根据你定义的先后顺序来返回返回值。Mocks 还可以根据传入参数的不同来定义不同的返回值。譬如说你的函数可以将anyString 或者 anyInt作为输入参数，然后定义其特定的放回值。 import static org.mockito.Mockito.*;import static org.junit.Assert.*;@Testpublic void test1() &#123; // 创建 mock MyClass test = Mockito.mock(MyClass.class); // 自定义 getUniqueId() 的返回值 when(test.getUniqueId()).thenReturn(43); // 在测试中使用mock对象 assertEquals(test.getUniqueId(), 43);&#125;// 返回多个值@Testpublic void testMoreThanOneReturnValue() &#123; Iterator i= mock(Iterator.class); when(i.next()).thenReturn("Mockito").thenReturn("rocks"); String result=i.next()+" "+i.next(); // 断言 assertEquals("Mockito rocks", result);&#125;// 如何根据输入来返回值@Testpublic void testReturnValueDependentOnMethodParameter() &#123; Comparable c= mock(Comparable.class); when(c.compareTo("Mockito")).thenReturn(1); when(c.compareTo("Eclipse")).thenReturn(2); // 断言 assertEquals(1,c.compareTo("Mockito"));&#125;// 如何让返回值不依赖于输入@Testpublic void testReturnValueInDependentOnMethodParameter() &#123; Comparable c= mock(Comparable.class); when(c.compareTo(anyInt())).thenReturn(-1); // 断言 assertEquals(-1 ,c.compareTo(9));&#125;// 根据参数类型来返回值@Testpublic void testReturnValueInDependentOnMethodParameter() &#123; Comparable c= mock(Comparable.class); when(c.compareTo(isA(Todo.class))).thenReturn(0); // 断言 Todo todo = new Todo(5); assertEquals(todo ,c.compareTo(new Todo(1)));&#125; 对于无返回值的函数，我们可以使用doReturn(…).when(…).methodCall来获得类似的效果。例如我们想在调用某些无返回值函数的时候抛出异常，那么可以使用doThrow 方法。如下面代码片段所示 import static org.mockito.Mockito.*;import static org.junit.Assert.*;// 下面测试用例描述了如何使用doThrow()方法@Test(expected=IOException.class)public void testForIOException() &#123; // 创建并配置 mock 对象 OutputStream mockStream = mock(OutputStream.class); doThrow(new IOException()).when(mockStream).close(); // 使用 mock OutputStreamWriter streamWriter= new OutputStreamWriter(mockStream); streamWriter.close();&#125; 验证 mock 对象方法是否被调用 Mockito 会跟踪 mock 对象里面所有的方法和变量。所以我们可以用来验证函数在传入特定参数的时候是否被调用。这种方式的测试称行为测试，行为测试并不会检查函数的返回值，而是检查在传入正确参数时候函数是否被调用。 import static org.mockito.Mockito.*;@Testpublic void testVerify() &#123; // 创建并配置 mock 对象 MyClass test = Mockito.mock(MyClass.class); when(test.getUniqueId()).thenReturn(43); // 调用mock对象里面的方法并传入参数为12 test.testing(12); test.getUniqueId(); test.getUniqueId(); // 查看在传入参数为12的时候方法是否被调用 verify(test).testing(Matchers.eq(12)); // 方法是否被调用两次 verify(test, times(2)).getUniqueId(); // 其他用来验证函数是否被调用的方法 verify(mock, never()).someMethod("never called"); verify(mock, atLeastOnce()).someMethod("called at least once"); verify(mock, atLeast(2)).someMethod("called at least twice"); verify(mock, times(5)).someMethod("called five times"); verify(mock, atMost(3)).someMethod("called at most 3 times");&#125; 使用 Spy 封装 java 对象 @Spy 或者spy()方法可以被用来封装 java 对象。被封装后，除非特殊声明（打桩 stub），否则都会真正的调用对象里面的每一个方法 import static org.mockito.Mockito.*;// Lets mock a LinkedListList list = new LinkedList();List spy = spy(list);// 可用 doReturn() 来打桩doReturn("foo").when(spy).get(0);// 下面代码不生效// 真正的方法会被调用// 将会抛出 IndexOutOfBoundsException 的异常，因为 List 为空when(spy.get(0)).thenReturn("foo"); 方法verifyNoMoreInteractions()允许你检查没有其他的方法被调用了。 使用 @InjectMocks 在 Mockito 中进行依赖注入 我们也可以使用@InjectMocks 注解来创建对象，它会根据类型来注入对象里面的成员方法和变量。假定我们有 ArticleManager 类 public class ArticleManager &#123; private User user; private ArticleDatabase database; ArticleManager(User user) &#123; this.user = user; &#125; void setDatabase(ArticleDatabase database) &#123; &#125;&#125; 这个类会被 Mockito 构造，而类的成员方法和变量都会被 mock 对象所代替，正如下面的代码片段所示： @RunWith(MockitoJUnitRunner.class)public class ArticleManagerTest &#123; @Mock ArticleCalculator calculator; @Mock ArticleDatabase database; @Most User user; @Spy private UserProvider userProvider = new ConsumerUserProvider(); @InjectMocks private ArticleManager manager; (1) @Test public void shouldDoSomething() &#123; // 假定 ArticleManager 有一个叫 initialize() 的方法被调用了 // 使用 ArticleListener 来调用 addListener 方法 manager.initialize(); // 验证 addListener 方法被调用 verify(database).addListener(any(ArticleListener.class)); &#125;&#125; 创建 ArticleManager 实例并注入 Mock 对象 更多的详情可以查看 http://docs.mockito.googlecode.com/hg/1.9.5/org/mockito/InjectMocks.html. 捕捉参数 ArgumentCaptor类允许我们在 verification 期间访问方法的参数。得到方法的参数后我们可以使用它进行测试。 import static org.hamcrest.Matchers.hasItem;import static org.junit.Assert.assertThat;import static org.mockito.Mockito.mock;import static org.mockito.Mockito.verify;import java.util.Arrays;import java.util.List;import org.junit.Rule;import org.junit.Test;import org.mockito.ArgumentCaptor;import org.mockito.Captor;import org.mockito.junit.MockitoJUnit;import org.mockito.junit.MockitoRule;public class MockitoTests &#123; @Rule public MockitoRule rule = MockitoJUnit.rule(); @Captor private ArgumentCaptor&lt;List&lt;String&gt;&gt; captor; @Test public final void shouldContainCertainListItem() &#123; List&lt;String&gt; asList = Arrays.asList("someElement_test", "someElement"); final List&lt;String&gt; mockedList = mock(List.class); mockedList.addAll(asList); verify(mockedList).addAll(captor.capture()); final List&lt;String&gt; capturedArgument = captor.getValue(); assertThat(capturedArgument, hasItem("someElement")); &#125;&#125; Mockito 的限制 Mockito 当然也有一定的限制。而下面三种数据类型则不能够被测试 final classes anonymous classes primitive types 在 Android 中使用 Mockito 在 Android 中的 Gradle 构建文件中加入 Mockito 依赖后就可以直接使用 Mockito 了。若想使用 Android Instrumented tests 的话，还需要添加 dexmaker 和 dexmaker-mockito 依赖到 Gradle 的构建文件中。（需要 Mockito 1.9.5 版本以上） dependencies &#123; testCompile 'junit:junit:4.12' // Mockito unit test 的依赖 testCompile 'org.mockito:mockito-core:1.+' // Mockito Android instrumentation tests 的依赖 androidTestCompile 'org.mockito:mockito-core:1.+' androidTestCompile "com.google.dexmaker:dexmaker:1.2" androidTestCompile "com.google.dexmaker:dexmaker-mockito:1.2"&#125; 实例：使用 Mockito 写一个 Instrumented Unit Test 创建一个测试的 Android 应用 创建一个包名为com.vogella.android.testing.mockito.contextmock的 Android 应用，添加一个静态方法 ，方法里面创建一个包含参数的 Intent，如下代码所示： public static Intent createQuery(Context context, String query, String value) &#123; // 简单起见，重用MainActivity Intent i = new Intent(context, MainActivity.class); i.putExtra("QUERY", query); i.putExtra("VALUE", value); return i;&#125; 在 app/build.gradle 文件中添加 Mockito 依赖 dependencies &#123; // Mockito 和 JUnit 的依赖 // instrumentation unit tests on the JVM androidTestCompile 'junit:junit:4.12' androidTestCompile 'org.mockito:mockito-core:2.0.57-beta' androidTestCompile 'com.android.support.test:runner:0.3' androidTestCompile "com.google.dexmaker:dexmaker:1.2" androidTestCompile "com.google.dexmaker:dexmaker-mockito:1.2" // Mockito 和 JUnit 的依赖 // tests on the JVM testCompile 'junit:junit:4.12' testCompile 'org.mockito:mockito-core:1.+'&#125; 创建测试 使用 Mockito 创建一个单元测试来验证在传递正确 extra data 的情况下，intent 是否被触发。 因此我们需要使用 Mockito 来 mock 一个Context对象，如下代码所示： package com.vogella.android.testing.mockitocontextmock;import android.content.Context;import android.content.Intent;import android.os.Bundle;import org.junit.Test;import org.junit.runner.RunWith;import org.mockito.Mockito;import static org.junit.Assert.assertEquals;import static org.junit.Assert.assertNotNull;public class TextIntentCreation &#123; @Test public void testIntentShouldBeCreated() &#123; Context context = Mockito.mock(Context.class); Intent intent = MainActivity.createQuery(context, "query", "value"); assertNotNull(intent); Bundle extras = intent.getExtras(); assertNotNull(extras); assertEquals("query", extras.getString("QUERY")); assertEquals("value", extras.getString("VALUE")); &#125;&#125; 实例：使用 Mockito 创建一个 mock 对象 目标 创建一个 Api，它可以被 Mockito 来模拟并做一些工作 创建一个 Twitter API 的例子 实现 TwitterClient类，它内部使用到了 ITweet 的实现。但是ITweet实例很难得到，譬如说他需要启动一个很复杂的服务来得到。 public interface ITweet &#123; String getMessage();&#125;public class TwitterClient &#123; public void sendTweet(ITweet tweet) &#123; String message = tweet.getMessage(); // send the message to Twitter &#125;&#125; 模拟 ITweet 的实例 为了能够不启动复杂的服务来得到 ITweet，我们可以使用 Mockito 来模拟得到该实例。 @Testpublic void testSendingTweet() &#123; TwitterClient twitterClient = new TwitterClient(); ITweet iTweet = mock(ITweet.class); when(iTweet.getMessage()).thenReturn("Using mockito is great"); twitterClient.sendTweet(iTweet);&#125; 现在 TwitterClient 可以使用 ITweet 接口的实现，当调用 getMessage() 方法的时候将会打印 “Using Mockito is great” 信息。 验证方法调用 确保 getMessage() 方法至少调用一次。 @Testpublic void testSendingTweet() &#123; TwitterClient twitterClient = new TwitterClient(); ITweet iTweet = mock(ITweet.class); when(iTweet.getMessage()).thenReturn("Using mockito is great"); twitterClient.sendTweet(iTweet); verify(iTweet, atLeastOnce()).getMessage();&#125; 验证 运行测试，查看代码是否测试通过。 模拟静态方法 使用 Powermock 来模拟静态方法 因为 Mockito 不能够 mock 静态方法，因此我们可以使用 Powermock。 import java.net.InetAddress;import java.net.UnknownHostException;public final class NetworkReader &#123; public static String getLocalHostname() &#123; String hostname = ""; try &#123; InetAddress addr = InetAddress.getLocalHost(); // Get hostname hostname = addr.getHostName(); &#125; catch ( UnknownHostException e ) &#123; &#125; return hostname; &#125;&#125; 我们模拟了 NetworkReader 的依赖，如下代码所示： import org.junit.runner.RunWith;import org.powermock.core.classloader.annotations.PrepareForTest;@RunWith( PowerMockRunner.class )@PrepareForTest( NetworkReader.class )public class MyTest &#123;// 测试代码 @Testpublic void testSomething() &#123; mockStatic( NetworkUtil.class ); when( NetworkReader.getLocalHostname() ).andReturn( "localhost" ); // 与 NetworkReader 协作的测试&#125; 用封装的方法代替 Powermock 有时候我们可以在静态方法周围包含非静态的方法来达到和 Powermock 同样的效果。 class FooWraper &#123; void someMethod() &#123; Foo.someStaticMethod() &#125;&#125; 引用和引申 官网 Github 使用强大的 Mockito 测试框架来测试你的代码]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 技巧]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fappendix%2Fdocker-recipe%2F</url>
    <content type="text"><![CDATA[Docker 技巧 命令技巧 停止所有的 docker container 这样才能够删除其中的images docker stop $(docker ps -a -q) 删除所有的 docker container docker rm $(docker ps -a -q) 删除所有的 docker images docker rmi $(docker images -q)]]></content>
  </entry>
  <entry>
    <title><![CDATA[树]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Falgorithm%2Fdata-structure%2Ftree%2Ftree%2F</url>
    <content type="text"><![CDATA[树 概念 什么是树？ 在计算器科学中，树（英语：tree）是一种抽象数据类型（ADT）或是实现这种抽象数据类型的数据结构，用来模拟具有树状结构性质的数据集合。它是由 n（n&gt;0）个有限节点组成一个具有层次关系的集合。把它叫做“树”是因为它看起来像一棵倒挂的树，也就是说它是根朝上，而叶朝下的。它具有以下的特点： 每个节点有零个或多个子节点； 没有父节点的节点称为根节点； 每一个非根节点有且只有一个父节点； 除了根节点外，每个子节点可以分为多个不相交的子树； 树的术语 节点的度 - 一个节点含有的子树的个数称为该节点的度； 树的度 - 一棵树中，最大的节点的度称为树的度； 叶节点或终端节点 - 度为零的节点； 非终端节点或分支节点 - 度不为零的节点； 父亲节点或父节点 - 若一个节点含有子节点，则这个节点称为其子节点的父节点； 孩子节点或子节点 - 一个节点含有的子树的根节点称为该节点的子节点； 兄弟节点 - 具有相同父节点的节点互称为兄弟节点； 节点的层次 - 从根开始定义起，根为第 1 层，根的子节点为第 2 层，以此类推； 深度 - 对于任意节点 n,n 的深度为从根到 n 的唯一路径长，根的深度为 0； 高度 - 对于任意节点 n,n 的高度为从 n 到一片树叶的最长路径长，所有树叶的高度为 0； 堂兄弟节点 - 父节点在同一层的节点互为堂兄弟； 节点的祖先 - 从根到该节点所经分支上的所有节点； 子孙 - 以某节点为根的子树中任一节点都称为该节点的子孙。 森林 - 由 m（m&gt;=0）棵互不相交的树的集合称为森林； 树的种类 无序树 - 树中任意节点的子节点之间没有顺序关系，这种树称为无序树，也称为自由树； 有序树 - 树中任意节点的子节点之间有顺序关系，这种树称为有序树； 二叉树 - 每个节点最多含有两个子树的树称为二叉树； 完全二叉树 - 对于一颗二叉树，假设其深度为 d（d&gt;1）。除了第 d 层外，其它各层的节点数目均已达最大值，且第 d 层所有节点从左向右连续地紧密排列，这样的二叉树被称为完全二叉树； 满二叉树 - 所有叶节点都在最底层的完全二叉树； 平衡二叉树（AVL 树） - 当且仅当任何节点的两棵子树的高度差不大于 1 的二叉树； 排序二叉树(二叉查找树（英语 - Binary Search Tree)) - 也称二叉搜索树、有序二叉树； 霍夫曼树 - 带权路径最短的二叉树称为哈夫曼树或最优二叉树； B 树 - 一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多于两个子树。 更多内容 https://zh.wikipedia.org/wiki/树_(数据结构)]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongodb 安装]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fnosql%2Fmongodb%2Finstall-mongodb%2F</url>
    <content type="text"><![CDATA[Mongodb 安装 安装 启动 脚本 安装 安装步骤如下： （1）下载并解压到本地 进入官网下载地址：https://www.mongodb.com/download-center#community ，选择合适的版本下载。 我选择的是最新稳定版本 3.6.3：https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.6.3.tgz 我个人喜欢存放在：/opt/mongodb wget -O /opt/mongodb/mongodb-linux-x86_64-3.6.3.tgz https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.6.3.tgzcd /opt/mongodbtar zxvf mongodb-linux-x86_64-3.6.3.tgzmv mongodb-linux-x86_64-3.6.3 mongodb-3.6.3mkdir -p /data/db 启动 启动 mongodb 服务 cd /opt/mongodb/mongodb-3.6.3/bin./mongod --dbpath=/data/db 启动 mongodb 客户端 cd /opt/mongodb/mongodb-3.6.3/bin./mongo 脚本 | 安装脚本 |]]></content>
  </entry>
  <entry>
    <title><![CDATA[MyBatis 指南]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Form%2Fmybatis%2F</url>
    <content type="text"><![CDATA[MyBatis 指南 MyBatis 的前身就是 iBatis ，是一个作用在数据持久层的对象关系映射（Object Relational Mapping，简称 ORM）框架。 简介 什么是 MyBatis？ MyBatis vs. Hibernate 入门 核心 API SqlSessionFactoryBuilder SqlSessionFactory SqlSession 映射器 原理 MyBatis 的架构 接口层 数据处理层 框架支撑层 引导层 主要组件 配置 SQL XML 映射文件 Mybatis 扩展 资料 简介 什么是 MyBatis？ MyBatis 是一款持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生类型、接口和 Java 的 POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。 MyBatis vs. Hibernate MyBatis 和 Hibernate 都是 Java 世界中比较流行的 ORM 框架。我们应该了解其各自的优势，根据项目的需要去抉择在开发中使用哪个框架。 Mybatis 优势 MyBatis 可以进行更为细致的 SQL 优化，可以减少查询字段。 MyBatis 容易掌握，而 Hibernate 门槛较高。 Hibernate 优势 Hibernate 的 DAO 层开发比 MyBatis 简单，Mybatis 需要维护 SQL 和结果映射。 Hibernate 对对象的维护和缓存要比 MyBatis 好，对增删改查的对象的维护要方便。 Hibernate 数据库移植性很好，MyBatis 的数据库移植性不好，不同的数据库需要写不同 SQL。 Hibernate 有更好的二级缓存机制，可以使用第三方缓存。MyBatis 本身提供的缓存机制不佳。 入门 Mybatis 官方文档之入门 已经写得很简洁易懂，不再赘述。 核心 API 核心 API 请参考：「 Mybatis 官方文档之 Java API 」 SqlSessionFactoryBuilder SqlSessionFactoryBuilder 职责 SqlSessionFactoryBuilder 负责创建 SqlSessionFactory 实例。而 SqlSessionFactoryBuilder 则可以从 XML 配置文件或一个预先定制的 Configuration 的实例构建出 SqlSessionFactory 的实例。 SqlSessionFactoryBuilder 生命周期 这个类可以被实例化、使用和丢弃，一旦创建了 SqlSessionFactory，就不再需要它了。 因此 SqlSessionFactoryBuilder 实例的最佳作用域是方法作用域（也就是局部方法变量）。 SqlSessionFactory SqlSessionFactory 职责 SqlSessionFactory 负责创建 SqlSession 实例。 SqlSessionFactory 生命周期 SqlSessionFactory 一旦被创建就应该在应用的运行期间一直存在，没有任何理由丢弃它或重新创建另一个实例。最简单的方法就是使用单例模式或者静态单例模式。 SqlSession SqlSession 职责 MyBatis 的主要 Java 接口就是 SqlSession。你可以通过这个接口来执行命令，获取映射器和管理事务。 详细内容可以参考：「 Mybatis 官方文档之 sqlSessions 」 。 SqlSession 生命周期 注意：当 Mybatis 与一些依赖注入框架（如 Spring 或者 Guice）同时使用时，SqlSessions 将被依赖注入框架所创建，所以你不需要使用 SqlSessionFactoryBuilder 或者 SqlSessionFactory。 每个线程都应该有它自己的 SqlSession 实例。 SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的作用域是请求或方法作用域。 绝对不能将 SqlSession 实例的引用放在一个类的静态域，甚至一个类的实例变量也不行。 也绝不能将 SqlSession 实例的引用放在任何类型的托管作用域中，比如 Servlet 框架中的 HttpSession。 正确在 Web 中使用 SqlSession 的场景是：每次收到的 HTTP 请求，就可以打开一个 SqlSession，返回一个响应，就关闭它。 编程模式： SqlSession session = sqlSessionFactory.openSession();try &#123; // 你的应用逻辑代码&#125; finally &#123; session.close();&#125; 映射器 映射器职责 映射器是一些由你创建的、绑定你映射的语句的接口。 各个 insert、update、delete 和 select 方法都很强大，但也有些繁琐。更通用的方式是使用映射器类来执行映射语句。一个映射器类就是一个仅需声明与 SqlSession 方法相匹配的方法的接口类。 下面的示例展示了一些方法签名以及它们是如何映射到 SqlSession 上的。 public interface AuthorMapper &#123; // (Author) selectOne("selectAuthor",5); Author selectAuthor(int id); // (List&lt;Author&gt;) selectList(“selectAuthors”) List&lt;Author&gt; selectAuthors(); // (Map&lt;Integer,Author&gt;) selectMap("selectAuthors", "id") @MapKey("id") Map&lt;Integer, Author&gt; selectAuthors(); // insert("insertAuthor", author) int insertAuthor(Author author); // updateAuthor("updateAuthor", author) int updateAuthor(Author author); // delete("deleteAuthor",5) int deleteAuthor(int id);&#125; 注意 映射器接口不需要去实现任何接口或继承自任何类。只要方法可以被唯一标识对应的映射语句就可以了。 映射器接口可以继承自其他接口。当使用 XML 来构建映射器接口时要保证语句被包含在合适的命名空间中。而且，唯一的限制就是你不能在两个继承关系的接口中拥有相同的方法签名（潜在的危险做法不可取）。 映射器生命周期 映射器接口的实例是从 SqlSession 中获得的。因此从技术层面讲，任何映射器实例的最大作用域是和请求它们的 SqlSession 相同的。尽管如此，映射器实例的最佳作用域是方法作用域。 也就是说，映射器实例应该在调用它们的方法中被请求，用过之后即可丢弃。 编程模式： SqlSession session = sqlSessionFactory.openSession();try &#123; BlogMapper mapper = session.getMapper(BlogMapper.class); // 你的应用逻辑代码&#125; finally &#123; session.close();&#125; 映射器注解 MyBatis 是一个 XML 驱动的框架。配置信息是基于 XML 的，而且映射语句也是定义在 XML 中的。MyBatis 3 以后，支持注解配置。注解配置基于配置 API；而配置 API 基于 XML 配置。 Mybatis 支持诸如 @Insert、@Update、@Delete、@Select、@Result 等注解。 详细内容请参考：Mybatis 官方文档之 sqlSessions，其中列举了 Mybatis 支持的注解清单，以及基本用法。 原理 MyBatis 的架构 接口层 接口层负责和数据库交互的方式 MyBatis 和数据库的交互有两种方式： 使用传统的 MyBatis 提供的 API； 使用 Mapper 接口 使用传统的 MyBatis 提供的 API 这是传统的传递 Statement Id 和查询参数给 SqlSession 对象，使用 SqlSession 对象完成和数据库的交互；MyBatis 提供了非常方便和简单的 API，供用户实现对数据库的增删改查数据操作，以及对数据库连接信息和 MyBatis 自身配置信息的维护操作。 上述使用 MyBatis 的方法，是创建一个和数据库打交道的 SqlSession 对象，然后根据 Statement Id 和参数来操作数据库，这种方式固然很简单和实用，但是它不符合面向对象语言的概念和面向接口编程的编程习惯。由于面向接口的编程是面向对象的大趋势，MyBatis 为了适应这一趋势，增加了第二种使用 MyBatis 支持接口（Interface）调用方式。 使用 Mapper 接口 MyBatis 将配置文件中的每一个 &lt;mapper&gt; 节点抽象为一个 Mapper 接口，而这个接口中声明的方法和跟 &lt;mapper&gt; 节点中的 &lt;select|update|delete|insert&gt; 节点相对应，即 &lt;select|update|delete|insert&gt; 节点的 id 值为 Mapper 接口中的方法名称，parameterType 值表示 Mapper 对应方法的入参类型，而 resultMap 值则对应了 Mapper 接口表示的返回值类型或者返回结果集的元素类型。 根据 MyBatis 的配置规范配置好后，通过 SqlSession.getMapper(XXXMapper.class) 方法，MyBatis 会根据相应的接口声明的方法信息，通过动态代理机制生成一个 Mapper 实例，我们使用 Mapper 接口的某一个方法时，MyBatis 会根据这个方法的方法名和参数类型，确定 Statement Id，底层还是通过SqlSession.select(&quot;statementId&quot;,parameterObject); 或者 SqlSession.update(&quot;statementId&quot;,parameterObject); 等等来实现对数据库的操作。 MyBatis 引用 Mapper 接口这种调用方式，纯粹是为了满足面向接口编程的需要。（其实还有一个原因是在于，面向接口的编程，使得用户在接口上可以使用注解来配置 SQL 语句，这样就可以脱离 XML 配置文件，实现“0 配置”）。 数据处理层 数据处理层可以说是 MyBatis 的核心，从大的方面上讲，它要完成两个功能： 通过传入参数构建动态 SQL 语句； SQL 语句的执行以及封装查询结果集成 List&lt;E&gt; 参数映射和动态 SQL 语句生成 动态语句生成可以说是 MyBatis 框架非常优雅的一个设计，MyBatis 通过传入的参数值，使用 Ognl 来动态地构造 SQL 语句，使得 MyBatis 有很强的灵活性和扩展性。 参数映射指的是对于 java 数据类型和 jdbc 数据类型之间的转换：这里有包括两个过程：查询阶段，我们要将 java 类型的数据，转换成 jdbc 类型的数据，通过 preparedStatement.setXXX() 来设值；另一个就是对 resultset 查询结果集的 jdbcType 数据转换成 java 数据类型。 SQL 语句的执行以及封装查询结果集成 List&lt;E&gt; 动态 SQL 语句生成之后，MyBatis 将执行 SQL 语句，并将可能返回的结果集转换成 List&lt;E&gt; 列表。MyBatis 在对结果集的处理中，支持结果集关系一对多和多对一的转换，并且有两种支持方式，一种为嵌套查询语句的查询，还有一种是嵌套结果集的查询。 框架支撑层 事务管理机制 对数据库的事务而言，应该具有以下几点：创建（create）、提交（commit）、回滚（rollback）、关闭（close）。对应地，MyBatis 将事务抽象成了 Transaction 接口。 MyBatis 的事务管理分为两种形式： 使用 JDBC 的事务管理机制：即利用 java.sql.Connection 对象完成对事务的提交（commit()）、回滚（rollback()）、关闭（close()）等。 使用 MANAGED 的事务管理机制：这种机制 MyBatis 自身不会去实现事务管理，而是让程序的容器如（JBOSS，Weblogic）来实现对事务的管理。 连接池管理机制 由于创建一个数据库连接所占用的资源比较大， 对于数据吞吐量大和访问量非常大的应用而言，连接池的设计就显得非常重要，对于连接池管理机制我已经在我的博文《深入理解 mybatis 原理》 Mybatis 数据源与连接池 中有非常详细的讨论，感兴趣的读者可以点击查看。 缓存机制 MyBatis 将数据缓存设计成两级结构，分为一级缓存、二级缓存： 一级缓存是 Session 会话级别的缓存，位于表示一次数据库会话的 SqlSession 对象之中，又被称之为本地缓存。一级缓存是 MyBatis 内部实现的一个特性，用户不能配置，默认情况下自动支持的缓存，用户没有定制它的权利（不过这也不是绝对的，可以通过开发插件对它进行修改）； 二级缓存是 Application 应用级别的缓存，它的是生命周期很长，跟 Application 的声明周期一样，也就是说它的作用范围是整个 Application 应用。 一级缓存的工作机制 一级缓存是 Session 会话级别的，一般而言，一个 SqlSession 对象会使用一个 Executor 对象来完成会话操作，Executor 对象会维护一个 Cache 缓存，以提高查询性能。 二级缓存的工作机制 如上所言，一个 SqlSession 对象会使用一个 Executor 对象来完成会话操作，MyBatis 的二级缓存机制的关键就是对这个 Executor 对象做文章。如果用户配置了 &quot;cacheEnabled=true&quot;，那么 MyBatis 在为 SqlSession 对象创建 Executor 对象时，会对 Executor 对象加上一个装饰者：CachingExecutor，这时 SqlSession 使用 CachingExecutor 对象来完成操作请求。CachingExecutor 对于查询请求，会先判断该查询请求在 Application 级别的二级缓存中是否有缓存结果，如果有查询结果，则直接返回缓存结果；如果缓存中没有，再交给真正的 Executor 对象来完成查询操作，之后 CachingExecutor 会将真正 Executor 返回的查询结果放置到缓存中，然后在返回给用户。 SQL 语句的配置方式 传统的 MyBatis 配置 SQL 语句方式就是使用 XML 文件进行配置的，但是这种方式不能很好地支持面向接口编程的理念，为了支持面向接口的编程，MyBatis 引入了 Mapper 接口的概念，面向接口的引入，对使用注解来配置 SQL 语句成为可能，用户只需要在接口上添加必要的注解即可，不用再去配置 XML 文件了，但是，目前的 MyBatis 只是对注解配置 SQL 语句提供了有限的支持，某些高级功能还是要依赖 XML 配置文件配置 SQL 语句。 引导层 引导层是配置和启动 MyBatis 配置信息的方式。MyBatis 提供两种方式来引导 MyBatis ： 基于 XML 配置文件的方式 基于 Java API 的方式 主要组件 从 MyBatis 代码实现的角度来看，MyBatis 的主要组件有以下几个： SqlSession - 作为 MyBatis 工作的主要顶层 API，表示和数据库交互的会话，完成必要数据库增删改查功能。 Executor - MyBatis 执行器，是 MyBatis 调度的核心，负责 SQL 语句的生成和查询缓存的维护。 StatementHandler - 封装了 JDBC Statement 操作，负责对 JDBC statement 的操作，如设置参数、将 Statement 结果集转换成 List 集合。 ParameterHandler - 负责对用户传递的参数转换成 JDBC Statement 所需要的参数。 ResultSetHandler - 负责将 JDBC 返回的 ResultSet 结果集对象转换成 List 类型的集合。 TypeHandler - 负责 java 数据类型和 jdbc 数据类型之间的映射和转换。 MappedStatement - MappedStatement 维护了一条 &lt;select|update|delete|insert&gt; 节点的封装。 SqlSource - 负责根据用户传递的 parameterObject，动态地生成 SQL 语句，将信息封装到 BoundSql 对象中，并返回。 BoundSql - 表示动态生成的 SQL 语句以及相应的参数信息。 Configuration - MyBatis 所有的配置信息都维持在 Configuration 对象之中。 它们的关系如下图所示： ## 配置 配置的详细内容请参考：「 Mybatis 官方文档之配置 」 。 MyBatis 的配置文件包含了会深深影响 MyBatis 行为的设置和属性信息。主要配置项有以下： properties（属性） settings（设置） typeAliases（类型别名） typeHandlers（类型处理器） objectFactory（对象工厂） plugins（插件） environments（环境配置） environment（环境变量） transactionManager（事务管理器） dataSource（数据源） databaseIdProvider（数据库厂商标识） mappers（映射器） SQL XML 映射文件 SQL XML 映射文件详细内容请参考：「 Mybatis 官方文档之 XML 映射文件 」。 XML 映射文件只有几个顶级元素： cache – 对给定命名空间的缓存配置。 cache-ref – 对其他命名空间缓存配置的引用。 resultMap – 是最复杂也是最强大的元素，用来描述如何从数据库结果集中来加载对象。 ~~parameterMap – 已被废弃！老式风格的参数映射。更好的办法是使用内联参数，此元素可能在将来被移除。文档中不会介绍此元素。~~ sql – 可被其他语句引用的可重用语句块。 insert – 映射插入语句 update – 映射更新语句 delete – 映射删除语句 select – 映射查询语句 Mybatis 扩展 mybatis-plus - CRUD、代码生成器、分页器等多功能 Mapper - CRUD Mybatis-PageHelper - Mybatis 通用分页插件 资料 官方 | Github | 官网 | MyBatis Generator | Spring 集成 | Spring Boot 集成 | 文章 深入理解 mybatis 原理 mybatis 源码中文注释 - mybatis 源码解读 MyBatis Generator 详解]]></content>
      <categories>
        <category>java</category>
        <category>javaweb</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaweb</tag>
        <tag>orm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系型数据库基本原理]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2F%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[关系型数据库基本原理 本文所述内容主要针对的是关系型数据库，nosql 数据库并不适用。 1. 索引 1.1. 索引的优点和缺点 1.2. 索引类型 1.3. 索引数据结构 1.4. 索引原则 2. 事务 2.1. ACID 2.2. 并发一致性问题 2.3. 事务隔离级别 2.4. 死锁 3. 并发控制 3.1. 锁粒度 3.2. 数据库锁的类型 3.3. 数据库锁的协议 4. 多版本并发控制 4.1. 版本号 4.2. Undo 日志 4.3. 实现过程 4.4. 快照读与当前读 5. 分库分表 5.1. 水平拆分 5.2. 垂直拆分 5.3. Sharding 策略 5.4. 分库分表的问题及解决方案 5.5. 常用的分库分表中间件 6. sql 优化 6.1. 使用执行计划进行分析 6.2. 优化数据访问 6.3. 重构查询方式 7. 关系数据库设计理论 7.1. 函数依赖 7.2. 异常 7.3. 范式 8. ER 图 8.1. 实体的三种联系 8.2. 表示出现多次的关系 8.3. 联系的多向性 8.4. 表示子类 9. 资料 1. 索引 索引能够轻易将查询性能提升几个数量级。 数据量小的表，使用全表扫描比建立索引更高效。 数据量大的表，使用索引更高效。 数据量特大的表，建立和维护索引的代价将会随之增长，可以使用分区技术。 1.1. 索引的优点和缺点 优点： 索引大大减少了服务器需要扫描的数据量。 索引可以帮助服务器避免排序和临时表。 索引可以将随机 I/O 变为顺序 I/O。 缺点： 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立组合索引那么需要的空间就会更大。 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 1.2. 索引类型 主流的关系型数据库一般都支持以下索引类型： 普通索引：最基本的索引，没有任何限制。 唯一索引：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。 主键索引：一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。一般是在建表的时候同时创建主键索引。 组合索引：多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合。 1.3. 索引数据结构 主流数据库的索引一般使用的数据结构为：B-Tree 或 B+Tree。 B-Tree B-Tree 不同于 Binary Tree（二叉树，最多有两个子树），它是平衡搜索树。 一棵 M 阶的 B-Tree 满足以下条件： 每个结点至多有 M 个孩子； 除根结点和叶结点外，其它每个结点至少有 M/2 个孩子； 根结点至少有两个孩子（除非该树仅包含一个结点）； 所有叶结点在同一层，叶结点不包含任何关键字信息； 有 K 个关键字的非叶结点恰好包含 K+1 个孩子； 对于任意结点，其内部的关键字 Key 是升序排列的。每个节点中都包含了 data。 对于每个结点，主要包含一个关键字数组 Key[]，一个指针数组（指向儿子）Son[]。 在 B-Tree 内，查找的流程是： 使用顺序查找（数组长度较短时）或折半查找方法查找 Key[]数组，若找到关键字 K，则返回该结点的地址及 K 在 Key[]中的位置； 否则，可确定 K 在某个 Key[i]和 Key[i+1]之间，则从 Son[i]所指的子结点继续查找，直到在某结点中查找成功； 或直至找到叶结点且叶结点中的查找仍不成功时，查找过程失败。 B+Tree B+Tree 是 B-Tree 的变种： 每个节点的指针上限为 2d 而不是 2d+1（d 为节点的出度）。 非叶子节点不存储 data，只存储 key；叶子节点不存储指针。 由于并不是所有节点都具有相同的域，因此 B+Tree 中叶节点和内节点一般大小不同。这点与 B-Tree 不同，虽然 B-Tree 中不同节点存放的 key 和指针可能数量不一致，但是每个节点的域和上限是一致的，所以在实现中 B-Tree 往往对每个节点申请同等大小的空间。 带有顺序访问指针的 B+Tree 一般在数据库系统或文件系统中使用的 B+Tree 结构都在经典 B+Tree 的基础上进行了优化，增加了顺序访问指针。 在 B+Tree 的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的 B+Tree。 这个优化的目的是为了提高区间访问的性能，例如上图中如果要查询 key 为从 18 到 49 的所有数据记录，当找到 18 后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 Hash Hash 索引只有精确匹配索引所有列的查询才有效。 对于每一行数据，对所有的索引列计算一个 hashcode。哈希索引将所有的 hashcode 存储在索引中，同时在 Hash 表中保存指向每个数据行的指针。 哈希索引的优点： 因为索引数据结构紧凑，所以查询速度非常快。 哈希索引的缺点： 哈希索引数据不是按照索引值顺序存储的，所以无法用于排序。 哈希索引不支持部分索引匹配查找。如，在数据列 (A,B) 上建立哈希索引，如果查询只有数据列 A，无法使用该索引。 哈希索引只支持等值比较查询，不支持任何范围查询，如 WHERE price &gt; 100。 哈希索引有可能出现哈希冲突，出现哈希冲突时，必须遍历链表中所有的行指针，逐行比较，直到找到符合条件的行。 1.4. 索引原则 独立的列 如果查询中的列不是独立的列，则数据库不会使用索引。 “独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。 ❌ 错误示例： SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5;SELECT ... WHERE TO_DAYS(CURRENT_DAT) - TO_DAYS(date_col) &lt;= 10; 前缀索引和索引选择性 有时候需要索引很长的字符列，这会让索引变得大且慢。 解决方法是：可以索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。 索引的选择性是指：不重复的索引值和数据表记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。 对于 BLOB/TEXT/VARCHAR 这种文本类型的列，必须使用前缀索引，因为数据库往往不允许索引这些列的完整长度。 要选择足够长的前缀以保证较高的选择性，同时又不能太长（节约空间）。 多列索引 不要为每个列创建独立的索引。 选择合适的索引列顺序 经验法则：将选择性高的列或基数大的列优先排在多列索引最前列。 但有时，也需要考虑 WHERE 子句中的排序、分组和范围条件等因素，这些因素也会对查询性能造成较大影响。 聚簇索引 聚簇索引不是一种单独的索引类型，而是一种数据存储方式。 聚簇表示数据行和相邻的键值紧凑地存储在一起。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 覆盖索引 索引包含所有需要查询的字段的值。 具有以下优点： 因为索引条目通常远小于数据行的大小，所以若只读取索引，能大大减少数据访问量。 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。 使用索引扫描来做排序 索引最好既满足排序，又用于查找行。这样，就可以使用索引来对结果排序。 = 和 in 可以乱序 比如 a = 1 and b = 2 and c = 3 建立（a,b,c）索引可以任意顺序，mysql 的查询优化器会帮你优化成索引可以识别的形式。 尽量的扩展索引，不要新建索引 比如表中已经有 a 的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 2. 事务 事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。 2.1. ACID 原子性（Automicity） 事务被视为不可分割的最小单元，事务中的所有操作要么全部提交成功，要么全部失败回滚。 回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistency） 数据库在事务执行前后都保持一致性状态。 在一致性状态下，所有事务对一个数据的读取结果都是相同的。 隔离性（Isolation） 一个事务所做的修改在最终提交以前，对其它事务是不可见的。 持久性（Durability） 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。 ACID 小结 事务的 ACID 特性概念简单，但不是很好理解，主要是因为这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时要只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并发执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对数据库奔溃的情况。 2.2. 并发一致性问题 在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题。 丢失修改 T1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。 脏数据 T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 不可重复读 T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 幻影读 T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 并发一致性解决方案 产生并发不一致性问题主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。 并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。 2.3. 事务隔离级别 未提交读（READ UNCOMMITTED） - 事务中的修改，即使没有提交，对其它事务也是可见的。 提交读（READ COMMITTED） - 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。 可重复读（REPEATABLE READ） - 保证在同一个事务中多次读取同样数据的结果是一样的。 可串行化（SERIALIXABLE） - 强制事务串行执行。 隔离级别 脏读 不可重复读 幻影读 未提交读 YES YES YES 提交读 NO YES YES 可重复读 NO NO YES 可串行化 NO NO NO 2.4. 死锁 死锁是指两个或者多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环的现象。当多个事务试图以不同的顺序锁定资源时，就可能会产生死锁。多个事务同时锁定一个资源时，也会产生死锁。 3. 并发控制 无论何时，只要有多个查询需要在同一时刻修改数据，就会产生并发控制的问题。 3.1. 锁粒度 应该尽量只锁定需要修改的那部分数据，而不是所有的资源。在给定的资源上，锁定的数据量越少，则系统的并发程度越高，只要相互之间不发生冲突即可。 但是加锁需要消耗资源，锁的各种操作，包括获取锁、释放锁、以及检查锁状态等，都会增加系统开销。因此锁粒度越小，系统开销就越大。 所谓锁策略，就是在锁的开销和并发程度之间寻求平衡，这种平衡自然也会影响到性能。 很多数据库都提供了表级锁和行级锁。 表级锁（table lock） - 锁定整张表。用户对表进行写操作前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有没有写锁时，其他用户才能获得读锁，读锁之间不会相互阻塞。 行级锁（row lock） - 仅对指定的行记录进行加锁，这样其它进程还是可以对同一个表中的其它记录进行操作。 3.2. 数据库锁的类型 读写锁 排它锁（Exclusive），简写为 X 锁，又称写锁。 共享锁（Shared），简写为 S 锁，又称读锁。 有以下两个规定： 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。 锁的兼容关系如下： - X S X NO NO S NO YES 意向锁 使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。 在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。 意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定： 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁； 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。 通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。 各种锁的兼容关系如下： - X IX S IS X NO NO NO NO IX NO YES NO YES S NO NO YES YES IS NO YES YES YES 解释如下： 任意 IS/IX 锁之间都是兼容的，因为它们只是表示想要对表加锁，而不是真正加锁； S 锁只与 S 锁和 IS 锁兼容，也就是说事务 T 想要对数据行加 S 锁，其它事务可以已经获得对表或者表中的行的 S 锁。 3.3. 数据库锁的协议 三级锁协议 一级锁协议 事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。 可以解决丢失修改问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。 T1 T2 lock-x(A) read A=20 lock-x(A) wait write A=19 . commit . unlock-x(A) . obtain read A=19 write A=21 commit unlock-x(A) 二级锁协议 在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。 可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。 T1 T2 lock-x(A) read A=20 write A=19 lock-s(A) wait rollback . A=20 . unlock-x(A) . obtain read A=20 commit unlock-s(A) 三级锁协议 在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。 可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变。 T1 T2 lock-s(A) read A=20 lock-x(A) wait read A=20 . commit . unlock-s(A) . obtain read A=20 write A=19 commit unlock-X(A) 两段锁协议 加锁和解锁分为两个阶段进行。 可串行化调度是指：通过并发控制，使得并发执行的事务结果与某个串行执行的事务结果相同。 事务遵循两段锁协议是保证可串行化调度的充分条件。例如以下操作满足两段锁协议，它是可串行化调度。 lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B) 但不是必要条件，例如以下操作不满足两段锁协议，但是它还是可串行化调度。 lock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C) 4. 多版本并发控制 多版本并发控制（Multi-Version Concurrency Control, MVCC）是实现隔离级别的一种具体方式。 Mysql、Oracle、PostgreSQL 等数据库都实现了 MVCC，但各自的实现机制不尽相同。 MVCC 用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，无需使用 MVCC；可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。 MVCC 可以视为行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此开销更低。 MVCC 的实现，是通过保存数据在某个时间的快照来实现的。 4.1. 版本号 系统版本号：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。 事务版本号：事务开始时的系统版本号。 MVCC 在每行记录后面都保存着两个隐藏的列，用来存储两个版本号： 创建版本号：指示创建一个数据行的快照时的系统版本号； 删除版本号：如果该快照的删除版本号大于当前事务版本号表示该快照有效，否则表示该快照已经被删除了。 4.2. Undo 日志 MVCC 使用到的快照存储在 Undo 日志中，该日志通过回滚指针把一个数据行（Record）的所有快照连接起来。 4.3. 实现过程 以下实现过程针对可重复读隔离级别。 SELECT 当开始新一个事务时，该事务的版本号肯定会大于当前所有数据行快照的创建版本号，理解这一点很关键。 多个事务必须读取到同一个数据行的快照，并且这个快照是距离现在最近的一个有效快照。但是也有例外，如果有一个事务正在修改该数据行，那么它可以读取事务本身所做的修改，而不用和其它事务的读取结果一致。 把没有对一个数据行做修改的事务称为 T，T 所要读取的数据行快照的创建版本号必须小于 T 的版本号，因为如果大于或者等于 T 的版本号，那么表示该数据行快照是其它事务的最新修改，因此不能去读取它。 除了上面的要求，T 所要读取的数据行快照的删除版本号必须大于 T 的版本号，因为如果小于等于 T 的版本号，那么表示该数据行快照是已经被删除的，不应该去读取它。 INSERT 将当前系统版本号作为数据行快照的创建版本号。 DELETE 将当前系统版本号作为数据行快照的删除版本号。 UPDATE 将当前系统版本号作为更新后的数据行快照的创建版本号，同时将当前系统版本号作为更新前的数据行快照的删除版本号。可以理解为先执行 DELETE 后执行 INSERT。 4.4. 快照读与当前读 快照读 使用 MVCC 读取的是快照中的数据，这样可以减少加锁所带来的开销。 select * from table ...; 当前读 读取的是最新的数据，需要加锁。以下第一个语句需要加 S 锁，其它都需要加 X 锁。 select * from table where ? lock in share mode;select * from table where ? for update;insert;update;delete; 5. 分库分表 分库分表的基本思想就要把一个数据库切分成多个部分放到不同的数据库(server)上，从而缓解单一数据库的性能问题。 当然，现实中更多是这两种情况混杂在一起，这时候需要根据实际情况做出选择，也可能会综合使用垂直与水平切分，从而将原有数据库切分成类似矩阵一样可以无限扩充的数据库(server)阵列。 5.1. 水平拆分 对于海量数据的数据库，如果表并不多，但每张表的数据非常多，这时候适合水平切分，即把表的数据按某种规则（比如按 ID 散列）切分到多个数据库(server)上。 水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。 5.2. 垂直拆分 垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。 如果是因为表多而数据多，这时候适合使用垂直切分，即把关系紧密（比如同一模块）的表切分出来放在一个 server 上。 5.3. Sharding 策略 哈希取模：hash(key) % NUM_DB 范围：可以是 ID 范围也可以是时间范围 映射表：使用单独的一个数据库来存储映射关系 5.4. 分库分表的问题及解决方案 事务问题 方案一：使用分布式事务 优点：交由数据库管理，简单有效 缺点：性能代价高，特别是 shard 越来越多时 方案二：由应用程序和数据库共同控制 原理：将一个跨多个数据库的分布式事务分拆成多个仅处于单个数据库上面的小事务，并通过应用程序来总控各个小事务。 优点：性能上有优势 缺点：需要应用程序在事务控制上做灵活设计。如果使用了 spring 的事务管理，改动起来会面临一定的困难。 跨节点 Join 的问题 只要是进行切分，跨节点 Join 的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的 id，根据这些 id 发起第二次请求得到关联数据。 跨节点的 count,order by,group by 以及聚合函数问题 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。 解决方案：与解决跨节点 join 问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和 join 不同的是每个节点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。 ID 唯一性 一旦数据库被切分到多个物理节点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的 ID 无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得 ID，以便进行 SQL 路由。 一些常见的主键生成策略： 使用全局唯一 ID：GUID。 为每个分片指定一个 ID 范围。 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)。 数据迁移，容量规划，扩容等问题 来自淘宝综合业务平台团队，它利用对 2 的倍数取余具有向前兼容的特性（如对 4 取余得 1 的数对 2 取余也是 1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了 Sharding 扩容的难度。 分库数量 分库数量首先和单库能处理的记录数有关，一般来说，Mysql 单库超过 5000 万条记录，Oracle 单库超过 1 亿条记录，DB 压力就很大(当然处理能力和字段数量/访问模式/记录长度有进一步关系)。 跨分片的排序分页 如果是在前台应用提供分页，则限定用户只能看前面 n 页，这个限制在业务上也是合理的，一般看后面的分页意义不大（如果一定要看，可以要求用户缩小范围重新查询）。 如果是后台批处理任务要求分批获取数据，则可以加大 page size，比如每次获取 5000 条记录，有效减少分页数（当然离线访问一般走备库，避免冲击主库）。 分库设计时，一般还有配套大数据平台汇总所有分库的记录，有些分页查询可以考虑走大数据平台。 5.5. 常用的分库分表中间件 简单易用的组件： 当当 sharding-jdbc 蘑菇街 TSharding 强悍重量级的中间件： sharding TDDL Smart Client 的方式（淘宝） Atlas(Qihoo 360) alibaba.cobar(是阿里巴巴（B2B）部门开发) MyCAT（基于阿里开源的 Cobar 产品而研发） Oceanus(58 同城数据库中间件) OneProxy(支付宝首席架构师楼方鑫开发) vitess（谷歌开发的数据库中间件） 6. sql 优化 6.1. 使用执行计划进行分析 执行计划 Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。 比较重要的字段有： select_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数 更多内容请参考：MySQL 性能优化神器 Explain 使用分析 6.2. 优化数据访问 减少请求的数据量 （一）只返回必要的列 最好不要使用 SELECT * 语句。 （二）只返回必要的行 使用 WHERE 语句进行查询过滤，有时候也需要使用 LIMIT 语句来限制返回的数据。 （三）缓存重复查询的数据 使用缓存可以避免在数据库中进行查询，特别要查询的数据经常被重复查询，缓存可以带来的查询性能提升将会是非常明显的。 减少服务器端扫描的行数 最有效的方式是使用索引来覆盖查询。 6.3. 重构查询方式 切分大查询 一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。 DELEFT FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH); rows_affected = 0do &#123; rows_affected = do_query( "DELETE FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000")&#125; while rows_affected &gt; 0 分解大连接查询 将一个大连接查询（JOIN）分解成对每一个表进行一次单表查询，然后将结果在应用程序中进行关联，这样做的好处有： 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。 减少锁竞争； 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可扩展。 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。 SELECT * FROM tagJOIN tag_post ON tag_post.tag_id=tag.idJOIN post ON tag_post.post_id=post.idWHERE tag.tag='mysql';SELECT * FROM tag WHERE tag='mysql';SELECT * FROM tag_post WHERE tag_id=1234;SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904); 7. 关系数据库设计理论 7.1. 函数依赖 记 A-&gt;B 表示 A 函数决定 B，也可以说 B 函数依赖于 A。 如果 {A1，A2，… ，An} 是关系的一个或多个属性的集合，该集合函数决定了关系的其它所有属性并且是最小的，那么该集合就称为键码。 对于 A-&gt;B，如果能找到 A 的真子集 A’，使得 A’-&gt; B，那么 A-&gt;B 就是部分函数依赖，否则就是完全函数依赖； 对于 A-&gt;B，B-&gt;C，则 A-&gt;C 是一个传递依赖。 7.2. 异常 以下的学生课程关系的函数依赖为 Sno, Cname -&gt; Sname, Sdept, Mname, Grade，键码为 {Sno, Cname}。也就是说，确定学生和课程之后，就能确定其它信息。 Sno Sname Sdept Mname Cname Grade 1 学生-1 学院-1 院长-1 课程-1 90 2 学生-2 学院-2 院长-2 课程-2 80 2 学生-2 学院-2 院长-2 课程-1 100 3 学生-3 学院-2 院长-2 课程-2 95 不符合范式的关系，会产生很多异常，主要有以下四种异常： 冗余数据：例如 学生-2 出现了两次。 修改异常：修改了一个记录中的信息，但是另一个记录中相同的信息却没有被修改。 删除异常：删除一个信息，那么也会丢失其它信息。例如如果删除了 课程-1，需要删除第一行和第三行，那么 学生-1 的信息就会丢失。 插入异常，例如想要插入一个学生的信息，如果这个学生还没选课，那么就无法插入。 7.3. 范式 范式理论是为了解决以上提到四种异常。 高级别范式的依赖于低级别的范式，1NF 是最低级别的范式。 第一范式 (1NF) 属性不可分； 第二范式 (2NF) 每个非主属性完全函数依赖于键码。 可以通过分解来满足。 分解前 Sno Sname Sdept Mname Cname Grade 1 学生-1 学院-1 院长-1 课程-1 90 2 学生-2 学院-2 院长-2 课程-2 80 2 学生-2 学院-2 院长-2 课程-1 100 3 学生-3 学院-2 院长-2 课程-2 95 以上学生课程关系中，{Sno, Cname} 为键码，有如下函数依赖： Sno -&gt; Sname, Sdept Sdept -&gt; Mname Sno, Cname-&gt; Grade Grade 完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。 Sname, Sdept 和 Mname 都部分依赖于键码，当一个学生选修了多门课时，这些数据就会出现多次，造成大量冗余数据。 分解后 关系-1 Sno Sname Sdept Mname 1 学生-1 学院-1 院长-1 2 学生-2 学院-2 院长-2 3 学生-3 学院-2 院长-2 有以下函数依赖： Sno -&gt; Sname, Sdept, Mname Sdept -&gt; Mname 关系-2 Sno Cname Grade 1 课程-1 90 2 课程-2 80 2 课程-1 100 3 课程-2 95 有以下函数依赖： Sno, Cname -&gt; Grade 第三范式 (3NF) 非主属性不传递依赖于键码。 上面的 关系-1 中存在以下传递依赖：Sno -&gt; Sdept -&gt; Mname，可以进行以下分解： 关系-11 Sno Sname Sdept 1 学生-1 学院-1 2 学生-2 学院-2 3 学生-3 学院-2 关系-12 Sdept Mname 学院-1 院长-1 学院-2 院长-2 8. ER 图 Entity-Relationship，有三个组成部分：实体、属性、联系。 用来进行关系型数据库系统的概念设计。 8.1. 实体的三种联系 包含一对一，一对多，多对多三种。 如果 A 到 B 是一对多关系，那么画个带箭头的线段指向 B；如果是一对一，画两个带箭头的线段；如果是多对多，画两个不带箭头的线段。下图的 Course 和 Student 是一对多的关系。 8.2. 表示出现多次的关系 一个实体在联系出现几次，就要用几条线连接。下图表示一个课程的先修关系，先修关系出现两个 Course 实体，第一个是先修课程，后一个是后修课程，因此需要用两条线来表示这种关系。 8.3. 联系的多向性 虽然老师可以开设多门课，并且可以教授多名学生，但是对于特定的学生和课程，只有一个老师教授，这就构成了一个三元联系。 一般只使用二元联系，可以把多元关系转换为二元关系。 8.4. 表示子类 用一个三角形和两条线来连接类和子类，与子类有关的属性和联系都连到子类上，而与父类和子类都有关的连到父类上。 9. 资料 数据库系统原理 分库分表需要考虑的问题及方案 数据库分库分表(sharding)系列(二) 全局主键生成策略 一种支持自由规划无须数据迁移和修改路由代码的 Sharding 扩容方案]]></content>
  </entry>
  <entry>
    <title><![CDATA[UML 教程]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdesign%2FUML%2F</url>
    <content type="text"><![CDATA[UML 教程 关键词：部署图, 组件图, 包图, 类图, 复合结构图, 对象图, 活动图, 状态机图, 用例图, 通信图, 交互概述图, 时序图, 时间图 简介 部署图 组件图 包图 类图 复合结构图 对象图 活动图 状态机图 用例图 通信图 交互概述图 时序图 时间图 UML 工具 更多内容 简介 UML 图类型 UML 图类型如下图所示： 结构式建模图 结构式建模图（Structure diagrams）强调的是系统式的建模。结构图定义了一个模型的静态架构。它们通常被用来对那些构成模型的‘要素’建模，诸如：类，对象，接口和物理组件。另外，它们也被用来对元素间关联和依赖关系进行建模。 类图 对象图 包图 组件图 部署图 复合结构图 行为式建模图 行为式建模图（Behavior diagrams）强调系统模型中触发的事。行为图用来记录在一个模型内部，随时间的变化，模型执行的交互变化和瞬间的状态；并跟踪系统在真实环境下如何表现，以及观察系统对一个操作或事件的反应，以及它的结果。 活动图 状态图 用例图 通信图 交互概述图 时序图 时间图 UML 概念 UML 从来源中使用相当多的概念。我们将之定义于统一建模语言术语汇表。下面仅列代表性的概念。 对于结构而言 - 执行者，属性，类，元件，接口，对象，包。 对于行为而言 - 活动（UML），事件（UML），消息（UML），方法（UML），操作（UML），状态（UML），用例（UML）。 对于关系而言 - 聚合，关联，组合，相依，广义化（or 继承）。 其他概念 构造型—这规范符号应用到的模型 多重性—多重性标记法与资料库建模基数对应，例如：1, 0..1, 1..* 部署图 部署图（Deployment Diagram）用于对系统的物理结构建模。部署图将显示系统中的软件组件和硬件组件之间的关系以及处理工作的物理分布。 节点 节点既可以是硬件元素，也可以是软件元素。它显示为一个立方体，如下图所示。 节点实例 图可以显示节点实例，实例与节点的区分是：实例的名称带下划线，冒号放在它的基本节点类型之前。实例在冒号之前可以有名称，也可以没有名称。下图显示了一个具名的计算机实例。 节点构造型 为节点提供了许多标准的构造型，分别命名为 «cdrom»， «cd-rom»， «computer»， «disk array»， «pc»， «pc client»， «pc server»， «secure»， «server»， «storage»， «unix server»， «user pc»。 并在节点符号的右上角显示适当的图标。 工件 工件是软件开发过程中的产品。包括过程模型（如：用例模型，设计模型等），源文件，执行文件，设计文档，测试报告，构造型，用户手册等等。 工件表示为带有工件名称的矩形，并显示«artifact»关键字和文档符号。 关联 在部署图的上下文联系中，关联代表节点间的联系通道。下图显示了一个网络系统的部署图，描述了网络协议为构造型和关联终端的多重性， 作为容器的节点 节点可以包含其他元素，如组件和工件。下图显示了一个嵌入式系统某个部分的部署图。描写了一个被主板节点包含的可执行工件。 组件图 组件图（Component Diagram）描绘了组成一个软件系统的模块和嵌入控件。组件图比类图具有更高层次的抽象－通常运行时一个组件被一个或多个类（或对象）实现。它们象积木那样使得组件能最终构成系统的绝大部分。 上图演示了一些组件和它们的内部关系。装配连接器（Assembly connectors）“连接”由&quot;Product&quot;和&quot;Customer&quot;的提供接口到由 &quot;Order&quot;指定的需求接口。 一个依赖关系映射了客户相关的帐户信息到“Order”需要的 &quot;Payment&quot;需求接口。 实际上，组件图同包图很相似，它们都有明确的界限，把元素分组到逻辑结构中。他们之间的不同是：组件图提供了语义更丰富的分组机制，在组件图中，所有的模型元素都是私有的，而包图只显示公有的成员。 表现组件 组件可表示为带关键字 «component»的矩形类元；也可用右上角有组件图标的矩形表示。 装配连接器 装配连接器在组件 “Component1”的需求接口和另一个组件 “Component2”的提供接口之间建立桥梁; 这个桥梁使得一个组件能提供另一个组件所需要的服务。 带端口组件 使用端口的组件图允许在它的环境指定一个服务和行为，同时这个服务和行为也是组件需要的。当端口进行双向操作的时候，它可以指定输入和输出。下图详述了用于在线服务的带端口组件，它有两个提供接口 “order entry”和 “tracking”，也有 “payment” 需求接口。 包图 包图（Package Diagram）用来表现包和它所包含元素的组织。当用来代表类元素时，包图提供了命名空间的可视化。包图最常用的用途是用来组织用例图和类图，尽管它不局限于这些 UML 元素。 下面是一个包图的例子。 包中的元素共享相同的命名空间，因此，一个指定命名空间的元素必须有唯一的名称。 包可以用来代表物理或逻辑关系。选择把类包括在指定的包里，有助于在同一个包里赋予这些类相同继承层次。通常认为把通过复合相关联的类，以及与它们相协作的类放在同一个包里。 在 UML2.5 中，包用文件夹来表示，包中的元素共享同一个命名空间，并且必须是可识别的，因此要有唯一的名称或类型。包必须显示包名，在附属方框部分有选择的显示包内的元素。 包的合并 - 包之间的合并连接符«merge»定义了源包元素与目标包同名元素之间的泛化关系。源包元素的定义被扩展来包含目标包元素定义。当源包元素与目标包内没有同名元素时，目标包元素的定义不受影响。 包的导入 - 导入连接符 «import»表明目标包的元素，在该例中是一个类 ，在源包中被引用要用非限定修饰名。源包的命名空间获得目标类的接口，目标包的命名空间则不受影响。 嵌套连接符 - 源包和目标包间的嵌套连接符说明目标包完全包含源包。 类图 类图（Class Diagram）展示了面向对象系统的构造模块。描绘了模型或部分模型的静态视图，显示它包含的属性和行为，而不是详细描述操作的功能或完善方法。类图最常用来表达多个类和接口之间的关系。泛化（Generalizations），聚合（aggregations）和关联（associations）分别是类之间继承，复合或应用，及连接的表现。 下面的图显示了类之间的聚合关系。弱聚合（浅色箭头）表现在类 “Account” 使用 “AddressBook”，但是不必要包含它的一个实例。强聚合（图中的黑色箭头）表示了目标类包含源类，例如，“Contact” 和 &quot;ContactGroup&quot;值被包含在 &quot;AddressBook&quot;中。 类（Classes） 类是定义对象所具有的属性和行为的元素。行为用类能理解的合适消息和适合每条消息的操作来描述。 类中也可能定义约束，标记值，构造型。 类的标柱（Class Notation） 类用矩形表示。除类的名称外，还可以选择性地显示属性和操作。 分栏分别用来显示类的名称，属性和操作。 在下面图中，类的类名显示在最上面的分栏，它下面的分栏显示详细属性，如：“center” 属性显示初始化的值。最后面的分栏显示操作，如： setWidth，setLength 和 setPosition 以及他们的参数。 属性和操作名前的标注表示了该属性或操作的可见性: 如果使用 &quot;+&quot;号，这个属性或操作是公共的 ; “-” 号则代表这个属性或操作是私有的。 “#“号是这个属性或操作被定义为保护的，” ~” 号代表包的可见性。 接口（Interfaces） 接口是实施者同意满足的行为规范，是一种约定。实现一个接口，类必需支持其要求的行为，使系统按照同样的方式，即公共的接口，处理不相关的元素。 接口有相似于类的外形风格，含有指定的操作，如下图所示。如果没有明确的详细操作，也可以画成一个圆环。当画成圆环的时候，到这个环形标柱的实现连接没有目标箭头。 表（Tables） 表尽管不是基本 UML 的一部分，仍然是“图型”能完成的实例用。在右上角画一个表的小图标来表示。表属性用“图型” «column»表示。 绝大多数表单有一个主键，是由一个或几个字段组成的一个唯一的字码组合加主键操作来访问表格，主键操作“图型”为«PK»。 一些表有一个或多个外键，使用一个或多个字段加一个外键操作，映射到相关表的主键上去，外键操作“图型”为«FK»。 关联（Associations） 关联表明两个模型元素之间有关系，通常用在一个类中被实现为一个实例变量。连接符可以包含两端的命名的角色，基数性，方向和约束。关联是元素之间普通的关系。如果多于两个元素，也可以使用菱形的关联关系。当从类图生成代码时，关联末端的对象将变成目标类中实例变量。见下图示例 “playsFor” 将变成&quot;Player&quot;类中的实例变量。 泛化（Generalizations） 泛化被用来说明继承关系。连接从特定类元到一般类元。泛化的含义是源类继承了目标类的特性。下图的图显示了一个父类泛化一个子类， 类“Circle”的一个实例将会有属性 “ x_position”，“ y_position” ， “radius” 和 方法 “display()”。 注意：类 “Shape” 是抽象的，类名显示为斜体。 下图显示了与上图相同信息的视图。 聚合（Aggregations） 聚合通常被用来描述由更小的组件所构成的元素。聚合关系表示为白色菱形箭头指向目标类或父类。 聚合的更强形式 -组合聚合（强聚合） - 显示为黑色菱形箭头，用来组合每次最大化的包含组件。如果一个组合聚合的父类被删除，通常与他相关的所有部分都会被删除，但是，如果一个部件从组合中去掉，将不用删除整个组合。组合是可迁，非对称的关系和递归的。 下面的图示：显示了弱聚合和强聚合的不同。“ address book” 由许多 “contacts” 和 “contact groups”组成。 “contact group” 是一个“contacts”的虚分组; “contact”可以被包含在不止一个 “ contact group”。 如果你删除一个“ address book”，所有的 “contacts” 和 “contact groups” 也将会被删除；如果你删除“ contact group”， 没有 “contacts”会被删除。 关联类（Association Classes） 关联类是一个允许关联连接有属性和操作的构造。下面的示例：显示了远不止简单连接两个类的连接，如给“employee”分配项目。“ employee”在项目中所起的作用是一个复杂的实体，既有自身的也有不属于“employee” 或 “project” 类的细节。 例如，“ employee”可以同时为几个项目工作，有不同的职务头衔和对应的安全权限。 依赖（Dependencies） 依赖被用来描述模型元素间广泛的依赖关系。通常在设计过程早期显示两个元素之间存在某种关系，因为是初期而不能确定具体是什么关系，在设计过程末期，该继承关系会被归入已有构造型 (构造型 可以是实例化 «instantiate»，跟踪 «trace»，导入 «import»， 和其它的关系)，或被替换成一个更明确类型的连接符。 跟踪（Traces） 跟踪关系是一种特殊化的依赖关系。连接模型元素或跨模型但是具有相同概念的模型元素集。跟踪被经常用来追踪需求和模型的变化。由于变化是双向的，这种依赖关系的顺序通常被忽略。这种关系的属性可以被指定为单向映射，但跟踪是双向的，非正式的和很少可计算的。 实现（Realizations） 是源对象执行或实现目标，实现被用来表达模型的可跟踪性和完整性－业务模型或需求被一个或多个用例实现，用例则被类实现，类被组件实现，等等。这种实现贯穿于系统设计的映射需求和类等，直至抽象建模水平级。从而确保整个系统的一张宏图，它也反映系统的所有微小组成，以及约束和定义它的细节。实现关系用带虚线的实箭头表示。 嵌套（Nestings） 嵌套连接符用来表示源元素嵌套在目标元素中。下图显示“ inner class”的定义，尽管在 EA 中，更多地按照着他们在项目层次视图中的位置来显示这种关系。 复合结构图 复合结构图显示类的内部结构，包括它与系统其他部分的交互点。也显示各部分的配置与关系，这些部分一起执行类元的行为。 类元素已经在类图部分被详细地阐述，这部分用来说明类表现复合元素的方式，如：暴露接口，包含端口和部件。 部件 部件是代表一组（一个或多个）实例的元素，这组实例的拥有者是一类元实例，例如：如果一个图的实例有一组图形元素，则这些图形元素可以被表示为部件，并可以对他们之间的某种关系建模。注意：一个部件可以在它的父类被删除之前从父类中被去掉，这样部件就不会被同时删除了。 部件在类或组件内部显示为不加修饰的方框。 端口 端口是类型化的元素，代表一个包含类元实例的外部可视的部分。端口定义了类元和它的环境之间的交互。端口显示在包含它的部件，类或组合结构的边缘上。端口指定了类元提供的服务，以及类元要求环境提供的服务。 端口显示为所属类元边界指定的方框。 接口 接口与类相似，但是有一些限制，所有的接口操作都是公共和抽象的，不提供任何默认的实现。所有的接口属性都必须是常量。然而，当一个类从一个单独的超级类继承而来，它可以实现多个接口。 当一个接口在图中单列出来，它既可以显示为类元素的方框，带 «interface» 关键字和表明它是抽象的斜体名称，也可以显示为圆环。 注意：圆环标注不显示接口操作。当接口显示为类所有的接口，它们会被当作暴露接口引用。暴露接口可以定义为是提供的，还是需求的。提供接口确认包含它的类元提供指定接口元素定义的操作，可通过类和接口间实现的连接来定义。需求接口说明该类元能与其他类元进行通信，这些类元提供了指定接口元素所定义的操作。需求接口可通过在类和接口间建立依赖连接来定义。 提供接口显示为“带棒球体”，依附在类元边缘。需求接口显示为“带棒杯体”，也是依附在类元边缘。 委托 委托连接器用来定义组件外部端口和接口的内部工作方式。委托连接器表示为带有 «delegate» 关键字的箭头。它连接组件的外部约定，表现为它的端口，到组件部件行为的内部实现。 协作 协作定义了一系列共同协作的角色，它们集体展示一个指定的设计功能。协作图应仅仅显示完成指定任务或功能的角色与属性。隔离主要角色是用来简化结构和澄清行为，也用于重用。一个协作通常实现一个模式。 协作元素显示为椭圆。 角色绑定 角色绑定连接器是一条从连接协作到所要完成该任务类元的连线。它显示为虚线，并在类元端显示作用名。 表现 表现连接器用于连接协作到类元来表示此类元中使用了该协作。显示为带关键字 «represents»的虚线箭头。 发生 发生连接器用于连接协作到类元来表示此协作表现了（同原文）该类元；显示为带关键字«occurrence»的虚线箭头。 对象图 对象图（Object Diagram）可以认为是类图的特殊情形，是类图元素子集，被用来及时强调在某些点，类的实例间的关系。这对理解类图很有帮助。他们在构造上与类图显示没有不同，但是反映出多样性和作用。 类和对象元素 下面的图显示了类元素和对象元素外观上的不同。注意：类元素包括三个部分，分别是名字栏，属性栏和操作栏；对象元素默认为没有分栏。名称显示也有不同：对象名称有下划线，并可能显示该对象实例化所用类元的名称。 运行状态 类元元素可以有任意数量的属性和操作。在对象实例中不会被显示出来。但可能定义对象的运行状态，显示特殊实例的属性设置值。 类和对象图示例 下图是一个对象图，其中插入了类定义图。它例示如何用对象图来测试类图中任务多重性的方法。“car” 类对 “wheel” 类有“1 对多” 的多重性，但是如果已经选择用“1 对 4” 来替代，那样就不会在对象图显示“3 个轮子”的汽车。 活动图 UML 中，活动图用来展示活动的顺序。显示了从起始点到终点的工作流，描述了活动图中存在于事件进程的判断路径。活动图可以用来详细阐述某些活动执行中发生并行处理的情况。活动图对业务建模也比较有用，用来详细描述发生在业务活动中的过程。 一个活动图的示例如下所示。 下面描述组成活动图的元素。 活动 活动是行为参数化顺序的规范。活动被表示为圆角矩形，内含全部的动作，工作流和其他组成活动的元素。 动作 一个动作代表活动中的一个步骤。动作用圆角矩形表示。 动作约束 动作可以附带约束，下图显示了一个带前置条件和后置条件的动作。 控制流 控制流显示一个动作到下一个动作的流。表示为带箭头实线 初始节点 一个开始或起始点用大黑圆点表示，如下图。 结束节点 结束节点有两种类型：活动结束节点和流结束节点。活动结束节点表示为中心带黑点的圆环。 流结束节点表示为内部为叉号的圆环。 这两种不同类型节点的区别为：流结束节点表明单独的控制流的终点。活动结束终点是活动图内所有控制流的结束。 对象和对象流 对象流是对象和数据转递的通道。对象显示为矩形。 对象流显示为带箭头的连接器，表明方向和通过的对象。 一个对象流在它的至少一个终端有一个对象。在上图中，可以采用带输入输出引脚的速记标柱表示。 数据存储显示为带 «datastore» 关键字的对象。 判断节点和合并节点 判断节点和合并节点是相同标注：菱形。它们可以被命名。从判断节点出来的控制流有监护条件，当监护条件满足时，可以对流控制。下图显示了判断节点和合并节点的使用。 分叉和结合节点 分叉和结合节点有同样的标柱：垂直或水平条（方向取决于工作流从左到右，还是从上到下）。它们说明了控制的并发线程的起始和终点，下图显示他们的使用示例。 结合节点与合并节点不同之处在于：结合节点同步两个输入量，产生一个单独的输出量。来自结合节点的输出量要接收到所有的输入量后才能执行。合并节点直接将控制流传递通过。如果两个或更多的输入量到达合并节点。则它的输出流指定的动作会被执行两次或更多次。 扩展域 扩展域是会执行多次的结构活动域。输入输出扩展节点表示为一组“3 厢” ，代表多个选择项。关键词 “iterative”， “parallel” 或 &quot;stream&quot;显示在区域的左上角 异常处理器 异常处理器在活动图中可以建模。 可中断活动区 可中断活动区环绕一组可以中断的动作。在下面非常简单的例子中： 当控制被传递到结束订单 “Close Order” 动作，定单处理&quot;Process Order&quot; 动作会执行直到完成，除非&quot;Cancel Request&quot;取消请求中断被接受，这会将控制传递给&quot;Cancel Order&quot;动作。 分割 一个活动分割显示为垂直或水平泳道。在下图中，分割被用来在活动图中分隔动作，有在 &quot;accounting department&quot;中执行的，有在 &quot;customer&quot;中执行的。 状态机图 状态机图（state-machine-diagram）对一个单独对象的行为建模，指明对象在它的整个生命周期里，响应不同事件时，执行相关事件的顺序。 如下示例， 下列的状态机图显示了门在它的整个生命周期里如何运作。 门可以处于以下的三种状态之一： &quot;Opened&quot;打开状态， &quot;Closed&quot;关闭状态，或者&quot;Locked&quot;锁定状态。 它分别响应事件：“Open”开门， “Close”关门， “Lock”锁门 和 “Unlock”解锁。 注意：不是所有的事件，在所有的状态下都是有效的。如：一个门打开的时候是不可能锁定的，除非你关上门。并且，状态转移可能有附加监护条件：假设门是开的，如果“doorWay-&gt;isEmpty”（门是空的）被满足，那么它只能响应关门事件。状态机图使用的语法和约定将在下面的部分进行讨论。 状态 状态被表示为圆角矩形，状态名写在里面。 起始和结束状态 初始状态表示为实心黑圆环，可以标注名称。结束状态表示为中心带黑点圆环，也可以被标注名称。 转移 一个状态到下一个状态的转移表示为带箭头实线。转移可以有一个“Trigger”触发器，一个“Guard”监护条件和一个“effect”效果。如下所示： &quot;Trigger&quot;触发器是转移的起因，它可以是某个条件下的一个信号，一个事件，一个变化或一个时间通路。&quot;Guard&quot;监护是一个条件，而且必须为真，以便于让触发器引起转移。效果&quot;Effect&quot;是直接作用到对象上的一个动作，该对象具有做为转移结果的状态机。 状态活动 在上面的状态转移示例中，一个效果与该转移相关联。如果目标状态有多个转移到达，并且每一个转移都有相同的效果与它相关联，那最好将该效果与目标状态相关联，而不与转移相关联。你可以通过为这个状态定义初始动作来实现。下图显示了一个带入口动作和出口动作的状态。 可以定义发生在事件上的动作或一直发生的动作。每一种类型的动作是可以定义任意数量的。 自转移 一个状态可能有一个返回到自身的转移，如下图。效果与转移关联是十分有帮助。 复合状态 一个状态机图可以有子状态机图，如下图所示： 可选择不同方式显示相同信息，如下图所示： 上面版本的标注说明&quot;Check PIN&quot;的子状态机图显示在单独的图中。 入口点 有时，你不想在正常的初始状态进入子状态机。例如下面的子状态机，它通常从&quot;初始化&quot;状态开始，但是如果因为某些原因，它不必执行初始化，可能靠转移到指定的入口点来从 “Ready” 状态开始。 下图显示了状态机的上一层。 出口点 有与入口点相类似的方式，它可能也指定可选择的出口点。下图给出了主处理状态执行后，所执行状态的去向将取决于该状态转移时所使用的路径。 选择伪状态 选择伪状态显示为菱形，有一个转移输入，两个或多个输出。下图显示不管到达哪一个状态，经过选择伪状态后的去向，取决于在伪状态中执行时所选择的消息格式。 连接伪状态 连接伪状态用来将多个状态转移链接在一起。一个单独的连接伪状态可以有一个或多个输入和一个或多个输出，监护可能应用于每一个转移，连接是没有语义的。连接可以把一个输入转移分成多个输出转移来实现一个静态分支。与之对照的是选择伪状态实现一个动态条件分支。 终止伪状态 进入终止伪状态是指状态机生命线已经终止。终止伪状态表示为叉号。 历史状态 历史状态用来当状态机中断时，恢复状态机之前状态。下面例图说明了历史状态的使用。这个例子是关于洗衣机的状态机。 在这个状态机中，当洗衣机运行时，它会按照&quot;Washing&quot; 到 Rinsing&quot;再到&quot;Spinning&quot;来进行。如果电源被切断 ，洗衣机会停止运行并进入&quot;Power Off&quot; 状态。当电源恢复，运行状态在&quot;History State&quot;符号处进入，表示它会从上次离开的地方恢复。 并发区 一个状态可以被分成几个不同的区，包含同时存在和执行的子状态。下面的例子显示状态 “Applying Brakes”， “front brake&quot;和&quot;rear brakes” 将同时独立运作。注意使用了分叉和结合伪状态而不是选择和合并伪状态。这些符号用来同步并发的线程。 用例图 用例图用来记录系统的需求，它提供系统与用户及其他参与者的一种通信手段。 执行者 用例图显示了系统和系统外实体之间的交互。这些实体被引用为执行者。执行者代表角色，可以包括：用户，外部硬件和其他系统。执行者往往被画成简笔画小人。也可以用带«actor»关键字的类矩形表示。 在下图中，执行者可以详细的泛化其他执行者: 用例 用例是有意义的单独工作单元。它向系统外部的人或事提供一个易于观察的高层次行为视图。 用例的标注符号是一个椭圆。 使用用例的符号是带可选择箭头的连接线，箭头显示控制的方向。下图说明执行者 &quot;Customer&quot;使用 &quot;Withdraw&quot;用例。 用途连接器（uses connector）可以有选择性的在每一个端点有多重性值，如下图，显示客户一次可能只执行一次取款交易。但是银行可以同时执行许多取款交易。 用例定义 一个典型的用例包括: 名称和描述 - 用例通常用一个动词词组定义，而且有一个简短的文字说明。 需求 - 需求定义了一个用例必须提供给终端用户的正式功能性需求。它们符合构造方法建立的功能性规范。一个需求是用例将执行一个动作或提供多个值给系统的约定或承诺。 约束 - 一个约束是一个用例运行的条件或限制。它包括：前置条件，后置条件和不变化条件 。前置条件指明了用例在发生之前需要符合的条件。后置条件用来说明在用例执行之后一些条件必须为&quot;真&quot;。不变化条件说明用例整个执行过程中该条件始终为&quot;真&quot;。 情形 - 情形是用例的实例在执行过程中，事件发生流程的形式描述。它定义了系统和外部执行者之间的事件指定顺序。通常用文本方式来表示，并对应时序图中的文字描述。 情形图 附加信息 包含用例 用例可能包含其他用例的功能来作为它正常处理的一部分。通常它假设，任何被包含的用例在基本程序运行时每一次都会被调用。下面例子：用例“卡的确认” 在运行时，被用例“取钱”当作一个子部分。 用例可以被一个或多个用例包含。通过提炼通用的行为，将它变成可以多次重复使用的用例。有助于降低功能重复级别。 扩展用例 一个用例可以被用来扩展另一个用例的行为，通常使用在特别情况下。例如：假设在修改一个特别类型的客户订单之前，用户必须得到某种更高级别的许可，然后“获得许可”用例将有选择的扩展常规的“修改订单”用例。 扩展点 - 扩展用例的加入点被定义为扩展点。 系统边界 - 它用来显示用例在系统内部，执行者在系统的外部。 通信图 通信图，以前称之为协作图，是一种交互图，所显示消息与时序图相似，但是它更侧重于对象间的联系。 在通信图中，对象之间显示关联连接器。消息附加到这些关联上，显示短箭头指向消息流的方向。消息的顺序通过编号码显示。 下面的两个图用通信图和时序图分别显示相同的信息。尽管我们可能从通信图的编号码得到消息顺序，但它不是立即可见的。通信图十分清楚的显示了邻近对象间全部完整的消息传递。 交互概述图 一个交互概览图是活动图的一种形式，它的节点代表交互图。交互图包含时序图，通信图，交互概览图和时间图。 大多数交互概览图标注与活动图一样。例如：起始，结束，判断，合并，分叉和结合节点是完全相同。并且，交互概览图介绍了两种新的元素：交互发生和交互元素。 交互发生 交互发生引用现有的交互图。显示为一个引用框，左上角显示 “ref” 。被引用的图名显示在框的中央。 交互元素 交互元素与交互发生相似之处在于都是在一个矩形框中显示一个现有的交互图。不同之处在内部显示参考图的内容不同。 将它们放在一起 所有的活动图控件，都可以相同地被使用于交互概览图，如：分叉，结合，合并等等。它把控制逻辑放入较低一级的图中。下面的例子就说明了一个典型的销售过程。子过程是从交互发生抽象而来。 时序图 时序图是交互图的一种形式，它显示对象沿生命线发展，对象之间随时间的交互表示为从源生命线指向目标生命线的消息。时序图能很好地显示那些对象与其它那些对象通信，什么消息触发了这些通信，时序图不能很好显示复杂过程的逻辑。 生命线 一条生命线在时序图中代表一个独立的参与者。表示为包含对象名的矩形，如果它的名字是&quot;self&quot;，则说明该生命线代表控制带时序图的类元。 有时，时序图会包含一个顶端是执行者的生命线。这情况说明掌握这个时序图的是用例。健壮图中的边界，控制和实体元素也可以有生命线。 消息 消息显示为箭头。消息可以完成传输，也可能丢失和找回，它可以是同步的，也可以是异步的，即可以是调用，也可以是信号。在下图中，第一条消息是同步消息(标为实箭头)完成传输，并隐含一条返回消息。第二条消息是异步消息 (标为实线箭头)，第三条是异步返回消息(标为虚线)。 执行发生 向下延伸的细条状矩形表示执行事件或控制焦点的激活。在上图中有三个执行事件。第一个是源对象发送两条消息和收到两条回复。第二个是目标对象收到一条同步消息并返回一条回复。第三个是目标对象收到一条异步消息并返回一条回复。 内部通信 内部消息表现为一个操作的递归调用，或一个方法调用属于同一个对象的其他方法。显示为生命线上执行事件的嵌套控制焦点。 迷路消息和拾取消息 迷路消息是那些发送了却没有到达指定接收者，或者到达的接收者不再当前图中。拾取消息是收到来自那些未知的发送者，或者来自没有显示在当前图的发送者的消息。它们都表明是去往或来自一个终点元素。 生命线开始与结束 生命线可以在时序图时间刻度范围内创建和销毁，在下面的例子中，生命线被停止符号（叉号）终止。在前面的例子中，生命线顶端的符号（Child）显示在比创建它的对象符号（parent）沿页面要低的位置上。下图显示创建和终止对象。 时间和期限约束 消息默认显示为水平线。因为生命线显示为沿屏幕向下的时间通道，所以当给实时系统建模，或是有时间约束的业务过程建模，考虑执行动作所需时间长度是很重要的。因此可以给消息设置一个期限约束，这样的消息显示为下斜线。 复合片段 如前面所说，时序图不适合表达复杂的过程逻辑。在一种情况下，有许多机制允许把一定程度的过程逻辑加入到图中，并把它们放到复合片段的标题下。复合片段是一个或多个处理顺序被包含在一个框架中，并在指定名称的环境下执行。片段可以是: 选择性片段 (显示 “alt”) 为 if…then…else 结构建模。 选项片段 (显示 “opt”) 为 “switch”(开关) 结构建模。 中断片段对被处理事件的可选择顺序建模，而不是该图的其他部分。 并行片段(显示 “par”) 为并发处理建模。 弱顺序片段 (显示 “seq”) 包含了一组消息，这组消息必须在后继片段开始之前被处理。但不会把片段内消息的先后顺序强加到不共享同一条生命线的消息上。 严格顺序片段 (显示 “strict”) 包含了一系列需要按照给定顺序处理的消息。 非片段 (显示 “neg”) 包含了一系列不可用的消息。 关键片段 具有关键部分。 忽略片段 声明一个没有意义的消息，如果它出现在当前上下文中。 考虑片段与忽略片段相反，不包含在考虑片段内的消息都应该被忽略。 断言片段 (显示 “assert”)标明任何没有显示为声明操作数的顺序都是无效的。 循环片段 包含一系列被重复的消息。 下图显示的是循环片段： 这也是一个类似于复合片段的交互发生。 交互发生被其他图参考，显示为左上角带&quot;ref&quot;，将被参考图名显示在方框的中间。 门 门是连接片段内消息和片段外消息的连接点。 在 EA 中，门显示为片段框架上的小正方形。作用为时序图与页面外的连接器。 用来表示进来的消息源，或者出去消息的终点。下面两个图显示它们在实践中的使用。注意：&quot; top level diagram&quot;中的门用消息箭头指向参考片段，在这里没有必要把它画成方块。 部分分解 一个对象可以引出多条生命线，使得对象内部和对象之间的消息显示在同一图上。 状态常量/延续 状态常量是生命线的约束，运行时始终为&quot;真&quot;。显示为两侧半圆的矩形，如下图： 延续虽与状态常量有同样的标注，但是被用于复合片段，并可以延伸跨越多条生命线。 时间图 UML 时间图被用来显示随时间变化，一个或多个元素的值或状态的更改。也显示时控事件之间的交互和管理它们的时间和期限约束。 状态生命线 状态生命线显示随时间变化，一个单项状态的改变。不论时间单位如何选择，X 轴显示经过的时间，Y 轴被标为给出状态的列表。状态生命线如下所示： 值生命线 值生命线显示随时间变化，一个单项的值的变化。X 轴显示经过的时间，时间单位为任意，和状态生命线一样。平行线之间显示值，每次值变化，平行线交叉。如下图所示。 将它们放在一起 状态和值的生命线能叠加组合。它们必须有相同的 X 轴。 消息可以从一个生命线传递到另一个。每一个状态和值的变换能有一个定义的事件，一个时间限制是指一个事件何时必须发生，和一个期限限制说明状态或值多长时间必须有效。一旦这些已经被应用，其时间图可能显示如下。 UML 工具 UML 工具非常多，到底哪种工具好，真的是仁者见仁智者见智。这里列举一些我接触过的 UML 工具： 亿图 国内开发的、收费的绘图工具。图形模板、素材非常全面，样式也很精美，可以导出为 word、pdf、图片。 亿图官网 Visio Office 的绘图工具，特点是简单、清晰。 Visio 官网 StarUML 样式精美，功能全面的 UML 工具。 StarUML 官网 Astah 样式不错，功能全面的绘图工具。 Astah 官网 ArgoUML UML 工具。 ArgoUML 官网 ProcessOn 在线绘图工具，特点是简洁、清晰。 ProcessOn 官网 drawio 开源的在线绘图工具，特点是简洁、清晰。 drawio 官网 更多内容 📓 本文已归档到：「blog」 参考资料 Wiki-UML Sparx UML 教程 OMG UML UML Tutorial W3Cschool UML 教程 UML 学习入门就这一篇文章 http://www.cnblogs.com/ywqu/category/223486.html]]></content>
      <categories>
        <category>design</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>uml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 容器]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fbasics%2Fdocker-container%2F</url>
    <content type="text"><![CDATA[Docker 容器 容器是独立运行的一个或一组应用，以及它们的运行态环境。对应的，虚拟机可以理解为模拟运行的一整套操作系统（提供了运行态环境和其他系统环境）和跑在上面的应用。 启动容器 新建并启动 启动已终止容器 后台运行 终止容器 进入容器 attach 命令 exec 命令 导出和导入容器 导出容器 导入容器快照 删除容器 清理所有处于终止状态的容器 启动容器 启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。 因为 Docker 的容器实在太轻量级了，很多时候用户都是随时删除和新创建容器。 新建并启动 所需要的命令主要为 docker run。 例如，下面的命令输出一个 “Hello World”，之后终止容器。 $ docker run ubuntu:14.04 /bin/echo 'Hello world'Hello world 这跟在本地直接执行 /bin/echo 'hello world' 几乎感觉不出任何区别。 下面的命令则启动一个 bash 终端，允许用户进行交互。 $ docker run -t -i ubuntu:14.04 /bin/bashroot@af8bae53bdd3:/# 其中，-t 选项让 Docker 分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开。 在交互模式下，用户可以通过所创建的终端来输入命令，例如 root@af8bae53bdd3:/# pwd/root@af8bae53bdd3:/# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括： 检查本地是否存在指定的镜像，不存在就从公有仓库下载 利用镜像创建并启动一个容器 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行完毕后容器被终止 启动已终止容器 可以利用 docker container start 命令，直接将一个已经终止的容器启动运行。 容器的核心为所执行的应用程序，所需要的资源都是应用程序运行所必需的。除此之外，并没有其它的资源。可以在伪终端中利用 ps 或 top 来查看进程信息。 root@ba267838cc1b:/# ps PID TTY TIME CMD 1 ? 00:00:00 bash 11 ? 00:00:00 ps 可见，容器中仅运行了指定的 bash 应用。这种特点使得 Docker 对资源的利用率极高，是货真价实的轻量级虚拟化。 后台运行 更多的时候，需要让 Docker 在后台运行而不是直接把执行命令的结果输出在当前宿主机下。此时，可以通过添加 -d 参数来实现。 下面举两个例子来说明一下。 如果不使用 -d 参数运行容器。 $ docker run ubuntu:18.04 /bin/sh -c "while true; do echo hello world; sleep 1; done"hello worldhello worldhello worldhello world 容器会把输出的结果 (STDOUT) 打印到宿主机上面 如果使用了 -d 参数运行容器。 $ docker run -d ubuntu:18.04 /bin/sh -c "while true; do echo hello world; sleep 1; done"77b2dc01fe0f3f1265df143181e7b9af5e05279a884f4776ee75350ea9d8017a 此时容器会在后台运行并不会把输出的结果 (STDOUT) 打印到宿主机上面(输出结果可以用 docker logs 查看)。 注： 容器是否会长久运行，是和 docker run 指定的命令有关，和 -d 参数无关。 使用 -d 参数启动后会返回一个唯一的 id，也可以通过 docker container ls 命令来查看容器信息。 $ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES77b2dc01fe0f ubuntu:18.04 /bin/sh -c 'while tr 2 minutes ago Up 1 minute agitated_wright 要获取容器的输出信息，可以通过 docker container logs 命令。 $ docker container logs [container ID or NAMES]hello worldhello worldhello world. . . 终止容器 可以使用 docker container stop 来终止一个运行中的容器。 此外，当 Docker 容器中指定的应用终结时，容器也自动终止。 例如对于上一章节中只启动了一个终端的容器，用户通过 exit 命令或 Ctrl+d 来退出终端时，所创建的容器立刻终止。 终止状态的容器可以用 docker container ls -a 命令看到。例如 docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba267838cc1b ubuntu:14.04 "/bin/bash" 30 minutes ago Exited (0) About a minute ago trusting_newton98e5efa7d997 training/webapp:latest "python app.py" About an hour ago Exited (0) 34 minutes ago backstabbing_pike 处于终止状态的容器，可以通过 docker container start 命令来重新启动。 此外，docker container restart 命令会将一个运行态的容器终止，然后再重新启动它。 进入容器 在使用 -d 参数时，容器启动后会进入后台。 某些时候需要进入容器进行操作，包括使用 docker attach 命令或 docker exec 命令，推荐大家使用 docker exec 命令，原因会在下面说明。 attach 命令 docker attach 是 Docker 自带的命令。下面示例如何使用该命令。 $ docker run -dit ubuntu243c32535da7d142fb0e6df616a3c3ada0b8ab417937c853a9e1c251f499f550$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES243c32535da7 ubuntu:latest "/bin/bash" 18 seconds ago Up 17 seconds nostalgic_hypatia$ docker attach 243croot@243c32535da7:/# 注意： 如果从这个 stdin 中 exit，会导致容器的停止。 exec 命令 -i -t 参数 docker exec 后边可以跟多个参数，这里主要说明 -i -t 参数。 只用 -i 参数时，由于没有分配伪终端，界面没有我们熟悉的 Linux 命令提示符，但命令执行结果仍然可以返回。 当 -i -t 参数一起使用时，则可以看到我们熟悉的 Linux 命令提示符。 $ docker run -dit ubuntu69d137adef7a8a689cbcb059e94da5489d3cddd240ff675c640c8d96e84fe1f6$ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES69d137adef7a ubuntu:latest "/bin/bash" 18 seconds ago Up 17 seconds zealous_swirles$ docker exec -i 69d1 bashlsbinbootdev...$ docker exec -it 69d1 bashroot@69d137adef7a:/# 如果从这个 stdin 中 exit，不会导致容器的停止。这就是为什么推荐大家使用 docker exec 的原因。 更多参数说明请使用 docker exec --help 查看。 导出和导入容器 导出容器 如果要导出本地某个容器，可以使用 docker export 命令。 $ docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7691a814370e ubuntu:14.04 "/bin/bash" 36 hours ago Exited (0) 21 hours ago test$ docker export 7691a814370e &gt; ubuntu.tar 这样将导出容器快照到本地文件。 导入容器快照 可以使用 docker import 从容器快照文件中再导入为镜像，例如 $ cat ubuntu.tar | docker import - test/ubuntu:v1.0$ docker image lsREPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZEtest/ubuntu v1.0 9d37a6082e97 About a minute ago 171.3 MB 此外，也可以通过指定 URL 或者某个目录来导入，例如 $ docker import http://example.com/exampleimage.tgz example/imagerepo 注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。 删除容器 可以使用 docker container rm 来删除一个处于终止状态的容器。例如 $ docker container rm trusting_newtontrusting_newton 如果要删除一个运行中的容器，可以添加 -f 参数。Docker 会发送 SIGKILL 信号给容器。 清理所有处于终止状态的容器 用 docker container ls -a 命令可以查看所有已经创建的包括终止状态的容器，如果数量太多要一个个删除可能会很麻烦，用下面的命令可以清理掉所有处于终止状态的容器。 $ docker container prune]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis 缓存淘汰策略]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fnosql%2Fredis%2FRedis%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[Redis 缓存淘汰策略 概述 最大缓存 Redis 允许通过 maxmemory 参数来设置内存最大值。 主键失效 作为一种定期清理无效数据的重要机制，在 Redis 提供的诸多命令中，EXPIRE、EXPIREAT、PEXPIRE、PEXPIREAT 以及 SETEX 和 PSETEX 均可以用来设置一条 Key-Value 对的失效时间，而一条 Key-Value 对一旦被关联了失效时间就会在到期后自动删除（或者说变得无法访问更为准确）。 淘汰机制 随着不断的向 redis 中保存数据，当内存剩余空间无法满足添加的数据时，redis 内就会施行数据淘汰策略，清除一部分内容然后保证新的数据可以保存到内存中。 内存淘汰机制是为了更好的使用内存，用一定得 miss 来换取内存的利用率，保证 redis 缓存中保存的都是热点数据。 ​ 非精准的 LRU 实际上 Redis 实现的 LRU 并不是可靠的 LRU，也就是名义上我们使用 LRU 算法淘汰键，但是实际上被淘汰的键并不一定是真正的最久没用的。 淘汰策略 内存淘汰只是 Redis 提供的一个功能，为了更好地实现这个功能，必须为不同的应用场景提供不同的策略，内存淘汰策略讲的是为实现内存淘汰我们具体怎么做，要解决的问题包括淘汰键空间如何选择？在键空间中淘汰键如何选择？ Redis 提供了下面几种淘汰策略供用户选择，其中默认的策略为 noeviction 策略： noeviction - 当内存使用达到阈值的时候，所有引起申请内存的命令会报错。 allkeys-lru - 在主键空间中，优先移除最近未使用的 key。 allkeys-random - 在主键空间中，随机移除某个 key。 volatile-lru - 在设置了过期时间的键空间中，优先移除最近未使用的 key。 volatile-random - 在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl - 在设置了过期时间的键空间中，具有更早过期时间的 key 优先移除。 这里补充一下主键空间和设置了过期时间的键空间，举个例子，假设我们有一批键存储在 Redis 中，则有那么一个哈希表用于存储这批键及其值，如果这批键中有一部分设置了过期时间，那么这批键还会被存储到另外一个哈希表中，这个哈希表中的值对应的是键被设置的过期时间。设置了过期时间的键空间为主键空间的子集。 如何选择淘汰策略 如果数据呈现幂律分布，也就是一部分数据访问频率高，一部分数据访问频率低，则使用 allkeys-lru。 如果数据呈现平等分布，也就是所有的数据访问频率都相同，则使用 allkeys-random。 volatile-lru 策略和 volatile-random 策略适合我们将一个 Redis 实例既应用于缓存和又应用于持久化存储的时候，然而我们也可以通过使用两个 Redis 实例来达到相同的效果。 将 key 设置过期时间实际上会消耗更多的内存，因此我们建议使用 allkeys-lru 策略从而更有效率的使用内存。 内部实现 Redis 删除失效主键的方法主要有两种： 消极方法（passive way），在主键被访问时如果发现它已经失效，那么就删除它。 主动方法（active way），周期性地从设置了失效时间的主键中选择一部分失效的主键删除。 主动删除：当前已用内存超过 maxmemory 限定时，触发主动清理策略，该策略由启动参数的配置决定主键具体的失效时间全部都维护在 expires 这个字典表中。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis 配置]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fnosql%2Fredis%2FRedis%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Redis 配置 通过配置文件配置 Redis 的配置文件名一般叫做：redis.conf。 redis.conf 文件中的配置指令参数格式为： keyword argument1 argument2 ... argumentN关键字 参数1 参数2 ... 参数N 通过命令行配置 自 Redis2.6 起就可以直接通过命令行传递 Redis 配置参数。这种方法可以用于测试。 例：这个例子配置一个新运行并以 6380 为端口的 Redis 实例，使配置它为 127.0.0.1:6379 Redis 实例的 slave。 ./redis-server --port 6380 --slaveof 127.0.0.1 6379 动态修改配置 Redis 允许在运行的过程中，在不重启服务器的情况下更改服务器配置，同时也支持 使用特殊的 CONFIG SET 和 CONFIG GET 命令用编程方式查询并设置配置。 并非所有的配置指令都支持这种使用方式，但是大部分是支持的。 配置 Redis 成为一个缓存 如果你想把 Redis 当做一个缓存来用，所有的 key 都有过期时间，那么你可以考虑 使用以下设置（假设最大内存使用量为 2M）： maxmemory 2mbmaxmemory-policy allkeys-lru 以上设置并不需要我们的应用使用 EXPIRE(或相似的命令)命令去设置每个 key 的过期时间，因为 只要内存使用量到达 2M，Redis 就会使用类 LRU 算法自动删除某些 key。 相比使用额外内存空间存储多个键的过期时间，使用缓存设置是一种更加有效利用内存的方式。而且相比每个键固定的 过期时间，使用 LRU 也是一种更加推荐的方式，因为这样能使应用的热数据(更频繁使用的键) 在内存中停留时间更久。 当我们把 Redis 当成缓存来使用的时候，如果应用程序同时也需要把 Redis 当成存储系统来使用，那么强烈建议 使用两个 Redis 实例。一个是缓存，使用上述方法进行配置，另一个是存储，根据应用的持久化需求进行配置，并且 只存储那些不需要被缓存的数据。 资料 https://redis.io/topics/config]]></content>
  </entry>
  <entry>
    <title><![CDATA[Redis 安装]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fnosql%2Fredis%2Finstall-redis%2F</url>
    <content type="text"><![CDATA[Redis 安装 安装 启动 脚本 安装 安装步骤如下： （1）下载并解压到本地 进入官网下载地址：https://redis.io/download ，选择合适的版本下载。 我选择的是最新稳定版本 4.0.8：http://download.redis.io/releases/redis-4.0.8.tar.gz 我个人喜欢存放在：/opt/redis wget -O /opt/redis/redis-4.0.8.tar.gz http://download.redis.io/releases/redis-4.0.8.tar.gzcd /opt/redistar zxvf redis-4.0.8.tar.gz （2）编译安装 执行以下命令： cd /opt/redis/redis-4.0.8make 启动 启动 redis 服务 cd /opt/redis/redis-4.0.8/src./redis-server 启动 redis 客户端 cd /opt/redis/redis-4.0.8/src./redis-cli 脚本 以上两种安装方式，我都写了脚本去执行： | 安装脚本 |]]></content>
  </entry>
  <entry>
    <title><![CDATA[计算机网络之数据链路层]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fcommunication%2Fnetwork-data-link%2F</url>
    <content type="text"><![CDATA[计算机网络之数据链路层 数据链路层（Data Link Layer） - 网络层针对的还是主机之间的数据传输服务，而主机之间可以有很多链路，链路层协议就是为同一链路的主机提供数据传输服务。数据链路层把网络层传下来的分组封装成帧。 主要协议：PPP、CSMA/CD 等。 数据单元：帧（frame）。 典型设备：二层交换机、网桥、网卡。 简介 基本问题 封装成帧 透明传输 差错检测 点对点信道 PPP 协议 广播信道 CSMA/CD 协议 局域网 以太网 MAC 地址 设备 适配器 集线器 网桥 以太网交换机 简介 链路是从一个节点到相邻节点的一段物理线路，数据链路则是在链路的基础上增加了一些必要的硬件（网络适配器）和软件（协议）。 数据链路层三个基本问题：封装成帧、透明传输、差错检测。 数据链路层有两种信道类型：点对点信道（主要使用 PPP）和广播信道（主要使用 CSMA/CD）。 以太网 MAC 层的地址。 适配器、转发器、集线器、网桥、以太网交换机的作用及使用场合。 基本问题 封装成帧 为网络层传下来的 IP 数据报添加首部和尾部，用于标记帧的开始和结束。 为了提高传输效率，应该让数据部分长度尽可能大于首部和尾部。但是，每种链路层协议都限制了帧的数据部分长度上线——最大传送单元 MTU（Maximum Transfer Unit） 透明传输 透明表示：某一个实际存在的事物看起来好像不存在一样。 帧使用首部和尾部进行定界，如果帧的数据部分含有和首部尾部相同的内容，那么帧的开始和结束位置就会被错误的判定。需要在数据部分出现首部尾部相同的内容前面插入转义字符。如果数据部分出现转义字符，那么就在转义字符前面再加个转义字符。在接收端进行处理之后可以还原出原始数据。这个过程透明传输的内容是转义字符，用户察觉不到转义字符的存在。 差错检测 目前数据链路层广泛使用了循环冗余检验 CRC（Cyclic redundancy check）来检查比特差错。 点对点信道 点对点信道使用一对一的点对点通信方式。 对于点对点的链路，点对点协议 PPP（Point-to-Point Protocol）是使用最广泛的数据链路层协议。 PPP 协议 互联网用户通常都要连接到某个 ISP 之后才能接入到互联网，PPP 协议是用户计算机和 ISP 进行通信时所使用的数据链路层协议。 PPP（点到点协议）是为在同等单元之间传输数据包这样的简单链路设计的链路层协议。这种链路提供全双工操作，并按照顺序传递数据包。设计目的主要是用来通过拨号或专线方式建立点对点连接发送数据，使其成为各种主机、网桥和路由器之间简单连接的一种共通的解决方案。 PPP 的帧格式： F 字段为帧的定界符 A 和 C 字段暂时没有意义 FCS 字段是使用 CRC 的检验序列 信息部分的长度不超过 1500 广播信道 广播信道(broadcast channel)是通过广播的方式传输信息的信息通道。 所有的节点都在同一个广播信道上发送数据，因此需要有专门的控制方法进行协调，避免发生冲突（冲突也叫碰撞）。 主要有两种控制方法进行协调，一个是使用信道复用技术，一是使用 CSMA/CD 协议。 CSMA/CD 协议 CSMA/CD（Carrier Sense Multiple Access with Collision Detection）即带冲突检测的载波监听多路访问技术(载波监听多点接入/碰撞检测)。 多点接入 ：说明这是总线型网络，许多计算机以多点接入的方式连接在一根总线上。 载波监听 ：每个主机都必须不停地监听信道。发送前监听，如果忙则等待，如果空闲则发送。 碰撞检测 ：即边发送边检测。若检测到信道有干扰信号，则表示产生了碰撞，于是就要停止发送数据，计算出退避等待时间，然后使用 CSMA 方法继续尝试发送。计算退避等待时间采用的是二进制指数退避算法。 局域网 局域网 LAN（Local Area Network）是指在某一区域内由多台计算机互联成的计算机组。 局域网的拓扑结构通常为总线型和环型。 局域网技术主要有：以太网、令牌环网、FDDI 网和无线局域网等。 以太网 以太网（Ethernet）是一种星型拓扑结构局域网。 以太网是目前应用最广泛的局域网。 以太网使用 CSMA/CD 协议。 MAC 地址 MAC 地址（Media Access Control Address），也称为以太网地址或物理地址，它是一个用来确认网上设备位置的地址。 MAC 地址长度为 6 字节（48 位），用于唯一标识网络适配器（网卡）。 一台主机拥有多少个网络适配器就有多少个 MAC 地址。 设备 适配器 网络适配器一般指网卡。 网卡是工作在链路层的网络组件，是局域网中连接计算机和传输介质的接口，不仅能实现与局域网传输介质之间的物理连接和电信号匹配，还涉及帧的发送与接收、帧的封装与拆封、介质访问控制、数据的编码与解码以及数据缓存的功能等。 网卡和局域网之间的通信是通过电缆或双绞线以串行传输方式进行的。而网卡和计算机之间的通信则是通过计算机主板上的 I/O 总线以并行传输方式进行。 集线器 集线器（Hub）的主要功能是对接收到的信号进行再生整形放大，以扩大网络的传输距离，同时把所有节点集中在以它为中心的节点上。 使用集线器可以在物理层扩展以太网。 网桥 网桥（Bridge）是早期的两端口二层网络设备，用来连接不同网段。网桥的两个端口分别有一条独立的交换信道，不是共享一条背板总线，可隔离冲突域。网桥比集线器（Hub）性能更好，集线器上各端口都是共享同一条背板总线的。后来，网桥被具有更多端口、同时也可隔离冲突域的交换机（Switch）所取代。 以太网交换机 以太网交换机是基于以太网传输数据的交换机，以太网采用共享总线型传输媒体方式的局域网。以太网交换机的结构是每个端口都直接与主机相连，并且一般都工作在全双工方式。交换机能同时连通许多对端口，使每一对相互通信的主机都能像独占通信媒体那样，进行无冲突地传输数据。 以太网交换机的每个端口都直接与主机相连，并且一般都工作在全双工方式。 交换机能同时连通许多对的端口，使每一对相互通信的主机都能像独占通信媒体那样，进行无冲突地传输数据。 用户独占传输媒体的带宽，若一个接口到主机的带宽是 10Mbit 每秒，那么有 10 个接口的交换机的总容量是 100Mbit 每秒。这是交换机的最大优点。]]></content>
      <categories>
        <category>communication</category>
      </categories>
      <tags>
        <tag>communication</tag>
        <tag>network</tag>
        <tag>data link</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql 教程]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Fmysql%2FREADME%2F</url>
    <content type="text"><![CDATA[Mysql 教程 📝 知识点 mysql 维护 mysql 命令 mysql 原理 📚 学习资源 官方 Mysql 官网 Mysql 官方文档 Mysql 官方文档之命令行客户端 书 MySQL 必知必会 - 适合入门者 高性能 MySQL - 经典，适合 DBA 或作为开发者的参考手册 在线教程 runoob.com MySQL 教程 - 入门级教程 更多资源 awesome-mysql 🚪 传送门 | 回首頁 |]]></content>
  </entry>
  <entry>
    <title><![CDATA[Oracle 语法]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Foracle%2Foracle-sql%2F</url>
    <content type="text"><![CDATA[Oracle 语法]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mysql 维护]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Fmysql%2Fmysql-maintain%2F</url>
    <content type="text"><![CDATA[Mysql 维护 安装配置 安装 mysql yum 源 安装 mysql 服务器 启动 mysql 服务 初始化数据库密码 配置远程访问 跳过登录认证 运维 备份与恢复 备份 恢复 卸载 问题 JDBC 与 Mysql 因 CST 时区协商无解导致偏差了 14 或 13 小时 参考资料 安装配置 通过 rpm 包安装 centos 的 yum 源中默认是没有 mysql 的，所以我们需要先去官网下载 mysql 的 repo 源并安装。 安装 mysql yum 源 官方下载地址：https://dev.mysql.com/downloads/repo/yum/ （1）下载 yum 源 $ wget https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm （2）安装 yum repo 文件并更新 yum 缓存 $ rpm -ivh mysql80-community-release-el7-1.noarch.rpm 执行结果： 会在 /etc/yum.repos.d/ 目录下生成两个 repo 文件 $ ls | grep mysqlmysql-community.repomysql-community-source.repo 更新 yum： $ yum clean all$ yum makecache （3）查看 rpm 安装状态 $ yum search mysql | grep servermysql-community-common.i686 : MySQL database common files for server and clientmysql-community-common.x86_64 : MySQL database common files for server andmysql-community-test.x86_64 : Test suite for the MySQL database server : administering MySQL serversmysql-community-server.x86_64 : A very fast and reliable SQL database server 通过 yum 安装 mysql 有几个重要目录： # 数据库目录/var/lib/mysql/# 配置文件/usr/share/mysql（mysql.server命令及配置文件）# 相关命令/usr/bin（mysqladmin mysqldump等命令）# 启动脚本/etc/rc.d/init.d/（启动脚本文件mysql的目录）# 配置文件/etc/my.cnf 安装 mysql 服务器 $ yum install mysql-community-server 启动 mysql 服务 # 启动 mysql 服务$ systemctl start mysqld.service# 查看运行状态$ systemctl status mysqld.service# 开机启动$ systemctl enable mysqld$ systemctl daemon-reload 初始化数据库密码 查看一下初始密码 $ grep "password" /var/log/mysqld.log2018-09-30T03:13:41.727736Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: %:lt+srWu4k1 执行命令： mysql -uroot -p 输入临时密码，进入 mysql ALTER user 'root'@'localhost' IDENTIFIED BY 'Tw#123456'; 注：密码强度默认为中等，大小写字母、数字、特殊符号，只有修改成功后才能修改配置再设置更简单的密码 配置远程访问 GRANT ALL ON *.* TO 'root'@'localhost';FLUSH PRIVILEGES; 跳过登录认证 vim /etc/my.cnf 在 [mysqld] 下面加上 skip-grant-tables 作用是登录时跳过登录认证，换句话说就是 root 什么密码都可以登录进去。 执行 service mysqld restart，重启 mysql 运维 备份与恢复 Mysql 备份数据使用 mysqldump 命令。 mysqldump 将数据库中的数据备份成一个文本文件，表的结构和表中的数据将存储在生成的文本文件中。 备份 （1）备份一个数据库 语法： mysqldump -u &lt;username&gt; -p &lt;database&gt; [&lt;table1&gt; &lt;table2&gt; ...] &gt; backup.sql username 数据库用户 dbname 数据库名称 table1 和 table2 参数表示需要备份的表的名称，为空则整个数据库备份； BackupName.sql 参数表设计备份文件的名称，文件名前面可以加上一个绝对路径。通常将数据库被分成一个后缀名为 sql 的文件 （2）备份多个数据库 mysqldump -u &lt;username&gt; -p --databases &lt;database1&gt; &lt;database2&gt; ... &gt; backup.sql （3）备份所有数据库 mysqldump -u &lt;username&gt; -p -all-databases &gt; backup.sql 恢复 Mysql 恢复数据使用 mysqldump 命令。 语法： mysql -u &lt;username&gt; -p &lt;database&gt; &lt; backup.sql 卸载 （1）查看已安装的 mysql $ rpm -qa | grep -i mysqlperl-DBD-MySQL-4.023-6.el7.x86_64mysql80-community-release-el7-1.noarchmysql-community-common-8.0.12-1.el7.x86_64mysql-community-client-8.0.12-1.el7.x86_64mysql-community-libs-compat-8.0.12-1.el7.x86_64mysql-community-libs-8.0.12-1.el7.x86_64 （2）卸载 mysql $ yum remove mysql-community-server.x86_64 问题 JDBC 与 Mysql 因 CST 时区协商无解导致偏差了 14 或 13 小时 现象 数据库中存储的 Timestamp 字段值比真实值少了 13 个小时。 原因 当 JDBC 与 MySQL 开始建立连接时，会获取服务器参数。 当 MySQL 的 time_zone 值为 SYSTEM 时，会取 system_time_zone 值作为协调时区，若得到的是 CST 那么 Java 会误以为这是 CST -0500 ，因此会给出错误的时区信息（国内一般是CST +0800，即东八区）。 查看时区方法： 通过 show variables like '%time_zone%'; 命令查看 Mysql 时区配置： &gt; mysql&gt; show variables like '%time_zone%';&gt; +------------------+--------+&gt; | Variable_name | Value |&gt; +------------------+--------+&gt; | system_time_zone | CST |&gt; | time_zone | SYSTEM |&gt; +------------------+--------+&gt; 解决方案 方案一 mysql&gt; set global time_zone = '+08:00';Query OK, 0 rows affected (0.00 sec)mysql&gt; set time_zone = '+08:00';Query OK, 0 rows affected (0.00 sec) 方案二 修改 my.cnf 文件，在 [mysqld] 节下增加 default-time-zone = '+08:00' ，然后重启。 参考资料 https://www.cnblogs.com/xiaopotian/p/8196464.html https://www.cnblogs.com/bigbrotherer/p/7241845.html https://blog.csdn.net/managementandjava/article/details/80039650 http://www.manongjc.com/article/6996.html https://www.cnblogs.com/xyabk/p/8967990.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[SQLite Java API]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Fsqlite%2Fsqlite-api-java%2F</url>
    <content type="text"><![CDATA[SQLite Java API 👉 完整示例源码 Quick Start （1）在官方下载地址下载 sqlite-jdbc-(VERSION).jar ，然后将 jar 包放在项目中的 classpath。 （2）通过 API 打开一个 SQLite 数据库连接。 执行方法： &gt; javac Sample.java&gt; java -classpath ".;sqlite-jdbc-(VERSION).jar" Sample # in Windowsor&gt; java -classpath ".:sqlite-jdbc-(VERSION).jar" Sample # in Mac or Linuxname = leoid = 1name = yuiid = 2 示例： public class Sample &#123; public static void main(String[] args) &#123; Connection connection = null; try &#123; // 创建数据库连接 connection = DriverManager.getConnection("jdbc:sqlite:sample.db"); Statement statement = connection.createStatement(); statement.setQueryTimeout(30); // 设置 sql 执行超时时间为 30s statement.executeUpdate("drop table if exists person"); statement.executeUpdate("create table person (id integer, name string)"); statement.executeUpdate("insert into person values(1, 'leo')"); statement.executeUpdate("insert into person values(2, 'yui')"); ResultSet rs = statement.executeQuery("select * from person"); while (rs.next()) &#123; // 读取结果集 System.out.println("name = " + rs.getString("name")); System.out.println("id = " + rs.getInt("id")); &#125; &#125; catch (SQLException e) &#123; // 如果错误信息是 "out of memory"，可能是找不到数据库文件 System.err.println(e.getMessage()); &#125; finally &#123; try &#123; if (connection != null) &#123; connection.close(); &#125; &#125; catch (SQLException e) &#123; // 关闭连接失败 System.err.println(e.getMessage()); &#125; &#125; &#125;&#125; 如何指定数据库文件 Windows Connection connection = DriverManager.getConnection("jdbc:sqlite:C:/work/mydatabase.db"); Unix (Linux, Mac OS X, etc) Connection connection = DriverManager.getConnection("jdbc:sqlite:/home/leo/work/mydatabase.db"); 如何使用内存数据库 Connection connection = DriverManager.getConnection("jdbc:sqlite::memory:"); 参考资料 https://github.com/xerial/sqlite-jdbc http://www.runoob.com/sqlite/sqlite-java.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mysql 命令]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Fmysql%2Fmysql-cli%2F</url>
    <content type="text"><![CDATA[Mysql 命令 登录 mysql 无密码登录 有密码登录 远程登录 账户 更改 root 密码 数据管理 清空所有表（数据库名是 test） 备份和恢复 数据库备份 数据库恢复 登录 mysql 语法： mysql -D 数据库名 -h 主机名 -u 用户名 -p '密码' 无密码登录 mysql -uroot 有密码登录 mysql -u root -p'yourpassword' 远程登录 mysql -uroot -p'yourpassword' -h&lt;ip&gt; -P3306 账户 更改 root 密码 mysqladmin -uroot password 'yourpassword' 数据管理 清空所有表（数据库名是 test） mysql -N -s information_schema -e "SELECT CONCAT('TRUNCATE TABLE ',TABLE_NAME,';') FROM TABLES WHERE TABLE_SCHEMA='test'" | mysql -f test 备份和恢复 数据库备份 备份所有数据库到指定位置： mysqldump -u root -p'yourpassword' -f --all-databases &gt; /home/zp/sql/all.sql 备份指定数据库到指定位置： mysqldump -u root -p'yourpassword' &lt;database1&gt; &lt;database2&gt; &lt;database3&gt; &gt; /home/zp/sql/all.sql 远程备份 mysqldump -u root -p'yourpassword' -h&lt;ip&gt; mysql &gt;/tmp/mysql.sql 数据库恢复 mysql -u root -p'yourpassword' mysql &lt; /home/zp/sql/all.sql]]></content>
  </entry>
  <entry>
    <title><![CDATA[SQLite 命令]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Fsqlite%2Fsqlite-cli%2F</url>
    <content type="text"><![CDATA[SQLite 命令 快速开始 进入 SQLite 控制台 $ sqlite3SQLite version 3.7.17 2013-05-20 00:56:22Enter ".help" for instructionsEnter SQL statements terminated with a ";"sqlite&gt; 进入 SQLite 控制台并指定数据库 $ sqlite3 test.dbSQLite version 3.7.17 2013-05-20 00:56:22Enter ".help" for instructionsEnter SQL statements terminated with a ";"sqlite&gt; 退出 SQLite 控制台 sqlite&gt;.quit 查看命令帮助 sqlite&gt;.help 常用命令清单 命令 描述 .backup ?DB? FILE 备份 DB 数据库（默认是 “main”）到 FILE 文件。 .bail ON|OFF 发生错误后停止。默认为 OFF。 .databases 列出数据库的名称及其所依附的文件。 .dump ?TABLE? 以 SQL 文本格式转储数据库。如果指定了 TABLE 表，则只转储匹配 LIKE 模式的 TABLE 表。 .echo ON|OFF 开启或关闭 echo 命令。 .exit 退出 SQLite 提示符。 .explain ON|OFF 开启或关闭适合于 EXPLAIN 的输出模式。如果没有带参数，则为 EXPLAIN on，及开启 EXPLAIN。 .header(s) ON|OFF 开启或关闭头部显示。 .help 显示消息。 .import FILE TABLE 导入来自 FILE 文件的数据到 TABLE 表中。 .indices ?TABLE? 显示所有索引的名称。如果指定了 TABLE 表，则只显示匹配 LIKE 模式的 TABLE 表的索引。 .load FILE ?ENTRY? 加载一个扩展库。 .log FILE|off 开启或关闭日志。FILE 文件可以是 stderr（标准错误）/stdout（标准输出）。 .mode MODE 设置输出模式，MODE 可以是下列之一：csv 逗号分隔的值column 左对齐的列html HTML 的 代码insert TABLE 表的 SQL 插入（insert）语句line 每行一个值list 由 .separator 字符串分隔的值tabs 由 Tab 分隔的值tcl TCL 列表元素 .nullvalue STRING 在 NULL 值的地方输出 STRING 字符串。 .output FILENAME 发送输出到 FILENAME 文件。 .output stdout 发送输出到屏幕。 .print STRING… 逐字地输出 STRING 字符串。 .prompt MAIN CONTINUE 替换标准提示符。 .quit 退出 SQLite 提示符。 .read FILENAME 执行 FILENAME 文件中的 SQL。 .schema ?TABLE? 显示 CREATE 语句。如果指定了 TABLE 表，则只显示匹配 LIKE 模式的 TABLE 表。 .separator STRING 改变输出模式和 .import 所使用的分隔符。 .show 显示各种设置的当前值。 .stats ON|OFF 开启或关闭统计。 .tables ?PATTERN? 列出匹配 LIKE 模式的表的名称。 .timeout MS 尝试打开锁定的表 MS 毫秒。 .width NUM NUM 为 “column” 模式设置列宽度。 .timer ON|OFF 开启或关闭 CPU 定时器。 实战 格式化输出 sqlite&gt;.header onsqlite&gt;.mode columnsqlite&gt;.timer onsqlite&gt; 输出结果到文件 sqlite&gt; .mode listsqlite&gt; .separator |sqlite&gt; .output test_file_1.txtsqlite&gt; select * from tbl1;sqlite&gt; .exit$ cat test_file_1.txthello|10goodbye|20$ 参考资料 SQLite 官方命令行手册 http://www.runoob.com/sqlite/sqlite-commands.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[SQLite]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Fsqlite%2FREADME%2F</url>
    <content type="text"><![CDATA[SQLite SQLite 是一个实现了自给自足的、无服务器的、零配置的、事务性的 SQL 数据库引擎。 简介 优点 局限 安装 语法 大小写敏感 注释 数据库操作 数据类型 SQLite 存储类 SQLite 亲和(Affinity)类型 SQLite 亲和类型(Affinity)及类型名称 Boolean 数据类型 Date 与 Time 数据类型 命令 进入 SQLite 控制台 进入 SQLite 控制台并指定数据库 退出 SQLite 控制台 查看命令帮助 更多内容 简介 优点 SQLite 是自给自足的，这意味着不需要任何外部的依赖。 SQLite 是无服务器的、零配置的，这意味着不需要安装或管理。 SQLite 事务是完全兼容 ACID 的，允许从多个进程或线程安全访问。 SQLite 是非常小的，是轻量级的，完全配置时小于 400KiB，省略可选功能配置时小于 250KiB。 SQLite 支持 SQL92（SQL2）标准的大多数查询语言的功能。 一个完整的 SQLite 数据库是存储在一个单一的跨平台的磁盘文件。 SQLite 使用 ANSI-C 编写的，并提供了简单和易于使用的 API。 SQLite 可在 UNIX（Linux, Mac OS-X, Android, iOS）和 Windows（Win32, WinCE, WinRT）中运行。 局限 特性 描述 RIGHT OUTER JOIN 只实现了 LEFT OUTER JOIN。 FULL OUTER JOIN 只实现了 LEFT OUTER JOIN。 ALTER TABLE 支持 RENAME TABLE 和 ALTER TABLE 的 ADD COLUMN variants 命令，不支持 DROP COLUMN、ALTER COLUMN、ADD CONSTRAINT。 Trigger 支持 支持 FOR EACH ROW 触发器，但不支持 FOR EACH STATEMENT 触发器。 VIEWs 在 SQLite 中，视图是只读的。您不可以在视图上执行 DELETE、INSERT 或 UPDATE 语句。 GRANT 和 REVOKE 可以应用的唯一的访问权限是底层操作系统的正常文件访问权限。 安装 Sqlite 可在 UNIX（Linux, Mac OS-X, Android, iOS）和 Windows（Win32, WinCE, WinRT）中运行。 一般，Linux 和 Mac 上会预安装 sqlite。如果没有安装，可以在官方下载地址下载合适安装版本，自行安装。 语法 这里不会详细列举所有 SQL 语法，仅列举 SQLite 除标准 SQL 以外的，一些自身特殊的 SQL 语法。 👉 扩展阅读：标准 SQL 基本语法 大小写敏感 SQLite 是不区分大小写的，但也有一些命令是大小写敏感的，比如 GLOB 和 glob 在 SQLite 的语句中有不同的含义。 注释 -- 单行注释/* 多行注释1 多行注释2 */ 数据库操作 创建数据库 如下，创建一个名为 test 的数据库： $ sqlite3 test.dbSQLite version 3.7.17 2013-05-20 00:56:22Enter ".help" for instructionsEnter SQL statements terminated with a ";" 查看数据库 sqlite&gt; .databasesseq name file--- --------------- ----------------------------------------------------------0 main /root/test.db 退出数据库 sqlite&gt; .quit 附加数据库 假设这样一种情况，当在同一时间有多个数据库可用，您想使用其中的任何一个。 SQLite 的 ATTACH DATABASE 语句是用来选择一个特定的数据库，使用该命令后，所有的 SQLite 语句将在附加的数据库下执行。 sqlite&gt; ATTACH DATABASE 'test.db' AS 'test';sqlite&gt; .databasesseq name file--- --------------- ----------------------------------------------------------0 main /root/test.db2 test /root/test.db 注意：数据库名 main 和 temp 被保留用于主数据库和存储临时表及其他临时数据对象的数据库。这两个数据库名称可用于每个数据库连接，且不应该被用于附加，否则将得到一个警告消息。 分离数据库 SQLite 的 DETACH DATABASE 语句是用来把命名数据库从一个数据库连接分离和游离出来，连接是之前使用 ATTACH 语句附加的。 sqlite&gt; .databasesseq name file--- --------------- ----------------------------------------------------------0 main /root/test.db2 test /root/test.dbsqlite&gt; DETACH DATABASE 'test';sqlite&gt; .databasesseq name file--- --------------- ----------------------------------------------------------0 main /root/test.db 备份数据库 如下，备份 test 数据库到 /home/test.sql $ sqlite3 test.db .dump &gt; /home/test.sql 恢复数据库 如下，根据 /home/test.sql 恢复 test 数据库 $ sqlite3 test.db &lt; test.sql 数据类型 SQLite 使用一个更普遍的动态类型系统。在 SQLite 中，值的数据类型与值本身是相关的，而不是与它的容器相关。 SQLite 存储类 每个存储在 SQLite 数据库中的值都具有以下存储类之一： 存储类 描述 NULL 值是一个 NULL 值。 INTEGER 值是一个带符号的整数，根据值的大小存储在 1、2、3、4、6 或 8 字节中。 REAL 值是一个浮点值，存储为 8 字节的 IEEE 浮点数字。 TEXT 值是一个文本字符串，使用数据库编码（UTF-8、UTF-16BE 或 UTF-16LE）存储。 BLOB 值是一个 blob 数据，完全根据它的输入存储。 SQLite 的存储类稍微比数据类型更普遍。INTEGER 存储类，例如，包含 6 种不同的不同长度的整数数据类型。 SQLite 亲和(Affinity)类型 SQLite 支持列的亲和类型概念。任何列仍然可以存储任何类型的数据，当数据插入时，该字段的数据将会优先采用亲缘类型作为该值的存储方式。SQLite 目前的版本支持以下五种亲缘类型： 亲和类型 描述 TEXT 数值型数据在被插入之前，需要先被转换为文本格式，之后再插入到目标字段中。 NUMERIC 当文本数据被插入到亲缘性为 NUMERIC 的字段中时，如果转换操作不会导致数据信息丢失以及完全可逆，那么 SQLite 就会将该文本数据转换为 INTEGER 或 REAL 类型的数据，如果转换失败，SQLite 仍会以 TEXT 方式存储该数据。对于 NULL 或 BLOB 类型的新数据，SQLite 将不做任何转换，直接以 NULL 或 BLOB 的方式存储该数据。需要额外说明的是，对于浮点格式的常量文本，如&quot;30000.0&quot;，如果该值可以转换为 INTEGER 同时又不会丢失数值信息，那么 SQLite 就会将其转换为 INTEGER 的存储方式。 INTEGER 对于亲缘类型为 INTEGER 的字段，其规则等同于 NUMERIC，唯一差别是在执行 CAST 表达式时。 REAL 其规则基本等同于 NUMERIC，唯一的差别是不会将&quot;30000.0&quot;这样的文本数据转换为 INTEGER 存储方式。 NONE 不做任何的转换，直接以该数据所属的数据类型进行存储。 SQLite 亲和类型(Affinity)及类型名称 下表列出了当创建 SQLite3 表时可使用的各种数据类型名称，同时也显示了相应的亲和类型： 数据类型 亲和类型 INT, INTEGER, TINYINT, SMALLINT, MEDIUMINT, BIGINT, UNSIGNED BIG INT, INT2, INT8 INTEGER CHARACTER(20), VARCHAR(255), VARYING CHARACTER(255), NCHAR(55), NATIVE CHARACTER(70), NVARCHAR(100), TEXT, CLOB TEXT BLOB, no datatype specified NONE REAL, DOUBLE, DOUBLE PRECISION, FLOAT REAL NUMERIC, DECIMAL(10,5), BOOLEAN, DATE, DATETIME NUMERIC Boolean 数据类型 SQLite 没有单独的 Boolean 存储类。相反，布尔值被存储为整数 0（false）和 1（true）。 Date 与 Time 数据类型 SQLite 没有一个单独的用于存储日期和/或时间的存储类，但 SQLite 能够把日期和时间存储为 TEXT、REAL 或 INTEGER 值。 存储类 日期格式 TEXT 格式为 “YYYY-MM-DD HH:MM:SS.SSS” 的日期。 REAL 从公元前 4714 年 11 月 24 日格林尼治时间的正午开始算起的天数。 INTEGER 从 1970-01-01 00:00:00 UTC 算起的秒数。 您可以以任何上述格式来存储日期和时间，并且可以使用内置的日期和时间函数来自由转换不同格式。 命令 👉 扩展阅读：SQLite 命令 进入 SQLite 控制台 $ sqlite3SQLite version 3.7.17 2013-05-20 00:56:22Enter ".help" for instructionsEnter SQL statements terminated with a ";"sqlite&gt; 进入 SQLite 控制台并指定数据库 $ sqlite3 test.dbSQLite version 3.7.17 2013-05-20 00:56:22Enter ".help" for instructionsEnter SQL statements terminated with a ";"sqlite&gt; 退出 SQLite 控制台 sqlite&gt;.quit 查看命令帮助 sqlite&gt;.help API 👉 扩展阅读：SQLite Java API 更多内容 📓 本文已归档到：「blog」 参考资料 SQLite 官网 SQLite 官方文档]]></content>
  </entry>
  <entry>
    <title><![CDATA[SQL 基本语法]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Fsql%2F</url>
    <content type="text"><![CDATA[SQL 基本语法 本文针对关系型数据库的一般语法。 1. 概念 2. SQL 基础 2.1. SQL 语法结构 2.2. SQL 语法要点 2.3. SQL 分类 3. 增删改查 3.1. 插入数据 3.2. 更新数据 3.3. 删除数据 3.4. 查询数据 4. 过滤 4.1. WHERE 4.2. IN 和 BETWEEN 4.3. AND、OR、NOT 4.4. LIKE 5. 函数 5.1. 文本处理 5.2. 日期和时间处理 5.3. 数值处理 5.4. 汇总 6. 排序和分组 6.1. ORDER BY 6.2. GROUP BY 6.3. HAVING 7. 子查询 8. 连接和组合 8.1. 连接（JOIN） 8.2. 组合（UNION） 8.3. JOIN vs UNION 9. 数据定义 9.1. 数据库（DATABASE） 9.2. 数据表（TABLE） 9.3. 视图（VIEW） 9.4. 索引（INDEX） 10. 约束 10.1. 创建表时使用约束条件 11. 事务处理 12. 权限控制 12.1. 创建账户 12.2. 修改账户名 12.3. 删除账户 12.4. 查看权限 12.5. 授予权限 12.6. 删除权限 12.7. 更改密码 13. 存储过程 13.1. 创建存储过程 13.2. 使用存储过程 14. 游标 15. 触发器 15.1. 创建触发器 15.2. 查看触发器 15.3. 删除触发器 16. 更多内容 1. 概念 数据库（database） - 保存有组织的数据的容器（通常是一个文件或一组文件）。 数据表（table） - 某种特定类型数据的结构化清单。 模式（schema） - 关于数据库和表的布局及特性的信息。模式定义了数据在表中如何存储，包含存储什么样的数据，数据如何分解，各部分信息如何命名等信息。数据库和表都有模式。 列（column） - 表中的一个字段。所有表都是由一个或多个列组成的。 行（row） - 表中的一个记录。 主键（primary key） - 一列（或一组列），其值能够唯一标识表中每一行。 2. SQL 基础 SQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。 2.1. SQL 语法结构 SQL 语法结构包括： 子句，是语句和查询的组成成分。（在某些情况下，这些都是可选的。） 表达式，可以产生任何标量值，或由列和行的数据库表 谓词，给需要评估的 SQL 三值逻辑（3VL）（true/false/unknown）或布尔真值指定条件，并限制语句和查询的效果，或改变程序流程。 查询，基于特定条件检索数据。这是 SQL 的一个重要组成部分。 语句，可以持久地影响纲要和数据，也可以控制数据库事务、程序流程、连接、会话或诊断。 2.2. SQL 语法要点 SQL 语句不区分大小写，但是数据库表名、列名和值是否区分，依赖于具体的 DBMS 以及配置。 例如：SELECT 与 select 、Select 是相同的。 多条 SQL 语句必须以分号（;）分隔。 处理 SQL 语句时，所有空格都被忽略。SQL 语句可以写成一行，也可以分写为多行。 -- 一行 SQL 语句UPDATE user SET username='robot', password='robot' WHERE username = 'root';-- 多行 SQL 语句UPDATE userSET username='robot', password='robot'WHERE username = 'root'; SQL 支持三种注释 ## 注释1-- 注释2/* 注释3 */ 2.3. SQL 分类 数据定义语言（DDL） 数据定义语言（Data Definition Language，DDL）是 SQL 语言集中负责数据结构定义与数据库对象定义的语言。 DDL 的主要功能是定义数据库对象。 DDL 的核心指令是 CREATE、ALTER、DROP。 数据操纵语言（DML） 数据操纵语言（Data Manipulation Language, DML）是用于数据库操作，对数据库其中的对象和数据运行访问工作的编程语句。 DML 的主要功能是访问数据，因此其语法都是以读写数据库为主。 DML 的核心指令是 INSERT、UPDATE、DELETE、SELECT。这四个指令合称 CRUD(Create, Read, Update, Delete)，即增删改查。 数据控制语言（DCL） 数据控制语言 (Data Control Language, DCL) 是一种可对数据访问权进行控制的指令，它可以控制特定用户账户对数据表、查看表、预存程序、用户自定义函数等数据库对象的控制权。 DCL 的核心指令是 GRANT、REVOKE。 DCL 以控制用户的访问权限为主，因此其指令作法并不复杂，可利用 DCL 控制的权限有： CONNECT SELECT INSERT UPDATE DELETE EXECUTE USAGE REFERENCES 根据不同的 DBMS 以及不同的安全性实体，其支持的权限控制也有所不同。 事务控制语言（TCL） 事务控制语言 (Transaction Control Language, TCL) 用于管理数据库中的事务。这些用于管理由 DML 语句所做的更改。它还允许将语句分组为逻辑事务。 TCL 的核心指令是 COMMIT、ROLLBACK。 3. 增删改查 3.1. 插入数据 INSERT INTO 语句用于向表中插入新记录。 插入完整的行 INSERT INTO userVALUES (10, 'root', 'root', 'xxxx@163.com'); 插入行的一部分 INSERT INTO user(username, password, email)VALUES ('admin', 'admin', 'xxxx@163.com'); 插入查询出来的数据 INSERT INTO user(username)SELECT nameFROM account; 3.2. 更新数据 UPDATE 语句用于更新表中的记录。 UPDATE userSET username='robot', password='robot'WHERE username = 'root'; 3.3. 删除数据 DELETE 语句用于删除表中的记录。 TRUNCATE TABLE 可以清空表，也就是删除所有行。 删除表中的指定数据 DELETE FROM userWHERE username = 'robot'; 清空表中的数据 TRUNCATE TABLE user; 3.4. 查询数据 SELECT 语句用于从数据库中查询数据。 DISTINCT 用于返回唯一不同的值。它作用于所有列，也就是说所有列的值都相同才算相同。 LIMIT 限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。 ASC ：升序（默认） DESC ：降序 查询单列 SELECT prod_nameFROM products; 查询多列 SELECT prod_id, prod_name, prod_priceFROM products; 查询所有列 ELECT *FROM products; 查询不同的值 SELECT DISTINCTvend_id FROM products; 限制查询结果 -- 返回前 5 行SELECT * FROM mytable LIMIT 5;SELECT * FROM mytable LIMIT 0, 5;-- 返回第 3 ~ 5 行SELECT * FROM mytable LIMIT 2, 3; 4. 过滤 4.1. WHERE WHERE 子句用于过滤记录，即缩小访问数据的范围。 WHERE 后跟一个返回 true 或 false 的条件。 WHERE 可以与 SELECT，UPDATE 和 DELETE 一起使用。 可以在 WHERE 子句中使用的操作符 运算符 描述 = 等于 &lt;&gt; 不等于。注释：在 SQL 的一些版本中，该操作符可被写成 != &gt; 大于 &lt; 小于 &gt;= 大于等于 &lt;= 小于等于 BETWEEN 在某个范围内 LIKE 搜索某种模式 IN 指定针对某个列的多个可能值 SELECT 语句中的 WHERE 子句 SELECT * FROM CustomersWHERE cust_name = 'Kids Place'; UPDATE 语句中的 WHERE 子句 UPDATE CustomersSET cust_name = 'Jack Jones'WHERE cust_name = 'Kids Place'; DELETE 语句中的 WHERE 子句 DELETE FROM CustomersWHERE cust_name = 'Kids Place'; 4.2. IN 和 BETWEEN IN 操作符在 WHERE 子句中使用，作用是在指定的几个特定值中任选一个值。 BETWEEN 操作符在 WHERE 子句中使用，作用是选取介于某个范围内的值。 IN 示例 SELECT *FROM productsWHERE vend_id IN ('DLL01', 'BRS01'); BETWEEN 示例 SELECT *FROM productsWHERE prod_price BETWEEN 3 AND 5; 4.3. AND、OR、NOT AND、OR、NOT 是用于对过滤条件的逻辑处理指令。 AND 优先级高于 OR，为了明确处理顺序，可以使用 ()。 AND 操作符表示左右条件都要满足。 OR 操作符表示左右条件满足任意一个即可。 NOT 操作符用于否定一个条件。 AND 示例 SELECT prod_id, prod_name, prod_priceFROM productsWHERE vend_id = 'DLL01' AND prod_price &lt;= 4; OR 示例 SELECT prod_id, prod_name, prod_priceFROM productsWHERE vend_id = 'DLL01' OR vend_id = 'BRS01'; NOT 示例 SELECT *FROM productsWHERE prod_price NOT BETWEEN 3 AND 5; 4.4. LIKE LIKE 操作符在 WHERE 子句中使用，作用是确定字符串是否匹配模式。 只有字段是文本值时才使用 LIKE。 LIKE 支持两个通配符匹配选项：% 和 _。 不要滥用通配符，通配符位于开头处匹配会非常慢。 % 表示任何字符出现任意次数。 _ 表示任何字符出现一次。 % 示例 SELECT prod_id, prod_name, prod_priceFROM productsWHERE prod_name LIKE '%bean bag%'; _ 示例 SELECT prod_id, prod_name, prod_priceFROM productsWHERE prod_name LIKE '__ inch teddy bear'; 5. 函数 各个 DBMS 的函数都是不相同的，因此不可移植。 5.1. 文本处理 函数 说明 LEFT() RIGHT() 左边或者右边的字符 LOWER() UPPER() 转换为小写或者大写 LTRIM() RTIM() 去除左边或者右边的空格 LENGTH() 长度 SOUNDEX() 转换为语音值 其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。 SELECT *FROM mytableWHERE SOUNDEX(col1) = SOUNDEX('apple') 5.2. 日期和时间处理 日期格式：YYYY-MM-DD 时间格式：HH:MM:SS 函 数 说 明 AddDate() 增加一个日期（天、周等） AddTime() 增加一个时间（时、分等） CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分 DateDiff() 计算两个日期之差 Date_Add() 高度灵活的日期运算函数 Date_Format() 返回一个格式化的日期或时间串 Day() 返回一个日期的天数部分 DayOfWeek() 对于一个日期，返回对应的星期几 Hour() 返回一个时间的小时部分 Minute() 返回一个时间的分钟部分 Month() 返回一个日期的月份部分 Now() 返回当前日期和时间 Second() 返回一个时间的秒部分 Time() 返回一个日期时间的时间部分 Year() 返回一个日期的年份部分 mysql&gt; SELECT NOW(); 2018-4-14 20:25:11 5.3. 数值处理 函数 说明 SIN() 正弦 COS() 余弦 TAN() 正切 ABS() 绝对值 SQRT() 平方根 MOD() 余数 EXP() 指数 PI() 圆周率 RAND() 随机数 5.4. 汇总 函 数 说 明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 AVG() 会忽略 NULL 行。 使用 DISTINCT 可以让汇总函数值汇总不同的值。 SELECT AVG(DISTINCT col1) AS avg_colFROM mytable 6. 排序和分组 6.1. ORDER BY ORDER BY 用于对结果集进行排序。 ASC ：升序（默认） DESC ：降序 可以按多个列进行排序，并且为每个列指定不同的排序方式 指定多个列的排序方向 SELECT * FROM productsORDER BY prod_price DESC, prod_name ASC; 6.2. GROUP BY GROUP BY 子句将记录分组到汇总行中。 GROUP BY 为每个组返回一个记录。 GROUP BY 通常还涉及聚合：COUNT，MAX，SUM，AVG 等。 GROUP BY 可以按一列或多列进行分组。 GROUP BY 按分组字段进行排序后，ORDER BY 可以以汇总字段来进行排序。 分组 SELECT cust_name, COUNT(cust_address) AS addr_numFROM Customers GROUP BY cust_name; 分组后排序 SELECT cust_name, COUNT(cust_address) AS addr_numFROM Customers GROUP BY cust_nameORDER BY cust_name DESC; 6.3. HAVING HAVING 用于对汇总的 GROUP BY 结果进行过滤。 HAVING 要求存在一个 GROUP BY 子句。 WHERE 和 HAVING 可以在相同的查询中。 HAVING vs WHERE WHERE 和 HAVING 都是用于过滤。 HAVING 适用于汇总的组记录；而 WHERE 适用于单个记录。 使用 WHERE 和 HAVING 过滤数据 SELECT cust_name, COUNT(*) AS numFROM CustomersWHERE cust_email IS NOT NULLGROUP BY cust_nameHAVING COUNT(*) &gt;= 1; 7. 子查询 子查询是嵌套在较大查询中的 SQL 查询。 子查询也称为内部查询或内部选择，而包含子查询的语句也称为外部查询或外部选择。 子查询可以嵌套在 SELECT，INSERT，UPDATE 或 DELETE 语句内或另一个子查询中。 子查询通常会在另一个 SELECT 语句的 WHERE 子句中添加。 您可以使用比较运算符，如 &gt;，&lt;，或 =。比较运算符也可以是多行运算符，如 IN，ANY 或 ALL。 子查询必须被圆括号 () 括起来。 内部查询首先在其父查询之前执行，以便可以将内部查询的结果传递给外部查询。执行过程可以参考下图： 子查询的子查询 SELECT cust_name, cust_contactFROM customersWHERE cust_id IN (SELECT cust_id FROM orders WHERE order_num IN (SELECT order_num FROM orderitems WHERE prod_id = 'RGAN01')); 8. 连接和组合 8.1. 连接（JOIN） 如果一个 JOIN 至少有一个公共字段并且它们之间存在关系，则该 JOIN 可以在两个或多个表上工作。 连接用于连接多个表，使用 JOIN 关键字，并且条件语句使用 ON 而不是 WHERE。 JOIN 保持基表（结构和数据）不变。 JOIN 有两种连接类型：内连接和外连接。 内连接又称等值连接，使用 INNER JOIN 关键字。在没有条件语句的情况下返回笛卡尔积。 自连接可以看成内连接的一种，只是连接的表是自身而已。 自然连接是把同名列通过 = 测试连接起来的，同名列可以有多个。 内连接 vs 自然连接 内连接提供连接的列，而自然连接自动连接所有同名列。 外连接返回一个表中的所有行，并且仅返回来自次表中满足连接条件的那些行，即两个表中的列是相等的。外连接分为左外连接、右外连接、全外连接（Mysql 不支持）。 左外连接就是保留左表没有关联的行。 右外连接就是保留右表没有关联的行。 连接 vs 子查询 连接可以替换子查询，并且比子查询的效率一般会更快。 内连接（INNER JOIN） SELECT vend_name, prod_name, prod_priceFROM vendors INNER JOIN productsON vendors.vend_id = products.vend_id; 自连接 SELECT c1.cust_id, c1.cust_name, c1.cust_contactFROM customers c1, customers c2WHERE c1.cust_name = c2.cust_nameAND c2.cust_contact = 'Jim Jones'; 自然连接（NATURAL JOIN） SELECT *FROM ProductsNATURAL JOIN Customers; 左连接（LEFT JOIN） SELECT customers.cust_id, orders.order_numFROM customers LEFT JOIN ordersON customers.cust_id = orders.cust_id; 右连接（RIGHT JOIN） SELECT customers.cust_id, orders.order_numFROM customers RIGHT JOIN ordersON customers.cust_id = orders.cust_id; 8.2. 组合（UNION） UNION 运算符将两个或更多查询的结果组合起来，并生成一个结果集，其中包含来自 UNION 中参与查询的提取行。 UNION 基本规则 所有查询的列数和列顺序必须相同。 每个查询中涉及表的列的数据类型必须相同或兼容。 通常返回的列名取自第一个查询。 默认会去除相同行，如果需要保留相同行，使用 UNION ALL。 只能包含一个 ORDER BY 子句，并且必须位于语句的最后。 应用场景 在一个查询中从不同的表返回结构数据。 对一个表执行多个查询，按一个查询返回数据。 组合查询 SELECT cust_name, cust_contact, cust_emailFROM customersWHERE cust_state IN ('IL', 'IN', 'MI')UNIONSELECT cust_name, cust_contact, cust_emailFROM customersWHERE cust_name = 'Fun4All'; 8.3. JOIN vs UNION JOIN vs UNION JOIN 中连接表的列可能不同，但在 UNION 中，所有查询的列数和列顺序必须相同。 UNION 将查询之后的行放在一起（垂直放置），但 JOIN 将查询之后的列放在一起（水平放置），即它构成一个笛卡尔积。 9. 数据定义 DDL 的主要功能是定义数据库对象（如：数据库、数据表、视图、索引等）。 9.1. 数据库（DATABASE） 创建数据库 CREATE DATABASE test; 删除数据库 DROP DATABASE test; 选择数据库 USE test; 9.2. 数据表（TABLE） 创建数据表 普通创建 CREATE TABLE user ( id int(10) unsigned NOT NULL COMMENT 'Id', username varchar(64) NOT NULL DEFAULT 'default' COMMENT '用户名', password varchar(64) NOT NULL DEFAULT 'default' COMMENT '密码', email varchar(64) NOT NULL DEFAULT 'default' COMMENT '邮箱') COMMENT='用户表'; 根据已有的表创建新表 CREATE TABLE vip_user ASSELECT * FROM user; 删除数据表 DROP TABLE user; 修改数据表 添加列 ALTER TABLE userADD age int(3); 删除列 ALTER TABLE userDROP COLUMN age; 修改列 ALTER TABLE `user`MODIFY COLUMN age tinyint; 添加主键 ALTER TABLE userADD PRIMARY KEY (id); 删除主键 ALTER TABLE userDROP PRIMARY KEY; 9.3. 视图（VIEW） 定义 视图是基于 SQL 语句的结果集的可视化的表。 视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。对视图的操作和对普通表的操作一样。 作用 简化复杂的 SQL 操作，比如复杂的联结； 只使用实际表的一部分数据； 通过只给用户访问视图的权限，保证数据的安全性； 更改数据格式和表示。 创建视图 CREATE VIEW top_10_user_view ASSELECT id, usernameFROM userWHERE id &lt; 10; 删除视图 DROP VIEW top_10_user_view; 9.4. 索引（INDEX） 作用 通过索引可以更加快速高效地查询数据。 用户无法看到索引，它们只能被用来加速查询。 注意 更新一个包含索引的表需要比更新一个没有索引的表花费更多的时间，这是由于索引本身也需要更新。因此，理想的做法是仅仅在常常被搜索的列（以及表）上面创建索引。 唯一索引 唯一索引表明此索引的每一个索引值只对应唯一的数据记录。 创建索引 CREATE INDEX user_indexON user (id); 创建唯一索引 CREATE UNIQUE INDEX user_indexON user (id); 删除索引 ALTER TABLE userDROP INDEX user_index; 10. 约束 SQL 约束用于规定表中的数据规则。 如果存在违反约束的数据行为，行为会被约束终止。 约束可以在创建表时规定（通过 CREATE TABLE 语句），或者在表创建之后规定（通过 ALTER TABLE 语句）。 约束类型 NOT NULL - 指示某列不能存储 NULL 值。 UNIQUE - 保证某列的每行必须有唯一的值。 PRIMARY KEY - NOT NULL 和 UNIQUE 的结合。确保某列（或两个列多个列的结合）有唯一标识，有助于更容易更快速地找到表中的一个特定的记录。 FOREIGN KEY - 保证一个表中的数据匹配另一个表中的值的参照完整性。 CHECK - 保证列中的值符合指定的条件。 DEFAULT - 规定没有给列赋值时的默认值。 10.1. 创建表时使用约束条件 CREATE TABLE Users ( Id INT(10) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '自增Id', Username VARCHAR(64) NOT NULL UNIQUE DEFAULT 'default' COMMENT '用户名', Password VARCHAR(64) NOT NULL DEFAULT 'default' COMMENT '密码', Email VARCHAR(64) NOT NULL DEFAULT 'default' COMMENT '邮箱地址', Enabled TINYINT(4) DEFAULT NULL COMMENT '是否有效', PRIMARY KEY (Id)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COMMENT='用户表'; 11. 事务处理 不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。 MySQL 默认是隐式提交，每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。 通过 set autocommit=0 可以取消自动提交，直到 set autocommit=1 才会提交；autocommit 标记是针对每个连接而不是针对服务器的。 指令 START TRANSACTION - 指令用于标记事务的起始点。 SAVEPOINT - 指令用于创建保留点。 ROLLBACK TO - 指令用于回滚到指定的保留点；如果没有设置保留点，则回退到 START TRANSACTION 语句处。 COMMIT - 提交事务。 -- 开始事务START TRANSACTION;-- 插入操作 AINSERT INTO `user`VALUES (1, 'root1', 'root1', 'xxxx@163.com');-- 创建保留点 updateASAVEPOINT updateA;-- 插入操作 BINSERT INTO `user`VALUES (2, 'root2', 'root2', 'xxxx@163.com');-- 回滚到保留点 updateAROLLBACK TO updateA;-- 提交事务，只有操作 A 生效COMMIT; 12. 权限控制 GRANT 和 REVOKE 可在几个层次上控制访问权限： 整个服务器，使用 GRANT ALL 和 REVOKE ALL； 整个数据库，使用 ON database.*； 特定的表，使用 ON database.table； 特定的列； 特定的存储过程。 新创建的账户没有任何权限。 账户用 username@host 的形式定义，username@% 使用的是默认主机名。 MySQL 的账户信息保存在 mysql 这个数据库中。&gt; USE mysql;&gt; SELECT user FROM user;&gt; 12.1. 创建账户 CREATE USER myuser IDENTIFIED BY 'mypassword'; 12.2. 修改账户名 UPDATE user SET user='newuser' WHERE user='myuser';FLUSH PRIVILEGES; 12.3. 删除账户 DROP USER myuser; 12.4. 查看权限 SHOW GRANTS FOR myuser; 12.5. 授予权限 GRANT SELECT, INSERT ON *.* TO myuser; 12.6. 删除权限 REVOKE SELECT, INSERT ON *.* FROM myuser; 12.7. 更改密码 SET PASSWORD FOR myuser = 'mypass'; 13. 存储过程 存储过程可以看成是对一系列 SQL 操作的批处理； 使用存储过程的好处 代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。 创建存储过程 命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。 包含 in、out 和 inout 三种参数。 给变量赋值都需要用 select into 语句。 每次只能给一个变量赋值，不支持集合的操作。 13.1. 创建存储过程 DROP PROCEDURE IF EXISTS `proc_adder`;DELIMITER ;;CREATE DEFINER=`root`@`localhost` PROCEDURE `proc_adder`(IN a int, IN b int, OUT sum int)BEGIN DECLARE c int; if a is null then set a = 0; end if; if b is null then set b = 0; end if; set sum = a + b;END;;DELIMITER ; 13.2. 使用存储过程 set @b=5;call proc_adder(2,@b,@s);select @s as sum; 14. 游标 游标（cursor）是一个存储在 DBMS 服务器上的数据库查询，它不是一条 SELECT 语句，而是被该语句检索出来的结果集。 在存储过程中使用游标可以对一个结果集进行移动遍历。 游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。 使用游标的四个步骤： 声明游标，这个过程没有实际检索出数据； 打开游标； 取出数据； 关闭游标； DELIMITER $CREATE PROCEDURE getTotal()BEGIN DECLARE total INT; -- 创建接收游标数据的变量 DECLARE sid INT; DECLARE sname VARCHAR(10); -- 创建总数变量 DECLARE sage INT; -- 创建结束标志变量 DECLARE done INT DEFAULT false; -- 创建游标 DECLARE cur CURSOR FOR SELECT id,name,age from cursor_table where age&gt;30; -- 指定游标循环结束时的返回值 DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = true; SET total = 0; OPEN cur; FETCH cur INTO sid, sname, sage; WHILE(NOT done) DO SET total = total + 1; FETCH cur INTO sid, sname, sage; END WHILE; CLOSE cur; SELECT total;END $DELIMITER ;-- 调用存储过程call getTotal(); 15. 触发器 触发器是一种与表操作有关的数据库对象，当触发器所在表上出现指定事件时，将调用该对象，即表的操作事件触发表上的触发器的执行。 可以使用触发器来进行审计跟踪，把修改记录到另外一张表中。 MySQL 不允许在触发器中使用 CALL 语句 ，也就是不能调用存储过程。 BEGIN 和 END 当触发器的触发条件满足时，将会执行 BEGIN 和 END 之间的触发器执行动作。 注意：在 MySQL 中，分号 ; 是语句结束的标识符，遇到分号表示该段语句已经结束，MySQL 可以开始执行了。因此，解释器遇到触发器执行动作中的分号后就开始执行，然后会报错，因为没有找到和 BEGIN 匹配的 END。 这时就会用到 DELIMITER 命令（DELIMITER 是定界符，分隔符的意思）。它是一条命令，不需要语句结束标识，语法为：DELIMITER new_delemiter。new_delemiter 可以设为 1 个或多个长度的符号，默认的是分号 ;，我们可以把它修改为其他符号，如 $ - DELIMITER $ 。在这之后的语句，以分号结束，解释器不会有什么反应，只有遇到了 $，才认为是语句结束。注意，使用完之后，我们还应该记得把它给修改回来。 NEW 和 OLD MySQL 中定义了 NEW 和 OLD 关键字，用来表示触发器的所在表中，触发了触发器的那一行数据。 在 INSERT 型触发器中，NEW 用来表示将要（BEFORE）或已经（AFTER）插入的新数据； 在 UPDATE 型触发器中，OLD 用来表示将要或已经被修改的原数据，NEW 用来表示将要或已经修改为的新数据； 在 DELETE 型触发器中，OLD 用来表示将要或已经被删除的原数据； 使用方法： NEW.columnName （columnName 为相应数据表某一列名） 15.1. 创建触发器 提示：为了理解触发器的要点，有必要先了解一下创建触发器的指令。 CREATE TRIGGER 指令用于创建触发器。 语法： CREATE TRIGGER trigger_nametrigger_timetrigger_eventON table_nameFOR EACH ROWBEGIN trigger_statementsEND; 说明： trigger_name：触发器名 trigger_time: 触发器的触发时机。取值为 BEFORE 或 AFTER。 trigger_event: 触发器的监听事件。取值为 INSERT、UPDATE 或 DELETE。 table_name: 触发器的监听目标。指定在哪张表上建立触发器。 FOR EACH ROW: 行级监视，Mysql 固定写法，其他 DBMS 不同。 trigger_statements: 触发器执行动作。是一条或多条 SQL 语句的列表，列表内的每条语句都必须用分号 ; 来结尾。 示例： DELIMITER $CREATE TRIGGER `trigger_insert_user`AFTER INSERT ON `user`FOR EACH ROWBEGIN INSERT INTO `user_history`(user_id, operate_type, operate_time) VALUES (NEW.id, 'add a user', now());END $DELIMITER ; 15.2. 查看触发器 SHOW TRIGGERS; 15.3. 删除触发器 DROP TRIGGER IF EXISTS trigger_insert_user; 16. 更多内容 📓 本文已归档到：「blog」 BenForta. SQL 必知必会 [M]. 人民邮电出版社, 2013. 『浅入深出』MySQL 中事务的实现 MySQL 的学习–触发器 维基百科词条 - SQL https://www.sitesbay.com/sql/index SQL Subqueries Quick breakdown of the types of joins SQL UNION SQL database security Mysql 中的存储过程]]></content>
  </entry>
  <entry>
    <title><![CDATA[Flyway]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2Fmiddleware%2Fflyway%2F</url>
    <content type="text"><![CDATA[Flyway Flyway 是一个数据迁移工具。 关键词： 简介 什么是 Flyway？ 为什么要使用数据迁移？ Flyway 如何工作？ 快速上手 命令行 JAVA API Maven Gradle 入门篇 概念 命令 支持的数据库 资料 简介 什么是 Flyway？ Flyway 是一个开源的数据库迁移工具。 为什么要使用数据迁移？ 为了说明数据迁移的作用，我们来举一个示例： （1）假设，有一个叫做 Shiny 的项目，它的架构是一个叫做 Shiny Soft 的 App 连接叫做 Shiny DB 的数据库。 （2）对于大多数项目而言，最简单的持续集成场景如下所示： 这意味着，我们不仅仅要处理一份环境中的修改，由此会引入一些版本冲突问题： 在代码侧（即应用软件）的版本问题比较容易解决： 有方便的版本控制工具 有可复用的构建和持续集成 规范的发布和部署过程 那么，数据库层面的版本问题如何解决呢？ 目前仍然没有方便的数据库版本工具。许多项目仍使用 sql 脚本来解决版本冲突，甚至是遇到冲突问题时才想起用 sql 语句去解决。 由此，引发一些问题： 机器上的数据库是什么状态？ 脚本到底生效没有？ 生产环境修复的问题是否也在测试环境修复了？ 如何建立一个新的数据库实例？ 数据迁移就是用来搞定这些混乱的问题： 通过草稿重建一个数据库。 在任何时候都可以清楚的了解数据库的状态。 以一种明确的方式将数据库从当前版本迁移到一个新版本。 Flyway 如何工作？ 最简单的场景是指定 Flyway 迁移到一个空的数据库。 Flyway 会尝试查找它的 schema 历史表，如果数据库是空的，Flyway 就不再查找，而是直接创建数据库。 现再你就有了一个仅包含一张空表的数据库，默认情况下，这张表叫 flyway_schema_history。 这张表将被用于追踪数据库的状态。 然后，Flyway 将开始扫描文件系统或应用 classpath 中的 migrations。这些 migrations 可以是 sql 或 java。 这些 migrations 将根据他们的版本号进行排序。 任意 migration 应用后，schema 历史表将更新。当元数据和初始状态替换后，可以称之为：迁移到新版本。 Flyway 一旦扫描了文件系统或应用 classpath 下的 migrations，这些 migrations 会检查 schema 历史表。如果它们的版本号低于或等于当前的版本，将被忽略。保留下来的 migrations 是等待的 migrations，有效但没有应用。 migrations 将根据版本号排序并按序执行。 快速上手 Flyway 有 4 种使用方式： 命令行 JAVA API Maven Gradle 命令行 适用于非 Java 用户，无需构建。 &gt; flyway migrate -url=... -user=... -password=... （1）下载解压 进入官方下载页面，选择合适版本，下载并解压到本地。 （2）配置 flyway 编辑 /conf/flyway.conf： flyway.url=jdbc:h2:file:./foobardbflyway.user=SAflyway.password= （3）创建第一个 migration 在 /sql 目录下创建 V1__Create_person_table.sql 文件，内容如下： create table PERSON ( ID int not null, NAME varchar(100) not null); （4）迁移数据库 运行 Flyway 来迁移数据库： flyway-5.1.4&gt; flyway migrate 运行正常的情况下，应该可以看到如下结果： Database: jdbc:h2:file:./foobardb (H2 1.4)Successfully validated 1 migration (execution time 00:00.008s)Creating Schema History table: "PUBLIC"."flyway_schema_history"Current version of schema "PUBLIC": &lt;&lt; Empty Schema &gt;&gt;Migrating schema "PUBLIC" to version 1 - Create person tableSuccessfully applied 1 migration to schema "PUBLIC" (execution time 00:00.033s) （5）添加第二个 migration 在 /sql 目录下创建 V2__Add_people.sql 文件，内容如下： insert into PERSON (ID, NAME) values (1, 'Axel');insert into PERSON (ID, NAME) values (2, 'Mr. Foo');insert into PERSON (ID, NAME) values (3, 'Ms. Bar'); 运行 Flyway flyway-5.1.4&gt; flyway migrate 运行正常的情况下，应该可以看到如下结果： Database: jdbc:h2:file:./foobardb (H2 1.4)Successfully validated 2 migrations (execution time 00:00.018s)Current version of schema "PUBLIC": 1Migrating schema "PUBLIC" to version 2 - Add peopleSuccessfully applied 1 migration to schema "PUBLIC" (execution time 00:00.016s) JAVA API （1）准备 Java8+ Maven 3.x （2）添加依赖 在 pom.xml 中添加依赖： &lt;project ...&gt; ... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.flywaydb&lt;/groupId&gt; &lt;artifactId&gt;flyway-core&lt;/artifactId&gt; &lt;version&gt;5.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;version&gt;1.3.170&lt;/version&gt; &lt;/dependency&gt; ... &lt;/dependencies&gt; ...&lt;/project&gt; （3）集成 Flyway 添加 App.java 文件，内容如下： import org.flywaydb.core.Flyway;public class App &#123; public static void main(String[] args) &#123; // Create the Flyway instance Flyway flyway = new Flyway(); // Point it to the database flyway.setDataSource("jdbc:h2:file:./target/foobar", "sa", null); // Start the migration flyway.migrate(); &#125;&#125; （4）创建第一个 migration 添加 src/main/resources/db/migration/V1__Create_person_table.sql 文件，内容如下： create table PERSON ( ID int not null, NAME varchar(100) not null); （5）执行程序 执行 App#main： 运行正常的情况下，应该可以看到如下结果： INFO: Creating schema history table: "PUBLIC"."flyway_schema_history"INFO: Current version of schema "PUBLIC": &lt;&lt; Empty Schema &gt;&gt;INFO: Migrating schema "PUBLIC" to version 1 - Create person tableINFO: Successfully applied 1 migration to schema "PUBLIC" (execution time 00:00.062s). （6）添加第二个 migration 添加 src/main/resources/db/migration/V2__Add_people.sql 文件，内容如下： insert into PERSON (ID, NAME) values (1, 'Axel');insert into PERSON (ID, NAME) values (2, 'Mr. Foo');insert into PERSON (ID, NAME) values (3, 'Ms. Bar'); 运行正常的情况下，应该可以看到如下结果： INFO: Current version of schema "PUBLIC": 1INFO: Migrating schema "PUBLIC" to version 2 - Add peopleINFO: Successfully applied 1 migration to schema "PUBLIC" (execution time 00:00.090s). Maven 与 Java API 方式大体相同，区别在 集成 Flyway 步骤： Maven 方式使用插件来集成 Flyway： &lt;project xmlns="..."&gt; ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.flywaydb&lt;/groupId&gt; &lt;artifactId&gt;flyway-maven-plugin&lt;/artifactId&gt; &lt;version&gt;5.1.4&lt;/version&gt; &lt;configuration&gt; &lt;url&gt;jdbc:h2:file:./target/foobar&lt;/url&gt; &lt;user&gt;sa&lt;/user&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;version&gt;1.4.191&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 因为用的是插件，所以执行方式不再是运行 Java 类，而是执行 maven 插件： &gt; mvn flyway:migrate 👉 参考：示例源码 Gradle 本人不用 Gradle，略。 入门篇 概念 Migrations 在 Flyway 中，对于数据库的任何改变都称之为 Migrations。 Migrations 可以分为 Versioned migrations 和 Repeatable migrations。 Versioned migrations 有 2 种形式：regular 和 undo。 Versioned migrations 和 Repeatable migrations 都可以使用 SQL 或 JAVA 来编写。 Versioned migrations 由一个版本号（version）、一段描述（description）、一个校验（checksum）组成。版本号必须是惟一的。Versioned migrations 只能按顺序执行一次。 一般用于： 增删改 tables/indexes/foreign keys/enums/UDTs。 引用数据更新 用户数据校正 Regular 示例： CREATE TABLE car ( id INT NOT NULL PRIMARY KEY, license_plate VARCHAR NOT NULL, color VARCHAR NOT NULL);ALTER TABLE owner ADD driver_license_id VARCHAR;INSERT INTO brand (name) VALUES ('DeLorean'); Undo migrations 注：仅专业版支持 Undo Versioned Migrations 负责撤销 Regular Versioned migrations 的影响。 Undo 示例： DELETE FROM brand WHERE name='DeLorean';ALTER TABLE owner DROP driver_license_id;DROP TABLE car; Repeatable migrations 由一段描述（description）、一个校验（checksum）组成。Versioned migrations 每次执行后，校验（checksum）会更新。 Repeatable migrations 用于管理可以通过一个文件来维护版本控制的数据库对象。 一般用于： 创建（重建）views/procedures/functions/packages 等。 大量引用数据重新插入 示例： CREATE OR REPLACE VIEW blue_cars AS SELECT id, license_plate FROM cars WHERE color='blue'; 基于 SQL 的 migrations migrations 最常用的编写形式就是 SQL。 基于 SQL 的 migrations 一般用于： DDL 变更（针对 TABLES,VIEWS,TRIGGERS,SEQUENCES 等的 CREATE/ALTER/DROP 操作） 简单的引用数据变更（引用数据表中的 CRUD） 简单的大量数据变更（常规数据表中的 CRUD） 命名规则 为了被 Flyway 自动识别，SQL migrations 的文件命名必须遵循规定的模式： Prefix - V 代表 versioned migrations (可配置), U 代表 undo migrations (可配置)、 R 代表 repeatable migrations (可配置) Version - 版本号通过.(点)或_(下划线)分隔 (repeatable migrations 不需要) Separator - __ (两个下划线) (可配置) Description - 下划线或空格分隔的单词 Suffix - .sql (可配置) 基于 JAVA 的 migrations 基于 JAVA 的 migrations 适用于使用 SQL 不容易表达的场景： BLOB 和 CLOB 变更 大量数据的高级变更（重新计算、高级格式变更） 命名规则 为了被 Flyway 自动识别，JAVA migrations 的文件命名必须遵循规定的模式： Prefix - V 代表 versioned migrations (可配置), U 代表 undo migrations (可配置)、 R 代表 repeatable migrations (可配置) Version - 版本号通过.(点)或_(下划线)分隔 (repeatable migrations 不需要) Separator - __ (两个下划线) (可配置) Description - 下划线或空格分隔的单词 👉 更多细节请参考：https://flywaydb.org/documentation/migrations Callbacks 注：部分 events 仅专业版支持。 尽管 Migrations 可能已经满足绝大部分场景的需要，但是某些情况下需要你一遍又一遍的执行相同的行为。这可能会重新编译存储过程，更新视图以及许多其他类型的开销。 因为以上原因，Flyway 提供了 Callbacks，用于在 Migrations 生命周期中添加钩子。 Callbacks 可以用 SQL 或 JAVA 来实现。 SQL Callbacks SQL Callbacks 的命名规则为：event 名 + SQL migration。 如： beforeMigrate.sql, beforeEachMigrate.sql, afterEachMigrate.sql 等。 SQL Callbacks 也可以包含描述（description）。这种情况下，SQL Callbacks 文件名 = event 名 + 分隔符 + 描述 + 后缀。例：beforeRepair__vacuum.sql 当同一个 event 有多个 SQL callbacks，将按照它们描述（description）的顺序执行。 注： Flyway 也支持你配置的 sqlMigrationSuffixes。 JAVA Callbacks 当 SQL Callbacks 不够方便时，才应考虑 JAVA Callbacks。 JAVA Callbacks 有 3 种形式： 基于 Java 的 Migrations - 实现 JdbcMigration、SpringJdbcMigration、MigrationInfoProvider、MigrationChecksumProvider、ConfigurationAware、FlywayConfiguration 基于 Java 的 Callbacks - 实现 org.flywaydb.core.api.callback 接口。 自定义 Migration resolvers 和 executors - 实现 MigrationResolver、MigrationExecutor、ConfigurationAware、FlywayConfiguration 接口。 👉 更多细节请参考：https://flywaydb.org/documentation/callbacks Error Handlers 注：仅专业版支持。 （略） Dry Runs 注：仅专业版支持。 （略） 命令 Flyway 的功能主要围绕着 7 个基本命令：Migrate、Clean、Info、Validate、Undo、Baseline 和 Repair。 注：各命令的使用方法细节请查阅官方文档。 支持的数据库 Oracle SQL Server DB2 MySQL MariaDB PostgreSQL Redshift CockroachDB SAP HANA Sybase ASE Informix H2 HSQLDB Derby SQLite 资料 | Github | 官方文档 |]]></content>
  </entry>
  <entry>
    <title><![CDATA[计算机网络之网络层]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fcommunication%2Fnetwork-network%2F</url>
    <content type="text"><![CDATA[计算机网络之网络层 网络层（network layer） - 为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组或包进行传送。 主要协议：IP、ICMP。 数据单元：IP 数据报（packet）。 典型设备：网关、路由器。 概述 IP 协议 相关协议 分类的 IP 地址 IP 地址与物理地址 IP 数据报格式 地址解析协议 ARP 网际控制报文协议 ICMP 1. Ping 2. Traceroute 虚拟专用网 VPN 网络地址转换 NAT 路由器的结构 路由器分组转发流程 路由选择协议 1. 内部网关协议 RIP 2. 内部网关协议 OSPF 3. 外部网关协议 BGP 概述 网络层向上只提供简单灵活的、无连接的、尽最大努力交付的数据报服务。网络层不提供服务质量的承诺，不保证分组交付的时限，所传送的分组可能出错、丢失、重复和失序。进程间通信的可靠性由运输层负责。 IP 协议 网际协议 IP (Internet Protocol) 定义了三种功能： IP 定义了在 TCP/IP 互联网上数据传送的基本单元和数据格式。 IP 软件完成路由选择功能，选择数据传送的路径。 IP 包含了一组不可靠分组传送的规则，指明了分组处理、差错信息发生以及分组的规则。 相关协议 与 IP 协议配套使用的还有三个协议： 地址解析协议 ARP（Address Resolution Protocol） 网际控制报文协议 ICMP（Internet Control Message Protocol） 网际组管理协议 IGMP（Internet Group Management Protocol） 分类的 IP 地址 IP 地址的编址方式经历了三个历史阶段： 分类 子网划分 无分类 1. 分类 由两部分组成，网络号和主机号，其中不同分类具有不同的网络号长度，并且是固定的。 IP 地址 ::= &#123;&lt; 网络号 &gt;, &lt; 主机号 &gt;&#125; 2. 子网划分 通过在主机号字段中拿一部分作为子网号，把两级 IP 地址划分为三级 IP 地址。 IP 地址 ::= &#123;&lt; 网络号 &gt;, &lt; 子网号 &gt;, &lt; 主机号 &gt;&#125; 要使用子网，必须配置子网掩码。一个 B 类地址的默认子网掩码为 255.255.0.0，如果 B 类地址的子网占两个比特，那么子网掩码为 11111111 11111111 11000000 00000000，也就是 255.255.192.0。 注意，外部网络看不到子网的存在。 3. 无分类 无分类编址 CIDR 消除了传统 A 类、B 类和 C 类地址以及划分子网的概念，使用网络前缀和主机号来对 IP 地址进行编码，网络前缀的长度可以根据需要变化。 IP 地址 ::= &#123;&lt; 网络前缀号 &gt;, &lt; 主机号 &gt;&#125; CIDR 的记法上采用在 IP 地址后面加上网络前缀长度的方法，例如 128.14.35.7/20 表示前 20 位为网络前缀。 CIDR 的地址掩码可以继续称为子网掩码，子网掩码首 1 长度为网络前缀的长度。 一个 CIDR 地址块中有很多地址，一个 CIDR 表示的网络就可以表示原来的很多个网络，并且在路由表中只需要一个路由就可以代替原来的多个路由，减少了路由表项的数量。把这种通过使用网络前缀来减少路由表项的方式称为路由聚合，也称为 构成超网 。 在路由表中的项目由“网络前缀”和“下一跳地址”组成，在查找时可能会得到不止一个匹配结果，应当采用最长前缀匹配来确定应该匹配哪一个。 IP 地址与物理地址 物理地址是数据链路层和物理层使用的地址。 IP 地址是网络层和以上各层使用的地址，是一种逻辑地址。 IP 数据报格式 版本 - 有 4（IPv4）和 6（IPv6）两个值。 首部长度 - 占 4 位，因此最大十进制数值为 15。值为 1 表示的是 1 个 32 位字的长度，也就是 4 字节。因为首部固定长度为 20 字节，因此该值最小为 5。如果可选字段的长度不是 4 字节的整数倍，就用尾部的填充部分来填充。 区分服务 - 用来获得更好的服务，一般情况下不使用。 总长度 - 包括首部长度和数据部分长度。占 16 位，因此数据报的最大长度为 2 16 - 1 = 65535 字节。 生存时间 - TTL，它的存在是为了防止无法交付的数据报在互联网中不断兜圈子。以路由器跳数为单位，当 TTL 为 0 时就丢弃数据报。 协议 - 指出携带的数据应该上交给哪个协议进行处理，例如 ICMP、TCP、UDP 等。 首部检验和 - 因为数据报每经过一个路由器，都要重新计算检验和，因此检验和不包含数据部分可以减少计算的工作量。 标识 - 在数据报长度过长从而发生分片的情况下，相同数据报的不同分片具有相同的标识符。 片偏移 - 和标识符一起，用于发生分片的情况。片偏移的单位为 8 字节。 地址解析协议 ARP 网络层实现主机之间的通信，而链路层实现具体每段链路之间的通信。因此在通信过程中，IP 数据报的源地址和目的地址始终不变，而 MAC 地址随着链路的改变而改变。 ARP 实现由 IP 地址得到 MAC 地址。 每个主机都有一个 ARP 高速缓存，里面有本局域网上的各主机和路由器的 IP 地址到 MAC 地址的映射表。 如果主机 A 知道主机 B 的 IP 地址，但是 ARP 高速缓存中没有该 IP 地址到 MAC 地址的映射，此时主机 A 通过广播的方式发送 ARP 请求分组，主机 B 收到该请求后会发送 ARP 响应分组给主机 A 告知其 MAC 地址，随后主机 A 向其高速缓存中写入主机 B 的 IP 地址到 MAC 地址的映射。 网际控制报文协议 ICMP ICMP 是为了更有效地转发 IP 数据报和提高交付成功的机会。它封装在 IP 数据报中，但是不属于高层协议。 ICMP 报文分为差错报告报文和询问报文。 1. Ping Ping 是 ICMP 的一个重要应用，主要用来测试两台主机之间的连通性。 Ping 的原理是通过向目的主机发送 ICMP Echo 请求报文，目的主机收到之后会发送 Echo 回答报文。Ping 会根据时间和成功响应的次数估算出数据包往返时间以及丢包率。 2. Traceroute Traceroute 是 ICMP 的另一个应用，用来跟踪一个分组从源点到终点的路径。 Traceroute 发送的 IP 数据报封装的是无法交付的 UDP 用户数据报，并由目的主机发送终点不可达差错报告报文。 源主机向目的主机发送一连串的 IP 数据报。第一个数据报 P1 的生存时间 TTL 设置为 1，当 P1 到达路径上的第一个路由器 R1 时，R1 收下它并把 TTL 减 1，此时 TTL 等于 0，R1 就把 P1 丢弃，并向源主机发送一个 ICMP 时间超过差错报告报文； 源主机接着发送第二个数据报 P2，并把 TTL 设置为 2。P2 先到达 R1，R1 收下后把 TTL 减 1 再转发给 R2，R2 收下后也把 TTL 减 1，由于此时 TTL 等于 0，R2 就丢弃 P2，并向源主机发送一个 ICMP 时间超过差错报文。 不断执行这样的步骤，直到最后一个数据报刚刚到达目的主机，主机不转发数据报，也不把 TTL 值减 1。但是因为数据报封装的是无法交付的 UDP，因此目的主机要向源主机发送 ICMP 终点不可达差错报告报文。 之后源主机知道了到达目的主机所经过的路由器 IP 地址以及到达每个路由器的往返时间。 虚拟专用网 VPN 由于 IP 地址的紧缺，一个机构能申请到的 IP 地址数往往远小于本机构所拥有的主机数。并且一个机构并不需要把所有的主机接入到外部的互联网中，机构内的计算机可以使用仅在本机构有效的 IP 地址（专用地址）。 有三个专用地址块： 10.0.0.0 ~ 10.255.255.255 172.16.0.0 ~ 172.31.255.255 192.168.0.0 ~ 192.168.255.255 VPN 使用公用的互联网作为本机构各专用网之间的通信载体。专用指机构内的主机只与本机构内的其它主机通信；虚拟指好像是，而实际上并不是，它有经过公用的互联网。 下图中，场所 A 和 B 的通信经过互联网，如果场所 A 的主机 X 要和另一个场所 B 的主机 Y 通信，IP 数据报的源地址是 10.1.0.1，目的地址是 10.2.0.3。数据报先发送到与互联网相连的路由器 R1，R1 对内部数据进行加密，然后重新加上数据报的首部，源地址是路由器 R1 的全球地址 125.1.2.3，目的地址是路由器 R2 的全球地址 194.4.5.6。路由器 R2 收到数据报后将数据部分进行解密，恢复原来的数据报，此时目的地址为 10.2.0.3，就交付给 Y。 网络地址转换 NAT 专用网内部的主机使用本地 IP 地址又想和互联网上的主机通信时，可以使用 NAT 来将本地 IP 转换为全球 IP。 在以前，NAT 将本地 IP 和全球 IP 一一对应，这种方式下拥有 n 个全球 IP 地址的专用网内最多只可以同时有 n 台主机接入互联网。为了更有效地利用全球 IP 地址，现在常用的 NAT 转换表把传输层的端口号也用上了，使得多个专用网内部的主机共用一个全球 IP 地址。使用端口号的 NAT 也叫做网络地址与端口转换 NAPT。 路由器的结构 路由器从功能上可以划分为：路由选择和分组转发。 分组转发结构由三个部分组成：交换结构、一组输入端口和一组输出端口。 路由器分组转发流程 从数据报的首部提取目的主机的 IP 地址 D，得到目的网络地址 N。 若 N 就是与此路由器直接相连的某个网络地址，则进行直接交付； 若路由表中有目的地址为 D 的特定主机路由，则把数据报传送给表中所指明的下一跳路由器； 若路由表中有到达网络 N 的路由，则把数据报传送给路由表中所指明的下一跳路由器； 若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器； 报告转发分组出错。 路由选择协议 路由选择协议都是自适应的，能随着网络通信量和拓扑结构的变化而自适应地进行调整。 互联网可以划分为许多较小的自治系统 AS，一个 AS 可以使用一种和别的 AS 不同的路由选择协议。 可以把路由选择协议划分为两大类： 自治系统内部的路由选择：RIP 和 OSPF 自治系统间的路由选择：BGP 1. 内部网关协议 RIP RIP 是一种基于距离向量的路由选择协议。距离是指跳数，直接相连的路由器跳数为 1。跳数最多为 15，超过 15 表示不可达。 RIP 按固定的时间间隔仅和相邻路由器交换自己的路由表，经过若干次交换之后，所有路由器最终会知道到达本自治系统中任何一个网络的最短距离和下一跳路由器地址。 距离向量算法： 对地址为 X 的相邻路由器发来的 RIP 报文，先修改报文中的所有项目，把下一跳字段中的地址改为 X，并把所有的距离字段加 1； 对修改后的 RIP 报文中的每一个项目，进行以下步骤： 若原来的路由表中没有目的网络 N，则把该项目添加到路由表中； 否则：若下一跳路由器地址是 X，则把收到的项目替换原来路由表中的项目；否则：若收到的项目中的距离 d 小于路由表中的距离，则进行更新（例如原始路由表项为 Net2, 5, P，新表项为 Net2, 4, X，则更新）；否则什么也不做。 若 3 分钟还没有收到相邻路由器的更新路由表，则把该相邻路由器标为不可达，即把距离置为 16。 RIP 协议实现简单，开销小。但是 RIP 能使用的最大距离为 15，限制了网络的规模。并且当网络出现故障时，要经过比较长的时间才能将此消息传送到所有路由器。 2. 内部网关协议 OSPF 开放最短路径优先 OSPF，是为了克服 RIP 的缺点而开发出来的。 开放表示 OSPF 不受某一家厂商控制，而是公开发表的；最短路径优先表示使用了 Dijkstra 提出的最短路径算法 SPF。 OSPF 具有以下特点： 向本自治系统中的所有路由器发送信息，这种方法是洪泛法。 发送的信息就是与相邻路由器的链路状态，链路状态包括与哪些路由器相连以及链路的度量，度量用费用、距离、时延、带宽等来表示。 只有当链路状态发生变化时，路由器才会发送信息。 所有路由器都具有全网的拓扑结构图，并且是一致的。相比于 RIP，OSPF 的更新过程收敛的很快。 3. 外部网关协议 BGP BGP（Border Gateway Protocol，边界网关协议） AS 之间的路由选择很困难，主要是由于： 互联网规模很大； 各个 AS 内部使用不同的路由选择协议，无法准确定义路径的度量； AS 之间的路由选择必须考虑有关的策略，比如有些 AS 不愿意让其它 AS 经过。 BGP 只能寻找一条比较好的路由，而不是最佳路由。 每个 AS 都必须配置 BGP 发言人，通过在两个相邻 BGP 发言人之间建立 TCP 连接来交换路由信息。]]></content>
      <categories>
        <category>communication</category>
      </categories>
      <tags>
        <tag>communication</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系型数据库面试题]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdatabase%2Fsql%2F%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[关系型数据库面试题 1. 存储引擎 1.1. mysql 有哪些存储引擎？有什么区别？ 2. 索引 2.1. 数据库索引有哪些数据结构？ 2.2. B-Tree 和 B+Tree 有什么区别？ 2.3. 索引原则有哪些？ 3. 事务 3.1. 数据库事务隔离级别？事务隔离级别分别解决什么问题？ 3.2. 如何解决分布式事务？若出现网络问题或宕机问题，如何解决？ 4. 锁 4.1. 数据库锁有哪些类型？如何实现？ 5. 分库分表 5.1. 为什么要分库分表？ 5.2. 分库分表的常见问题以及解决方案？ 5.3. 如何设计可以动态扩容缩容的分库分表方案？ 5.4. 有哪些分库分表中间件？各自有什么优缺点？底层实现原理？ 6. 数据库优化 6.1. 什么是执行计划？ 7. 数据库架构设计 7.1. 高并发系统数据层面如何设计？ 1. 存储引擎 1.1. mysql 有哪些存储引擎？有什么区别？ InnoDB - Mysql 的默认事务型存储引擎。性能不错且支持自动崩溃恢复。 MyISAM - Mysql 5.1 版本前的默认存储引擎。特性丰富但不支持事务，也没有崩溃恢复功能。 CSV - 可以将 CSV 文件作为 Mysql 的表来处理，但这种表不支持索引。 Memory - 适合快速访问数据，且数据不会被修改，重启丢失也没有关系。 NDB - 用于 Mysql 集群场景。 2. 索引 2.1. 数据库索引有哪些数据结构？ B-Tree B+Tree Hash 2.1.1. B-Tree 一棵 M 阶的 B-Tree 满足以下条件： 每个结点至多有 M 个孩子； 除根结点和叶结点外，其它每个结点至少有 M/2 个孩子； 根结点至少有两个孩子（除非该树仅包含一个结点）； 所有叶结点在同一层，叶结点不包含任何关键字信息； 有 K 个关键字的非叶结点恰好包含 K+1 个孩子； 对于任意结点，其内部的关键字 Key 是升序排列的。每个节点中都包含了 data。 对于每个结点，主要包含一个关键字数组 Key[]，一个指针数组（指向儿子）Son[]。 在 B-Tree 内，查找的流程是： 使用顺序查找（数组长度较短时）或折半查找方法查找 Key[]数组，若找到关键字 K，则返回该结点的地址及 K 在 Key[]中的位置； 否则，可确定 K 在某个 Key[i]和 Key[i+1]之间，则从 Son[i]所指的子结点继续查找，直到在某结点中查找成功； 或直至找到叶结点且叶结点中的查找仍不成功时，查找过程失败。 2.1.2. B+Tree B+Tree 是 B-Tree 的变种： 每个节点的指针上限为 2d 而不是 2d+1（d 为节点的出度）。 非叶子节点不存储 data，只存储 key；叶子节点不存储指针。 由于并不是所有节点都具有相同的域，因此 B+Tree 中叶节点和内节点一般大小不同。这点与 B-Tree 不同，虽然 B-Tree 中不同节点存放的 key 和指针可能数量不一致，但是每个节点的域和上限是一致的，所以在实现中 B-Tree 往往对每个节点申请同等大小的空间。 带有顺序访问指针的 B+Tree 一般在数据库系统或文件系统中使用的 B+Tree 结构都在经典 B+Tree 的基础上进行了优化，增加了顺序访问指针。 在 B+Tree 的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的 B+Tree。 这个优化的目的是为了提高区间访问的性能，例如上图中如果要查询 key 为从 18 到 49 的所有数据记录，当找到 18 后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 2.1.3. Hash Hash 索引只有精确匹配索引所有列的查询才有效。 对于每一行数据，对所有的索引列计算一个 hashcode。哈希索引将所有的 hashcode 存储在索引中，同时在 Hash 表中保存指向每个数据行的指针。 哈希索引的优点： 因为索引数据结构紧凑，所以查询速度非常快。 哈希索引的缺点： 哈希索引数据不是按照索引值顺序存储的，所以无法用于排序。 哈希索引不支持部分索引匹配查找。如，在数据列 (A,B) 上建立哈希索引，如果查询只有数据列 A，无法使用该索引。 哈希索引只支持等值比较查询，不支持任何范围查询，如 WHERE price &gt; 100。 哈希索引有可能出现哈希冲突，出现哈希冲突时，必须遍历链表中所有的行指针，逐行比较，直到找到符合条件的行。 2.2. B-Tree 和 B+Tree 有什么区别？ B+Tree 更适合外部存储(一般指磁盘存储)，由于内节点(非叶子节点)不存储 data，所以一个节点可以存储更多的内节点，每个节点能索引的范围更大更精确。也就是说使用 B+Tree 单次磁盘 IO 的信息量相比较 B-Tree 更大，IO 效率更高。 mysql 是关系型数据库，经常会按照区间来访问某个索引列，B+Tree 的叶子节点间按顺序建立了链指针，加强了区间访问性，所以 B+Tree 对索引列上的区间范围查询很友好。而 B-Tree 每个节点的 key 和 data 在一起，无法进行区间查找。 2.3. 索引原则有哪些？ 2.3.1. 独立的列 如果查询中的列不是独立的列，则数据库不会使用索引。 “独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。 ❌ 错误示例： SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5;SELECT ... WHERE TO_DAYS(CURRENT_DAT) - TO_DAYS(date_col) &lt;= 10; 2.3.2. 前缀索引和索引选择性 有时候需要索引很长的字符列，这会让索引变得大且慢。 解决方法是：可以索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。 索引的选择性是指：不重复的索引值和数据表记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。 对于 BLOB/TEXT/VARCHAR 这种文本类型的列，必须使用前缀索引，因为数据库往往不允许索引这些列的完整长度。 要选择足够长的前缀以保证较高的选择性，同时又不能太长（节约空间）。 2.3.3. 多列索引 不要为每个列创建独立的索引。 2.3.4. 选择合适的索引列顺序 经验法则：将选择性高的列或基数大的列优先排在多列索引最前列。 但有时，也需要考虑 WHERE 子句中的排序、分组和范围条件等因素，这些因素也会对查询性能造成较大影响。 2.3.5. 聚簇索引 聚簇索引不是一种单独的索引类型，而是一种数据存储方式。 聚簇表示数据行和相邻的键值紧凑地存储在一起。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。 2.3.6. 覆盖索引 索引包含所有需要查询的字段的值。 具有以下优点： 因为索引条目通常远小于数据行的大小，所以若只读取索引，能大大减少数据访问量。 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。 2.3.7. 使用索引扫描来做排序 索引最好既满足排序，又用于查找行。这样，就可以使用索引来对结果排序。 2.3.8. = 和 in 可以乱序 比如 a = 1 and b = 2 and c = 3 建立（a,b,c）索引可以任意顺序，mysql 的查询优化器会帮你优化成索引可以识别的形式。 2.3.9. 尽量的扩展索引，不要新建索引 比如表中已经有 a 的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 3. 事务 3.1. 数据库事务隔离级别？事务隔离级别分别解决什么问题？ 未提交读（READ UNCOMMITTED） - 事务中的修改，即使没有提交，对其它事务也是可见的。 提交读（READ COMMITTED） - 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。 可重复读（REPEATABLE READ） - 保证在同一个事务中多次读取同样数据的结果是一样的。 可串行化（SERIALIXABLE） - 强制事务串行执行。 隔离级别 脏读 不可重复读 幻影读 未提交读 YES YES YES 提交读 NO YES YES 可重复读 NO NO YES 可串行化 NO NO NO 3.2. 如何解决分布式事务？若出现网络问题或宕机问题，如何解决？ 4. 锁 4.1. 数据库锁有哪些类型？如何实现？ 4.1.1. 锁粒度 表级锁（table lock） - 锁定整张表。用户对表进行写操作前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有没有写锁时，其他用户才能获得读锁，读锁之间不会相互阻塞。 行级锁（row lock） - 仅对指定的行记录进行加锁，这样其它进程还是可以对同一个表中的其它记录进行操作。 InnoDB 行锁是通过给索引上的索引项加锁来实现的。只有通过索引条件检索数据，InnoDB 才使用行级锁；否则，InnoDB 将使用表锁！ 索引分为主键索引和非主键索引两种，如果一条 sql 语句操作了主键索引，MySQL 就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL 会先锁定该非主键索引，再锁定相关的主键索引。在 UPDATE、DELETE 操作时，MySQL 不仅锁定 WHERE 条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的 next-key locking。 当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。发生死锁后，InnoDB 一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。 4.1.2. 读写锁 排它锁（Exclusive），简写为 X 锁，又称写锁。 共享锁（Shared），简写为 S 锁，又称读锁。 有以下两个规定： 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。 锁的兼容关系如下： - X S X NO NO S NO YES 使用： 排他锁：SELECT ... FOR UPDATE; 共享锁：SELECT ... LOCK IN SHARE MODE; innodb 下的记录锁（也叫行锁），间隙锁，next-key 锁统统属于排他锁。 在 InnoDB 中，行锁是通过给索引上的索引项加锁来实现的。如果没有索引，InnoDB 将会通过隐藏的聚簇索引来对记录加锁。另外，根据针对 sql 语句检索条件的不同，加锁又有以下三种情形需要我们掌握。 Record lock：对索引项加锁。若没有索引项则使用表锁。 Gap lock：对索引项之间的间隙加锁。 Next-key lock：1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。当利用范围条件而不是相等条件获取排他锁时，innoDB 会给符合条件的所有数据加锁。对于在条件范围内但是不存在的记录，叫做间隙。innoDB 也会对这个间隙进行加锁。另外，使用相等的检索条件时，若指定了本身不存在的记录作为检索条件的值的话，则此值对应的索引项也会加锁。 4.1.3. 意向锁 使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。 在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。 意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定： 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁； 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。 通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。 各种锁的兼容关系如下： - X IX S IS X NO NO NO NO IX NO YES NO YES S NO NO YES YES IS NO YES YES YES 解释如下： 任意 IS/IX 锁之间都是兼容的，因为它们只是表示想要对表加锁，而不是真正加锁； S 锁只与 S 锁和 IS 锁兼容，也就是说事务 T 想要对数据行加 S 锁，其它事务可以已经获得对表或者表中的行的 S 锁。 意向锁是 InnoDB 自动加的，不需要用户干预。 5. 分库分表 5.1. 为什么要分库分表？ 分库分表的基本思想就要把一个数据库切分成多个部分放到不同的数据库(server)上，从而缓解单一数据库的性能问题。 5.2. 分库分表的常见问题以及解决方案？ 5.2.1. 事务问题 方案一：使用分布式事务 优点：交由数据库管理，简单有效 缺点：性能代价高，特别是 shard 越来越多时 方案二：由应用程序和数据库共同控制 原理：将一个跨多个数据库的分布式事务分拆成多个仅处于单个数据库上面的小事务，并通过应用程序来总控各个小事务。 优点：性能上有优势 缺点：需要应用程序在事务控制上做灵活设计。如果使用了 spring 的事务管理，改动起来会面临一定的困难。 5.2.2. 跨节点 Join 的问题 只要是进行切分，跨节点 Join 的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的 id，根据这些 id 发起第二次请求得到关联数据。 5.2.3. 跨节点的 count,order by,group by 以及聚合函数问题 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。 解决方案：与解决跨节点 join 问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和 join 不同的是每个节点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。 5.2.4. ID 唯一性 一旦数据库被切分到多个物理节点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的 ID 无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得 ID，以便进行 SQL 路由。 一些常见的主键生成策略： 使用全局唯一 ID：GUID。 为每个分片指定一个 ID 范围。 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)。 5.2.5. 数据迁移，容量规划，扩容等问题 来自淘宝综合业务平台团队，它利用对 2 的倍数取余具有向前兼容的特性（如对 4 取余得 1 的数对 2 取余也是 1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了 Sharding 扩容的难度。 5.2.6. 分库数量 分库数量首先和单库能处理的记录数有关，一般来说，Mysql 单库超过 5000 万条记录，Oracle 单库超过 1 亿条记录，DB 压力就很大(当然处理能力和字段数量/访问模式/记录长度有进一步关系)。 5.2.7. 跨分片的排序分页 如果是在前台应用提供分页，则限定用户只能看前面 n 页，这个限制在业务上也是合理的，一般看后面的分页意义不大（如果一定要看，可以要求用户缩小范围重新查询）。 如果是后台批处理任务要求分批获取数据，则可以加大 page size，比如每次获取 5000 条记录，有效减少分页数（当然离线访问一般走备库，避免冲击主库）。 分库设计时，一般还有配套大数据平台汇总所有分库的记录，有些分页查询可以考虑走大数据平台。 5.3. 如何设计可以动态扩容缩容的分库分表方案？ 5.4. 有哪些分库分表中间件？各自有什么优缺点？底层实现原理？ 简单易用的组件： 当当 sharding-jdbc 蘑菇街 TSharding 强悍重量级的中间件： sharding TDDL Smart Client 的方式（淘宝） Atlas(Qihoo 360) alibaba.cobar(是阿里巴巴（B2B）部门开发) MyCAT（基于阿里开源的 Cobar 产品而研发） Oceanus(58 同城数据库中间件) OneProxy(支付宝首席架构师楼方鑫开发) vitess（谷歌开发的数据库中间件） 6. 数据库优化 6.1. 什么是执行计划？ 7. 数据库架构设计 7.1. 高并发系统数据层面如何设计？ 读写分离的原理 主服务器用来处理写操作以及实时性要求比较高的读操作，而从服务器用来处理读操作。 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。 MySQL 读写分离能提高性能的原因在于： 主从服务器负责各自的读和写，极大程度缓解了锁的争用； 从服务器可以配置 MyISAM 引擎，提升查询性能以及节约系统开销； 增加冗余，提高可用性。 Mysql 的复制原理 Mysql 支持两种复制：基于行的复制和基于语句的复制。 这两种方式都是在主库上记录二进制日志，然后在从库重放日志的方式来实现异步的数据复制。这意味着：复制过程存在时延，这段时间内，主从数据可能不一致。 主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。 binlog 线程 ：负责将主服务器上的数据更改写入二进制文件（binlog）中。 I/O 线程 ：负责从主服务器上读取二进制日志文件，并写入从服务器的中继日志中。 SQL 线程 ：负责读取中继日志并重放其中的 SQL 语句。 垂直切分 按照业务线或功能模块拆分为不同数据库。 更进一步是服务化改造，将强耦合的系统拆分为多个服务。 水平切分 哈希取模：hash(key) % NUM_DB 范围：可以是 ID 范围也可以是时间范围 映射表：使用单独的一个数据库来存储映射关系]]></content>
  </entry>
  <entry>
    <title><![CDATA[Node.js 入门]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fnodejs%2Fnodejs%2F</url>
    <content type="text"><![CDATA[Node.js 入门 Node.js 是一个能够在服务器端运行 JavaScript 源代码的跨平台运行环境。 关键词： nodejs, REPL， require, exports 安装配置 Windows / IOS Linux / Ubuntu / Debian 第一个应用 交互式解释器（REPL） 简单的表达式运算 使用变量 多行表达式 Node 的代码组织 模块路径解析规则 包（package） index.js package.json 命令行程序 工程目录 Node 的 IO 操作 Buffer Stream 文件系统 Node.js 工具 nvm 更多内容 安装配置 Windows / IOS 可以在官方下载安装文件：https://nodejs.org/en/download/。 Linux / Ubuntu / Debian 命令格式如下： curl -sL https://deb.nodesource.com/setup_7.x | sudo -E bash -sudo apt-get install -y nodejs 详细内容参考官方文档：通过包管理器安装 Node.js 第一个应用 创建 helloworld.js 文件，内容如下： function helloworld() &#123; console.log("Hello World!");&#125;helloworld(); 执行命令 node helloworld.js 运行脚本。 交互式解释器（REPL） Node.js REPL(Read Eval Print Loop:交互式解释器) 表示一个电脑的环境，类似 Window 系统的终端或 Unix/Linux shell，我们可以在终端中输入命令，并接收系统的响应。 Node 自带了交互式解释器，可以执行以下任务： 读取 - 读取用户输入，解析输入了 Javascript 数据结构并存储在内存中。 执行 - 执行输入的数据结构 打印 - 输出结果 循环 - 循环操作以上步骤直到用户两次按下 ctrl-c 按钮退出。 Node 的交互式解释器可以很好的调试 Javascript 代码。 我们可以输入 node 命令来启动 Node 的终端： 简单的表达式运算 $ node&gt; 1 + 45&gt; 1 + ( 2 * 3 ) - 42.5 使用变量 你可以将数据存储在变量中，并在你需要的时候使用它。 变量声明需要使用 var 关键字，如果没有使用 var 关键字变量会直接打印出来。 使用 var 关键字的变量可以使用 console.log() 来输出变量。 $ node&gt; x = 1010&gt; var y = 10undefined&gt; x + y20&gt; console.log(&quot;Hello World&quot;)Hello Worldundefined&gt; console.log(&quot;www.runoob.com&quot;)www.runoob.comundefined 多行表达式 Node REPL 支持输入多行表达式，这就有点类似 JavaScript。接下来让我们来执行一个 do-while 循环： $ node&gt; var x = 0undefined&gt; do &#123;... x++;... console.log(&quot;x: &quot; + x);... &#125; while ( x &lt; 5 );x: 1x: 2x: 3x: 4x: 5undefined&gt; … 三个点的符号是系统自动生成的，你回车换行后即可。Node 会自动检测是否为连续的表达式。 不会打乱对象原有的继承关系。 Node 的代码组织 有经验的 C 程序员在编写一个新程序时首先从 make 文件写起。同样的，使用 Node.js 编写程序前，为了有个良好的开端，首先需要准备好代码的目录结构和部署方式，就如同修房子要先搭脚手架。本章将介绍与之相关的各种知识。 模块路径解析规则 我们已经知道，require 函数支持斜杠（/）或盘符（C:）开头的绝对路径，也支持./开头的相对路径。但这两种路径在模块之间建立了强耦合关系，一旦某个模块文件的存放位置需要变更，使用该模块的其它模块的代码也需要跟着调整，变得牵一发动全身。因此，require函数支持第三种形式的路径，写法类似于foo/bar，并依次按照以下规则解析路径，直到找到模块位置。 内置模块 如果传递给 require 函数的是 Node.js 内置模块名称，不做路径解析，直接返回内部模块的导出对象，例如require(&quot;fs&quot;)。 node_modules 目录 Node.js 定义了一个特殊的 node_modules 目录用于存放模块。 例如某个模块的绝对路径是 /home/user/hello.js，在该模块中使用 require('foo/bar') 方式加载模块时，则 Node.js 会依次尝试使用以下路径。 /home/user/node_modules/foo/bar/home/node_modules/foo/bar/node_modules/foo/bar NODE_PATH 环境变量 与 PATH 环境变量类似，Node.js 允许通过 NODE_PATH 环境变量来指定额外的模块搜索路径。 NODE_PATH 环境变量中包含一到多个目录路径，路径之间在 Linux 下使用:分隔，在 Windows 下使用;分隔。例如定义了以下 NODE_PATH 环境变量： NODE_PATH=/home/user/lib:/home/lib 当使用 require('foo/bar') 的方式加载模块时，则 Node.js 依次尝试以下路径。 /home/user/lib/foo/bar/home/lib/foo/bar 包（package） 我们已经知道了 JS 模块的基本单位是单个 JS 文件，但复杂些的模块往往由多个子模块组成。为了便于管理和使用，我们可以把由多个子模块组成的大模块称做包，并把所有子模块放在同一个目录里。 在组成一个包的所有子模块中，需要有一个入口模块，入口模块的导出对象被作为包的导出对象。例如有以下目录结构。 - /home/user/lib/ - cat/ head.js body.js main.js 其中cat目录定义了一个包，其中包含了 3 个子模块。main.js作为入口模块，其内容如下： var head = require("./head");var body = require("./body");exports.create = function(name) &#123; return &#123; name: name, head: head.create(), body: body.create() &#125;;&#125;; 在其它模块里使用包的时候，需要加载包的入口模块。接着上例，使用require('/home/user/lib/cat/main')能达到目的，但是入口模块名称出现在路径里看上去不是个好主意。因此我们需要做点额外的工作，让包使用起来更像是单个模块。 index.js 当模块的文件名是index.js，加载模块时可以使用模块所在目录的路径代替模块文件路径，因此接着上例，以下两条语句等价。 var cat = require("/home/user/lib/cat");var cat = require("/home/user/lib/cat/index"); 这样处理后，就只需要把包目录路径传递给require函数，感觉上整个目录被当作单个模块使用，更有整体感。 package.json 如果想自定义入口模块的文件名和存放位置，就需要在包目录下包含一个package.json文件，并在其中指定入口模块的路径。上例中的cat模块可以重构如下。 - /home/user/lib/ - cat/ + doc/ - lib/ head.js body.js main.js + tests/ package.json 其中package.json内容如下。 &#123; "name": "cat", "main": "./lib/main.js"&#125; 如此一来，就同样可以使用require('/home/user/lib/cat')的方式加载模块。Node.js 会根据包目录下的package.json找到入口模块所在位置。 命令行程序 使用 Node.js 编写的东西，要么是一个包，要么是一个命令行程序，而前者最终也会用于开发后者。因此我们在部署代码时需要一些技巧，让用户觉得自己是在使用一个命令行程序。 例如我们用 Node.js 写了个程序，可以把命令行参数原样打印出来。该程序很简单，在主模块内实现了所有功能。并且写好后，我们把该程序部署在/home/user/bin/node-echo.js这个位置。为了在任何目录下都能运行该程序，我们需要使用以下终端命令。 $ node /home/user/bin/node-echo.js Hello WorldHello World 这种使用方式看起来不怎么像是一个命令行程序，下边的才是我们期望的方式。 $ node-echo Hello World Linux 在 Linux 系统下，我们可以把 JS 文件当作 shell 脚本来运行，从而达到上述目的，具体步骤如下： （1）在 shell 脚本中，可以通过#!注释来指定当前脚本使用的解析器。所以我们首先在node-echo.js文件顶部增加以下一行注释，表明当前脚本使用 Node.js 解析。 #! /usr/bin/env node Node.js 会忽略掉位于 JS 模块首行的#!注释，不必担心这行注释是非法语句。 （2）然后，我们使用以下命令赋予node-echo.js文件执行权限。 $ chmod +x /home/user/bin/node-echo.js （3）最后，我们在 PATH 环境变量中指定的某个目录下，例如在/usr/local/bin下边创建一个软链文件，文件名与我们希望使用的终端命令同名，命令如下： $ sudo ln -s /home/user/bin/node-echo.js /usr/local/bin/node-echo 这样处理后，我们就可以在任何目录下使用node-echo命令了。 Windows 在 Windows 系统下的做法完全不同，我们得靠.cmd文件来解决问题。假设node-echo.js存放在C:\Users\user\bin目录，并且该目录已经添加到 PATH 环境变量里了。接下来需要在该目录下新建一个名为node-echo.cmd的文件，文件内容如下： @node "C:\User\user\bin\node-echo.js" %* 这样处理后，我们就可以在任何目录下使用node-echo命令了。 工程目录 了解了以上知识后，现在我们可以来完整地规划一个工程目录了。以编写一个命令行程序为例，一般我们会同时提供命令行模式和 API 模式两种使用方式，并且我们会借助三方包来编写代码。除了代码外，一个完整的程序也应该有自己的文档和测试用例。因此，一个标准的工程目录都看起来像下边这样。 - /home/user/workspace/node-echo/ ## 工程目录 - bin/ ## 存放命令行相关代码 node-echo + docs/ ## 存放文档 - libs/ ## 存放API相关代码 echo.js - node_modules/ ## 存放三方包 + argv/ + tests/ ## 存放测试用例 package.json ## 元数据文件 README.md ## 说明文件 其中部分文件内容如下： /* bin/node-echo */var argv = require('argv'), echo = require('../lib/echo');console.log(echo(argv.join(' ')));/* lib/echo.js */module.exports = function (message) &#123; return message;&#125;;/* package.json */&#123; "name": "node-echo", "main": "./lib/echo.js"&#125; 以上例子中分类存放了不同类型的文件，并通过node_moudles目录直接使用三方包名加载模块。此外，定义了package.json之后，node-echo目录也可被当作一个包来使用。 Node 的 IO 操作 Buffer JavaScript 语言自身只有字符串数据类型，没有二进制数据类型。 但在处理像 TCP 流或文件流时，必须使用到二进制数据。因此在 Node.js 中，定义了一个 Buffer 类，该类用来创建一个专门存放二进制数据的缓存区。 在 Node.js 中，Buffer 类是随 Node 内核一起发布的核心库。Buffer 库为 Node.js 带来了一种存储原始数据的方法，可以让 Node.js 处理二进制数据，每当需要在 Node.js 中处理 I/O 操作中移动的数据时，就有可能使用 Buffer 库。原始数据存储在 Buffer 类的实例中。一个 Buffer 类似于一个整数数组，但它对应于 V8 堆内存之外的一块原始内存。 创建 Buffer Node Buffer 类可以通过多种方式来创建。 创建指定长度的的 Buffer 实例： var buf = new Buffer(10); 通过给定的数组创建 Buffer 实例： var buf = new Buffer([10, 20, 30, 40, 50]); 通过一个字符串来创建 Buffer 实例： var buf = new Buffer("How are you?", "utf-8"); utf-8 是默认的编码方式，此外它同样支持以下编码：ascii, utf8, utf16le, ucs2, base64 和 hex。 写入缓冲区 语法 buf.write(string[, offset[, length]][, encoding]) 参数 string - 写入缓冲区的字符串。 offset - 缓冲区开始写入的索引值，默认为 0 。 length - 写入的字节数，默认为 buffer.length encoding - 使用的编码。默认为 ‘utf8’ 。 返回值 返回实际写入的大小。如果 buffer 空间不足， 则只会写入部分字符串。 示例 var buf1 = new Buffer(10);console.log("buf1 写入字节数: " + buf1.write("0123456789"));var buf2 = new Buffer(5);console.log("buf2 写入字节数: " + buf2.write("0123456789")); 执行以上代码，输出结果为： 105 从缓冲区读取数据 语法 buf.toString([encoding[, start[, end]]]) 参数 encoding - 使用的编码。默认为 ‘utf8’ 。 start - 指定开始读取的索引位置，默认为 0。 end - 结束位置，默认为缓冲区的末尾。 返回值 解码缓冲区数据并使用指定的编码返回字符串。 实例 buf = new Buffer(26);for (var i = 0; i &lt; 26; i++) &#123; buf[i] = i + 97;&#125;console.log(buf.toString("ascii")); // 输出: abcdefghijklmnopqrstuvwxyzconsole.log(buf.toString("ascii", 0, 5)); // 输出: abcdeconsole.log(buf.toString("utf8", 0, 5)); // 输出: abcdeconsole.log(buf.toString(undefined, 0, 5)); // 使用 'utf8' 编码, 并输出: abcde 将 Buffer 转换为 JSON 对象 语法 buf.toJSON(); 返回值 返回 JSON 对象。 实例 var buf = new Buffer("goodbye");var json = buf.toJSON(buf);console.log(json); 执行以上代码，输出结果为： &#123; type: 'Buffer', data: [ 103, 111, 111, 100, 98, 121, 101 ] &#125; 缓冲区合并 语法 Buffer.concat(list[, totalLength]) 参数 list - 用于合并的 Buffer 对象数组列表。 totalLength - 指定合并后 Buffer 对象的总长度。 返回值 返回一个多个成员合并的新 Buffer 对象。 实例 var buffer1 = new Buffer("Nothing is ");var buffer2 = new Buffer("impossible");var buffer3 = Buffer.concat([buffer1, buffer2]);console.log("buffer3 内容: " + buffer3.toString()); 执行以上代码，输出结果为： buffer3 内容: Nothing is impossible 缓冲区比较 语法 buf.compare(otherBuffer); 参数 参数描述如下： otherBuffer 与 buf 对象比较的另外一个 Buffer 对象。 返回值 返回一个数字，表示 buf 在 otherBuffer 之前，之后或相同。 实例 var buffer1 = new Buffer("ABC");var buffer2 = new Buffer("ABCD");var result = buffer1.compare(buffer2);if (result &lt; 0) &#123; console.log(buffer1 + " 在 " + buffer2 + "之前");&#125; else if (result == 0) &#123; console.log(buffer1 + " 与 " + buffer2 + "相同");&#125; else &#123; console.log(buffer1 + " 在 " + buffer2 + "之后");&#125; 执行以上代码，输出结果为： ABC在ABCD之前 拷贝缓冲区 语法 buf.copy(targetBuffer[, targetStart[, sourceStart[, sourceEnd]]]) 参数 targetBuffer - 要拷贝的 Buffer 对象。 targetStart - 数字, 可选, 默认: 0 sourceStart - 数字, 可选, 默认: 0 sourceEnd - 数字, 可选, 默认: buffer.length 返回值 没有返回值。 实例 var buffer1 = new Buffer("ABC");var buffer2 = new Buffer(3);buffer1.copy(buffer2);console.log("buffer2 内容: " + buffer2.toString()); 执行以上代码，输出结果为： buffer2 内容: ABC 剪切缓冲区 语法 buf.slice([start[, end]]) 参数 start - 数字, 可选, 默认: 0 end - 数字, 可选, 默认: buffer.length 返回值 返回一个新的缓冲区，它和旧缓冲区指向同一块内存，但是从索引 start 到 end 的位置剪切。 实例 var buffer1 = new Buffer("goodbye");var buffer2 = buffer1.slice(0, 2);console.log("buffer2 内容: " + buffer2.toString()); 执行以上代码，输出结果为： buffer2 内容: go 缓冲区长度 语法 buf.length; 返回值 返回 Buffer 对象所占据的内存长度。 实例 var buffer = new Buffer("made in China");console.log("buffer length: " + buffer.length); 执行以上代码，输出结果为： buffer length: 13 Stream Stream 是一个抽象接口，Node 中有很多对象实现了这个接口。例如，对 http 服务器发起请求的 request 对象就是一个 Stream，还有 stdout（标准输出）。 Node.js，Stream 有四种流类型： Readable - 可读操作。 Writable - 可写操作。 Duplex - 可读可写操作. Transform - 操作被写入数据，然后读出结果。 所有的 Stream 对象都是 EventEmitter 的实例。常用的事件有： data - 当有数据可读时触发。 end - 没有更多的数据可读时触发。 error - 在接收和写入过程中发生错误时触发。 finish - 所有数据已被写入到底层系统时触发。 本教程会为大家介绍常用的流操作。 读取流 获取读取流 var readerStream = fs.createReadStream(pathname);readerStream.on("data", function(chunk) &#123; doSomething(chunk);&#125;);readerStream.on("end", function() &#123; cleanUp();&#125;); 完整示例代码见：codes/chapter01/node/stream/streamDemo01.js 写入流 获取写入流 var writerStream = fs.createWriteStream("output.txt");writerStream.write(data, "UTF8");writerStream.end(); 完整示例代码见：codes/chapter01/node/stream/streamDemo02.js 管道流 管道提供了一个输出流到输入流的机制。通常我们用于从一个流中获取数据并将数据传递到另外一个流中。 var fs = require("fs");var readerStream = fs.createReadStream("input.txt");var writerStream = fs.createWriteStream("output.txt");readerStream.pipe(writerStream); 完整示例代码见：codes/chapter01/node/stream/streamDemo03.js 链式流 链式是通过连接输出流到另外一个流并创建多个对个流操作链的机制。链式流一般用于管道操作。 var fs = require("fs");var zlib = require("zlib");fs.createReadStream("input.txt") .pipe(zlib.createGzip()) .pipe(fs.createWriteStream("input.txt.gz")); 完整示例代码见：codes/chapter01/node/stream/streamDemo04.js 文件系统 Node.js 提供一组类似 UNIX（POSIX）标准的文件操作 API。 Node 导入文件系统模块(fs)语法如下所示： var fs = require("fs"); 异步和同步 Node.js 文件系统（fs 模块）模块中的方法均有异步和同步版本，例如读取文件内容的函数有异步的 fs.readFile() 和同步的 fs.readFileSync()。 异步的方法函数最后一个参数为回调函数，回调函数的第一个参数包含了错误信息(error)。 建议大家是用异步方法，比起同步，异步方法性能更高，速度更快，而且没有阻塞。 完整代码示例：codes/chapter01/node/fs/fsDemo01.js 打开文件 语法 fs.open(path, flags[, mode], callback) 参数 path - 文件的路径。 flags - 文件打开的行为。具体值详见下文。 mode - 设置文件模式(权限)，文件创建默认权限为 0666(可读，可写)。 callback - 回调函数，带有两个参数如：callback(err, fd)。 flags 参数可以是以下值： Flag 描述 r 以读取模式打开文件。如果文件不存在抛出异常。 r+ 以读写模式打开文件。如果文件不存在抛出异常。 rs 以同步的方式读取文件。 rs+ 以同步的方式读取和写入文件。 w 以写入模式打开文件，如果文件不存在则创建。 wx 类似 ‘w’，但是如果文件路径存在，则文件写入失败。 w+ 以读写模式打开文件，如果文件不存在则创建。 wx+ 类似 ‘w+’， 但是如果文件路径存在，则文件读写失败。 a 以追加模式打开文件，如果文件不存在则创建。 ax 类似 ‘a’， 但是如果文件路径存在，则文件追加失败。 a+ 以读取追加模式打开文件，如果文件不存在则创建。 ax+ 类似 ‘a+’， 但是如果文件路径存在，则文件读取追加失败。 完整代码示例：codes/chapter01/node/fs/fsDemo02.js 获取文件信息 语法 fs.stat(path, callback); 参数 path - 文件路径。 callback - 回调函数，带有两个参数如：(err, stats), stats 是 fs.Stats 对象。 fs.stat(path)执行后，会将 stats 类的实例返回给其回调函数。可以通过 stats 类中的提供方法判断文件的相关属性。 stats 类中的方法有： 方法 描述 stats.isFile() 如果是文件返回 true，否则返回 false。 stats.isDirectory() 如果是目录返回 true，否则返回 false。 stats.isBlockDevice() 如果是块设备返回 true，否则返回 false。 stats.isCharacterDevice() 如果是字符设备返回 true，否则返回 false。 stats.isSymbolicLink() 如果是软链接返回 true，否则返回 false。 stats.isFIFO() 如果是 FIFO，返回 true，否则返回 false。FIFO 是 UNIX 中的一种特殊类型的命令管道。 stats.isSocket() 如果是 Socket 返回 true，否则返回 false。 完整代码示例：codes/chapter01/node/fs/fsDemo03.js 写入文件 语法 fs.writeFile(file, data[, options], callback) 如果文件存在，该方法写入的内容会覆盖旧的文件内容。 参数 file - 文件名或文件描述符。 data - 要写入文件的数据，可以是 String(字符串) 或 Buffer(流) 对象。 options - 该参数是一个对象，包含 {encoding, mode, flag}。默认编码为 utf8, 模式为 0666 ， flag 为 ‘w’ callback - 回调函数，回调函数只包含错误信息参数(err)，在写入失败时返回。 完整代码示例：codes/chapter01/node/fs/fsDemo04.js 读取文件 语法 fs.read(fd, buffer, offset, length, position, callback); 该方法使用了文件描述符来读取文件。 参数 fd - 通过 fs.open() 方法返回的文件描述符。 buffer - 数据写入的缓冲区。 offset - 缓冲区写入的写入偏移量。 length - 要从文件中读取的字节数。 position - 文件读取的起始位置，如果 position 的值为 null，则会从当前文件指针的位置读取。 callback - 回调函数，有三个参数 err, bytesRead, buffer，err 为错误信息， bytesRead 表示读取的字节数，buffer 为缓冲区对象。 完整代码示例：codes/chapter01/node/fs/fsDemo05.js 关闭文件 语法 fs.close(fd, callback); 该方法使用了文件描述符来读取文件。 参数 fd - 通过 fs.open() 方法返回的文件描述符。 callback - 回调函数，没有参数。 完整代码示例：codes/chapter01/node/fs/fsDemo06.js 截取文件 语法 fs.ftruncate(fd, len, callback); 该方法使用了文件描述符来读取文件。 参数 fd - 通过 fs.open() 方法返回的文件描述符。 len - 文件内容截取的长度。 callback - 回调函数，没有参数。 完整代码示例：codes/chapter01/node/fs/fsDemo07.js 删除文件 语法 fs.unlink(path, callback); 参数 path - 文件路径。 callback - 回调函数，没有参数。 完整代码示例：codes/chapter01/node/fs/fsDemo08.js 创建目录 语法 fs.mkdir(path[, mode], callback) 参数 path - 文件路径。 mode - 设置目录权限，默认为 0777。 callback - 回调函数，没有参数。 完整代码示例：codes/chapter01/node/fs/fsDemo09.js 读取目录 语法 fs.readdir(path, callback); 参数 path - 文件路径。 callback - 回调函数，回调函数带有两个参数 err, files，err 为错误信息，files 为 目录下的文件数组列表。 完整代码示例：codes/chapter01/node/fs/fsDemo10.js 删除目录 语法 fs.rmdir(path, callback); 参数 path - 文件路径。 callback - 回调函数，没有参数。 完整代码示例：codes/chapter01/node/fs/fsDemo11.js Node.js 工具 nvm nvm 是 Node 版本管理器。 安装 # 两条命令效果相同wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.11/install.sh | bashcurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.11/install.sh | bash 设置环境变量 export NVM_DIR="$&#123;XDG_CONFIG_HOME/:-$HOME/.&#125;nvm"[ -s "$NVM_DIR/nvm.sh" ] &amp;&amp; \. "$NVM_DIR/nvm.sh" 使用 nvm install 6.14.4 # 安装 Node.js 版本nvm use node6.14.4 # 指定 Node.js 版本（必须已安装） 更多内容 📚 拓展阅读 Node.js Npm Yarn 📦 本文归档在 前端技术教程系列：frontend-tutorial 官方 Node.js 官网 Node.js Github 教程 Node.js 包教不包会 一起学 Node.js 七天学会 NodeJS 规范 Node.JS 最佳实践 工具 nvm - Node 版本管理器 更多资源 awesome-nodejs]]></content>
  </entry>
  <entry>
    <title><![CDATA[计算机网络之传输层]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fcommunication%2Fnetwork-transport%2F</url>
    <content type="text"><![CDATA[计算机网络之传输层 网络层只把分组发送到目的主机，但是真正通信的并不是主机而是主机中的进程。传输层提供了进程间的逻辑通信，传输层向高层用户屏蔽了下面网络层的核心细节，使应用程序看起来像是在两个传输层实体之间有一条端到端的逻辑通信信道。 UDP 和 TCP 的特点 UDP 首部格式 TCP 首部格式 TCP 的三次握手 TCP 的四次挥手 TCP 可靠传输 TCP 滑动窗口 TCP 流量控制 TCP 拥塞控制 1. 慢开始与拥塞避免 2. 快重传与快恢复 UDP 和 TCP 的特点 用户数据报协议 UDP（User Datagram Protocol）是无连接的，尽最大可能交付，没有拥塞控制，面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），支持一对一、一对多、多对一和多对多的交互通信。 传输控制协议 TCP（Transmission Control Protocol）是面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信，面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块），每一条 TCP 连接只能是点对点的（一对一）。 UDP 首部格式 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。 TCP 首部格式 序号 ：用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。 确认号 ：期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。 数据偏移 ：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。 确认 ACK ：当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。 同步 SYN ：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。 终止 FIN ：用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口 ：窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。 TCP 的三次握手 假设 A 为客户端，B 为服务器端。 首先 B 处于 LISTEN（监听）状态，等待客户的连接请求。 A 向 B 发送连接请求报文，SYN=1，ACK=0，选择一个初始的序号 x。 B 收到连接请求报文，如果同意建立连接，则向 A 发送连接确认报文，SYN=1，ACK=1，确认号为 x+1，同时也选择一个初始的序号 y。 A 收到 B 的连接确认报文后，还要向 B 发出确认，确认号为 y+1，序号为 x+1。 B 收到 A 的确认后，连接建立。 三次握手的原因 第三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。 客户端发送的连接请求如果在网络中滞留，那么就会隔很长一段时间才能收到服务器端发回的连接确认。客户端等待一个超时重传时间之后，就会重新请求连接。但是这个滞留的连接请求最后还是会到达服务器，如果不进行三次握手，那么服务器就会打开两个连接。如果有第三次握手，客户端会忽略服务器之后发送的对滞留连接请求的连接确认，不进行第三次握手，因此就不会再次打开连接。 TCP 的四次挥手 以下描述不讨论序号和确认号，因为序号和确认号的规则比较简单。并且不讨论 ACK，因为 ACK 在连接建立之后都为 1。 A 发送连接释放报文，FIN=1。 B 收到之后发出确认，此时 TCP 属于半关闭状态，B 能向 A 发送数据但是 A 不能向 B 发送数据。 当 B 不再需要连接时，发送连接释放报文，FIN=1。 A 收到后发出确认，进入 TIME-WAIT 状态，等待 2 MSL（最大报文存活时间）后释放连接。 B 收到 A 的确认后释放连接。 四次挥手的原因 客户端发送了 FIN 连接释放报文之后，服务器收到了这个报文，就进入了 CLOSE-WAIT 状态。这个状态是为了让服务器端发送还未传送完毕的数据，传送完毕之后，服务器会发送 FIN 连接释放报文。 TIME_WAIT 客户端接收到服务器端的 FIN 报文后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间 2MSL。这么做有两个理由： 确保最后一个确认报文能够到达。如果 B 没收到 A 发送来的确认报文，那么就会重新发送连接释放请求报文，A 等待一段时间就是为了处理这种情况的发生。 等待一段时间是为了让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。 TCP 可靠传输 TCP 使用超时重传来实现可靠传输：如果一个已经发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。 一个报文段从发送再到接收到确认所经过的时间称为往返时间 RTT，加权平均往返时间 RTTs 计算如下： 其中，0 ≤ a ＜ 1，RTTs 随着 a 的增加更容易受到 RTT 的影响。 超时时间 RTO 应该略大于 RTTs，TCP 使用的超时时间计算如下： 其中 RTTd 为偏差的加权平均值。 TCP 滑动窗口 窗口是缓存的一部分，用来暂时存放字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。 发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。 接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前的所有字节都已经被接收。 TCP 流量控制 流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 TCP 拥塞控制 如果网络出现拥塞，分组将会丢失，此时发送方会继续重传，从而导致网络拥塞程度更高。因此当出现拥塞时，应当控制发送方的速率。这一点和流量控制很像，但是出发点不同。流量控制是为了让接收方能来得及接收，而拥塞控制是为了降低整个网络的拥塞程度。 TCP 主要通过四个算法来进行拥塞控制：慢开始、拥塞避免、快重传、快恢复。 发送方需要维护一个叫做拥塞窗口（cwnd）的状态变量，注意拥塞窗口与发送方窗口的区别：拥塞窗口只是一个状态变量，实际决定发送方能发送多少数据的是发送方窗口。 为了便于讨论，做如下假设： 接收方有足够大的接收缓存，因此不会发生流量控制； 虽然 TCP 的窗口基于字节，但是这里设窗口的大小单位为报文段。 1. 慢开始与拥塞避免 发送的最初执行慢开始，令 cwnd = 1，发送方只能发送 1 个报文段；当收到确认后，将 cwnd 加倍，因此之后发送方能够发送的报文段数量为：2、4、8 … 注意到慢开始每个轮次都将 cwnd 加倍，这样会让 cwnd 增长速度非常快，从而使得发送方发送的速度增长速度过快，网络拥塞的可能性也就更高。设置一个慢开始门限 ssthresh，当 cwnd &gt;= ssthresh 时，进入拥塞避免，每个轮次只将 cwnd 加 1。 如果出现了超时，则令 ssthresh = cwnd / 2，然后重新执行慢开始。 2. 快重传与快恢复 在接收方，要求每次接收到报文段都应该对最后一个已收到的有序报文段进行确认。例如已经接收到 M1 和 M2，此时收到 M4，应当发送对 M2 的确认。 在发送方，如果收到三个重复确认，那么可以知道下一个报文段丢失，此时执行快重传，立即重传下一个报文段。例如收到三个 M2，则 M3 丢失，立即重传 M3。 在这种情况下，只是丢失个别报文段，而不是网络拥塞。因此执行快恢复，令 ssthresh = cwnd / 2 ，cwnd = ssthresh，注意到此时直接进入拥塞避免。 慢开始和快恢复的快慢指的是 cwnd 的设定值，而不是 cwnd 的增长速率。慢开始 cwnd 设定为 1，而快恢复 cwnd 设定为 ssthresh。]]></content>
      <categories>
        <category>communication</category>
      </categories>
      <tags>
        <tag>communication</tag>
        <tag>network</tag>
        <tag>transport</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES6]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fes6%2FES6%2F</url>
    <content type="text"><![CDATA[ES6 关键词： ES6, ECMAScript, arrow, this, let, const, class, extends, super, arrow … 简介 最常用的 ES6 特性 arrow, this let, const class, extends, super template strings Destructuring default, rest, spread 增强的对象字面量 Iterator，for…Of Generators Unicode Module Module Loader Map + Set + WeakMap + WeakSet Proxy Symbols 可以被继承的内建对象 Math + Number + String + Object APIs 二进制和八进制字面量 Promises Reflect API Tail Calls(尾调用) 更多内容 简介 ECMAScript 6（以下简称 ES6）是 JavaScript 语言的下一代标准。 因为当前版本的 ES6 是在 2015 年发布的，所以又称 ECMAScript 2015。 也就是说，ES6 就是 ES2015。 注意：并非所有的浏览器都支持 ES6。所以，如果要让浏览器能够理解 ES6 的语义，需要使用转码器，比如最流行的 Babel。 最常用的 ES6 特性 arrow, this 这个恐怕是 ES6 最最常用的一个新特性了，用它来写 function 比原来的写法要简洁清晰很多。 箭头函数用 =&gt; 来代表一个函数，语法上类似于 C#, Java8 和 CoffeeScript 中的相关特性。它同时支持表达式（Expression bodies）和语句（Statement bodies）的写法。值得注意的是，与一般的函数不同，箭头函数与包裹它的代码共享相同的 this 对象，如果箭头函数在其它函数的内部，它也将共享该函数的 arguments 变量。 function(i)&#123; return i + 1; &#125; //ES5(i) =&gt; i + 1 //ES6 简直是简单的不像话对吧… 如果函数比较复杂，则需要用 {} 把代码包起来： function(x, y) &#123; x++; y--; return x + y;&#125;(x, y) =&gt; &#123;x++; y--; return x+y&#125; 除了看上去更简洁以外，arrow function 还有一项超级无敌的功能！ 长期以来，JavaScript 语言的 this 对象一直是一个令人头痛的问题，在对象方法中使用 this，必须非常小心。例如： class Animal &#123; constructor() &#123; this.type = "animal"; &#125; says(say) &#123; setTimeout(function() &#123; console.log(this.type + " says " + say); &#125;, 1000); &#125;&#125;var animal = new Animal();animal.says("hi"); //undefined says hi 运行上面的代码会报错，这是因为setTimeout中的this指向的是全局对象。所以为了让它能够正确的运行，传统的解决方法有两种： （1）第一种是将 this 传给 self,再用 self 来指代 this says(say)&#123; var self = this; setTimeout(function()&#123; console.log(self.type + ' says ' + say) &#125;, 1000) （2）第二种方法是用bind(this),即 says(say)&#123; setTimeout(function()&#123; console.log(self.type + ' says ' + say) &#125;.bind(this), 1000) 但现在我们有了箭头函数，就不需要这么麻烦了： class Animal &#123; constructor() &#123; this.type = "animal"; &#125; says(say) &#123; setTimeout(() =&gt; &#123; console.log(this.type + " says " + say); &#125;, 1000); &#125;&#125;var animal = new Animal();animal.says("hi"); //animal says hi 当我们使用箭头函数时，函数体内的 this 对象，就是定义时所在的对象，而不是使用时所在的对象。 并不是因为箭头函数内部有绑定 this 的机制，实际原因是箭头函数根本没有自己的 this，它的 this 是继承外面的，因此内部的 this 就是外层代码块的 this。 let, const 这两个关键字具有块级作用域。 let 用于声明变量； const 用于声明常量。const 仅允许被赋值一次，通过静态限制（Static restrictions ）的方式阻止变量在赋值前被使用。 let ES5 只有全局作用域和函数作用域，没有块级作用域，这带来很多不合理的场景。 （1）内层变量覆盖外层变量 var vs. let 示例 1： // ES5 示例var name = "zach";while (true) &#123; var name = "obama"; console.log(name); //obama break;&#125;console.log(name); //obama--------------------------------------// ES6 示例let name = "zach";while (true) &#123; let name = "obama"; console.log(name); //obama break;&#125;console.log(name); //zach 说明：使用 var 两次输出都是 obama。而 let 则实际上为 JavaScript 新增了块级作用域。用它所声明的变量，只在 let 命令所在的代码块内有效。 （2）用来计数的循环变量泄露为全局变量 var vs. let 示例 2： // ES5var a = [];for (var i = 0; i &lt; 10; i++) &#123; a[i] = function() &#123; console.log(i); &#125;;&#125;a[6](); // 10// ES6let a = [];for (let i = 0; i &lt; 10; i++) &#123; a[i] = function() &#123; console.log(i); &#125;;&#125;a[6](); // 6 说明：上面代码中，变量 i 是 var 声明的，在全局范围内都有效。所以每一次循环，新的 i 值都会覆盖旧值，导致最后输出的是最后一轮的 i 的值。而使用 let 则不会出现这个问题。 const const 用于声明常量，一旦声明，常量的值就不能改变。 const PI = Math.PI;PI = 23; //Module build failed: SyntaxError: /es6/app.js: "PI" is read-only 当我们尝试去改变用 const 声明的常量时，浏览器就会报错。 const 有一个很好的应用场景，就是当我们引用第三方库的时声明的变量，用 const 来声明可以避免未来不小心重命名而导致出现 bug： const monent = require("moment"); class, extends, super 这三个特性涉及了 ES5 中最令人头疼的的几个部分：原型、构造函数，继承。 有了 ES6 我们不再烦恼！ ES6 提供了更接近传统语言的写法，引入了 Class（类）这个概念。新的 class 写法让对象原型的写法更加清晰、更像面向对象编程的语法，也更加通俗易懂。 class Animal &#123; constructor() &#123; this.type = "animal"; &#125; says(say) &#123; console.log(this.type + " says " + say); &#125;&#125;let animal = new Animal();animal.says("hello"); //animal says helloclass Cat extends Animal &#123; constructor() &#123; super(); this.type = "cat"; &#125;&#125;let cat = new Cat();cat.says("hello"); //cat says hello 上面代码首先用 class 定义了一个类，可以看到里面有一个 constructor 方法，这就是构造方法，而 this 关键字则代表实例对象。简单地说，constructor 内定义的方法和属性是实例对象自己的，而 constructor 外定义的方法和属性则是所有实力对象可以共享的。 ES2015 的类只是一个语法糖，通过 class 关键字让语法更接近传统的面向对象模式，本质上还是基于原型的。使用单一便捷的声明格式，使得类使用起来更方便，也更具互操作性。类支持基于原型的继承，调用父类的构造函数，生成实例，静态方法和构造函数。 Class 之间可以通过 extends 关键字实现继承，这比 ES5 的通过修改原型链实现继承，要清晰和方便很多。上面定义了一个 Cat 类，该类通过 extends 关键字，继承了 Animal 类的所有属性和方法。 super 关键字，它指代父类的实例（即父类的 this 对象）。子类必须在 constructor 方法中调用 super 方法，否则新建实例时会报错。这是因为子类没有自己的 this 对象，而是继承父类的 this 对象，然后对其进行加工。如果不调用 super 方法，子类就得不到 this 对象。 ES6 的继承机制，实质是先创造父类的实例对象 this（所以必须先调用 super 方法），然后再用子类的构造函数修改 this。 P.S 如果你写 react 的话，就会发现以上三个东西在最新版 React 中出现得很多。创建的每个 component 都是一个继承React.Component的类。详见 react 文档 template strings 模版字符串提供了构建字符串的语法糖，类似于 Perl，Python 等语言中的字符串插值。可以构建一个自定义标签，避免注入攻击或者从字符串内容中构建更加高级的数据结构。 当我们要插入大段的 html 内容到文档中时，传统的写法非常麻烦： // ES5$("#result").append( "There are &lt;b&gt;" + basket.count + "&lt;/b&gt; " + "items in your basket, " + "&lt;em&gt;" + basket.onSale + "&lt;/em&gt; are on sale!"); 我们要用一堆的’+'号来连接文本与变量，而使用 ES6 的新特性模板字符串 `` 后，我们可以直接这么来写： // ES6$("#result").append(` There are &lt;b&gt;$&#123;basket.count&#125;&lt;/b&gt; items in your basket, &lt;em&gt;$&#123;basket.onSale&#125;&lt;/em&gt; are on sale!`); 用反引号 ``` 来标识起始，用 ${}来引用变量，而且所有的空格和缩进都会被保留在输出之中。 // 创建基本的模板字符串`This is a pretty little template string.``In ES5 this is // 多行字符串 not legal.`;// 插入变量var name = "Bob", time = "today";`Hello $&#123;name&#125;, how are you $&#123;time&#125;?`;// 不用转义String.raw`In ES5 "\n" is a line-feed.`;// 创建一个HTTP请求头的模版字符串，通过替换内容来构建请求GET`http://foo.org/bar?a=$&#123;a&#125;&amp;b=$&#123;b&#125; Content-Type: application/json X-Credentials: $&#123;credentials&#125; &#123; "foo": $&#123;foo&#125;, "bar": $&#123;bar&#125; &#125;`(myOnReadyStateChangeHandler); Destructuring ES6 允许按照一定模式，从数组和对象中提取值，对变量进行赋值，这被称为解构（Destructuring）。 看下面的例子： var cat = "ken";var dog = "lili";var zoo = &#123; cat: cat, dog: dog &#125;;console.log(zoo); //Object &#123;cat: "ken", dog: "lili"&#125; 用 ES6 完全可以像下面这么写： let cat = "ken";let dog = "lili";let zoo = &#123; cat, dog &#125;;console.log(zoo); //Object &#123;cat: "ken", dog: "lili"&#125; 反过来可以这么写： let dog = &#123; type: "animal", many: 2 &#125;;let &#123; type, many &#125; = dog;console.log(type, many); //animal 2 解构允许使用模式匹配的方式进行绑定，并支持匹配 数组和对象。解构具有一定的容错机制，就像查找普通对象foo['foo']这样，当没有找到时会返回undefined（而不会直接报错）。 注：当上层结构都不存在时，解构是会报错的，如const [{id: id}] = []，解构数组为空，导致整个 obj 为undefined，此时再去找obj.id就会报错。 // 列表（数组）匹配var [a, , b] = [1, 2, 3];// 对象匹配var &#123; op: a, lhs: &#123; op: b &#125;, rhs: c&#125; = getASTNode();// 对象匹配的简写// 绑定当前作用域的 `op`, `lhs` 和 `rhs`var &#123; op, lhs, rhs &#125; = getASTNode();// 可以用在函数的参数中function g(&#123; name: x &#125;) &#123; console.log(x);&#125;g(&#123; name: 5 &#125;);// 解构容错机制var [a] = [];a === undefined;// 带默认值的解构容错var [a = 1] = [];a === 1;// 解构 + 默认参数function r(&#123; x, y, w = 10, h = 10 &#125;) &#123; return x + y + w + h;&#125;r(&#123; x: 1, y: 2 &#125;) === 23; default, rest, spread （1）默认参数 默认参数(default)的功能是在函数被调用时对参数做自动估值(若没被赋值，则自动赋值) 大家可以看下面的例子，调用animal()方法时忘了传参数，传统的做法就是加上这一句type = type || 'cat'来指定默认值。 // ES5function animal(type) &#123; type = type || "cat"; console.log(type);&#125;animal();// ES6function animal(type = "cat") &#123; console.log(type);&#125;animal(); （2）不定参数 不定参数(rest)用在参数末尾，将最末尾的参数转换为数组。不定参数(rest)让我们不再需要arguments，更直接地解决了一些常见的问题。 最后一个 rest 语法也很简单，直接看例子： function animals(...types) &#123; console.log(types);&#125;animals("cat", "dog", "fish"); //["cat", "dog", "fish"] 而如果不用 ES6 的话，我们则得使用 ES5 的arguments。 （3）扩展运算符 扩展运算符(spread)则可以将数组转换为连续的函数参数， function f(x, y, z) &#123; return x + y + z;&#125;// 将数组中的每个元素展开为函数参数f(...[1, 2, 3]) == 6; 增强的对象字面量 经扩展后的对象字面量，允许在结构中设置原型，简化了foo: foo这样的赋值，定义方法和调用父级。这样使得对象字面量（Object Literals）更接近类的声明，并且使得基于对象的设计更加方便。 var obj = &#123; // 设置 prototype __proto__: theProtoObj, // 计算属性不会重复设置__proto__，或者将直接触发错误。 ["__proto__"]: somethingElse, // ‘handler: handler’ 简写 handler, // 方法 toString() &#123; // 调用父级方法 return "d " + super.toString(); &#125;, // 设置动态的属性名 ["prop_" + (() =&gt; 42)()]: 42&#125;; __proto__ 需要原生支持, 并且在 之前的 ECMAScript 版本中已被弃用。虽然现在大多数引擎支持, 但是 仍有些引擎是不支持的。另外，值得注意的是，如同附录 B所示，只有 web 浏览器 仍然需要支持该属性。在 node 中已经被支持。 Iterator，for…Of ES6 中的迭代器(Iterators)对象允许像 CLR(Common Language Runtime)的 IEnumerable 接口或者 Java 的 Iterable 一样创建自定义迭代器，可以将for..in这种遍历模式更加一般化为for..of的形式。它是支持惰性模式的，不需要真正实现一个数组（只需要实现 Iterator 接口），就像 LINQ 语言那样。 // 实现斐波那契数列的迭代器let fibonacci = &#123; [Symbol.iterator]() &#123; let pre = 0, cur = 1; return &#123; next() &#123; [pre, cur] = [cur, pre + cur]; return &#123; done: false, value: cur &#125;; &#125; &#125;; &#125;&#125;;for (var n of fibonacci) &#123; // 循环将在n &gt; 1000 时结束 if (n &gt; 1000) break; console.log(n);&#125; 迭代器还可以基于”鸭子类型”来实现（使用TypeScript 这种基于类型的语法来说明）： interface IteratorResult &#123; done: boolean; value: any;&#125;interface Iterator &#123; next(): IteratorResult;&#125;interface Iterable &#123; [Symbol.iterator](): Iterator&#125; 通过 polyfill 支持 为了使用迭代器你必须引入 Babel 的 polyfill. Generators Generator 通过使用function*和yield关键字简化了迭代器的编写。通过function*声明的函数会返回一个 Generators 实例。Generator 可以看做是迭代器的子类，包含了额外的next和throw方法。这些特性可以让得到的结果值再传回 Generator，因此yield是一个具有返回值（或抛出一个值）的表达式。 注意：Generator 也可以用于使用‘await’这样的异步编程中，详见 ES7 await 协议. var fibonacci = &#123; [Symbol.iterator]: function*() &#123; var pre = 0, cur = 1; for (;;) &#123; var temp = pre; pre = cur; cur += temp; yield cur; &#125; &#125;&#125;;for (var n of fibonacci) &#123; // truncate the sequence at 1000 if (n &gt; 1000) break; console.log(n);&#125; Generator 接口 (使用TypeScript 这种基于类型的语法来说明): interface Generator extends Iterator &#123; next(value?: any): IteratorResult; throw(exception: any);&#125; 通过 polyfill 支持 要使用 Generator，你需要在项目中包含 Babel 的 polyfill. Unicode ES6 加强了对 Unicode 编码 的支持，包括新的 unicode 表示法，正则表达式的u模式来匹配码点（code points），也提供新的 API 去处理 21 位的码点（code points）。这些新特性允许我们使用 JavaScript 构建国际化的应用。 // 和ES5.1相同"𠮷".length == 2;// 正则表达式新的u模式"𠮷".match(/./u)[0].length == 2;// 新的unicode表示法("\u&#123;20BB7&#125;" == "𠮷") == "\uD842\uDFB7";// 新的字符串方法"𠮷".codePointAt(0) == 0x20bb7;// for of迭代码点for (var c of "𠮷") &#123; console.log(c);&#125; Module ES6 从语言层面对模块进行了支持。编写方式借鉴了流行的 JavaScript 模块加载器（AMD, CommonJS）。由宿主环境的默认加载器定义模块运行时的行为，采取隐式异步模式——在模块可以被获取和加载前不会有代码执行。 // lib/math.jsexport function sum(x, y) &#123; return x + y;&#125;export var pi = 3.141593;// app.jsimport * as math from "lib/math";console.log("2π = " + math.sum(math.pi, math.pi));// otherApp.jsimport &#123; sum, pi &#125; from "lib/math";console.log("2π = " + sum(pi, pi)); 还有的功能包括：export default and export *: // lib/mathplusplus.jsexport * from "lib/math";export var e = 2.71828182846;export default function(x) &#123; return Math.exp(x);&#125;// app.jsimport exp, &#123; pi, e &#125; from "lib/mathplusplus";console.log("e^π = " + exp(pi)); 模块的格式： Babel 可以将 ES2015 的模块转换为一下几种格式：Common.js，AMD，System，以及 UMD。你甚至可以创建你自己的方式。详见模块文档. Module Loader 非 ES2015 部分 这并不是 ES2015 的一部分：这部分 ECMAScript 2015 规范是由实现定义（implementation-defined）的。最终的标准将在 WHATWG 的Loader 规范中确定，目前这项工作正在进行中，下面的内容来自于之前的 ES2015 草稿。 模块加载器支持以下功能： 动态加载（Dynamic loading） 状态一致性（State isolation） 全局空间一致性（Global namespace isolation） 编译钩子（Compilation hooks） 嵌套虚拟化（Nested virtualization） 你可以对默认的加载器进行配置，构建出新的加载器，可以被加载于独立或受限的执行环境。 // 动态加载 – ‘System’ 是默认的加载器System.import("lib/math").then(function(m) &#123; alert("2π = " + m.sum(m.pi, m.pi));&#125;);// 创建执行沙箱 – new Loadersvar loader = new Loader(&#123; global: fixup(window) // replace ‘console.log’&#125;);loader.eval('console.log("hello world!");');// 直接操作模块的缓存System.get("jquery");System.set("jquery", Module(&#123; $: $ &#125;)); // WARNING: not yet finalized 需要额外的 polyfill 由于 Babel 默认使用 common.js 的模块，你需要一个 polyfill 来使用加载器 API。 点击获取. 使用模块加载器 为了使用此功能，你需要告诉 Babel 使用system模块格式化工具。在此查看 System.js Map + Set + WeakMap + WeakSet 为常见算法的实现提供了更有效的数据结构。WeakMaps 提供了对对象的弱引用（不会被垃圾回收计数）。 // Setsvar s = new Set();s.add("hello") .add("goodbye") .add("hello");s.size === 2;s.has("hello") === true;// Mapsvar m = new Map();m.set("hello", 42);m.set(s, 34);m.get(s) == 34;// Weak Mapsvar wm = new WeakMap();wm.set(s, &#123; extra: 42 &#125;);wm.size === undefined;// Weak Setsvar ws = new WeakSet();ws.add(&#123; data: 42 &#125;);// 由于传入的对象没有其它引用，故将不会被set保存。 需要 polyfill 支持 为了在所有环境下使用 Maps，Sets，WeakMaps 和 WeakSets，你需要使用 Babel 的 polyfill. Proxy Proxy(代理对象) 允许创建一个可以全范围控制宿主对象行为的对象，可用于拦截，对象的虚拟化，日志记录/性能分析等。 // 代理普通对象var target = &#123;&#125;;var handler = &#123; get: function(receiver, name) &#123; return `Hello, $&#123;name&#125;!`; &#125;&#125;;var p = new Proxy(target, handler);p.world === "Hello, world!";// 代理函数对象var target = function() &#123; return "I am the target";&#125;;var handler = &#123; apply: function(receiver, ...args) &#123; return "I am the proxy"; &#125;&#125;;var p = new Proxy(target, handler);p() === "I am the proxy"; 下面是完全在运行态的元操作（meta-operations）中可能出现的 trap： var handler =&#123; // target.prop get: ..., // target.prop = value set: ..., // 'prop' in target has: ..., // delete target.prop deleteProperty: ..., // target(...args) apply: ..., // new target(...args) construct: ..., // Object.getOwnPropertyDescriptor(target, 'prop') getOwnPropertyDescriptor: ..., // Object.defineProperty(target, 'prop', descriptor) defineProperty: ..., // Object.getPrototypeOf(target), Reflect.getPrototypeOf(target), // target.__proto__, object.isPrototypeOf(target), object instanceof target getPrototypeOf: ..., // Object.setPrototypeOf(target), Reflect.setPrototypeOf(target) setPrototypeOf: ..., // for (let i in target) &#123;&#125; enumerate: ..., // Object.keys(target) ownKeys: ..., // Object.preventExtensions(target) preventExtensions: ..., // Object.isExtensible(target) isExtensible :...&#125; 不支持的特性 由于 ES5 的局限性，Proxies 无法被转换或者通过 polyfill 兼容，查看不同 JavaScript 引擎. Symbols Symbol 对对象的状态进行访问控制。Symbol 允许对象的属性不仅可以通过string（ES5）命名，还可以通过symbol命名。symbol是一种基本数据类型。可选的name参数用于调试——但并不是它本身的一部分。Symbol 是唯一的，但不是私有的，因为它们使用诸如Object.getOwnPropertySymbols这样的方法来使用。 (function() &#123; // 模块内的 symbol var key = Symbol("key"); function MyClass(privateData) &#123; this[key] = privateData; &#125; MyClass.prototype = &#123; doStuff: function() &#123; ... this[key] ... &#125; &#125;; // Bable只能有限支持，完全支持需要原生实现 typeof key === "symbol"&#125;)();var c = new MyClass("hello")c["key"] === undefined 通过 polyfill 部分实现： 通过 Babel 的polyfill.部分实现。由于语言的限制，部分功能不能转换或通过 polyfill 兼容。您可以查看 code.js 的 注意事项 获取更多信息. 可以被继承的内建对象 在 ES2015 中，可以创建内建对象如Array，Date以及DOMElement的子类。 // 创建Array的子类class MyArray extends Array &#123; constructor(...args) &#123; super(...args); &#125;&#125;var arr = new MyArray();arr[1] = 12;arr.length == 2; 部分支持 部分支持：由于 ES5 引擎的限制，可以创建 HTMLElement 的子类，但不能创建诸如 Array，Date 和 Error 等对象的子类。 Math + Number + String + Object APIs 新增很多功能，如核心的 Math 库，数组转换和用于对象复制的 Object.assign()。 Number.EPSILON;Number.isInteger(Infinity); // falseNumber.isNaN("NaN"); // falseMath.acosh(3); // 1.762747174039086Math.hypot(3, 4); // 5Math.imul(Math.pow(2, 32) - 1, Math.pow(2, 32) - 2); // 2"abcde".includes("cd"); // true"abc".repeat(3); // "abcabcabc"Array.from(document.querySelectorAll("*")); // Returns a real ArrayArray.of(1, 2, 3) // Similar to new Array(...), but without special one-arg behavior [(0, 0, 0)].fill(7, 1) // [0,7,7] [(1, 2, 3)].findIndex(x =&gt; x == 2) // 1 [("a", "b", "c")].entries() // iterator [0, "a"], [1,"b"], [2,"c"] [("a", "b", "c")].keys() // iterator 0, 1, 2 [("a", "b", "c")].values(); // iterator "a", "b", "c"Object.assign(Point, &#123; origin: new Point(0, 0) &#125;); 通过 polyfill 有限的支持 上述许多 API 都通过 polyfill 进行了支持，但是部分特性由于多种原因没有被实现（如，String.prototype.normalize 需要编写大量额外的代码来实现），你可以在 这里找到更多的 polyfill。 二进制和八进制字面量 新增两种数字字面量：二进制b和八进制o。 0b111110111 === 503; // true0o767 === 503; // true 仅支持字面模式 Babel 仅可以转换 0o767，并不能转换 Number(“0o767”)。 Promises Promises 是一种异步编程的方式。Promises 在将来可能会得到支持。目前很多的 JavaScript 库都使用了 Promises。 function timeout(duration = 0) &#123; return new Promise((resolve, reject) =&gt; &#123; setTimeout(resolve, duration); &#125;);&#125;var p = timeout(1000) .then(() =&gt; &#123; return timeout(2000); &#125;) .then(() =&gt; &#123; throw new Error("hmm"); &#125;) .catch(err =&gt; &#123; return Promise.all([timeout(100), timeout(200)]); &#125;); 通过 polyfill 要使用 Promises，你需要引入 Babel 的 polyfill. Reflect API 完整的 Reflect API 暴露在对象的运行级元操作上。它可以用来有效地还原 Proxy API，并允许调用相应的 proxy traps，尤其是在执行 proxies 时非常有用。 var O = &#123; a: 1 &#125;;Object.defineProperty(O, "b", &#123; value: 2 &#125;);O[Symbol("c")] = 3;Reflect.ownKeys(O); // ['a', 'b', Symbol(c)]function C(a, b) &#123; this.c = a + b;&#125;var instance = Reflect.construct(C, [20, 22]);instance.c; // 42 通过 polyfill 要使用 Reflect API，你需要引入 Babel 的 polyfill. Tail Calls(尾调用) 尾递归调用可以保证调用栈不会无限增长，使得在无界输入的情况下，递归算法是安全的。 function factorial(n, acc = 1) &#123; "use strict"; if (n &lt;= 1) return acc; return factorial(n - 1, n * acc);&#125;// 在绝大多数JS引擎中运行这段代码会导致栈溢出// 但是在ES2015中，即便输入很随意也可以安全运行factorial(100000) 更多内容 📦 本文归档在 我的前端技术教程系列：frontend-tutorial 官方 ECMAScript 2015 教程 ECMAScript 6 入门 ES6-Learning 文章 30 分钟掌握 ES6/ES2015 核心内容 ECMAScript 6 in WebStorm: Transpiling ECMAScript 6 特性]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java 资源]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavastack%2Fappendix%2Fresources%2F</url>
    <content type="text"><![CDATA[Java 资源 经典书籍 javase 《Effective Java 中文版》 豆瓣评分：9.1【1235 人评价】 推荐理由：本书介绍了在Java编程中78条极具实用价值的经验规则，这些经验规则涵盖了大多数开发人员每天所面临的问题的解决方案。 友情提示：同推荐《重构 : 改善既有代码的设计》、《代码整洁之道》、《代码大全》，有一定的内容重叠。 《Java并发编程实战》 豆瓣评分：9.0 【651 人评价】 推荐理由：本书深入浅出地介绍了Java线程和并发，是一本完美的Java并发参考手册。 《深入理解Java虚拟机：JVM高级特性与最佳实践》 豆瓣评分：8.9 【657 人评价】 推荐理由：不去了解 JVM 的工程师，和咸鱼有什么区别？ javaee javatool 《Maven 实战》 豆瓣评分：8.1【563 人评价】 推荐理由：国内最权威的Maven专家的力作，唯一一本哦！ database 《Redis设计与实现》 豆瓣评分：8.5 【427 人评价】 推荐理由：系统而全面地描述了 Redis 内部运行机制。图示丰富，描述清晰，并给出大量参考信息，是NoSQL数据库开发人员案头必备。 others 《鸟哥的Linux私房菜 （基础学习篇）》 豆瓣评分：9.1【2269 人评价】 推荐理由：本书是最具知名度的Linux入门书《鸟哥的Linux私房菜基础学习篇》的最新版，全面而详细地介绍了Linux操作系统。 友情提示：内容非常全面，建议挑选和自己实际工作相关度较高的，其他部分有需要再阅读。 《Head First 设计模式》 豆瓣评分：9.2【2394 人评价】 推荐理由：《Head First设计模式》(中文版)共有14章，每章都介绍了几个设计模式，完整地涵盖了四人组版本全部23个设计模式。 《HTTP权威指南》 豆瓣评分：8.7 【1126 人评价】 推荐理由：本书尝试着将HTTP中一些互相关联且常被误解的规则梳理清楚，并编写了一系列基于各种主题的章节，对HTTP各方面的特性进行了介绍。纵观全书，对HTTP“为什么”这样做进行了详细的解释，而不仅仅停留在它是“怎么做”的。 《TCP/IP详解 系列》 豆瓣评分：9.3 【1883 人评价】 推荐理由：完整而详细的TCP/IP协议指南。针对任何希望理解TCP/IP协议是如何实现的读者设计。 《剑指Offer：名企面试官精讲典型编程题》 豆瓣评分：8.5【508 人评价】 推荐理由：剖析了80个典型的编程面试题，系统整理基础知识、代码质量、解题思路、优化效率和综合能力这5个面试要点。 推荐网站：牛客网-专业IT笔试面试备考平台]]></content>
  </entry>
  <entry>
    <title><![CDATA[vscode]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavatool%2Fide%2Fvscode%2F</url>
    <content type="text"><![CDATA[vscode 快捷键 命令面板（Command Palette） 根据您当前的上下文访问所有可用命令。 Mac: cmd+shift+p or f1 Windows / Linux: ctrl+shift+p or f1 快速打开文件（Quick Open） Mac: cmd+p Windows / Linux: ctrl+p Status Bar Mac: shift+cmd+m Windows / Linux: ctrl+shift+m 改变语言模式 Mac: cmd+k m Windows / Linux: ctrl+k m 设置 Mac: cmd+, Windows / Linux: File &gt; Preferences &gt; Settings or ctrl+, 插件 Chinese (Simplified) Language Pack for Visual Studio Code Prettier - Code formatter IntelliJ IDEA Keybindings EditorConfig for VS Code Git History 更多内容 官方 https://github.com/Microsoft/vscode https://github.com/Microsoft/vscode-docs https://github.com/Microsoft/vscode-tips-and-tricks 更多资源 https://github.com/viatsko/awesome-vscode]]></content>
  </entry>
  <entry>
    <title><![CDATA[CSS 快速入门]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Ffrontend%2Fbase%2Fcss%2F</url>
    <content type="text"><![CDATA[CSS 快速入门 层叠样式表（英语：Cascading Style Sheets，简写 CSS），又称串樣式列表、级联样式表、串接样式表、階層式樣式表，一种用来为结构化文档（如 HTML 文档或 XML 应用）添加样式（字体、间距和颜色等）的计算机语言。 关键词： css 简介 框模型 圆角边框 边框阴影 边框图片 样式 背景 文本 字体 布局 创建多列 规定列之间的间隔 列规则 列属性 转换 2D 转换 3D 转换 过渡 它如何工作？ 多项改变 过渡属性 动画 @keyframes 规则 什么是 CSS3 中的动画？ 动画属性 更多内容 CSS3 是最新的 CSS 标准。 CSS3 完全向后兼容，因此您不必改变现有的设计。浏览器通常支持 CSS2。 简介 CSS3 被划分为模块。 其中最重要的 CSS3 模块包括： 选择器 框模型 样式 布局 2D/3D 转换 动画 用户界面 框模型 通过 CSS3，您能够创建圆角边框，向矩形添加阴影，使用图片来绘制边框 - 并且不需使用设计软件，比如 PhotoShop。 在本章中，您将学到以下边框属性： border-radius box-shadow border-image 圆角边框 在 CSS2 中添加圆角矩形需要技巧。我们必须为每个圆角使用不同的图片。 在 CSS3 中，创建圆角是非常容易的。 在 CSS3 中，border-radius 属性用于创建圆角： 这个矩形有圆角哦！ 实例 向 div 元素添加圆角： div &#123; border: 2px solid; border-radius: 25px; -moz-border-radius: 25px; /* Old Firefox */&#125; 边框阴影 在 CSS3 中，box-shadow 用于向方框添加阴影： 实例 向 div 元素添加方框阴影： div &#123; box-shadow: 10px 10px 5px #888888;&#125; 边框图片 通过 CSS3 的 border-image 属性，您可以使用图片来创建边框： border-image 属性允许您规定用于边框的图片！ 用于创建上面的边框的原始图片： 实例 使用图片创建围绕 div 元素的边框： div&#123;border-image:url(border.png) 30 30 round;-moz-border-image:url(border.png) 30 30 round; /* 老的 Firefox */-webkit-border-image:url(border.png) 30 30 round; /* Safari 和 Chrome */-o-border-image:url(border.png) 30 30 round; /* Opera */&#125; 样式 背景 CSS3 包含多个新的背景属性，它们提供了对背景更强大的控制。 在本章，您将学到以下背景属性： background-size background-origin 您也将学到如何使用多重背景图片。 background-size 属性 background-size 属性规定背景图片的尺寸。 在 CSS3 之前，背景图片的尺寸是由图片的实际尺寸决定的。在 CSS3 中，可以规定背景图片的尺寸，这就允许我们在不同的环境中重复使用背景图片。 您能够以像素或百分比规定尺寸。如果以百分比规定尺寸，那么尺寸相对于父元素的宽度和高度。 实例：调整背景图片的大小 div &#123; background: url(bg_flower.gif); -moz-background-size: 63px 100px; /* 老版本的 Firefox */ background-size: 63px 100px; background-repeat: no-repeat;&#125; 实例：对背景图片进行拉伸，使其完成填充内容区域 div &#123; background: url(bg_flower.gif); -moz-background-size: 40% 100%; /* 老版本的 Firefox */ background-size: 40% 100%; background-repeat: no-repeat;&#125; background-origin 属性 background-origin 属性规定背景图片的定位区域。 背景图片可以放置于 content-box、padding-box 或 border-box 区域。 实例：在 content-box 中定位背景图片 div &#123; background: url(bg_flower.gif); background-repeat: no-repeat; background-size: 100% 100%; -webkit-background-origin: content-box; /* Safari */ background-origin: content-box;&#125; 多重背景图片 CSS3 允许您为元素使用多个背景图像。 实例：为 body 元素设置两幅背景图片 body &#123; background-image: url(bg_flower.gif), url(bg_flower_2.gif);&#125; 文本 CSS3 包含多个新的文本特性。 在本章中，您将学到如下文本属性： text-shadow word-wrap 文本阴影 在 CSS3 中，text-shadow 可向文本应用阴影。 您能够规定水平阴影、垂直阴影、模糊距离，以及阴影的颜色： 实例：向标题添加阴影 h1 &#123; text-shadow: 5px 5px 5px #ff0000;&#125; 自动换行 单词太长的话就可能无法超出某个区域： This paragraph contains a very long word: thisisaveryveryveryveryveryverylongword. The long word will break and wrap to the next line. 在 CSS3 中，word-wrap 属性允许您允许文本强制文本进行换行 - 即使这意味着会对单词进行拆分： This paragraph contains a very long word: thisisaveryveryveryveryveryverylongword. The long word will break and wrap to the next line. 下面是 CSS 代码： 实例：允许对长单词进行拆分，并换行到下一行 p &#123; word-wrap: break-word;&#125; 字体 @font-face 规则 在 CSS3 之前，web 设计师必须使用已在用户计算机上安装好的字体。 通过 CSS3，web 设计师可以使用他们喜欢的任意字体。 当您您找到或购买到希望使用的字体时，可将该字体文件存放到 web 服务器上，它会在需要时被自动下载到用户的计算机上。 您“自己的”的字体是在 CSS3 @font-face 规则中定义的。 使用您需要的字体 在新的 @font-face 规则中，您必须首先定义字体的名称（比如 myFirstFont），然后指向该字体文件。 如需为 HTML 元素使用字体，请通过 font-family 属性来引用字体的名称 (myFirstFont)： 实例 &lt;style&gt;@font-face &#123; font-family: myFirstFont; src: url('../../assets/fonts/sansation-light.ttf'); /* IE9+ */&#125;div &#123; font-family: myFirstFont;&#125;&lt;/style&gt; 使用粗体字体 您必须为粗体文本添加另一个包含描述符的 @font-face： 实例 @font-face &#123; font-family: myFirstFont; src: url("../../assets/fonts/sansation-light.ttf"); /* IE9+ */&#125;@font-face &#123; font-family: myFirstFont; src: url("../../assets/fonts/sansation-bold.ttf"); /* IE9+ */ font-weight: bold;&#125; 文件 “Sansation_Bold.ttf” 是另一个字体文件，它包含了 Sansation 字体的粗体字符。 只要 font-family 为 “myFirstFont” 的文本需要显示为粗体，浏览器就会使用该字体。 通过这种方式，我们可以为相同的字体设置许多 @font-face 规则。 字体描述符 下面的表格列出了能够在 @font-face 规则中定义的所有字体描述符： 描述符 值 描述 font-family name 必需。规定字体的名称。 src URL 必需。定义字体文件的 URL。 font-stretch normalcondensedultra-condensedextra-condensedsemi-condensedexpandedsemi-expandedextra-expandedultra-expanded 可选。定义如何拉伸字体。默认是 “normal”。 font-style ormalitalicoblique 可选。定义字体的样式。默认是 “normal”。 font-weight normalbold100200300400500600700800900 可选。定义字体的粗细。默认是 “normal”。 unicode-range unicode-range 可选。定义字体支持的 UNICODE 字符范围。默认是 “U+0-10FFFF”。 布局 通过 CSS3，您能够创建多个列来对文本进行布局 - 就像报纸那样！ 在本章中，您将学习如下多列属性： column-count column-gap column-rule 创建多列 column-count 属性规定元素应该被分隔的列数： 实例：把 div 元素中的文本分隔为三列 div &#123; -moz-column-count: 3; /* Firefox */ -webkit-column-count: 3; /* Safari and Chrome */ column-count: 3;&#125; 规定列之间的间隔 column-gap 属性规定列之间的间隔： 实例：规定列之间 40 像素的间隔 div &#123; -moz-column-gap: 30px; /* Firefox */ -webkit-column-gap: 30px; /* Safari and Chrome */ column-gap: 30px;&#125; 列规则 column-rule 属性设置列之间的宽度、样式和颜色规则。 实例 规定列之间的宽度、样式和颜色规则： div &#123; -moz-column-rule: 3px outset #ff0000; /* Firefox */ -webkit-column-rule: 3px outset #ff0000; /* Safari and Chrome */ column-rule: 3px outset #ff0000;&#125; 列属性 属性 描述 column-count 规定元素应该被分隔的列数。 column-fill 规定如何填充列。 column-gap 规定列之间的间隔。 column-rule 设置所有 column-rule-* 属性的简写属性。 column-rule-color 规定列之间规则的颜色。 column-rule-style 规定列之间规则的样式。 column-rule-width 规定列之间规则的宽度。 column-span 规定元素应该横跨的列数。 column-width 规定列的宽度。 columns 规定设置 column-width 和 column-count 的简写属性。 转换 通过 CSS3 转换，我们能够对元素进行移动、缩放、转动、拉长或拉伸。 转换是使元素改变形状、尺寸和位置的一种效果。 您可以使用 2D 或 3D 转换来转换您的元素。 2D 转换 在本章中，您将学到如下 2D 转换方法： translate() rotate() scale() skew() matrix() 实例 div &#123; transform: rotate(30deg); -ms-transform: rotate(30deg); /* IE 9 */ -webkit-transform: rotate(30deg); /* Safari and Chrome */ -o-transform: rotate(30deg); /* Opera */ -moz-transform: rotate(30deg); /* Firefox */&#125; translate() 方法 通过 translate() 方法，元素从其当前位置移动，根据给定的 left（x 坐标） 和 top（y 坐标） 位置参数： 实例 div &#123; transform: translate(50px, 100px); -ms-transform: translate(50px, 100px); /* IE 9 */ -webkit-transform: translate(50px, 100px); /* Safari and Chrome */ -o-transform: translate(50px, 100px); /* Opera */ -moz-transform: translate(50px, 100px); /* Firefox */&#125; 值 translate(50px,100px) 把元素从左侧移动 50 像素，从顶端移动 100 像素。 rotate() 方法 通过 rotate() 方法，元素顺时针旋转给定的角度。允许负值，元素将逆时针旋转。 实例 div &#123; transform: rotate(30deg); -ms-transform: rotate(30deg); /* IE 9 */ -webkit-transform: rotate(30deg); /* Safari and Chrome */ -o-transform: rotate(30deg); /* Opera */ -moz-transform: rotate(30deg); /* Firefox */&#125; 值 rotate(30deg) 把元素顺时针旋转 30 度。 scale() 方法 通过 scale() 方法，元素的尺寸会增加或减少，根据给定的宽度（X 轴）和高度（Y 轴）参数： 实例 div &#123; transform: scale(2, 4); -ms-transform: scale(2, 4); /* IE 9 */ -webkit-transform: scale(2, 4); /* Safari 和 Chrome */ -o-transform: scale(2, 4); /* Opera */ -moz-transform: scale(2, 4); /* Firefox */&#125; 值 scale(2,4) 把宽度转换为原始尺寸的 2 倍，把高度转换为原始高度的 4 倍。 skew() 方法 通过 skew() 方法，元素翻转给定的角度，根据给定的水平线（X 轴）和垂直线（Y 轴）参数： 实例 div &#123; transform: skew(30deg, 20deg); -ms-transform: skew(30deg, 20deg); /* IE 9 */ -webkit-transform: skew(30deg, 20deg); /* Safari and Chrome */ -o-transform: skew(30deg, 20deg); /* Opera */ -moz-transform: skew(30deg, 20deg); /* Firefox */&#125; 值 skew(30deg,20deg) 围绕 X 轴把元素翻转 30 度，围绕 Y 轴翻转 20 度。 matrix() 方法 matrix() 方法把所有 2D 转换方法组合在一起。 matrix() 方法需要六个参数，包含数学函数，允许您：旋转、缩放、移动以及倾斜元素。 实例 如何使用 matrix 方法将 div 元素旋转 30 度： div &#123; transform: matrix(0.866, 0.5, -0.5, 0.866, 0, 0); -ms-transform: matrix(0.866, 0.5, -0.5, 0.866, 0, 0); /* IE 9 */ -moz-transform: matrix(0.866, 0.5, -0.5, 0.866, 0, 0); /* Firefox */ -webkit-transform: matrix( 0.866, 0.5, -0.5, 0.866, 0, 0 ); /* Safari and Chrome */ -o-transform: matrix(0.866, 0.5, -0.5, 0.866, 0, 0); /* Opera */&#125; 新的转换属性 下面的表格列出了所有的转换属性： 属性 描述 transform 向元素应用 2D 或 3D 转换。 transform-origin 允许你改变被转换元素的位置。 2D Transform 方法 函数 描述 matrix(n,n,n,n,n,n) 定义 2D 转换，使用六个值的矩阵。 translate(x,y) 定义 2D 转换，沿着 X 和 Y 轴移动元素。 translateX(n) 定义 2D 转换，沿着 X 轴移动元素。 translateY(n) 定义 2D 转换，沿着 Y 轴移动元素。 scale(x,y) 定义 2D 缩放转换，改变元素的宽度和高度。 scaleX(n) 定义 2D 缩放转换，改变元素的宽度。 scaleY(n) 定义 2D 缩放转换，改变元素的高度。 rotate(angle) 定义 2D 旋转，在参数中规定角度。 skew(x-angle,y-angle) 定义 2D 倾斜转换，沿着 X 和 Y 轴。 skewX(angle) 定义 2D 倾斜转换，沿着 X 轴。 skewY(angle) 定义 2D 倾斜转换，沿着 Y 轴。 3D 转换 CSS3 允许您使用 3D 转换来对元素进行格式化。 在本章中，您将学到其中的一些 3D 转换方法： rotateX() rotateY() 点击下面的元素，来查看 2D 转换与 3D 转换之间的不同之处： rotateX() 方法 通过 rotateX() 方法，元素围绕其 X 轴以给定的度数进行旋转。 实例 div &#123; transform: rotateX(120deg); -webkit-transform: rotateX(120deg); /* Safari 和 Chrome */ -moz-transform: rotateX(120deg); /* Firefox */&#125; rotateY() 旋转 通过 rotateY() 方法，元素围绕其 Y 轴以给定的度数进行旋转。 实例 div &#123; transform: rotateY(130deg); -webkit-transform: rotateY(130deg); /* Safari 和 Chrome */ -moz-transform: rotateY(130deg); /* Firefox */&#125; 转换属性 下面的表格列出了所有的转换属性： 属性 描述 transform 向元素应用 2D 或 3D 转换。 transform-origin 允许你改变被转换元素的位置。 transform-style 规定被嵌套元素如何在 3D 空间中显示。 perspective 规定 3D 元素的透视效果。 perspective-origin 规定 3D 元素的底部位置。 backface-visibility 定义元素在不面对屏幕时是否可见。 过渡 通过 CSS3，我们可以在不使用 Flash 动画或 JavaScript 的情况下，当元素从一种样式变换为另一种样式时为元素添加效果。 它如何工作？ CSS3 过渡是元素从一种样式逐渐改变为另一种的效果。 要实现这一点，必须规定两项内容： 规定您希望把效果添加到哪个 CSS 属性上 规定效果的时长 实例 应用于宽度属性的过渡效果，时长为 2 秒： div &#123; transition: width 2s; -moz-transition: width 2s; /* Firefox 4 */ -webkit-transition: width 2s; /* Safari 和 Chrome */ -o-transition: width 2s; /* Opera */&#125; 注释：如果时长未规定，则不会有过渡效果，因为默认值是 0。 效果开始于指定的 CSS 属性改变值时。CSS 属性改变的典型时间是鼠标指针位于元素上时： 实例 规定当鼠标指针悬浮于 &lt;div&gt; 元素上时： div:hover &#123; width: 300px;&#125; 注释：当指针移出元素时，它会逐渐变回原来的样式。 多项改变 如需向多个样式添加过渡效果，请添加多个属性，由逗号隔开： 实例 向宽度、高度和转换添加过渡效果： div &#123; transition: width 2s, height 2s, transform 2s; -moz-transition: width 2s, height 2s, -moz-transform 2s; -webkit-transition: width 2s, height 2s, -webkit-transform 2s; -o-transition: width 2s, height 2s, -o-transform 2s;&#125; 过渡属性 下面的表格列出了所有的转换属性： 属性 描述 transition 简写属性，用于在一个属性中设置四个过渡属性。 transition-property 规定应用过渡的 CSS 属性的名称。 transition-duration 定义过渡效果花费的时间。默认是 0。 transition-timing-function 规定过渡效果的时间曲线。默认是 “ease”。 transition-delay 规定过渡效果何时开始。默认是 0。 下面的两个例子设置所有过渡属性： 实例：在一个例子中使用所有过渡属性 div &#123; transition-property: width; transition-duration: 1s; transition-timing-function: linear; transition-delay: 2s; /* Firefox 4 */ -moz-transition-property: width; -moz-transition-duration: 1s; -moz-transition-timing-function: linear; -moz-transition-delay: 2s; /* Safari 和 Chrome */ -webkit-transition-property: width; -webkit-transition-duration: 1s; -webkit-transition-timing-function: linear; -webkit-transition-delay: 2s; /* Opera */ -o-transition-property: width; -o-transition-duration: 1s; -o-transition-timing-function: linear; -o-transition-delay: 2s;&#125; 实例：与上面的例子相同的过渡效果，但是使用了简写的 transition 属性 div &#123; transition: width 1s linear 2s; /* Firefox 4 */ -moz-transition: width 1s linear 2s; /* Safari and Chrome */ -webkit-transition: width 1s linear 2s; /* Opera */ -o-transition: width 1s linear 2s;&#125; 动画 通过 CSS3，我们能够创建动画，这可以在许多网页中取代动画图片、Flash 动画以及 JavaScript。 @keyframes 规则 如需在 CSS3 中创建动画，您需要学习 @keyframes 规则。 @keyframes 规则用于创建动画。在 @keyframes 中规定某项 CSS 样式，就能创建由当前样式逐渐改为新样式的动画效果。 实例 @keyframes myfirst &#123; from &#123; background: red; &#125; to &#123; background: yellow; &#125;&#125;/* Firefox */@-moz-keyframes myfirst &#123; from &#123; background: red; &#125; to &#123; background: yellow; &#125;&#125;/* Safari 和 Chrome */@-webkit-keyframes myfirst &#123; from &#123; background: red; &#125; to &#123; background: yellow; &#125;&#125;/* Opera */@-o-keyframes myfirst &#123; from &#123; background: red; &#125; to &#123; background: yellow; &#125;&#125; 当您在 @keyframes 中创建动画时，请把它捆绑到某个选择器，否则不会产生动画效果。 通过规定至少以下两项 CSS3 动画属性，即可将动画绑定到选择器： 规定动画的名称 规定动画的时长 实例：把 “myfirst” 动画捆绑到 div 元素，时长：5 秒 div &#123; animation: myfirst 5s; -moz-animation: myfirst 5s; /* Firefox */ -webkit-animation: myfirst 5s; /* Safari 和 Chrome */ -o-animation: myfirst 5s; /* Opera */&#125; 注释：您必须定义动画的名称和时长。如果忽略时长，则动画不会允许，因为默认值是 0。 什么是 CSS3 中的动画？ 动画是使元素从一种样式逐渐变化为另一种样式的效果。 您可以改变任意多的样式任意多的次数。 请用百分比来规定变化发生的时间，或用关键词 “from” 和 “to”，等同于 0% 和 100%。 0% 是动画的开始，100% 是动画的完成。 为了得到最佳的浏览器支持，您应该始终定义 0% 和 100% 选择器。 实例：当动画为 25% 及 50% 时改变背景色，然后当动画 100% 完成时再次改变 @keyframes myfirst &#123; 0% &#123; background: red; &#125; 25% &#123; background: yellow; &#125; 50% &#123; background: blue; &#125; 100% &#123; background: green; &#125;&#125;/* Firefox */@-moz-keyframes myfirst &#123; 0% &#123; background: red; &#125; 25% &#123; background: yellow; &#125; 50% &#123; background: blue; &#125; 100% &#123; background: green; &#125;&#125;/* Safari 和 Chrome */@-webkit-keyframes myfirst &#123; 0% &#123; background: red; &#125; 25% &#123; background: yellow; &#125; 50% &#123; background: blue; &#125; 100% &#123; background: green; &#125;&#125;/* Opera */@-o-keyframes myfirst &#123; 0% &#123; background: red; &#125; 25% &#123; background: yellow; &#125; 50% &#123; background: blue; &#125; 100% &#123; background: green; &#125;&#125; 实例：改变背景色和位置 @keyframes myfirst &#123; 0% &#123; background: red; left: 0px; top: 0px; &#125; 25% &#123; background: yellow; left: 200px; top: 0px; &#125; 50% &#123; background: blue; left: 200px; top: 200px; &#125; 75% &#123; background: green; left: 0px; top: 200px; &#125; 100% &#123; background: red; left: 0px; top: 0px; &#125;&#125;/* Firefox */@-moz-keyframes myfirst &#123; 0% &#123; background: red; left: 0px; top: 0px; &#125; 25% &#123; background: yellow; left: 200px; top: 0px; &#125; 50% &#123; background: blue; left: 200px; top: 200px; &#125; 75% &#123; background: green; left: 0px; top: 200px; &#125; 100% &#123; background: red; left: 0px; top: 0px; &#125;&#125;/* Safari 和 Chrome */@-webkit-keyframes myfirst &#123; 0% &#123; background: red; left: 0px; top: 0px; &#125; 25% &#123; background: yellow; left: 200px; top: 0px; &#125; 50% &#123; background: blue; left: 200px; top: 200px; &#125; 75% &#123; background: green; left: 0px; top: 200px; &#125; 100% &#123; background: red; left: 0px; top: 0px; &#125;&#125;/* Opera */@-o-keyframes myfirst &#123; 0% &#123; background: red; left: 0px; top: 0px; &#125; 25% &#123; background: yellow; left: 200px; top: 0px; &#125; 50% &#123; background: blue; left: 200px; top: 200px; &#125; 75% &#123; background: green; left: 0px; top: 200px; &#125; 100% &#123; background: red; left: 0px; top: 0px; &#125;&#125; 动画属性 下面的表格列出了 @keyframes 规则和所有动画属性： 属性 描述 @keyframes 规定动画。 animation 所有动画属性的简写属性，除了 animation-play-state 属性。 animation-name 规定 @keyframes 动画的名称。 animation-duration 规定动画完成一个周期所花费的秒或毫秒。默认是 0。 animation-timing-function 规定动画的速度曲线。默认是 “ease”。 animation-delay 规定动画何时开始。默认是 0。 animation-iteration-count 规定动画被播放的次数。默认是 1。 animation-direction 规定动画是否在下一周期逆向地播放。默认是 “normal”。 animation-play-state 规定动画是否正在运行或暂停。默认是 “running”。 animation-fill-mode 规定对象动画时间之外的状态。 下面的两个例子设置了所有动画属性： 实例：运行名为 myfirst 的动画，其中设置了所有动画属性 div &#123; animation-name: myfirst; animation-duration: 5s; animation-timing-function: linear; animation-delay: 2s; animation-iteration-count: infinite; animation-direction: alternate; animation-play-state: running; /* Firefox: */ -moz-animation-name: myfirst; -moz-animation-duration: 5s; -moz-animation-timing-function: linear; -moz-animation-delay: 2s; -moz-animation-iteration-count: infinite; -moz-animation-direction: alternate; -moz-animation-play-state: running; /* Safari 和 Chrome: */ -webkit-animation-name: myfirst; -webkit-animation-duration: 5s; -webkit-animation-timing-function: linear; -webkit-animation-delay: 2s; -webkit-animation-iteration-count: infinite; -webkit-animation-direction: alternate; -webkit-animation-play-state: running; /* Opera: */ -o-animation-name: myfirst; -o-animation-duration: 5s; -o-animation-timing-function: linear; -o-animation-delay: 2s; -o-animation-iteration-count: infinite; -o-animation-direction: alternate; -o-animation-play-state: running;&#125; 实例：与上面的动画相同，但是使用了简写的动画 animation 属性 div &#123; animation: myfirst 5s linear 2s infinite alternate; /* Firefox: */ -moz-animation: myfirst 5s linear 2s infinite alternate; /* Safari 和 Chrome: */ -webkit-animation: myfirst 5s linear 2s infinite alternate; /* Opera: */ -o-animation: myfirst 5s linear 2s infinite alternate;&#125; 更多内容 📚 拓展阅读 Html Css Javascript 📦 本文归档在 我的前端技术教程系列：frontend-tutorial W3school css 教程]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 术语]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fappendix%2Fdocker-glossary%2F</url>
    <content type="text"><![CDATA[Docker 术语 Docker 镜像(Images)：Docker 镜像是用于创建 Docker 容器的模板。 Docker 容器(Container)：容器是独立运行的一个或一组应用。 Docker 客户端(Client)：Docker 客户端通过命令行或者其他工具使用 Docker API (https://docs.docker.com/reference/api/docker_remote_api) 与 Docker 的守护进程通信。 Docker 主机(Host)：一个物理或者虚拟的机器用于执行 Docker 守护进程和容器。 Docker 仓库(Registry)：Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。：Docker Hub(https://hub.docker.com) 提供了庞大的镜像集合供使用。 Docker Machine：Docker Machine是一个简化Docker安装的命令行工具，通过一个简单的命令行即可在相应的平台上安装Docker，比如VirtualBox、 Digital Ocean、Microsoft Azure。 Docker Machine：Dockerfile是一个文本文档，其中包含您通常为了构建Docker镜像而手动执行的所有命令。 Docker可以通过从Dockerfile读取指令自动构建映像。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 命令]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fappendix%2Fdocker-cli%2F</url>
    <content type="text"><![CDATA[Docker 命令 镜像(Images) 容器(Container) 生命周期 启动和停止 信息 导入 / 导出 执行命令 网络(Networks) 生命周期 信息 链接 仓管中心和仓库(Registry &amp; Repository) Dockerfile 卷标(Volumes) 生命周期 信息 引用和引申 镜像(Images) docker image ls - 查看所有镜像。 docker image rm - 删除本地镜像。 docker import - 从压缩文件中创建镜像。 docker export - 导出既有容器。 docker build - 从 Dockerfile 创建镜像。 docker commit - 为容器创建镜像，如果容器正在运行则会临时暂停。 docker rmi - 删除镜像。 docker load - 通过 STDIN 从压缩包加载镜像，包括镜像和标签(images and tags) (0.7 起). docker save - 通过 STDOUT 保存镜像到压缩包，包括所有的父层，标签和版本(parent layers, tags &amp; versions) (0.7 起). docker history - 查看镜像历史记录。 docker tag - 给镜像命名打标(tags) (本地或者仓库)。 容器(Container) 生命周期 docker create - 创建一个容器但是不启动。 docker rename - 允许重命名容器。 docker run - 在同一个操作中创建并启动一个容器。 docker rm - 删除容器。 docker update - 更新容器的资源限制。 启动和停止 docker start - 启动容器。 docker stop - 停止运行中的容器。 docker restart - 停止之后再启动容器。 docker pause - 暂停运行中的容器，将其 “冻结” 在当前状态。 docker unpause - 结束容器暂停状态。 docker wait - 阻塞，到运行中的容器停止为止。 docker kill - 向运行中容器发送 SIGKILL 指令。 docker attach - 链接到运行中容器。 信息 docker ps - 查看运行中的所有容器。 docker logs - 从容器中获取日志。(你也可以使用自定义日志驱动，不过在 1.10 中，它只支持 json-file 和 journald) docker inspect - 查看某个容器的所有信息(包括 IP 地址)。 docker events - 从容器中获取事件(events)。 docker port - 查看容器的公开端口。 docker top - 查看容器中活动进程。 docker stats - 查看容器的资源使用情况统计信息。 docker diff - 查看容器的 FS 中有变化文件信息。 导入 / 导出 docker cp 在容器和本地文件系统之间复制文件或文件夹。 docker export 将容器的文件系统切换为压缩包(tarball archive stream)输出到 STDOUT。 执行命令 docker exec 在容器中执行命令。 网络(Networks) 生命周期 docker network create docker network rm 信息 docker network ls docker network inspect 链接 docker network connect docker network disconnect 仓管中心和仓库(Registry &amp; Repository) docker login - 登入仓管中心。 docker logout - 登出仓管中心。 docker search - 从仓管中心检索镜像。 docker pull - 从仓管中心拉去镜像到本地。 docker push - 从本地推送镜像到仓管中心。 Dockerfile .dockerignore FROM 为其他指令设置基础镜像(Base Image)。 MAINTAINER 为生成的镜像设置作者字段。 RUN 在当前镜像的基础上生成一个新层并执行命令。 CMD 设置容器默认执行命令。 EXPOSE 告知 Docker 容器在运行时所要监听的网络端口。注意：并没有实际上将端口设置为可访问。 ENV 设置环境变量。 ADD 将文件，文件夹或者远程文件复制到容器中。缓存无效。尽量用 COPY 代替 ADD。 COPY 将文件或文件夹复制到容器中。 ENTRYPOINT 将一个容器设置为可执行。 VOLUME 为外部挂载卷标或其他容器设置挂载点(mount point)。 USER 设置执行 RUN / CMD / ENTRYPOINT 命令的用户名。 WORKDIR 设置工作目录。 ARG 定义编译时(build-time)变量。 ONBUILD 添加触发指令，当该镜像被作为其他镜像的基础镜像时该指令会被触发。 STOPSIGNAL 设置通过系统向容器发出退出指令。 LABEL 将键值对元数据(key/value metadata)应用到你的镜像，容器，或者守护进程。 卷标(Volumes) 生命周期 docker volume create docker volume rm 信息 docker volume ls docker volume inspect 引用和引申 https://github.com/wsargent/docker-cheat-sheet/tree/master/zh-cn]]></content>
  </entry>
  <entry>
    <title><![CDATA[军事]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fminds%2F%E4%BA%BA%E6%96%87%2F%E5%86%9B%E4%BA%8B%2FREADME%2F</url>
    <content type="text"><![CDATA[军事 军事是与战争、军队、军人等有关事务的总称。 关键词 战争、战役、战斗、战士、战略、战术、武器、兵种、进攻、防御、指挥、兵法、军衔]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spring for Apache Kafka]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fjava%2Fjavaweb%2Fspring-boot%2Fspring-boot-kafka%2F</url>
    <content type="text"><![CDATA[Spring for Apache Kafka spring 基于 kafka-clients jar 包。 简介 特性 快速入门 发送消息 接收消息 流处理 其他 Kafka 属性 引用和引申 简介 特性 KafkaTemplate KafkaMessageListenerContainer @KafkaListener KafkaTransactionManager spring-kafka-test 快速入门 spring-kafka 项目支持在 Spring Boot 中自动配置 Apache Kafka Kafka 配置在 spring.kafka.* 属性中进行控制。例如：你可以在 application.properties 中声明这些配置属性。 spring.kafka.bootstrap-servers=localhost:9092spring.kafka.consumer.group-id=myGroup 注意：更详细的配置可以参考 - KafkaProperties 发送消息 Spring’s KafkaTemplate is auto-configured, and you can autowire it directly in your own beans, as shown in the following example: @Componentpublic class MyBean &#123; private final KafkaTemplate kafkaTemplate; @Autowired public MyBean(KafkaTemplate kafkaTemplate) &#123; this.kafkaTemplate = kafkaTemplate; &#125; // ...&#125; 注意：如果定义了 spring.kafka.producer.transaction-id-prefix 属性，一个 KafkaTransactionManager 实例会被自动配置。同时，一个 RecordMessageConverter bean 也会被定义，并且它被自动关联到 KafkaTemplate。 接收消息 @KafkaListener 注解用于创建一个 listener。如果没有定义 KafkaListenerContainerFactory，会根据 spring.kafka.listener.* 的属性自动配置一个。 下面是一个监听 someTopic Topic 的示例： @Componentpublic class MyBean &#123; @KafkaListener(topics = "someTopic") public void processMessage(String content) &#123; // ... &#125;&#125; 如果定义了 KafkaTransactionManager Bean，它会被自动关联到容器工厂。类似的，如果 RecordMessageConverter, ErrorHandler 或 AfterRollbackProcessor Bean 被定义，也会壁咚关联到默认工厂。 流处理 如果项目的 classpath 出现了 kafka-streams, Spring Boot 会自动配置 KafkaStreamsConfiguration 需要的 Bean，并且 Kafka 流通过 @EnableKafkaStreams 注解启动来开启。 启动 Kafka 流意味着 application id 和 bootstrap servers 必须设置。可以通过 spring.kafka.streams.application-id 属性配置，如果没有设置，默认为 spring.application.name。 示例： @Configuration@EnableKafkaStreamsstatic class KafkaStreamsExampleConfiguration &#123; @Bean public KStream&lt;Integer, String&gt; kStream(StreamsBuilder streamsBuilder) &#123; KStream&lt;Integer, String&gt; stream = streamsBuilder.stream("ks1In"); stream.map((k, v) -&gt; new KeyValue&lt;&gt;(k, v.toUpperCase())).to("ks1Out", Produced.with(Serdes.Integer(), new JsonSerde&lt;&gt;())); return stream; &#125;&#125; 默认，流是由 StreamBuilder 对象管理的。可以使用 spring.kafka.streams.auto-startup 属性定制它的行为。 其他 Kafka 属性 参考：了解更多属性，请参考 - Appendix A, Common application properties KafkaProperties 类只支持 Kafka 配置属性的部分子集。如果需要配置一些附加属性，可以使用如下属性： spring.kafka.properties.prop.one=firstspring.kafka.admin.properties.prop.two=secondspring.kafka.consumer.properties.prop.three=thirdspring.kafka.producer.properties.prop.four=fourthspring.kafka.streams.properties.prop.five=fifth 配置 JsonDeserializer 的属性： spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializerspring.kafka.consumer.properties.spring.json.value.default.type=com.example.Invoicespring.kafka.consumer.properties.spring.json.trusted.packages=com.example,org.acme 配置 JsonSerializer 的属性： spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializerspring.kafka.producer.properties.spring.json.add.type.headers=false 引用和引申 Spring for Apache Kafka Spring Boot 官方文档之 Apache Kafka Support]]></content>
  </entry>
  <entry>
    <title><![CDATA[Docker 的设计]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fos%2Fdocker%2Fadvanced%2Fdocker-design%2F</url>
    <content type="text"><![CDATA[Docker 的设计 Docker 架构 Docker 守护进程（docker daemon） Docker 客户端 Docker 注册中心 Docker 对象 镜像 容器 服务 底层技术 命名空间 控制组 联合文件系统 容器格式 资料 Docker 的设计 Docker 架构 Docker 使用 C/S 体系结构。Docker 守护进程，负责构建、运行和分发 Docker 容器；Docker 客户端与 Docker 守护进程通信。Docker 客户端和守护进程可以在同一个系统上运行，也可以将 Docker 客户端连接到远程 Docker 守护进程。Docker 客户端和守护进程使用 REST API，并通过 UNIX 套接字或网络接口进行通信。 Docker 守护进程（docker daemon） Docker 守护进程（dockerd）监听 Docker API 请求并管理 Docker 对象（如镜像，容器，网络和卷）。守护进程还可以与其他守护进程通信来管理 Docker 服务。 Docker 客户端 Docker 客户端（docker）是许多 Docker 用户与 Docker 进行交互的主要方式。当你使用诸如 docker run 之类的命令时，客户端将这些命令发送到 dockerd，dockerd 执行这些命令。 docker 命令使用 Docker API。 Docker 客户端可以与多个守护进程进行通信。 Docker 注册中心 Docker 注册中心存储 Docker 镜像。Docker Hub 和 Docker Cloud 是任何人都可以使用的公共注册中心，并且 Docker 默认配置为在 Docker Hub 上查找镜像。你甚至可以运行你自己的私人注册中心。如果您使用 Docker Datacenter（DDC），它包括 Docker Trusted Registry（DTR）。 当您使用 docker pull 或 docker run 命令时，所需的镜像将从配置的注册中心中提取。当您使用 docker push 命令时，您的镜像将被推送到您配置的注册中心。 Docker 商店 允许您购买和销售 Docker 镜像或免费发布。例如，您可以购买包含来自软件供应商的应用程序或服务的 Docker 镜像，并使用该镜像将应用程序部署到您的测试，临时和生产环境中。您可以通过拉取新版本的镜像并重新部署容器来升级应用程序。 Docker 对象 镜像 镜像是一个只读模板，带有创建 Docker 容器的说明。通常，镜像基于另一个镜像，并具有一些额外的自定义功能。例如，您可以构建基于 ubuntu 镜像的镜像，但会安装 Apache Web 服务器和应用程序，以及使应用程序运行所需的配置细节。 您可能会创建自己的镜像，或者您可能只能使用其他人创建并在注册中心中发布的镜像。为了构建您自己的镜像，您可以使用简单的语法创建 Dockerfile，以定义创建镜像并运行所需的步骤。 Dockerfile 中的每条指令都会在镜像中创建一个图层。当您更改 Dockerfile 并重建镜像时，只重建那些已更改的图层。与其他虚拟化技术相比，这是使镜像轻量，小巧，快速的一部分。 容器 容器是镜像的可运行实例。您可以使用 Docker API 或 CLI 创建、启动、停止、移动或删除容器。您可以将容器连接到一个或多个网络，将存储器连接到它，甚至可以根据其当前状态创建新镜像。 默认情况下，容器与其他容器及其主机相对隔离。您可以控制容器的网络、存储或其他底层子系统与其他容器或主机的隔离程度。 容器由其镜像以及您在创建或启动时提供给它的任何配置选项来定义。当一个容器被移除时，其未被存储在永久存储器中的状态将消失。 服务 通过服务，您可以跨多个 Docker 守护进程扩展容器，这些守护进程可以作为一个群组与多个管理人员、工作人员一起工作。集群中的每个成员都是 Docker 守护进程，守护进程都使用 Docker API 进行通信。服务允许您定义所需的状态，例如在任何给定时间必须可用的服务的副本数量。默认情况下，该服务在所有工作节点之间进行负载平衡。对于消费者来说，Docker 服务似乎是一个单一的应用程序。Docker 引擎在 Docker 1.12 及更高版本中支持集群模式。 底层技术 Docker 使用 Go 编写，利用 Linux 内核的几个特性来提供其功能。 命名空间 Docker 使用名为 namespaces 的技术来提供独立工作空间（即容器）。当你运行一个容器时，Docker 会为该容器创建一组命名空间。 这些命名空间提供了一个隔离层。容器的每个方面都在单独的命名空间中运行，并且其访问权限限于该命名空间。 Docker 引擎在 Linux 上使用如下的命名空间： pid 命名空间：进程隔离（PID：进程ID）。 net 命名空间：管理网络接口（NET：网络）。 ipc 命名空间：管理对IPC资源的访问（IPC：InterProcess Communication）。 mnt 命名空间：管理文件系统挂载点（MNT：挂载）。 uts 命名空间：隔离内核和版本标识符。 （UTS：Unix分时系统）。 控制组 Linux 上的 Docker Engine 也依赖于另一种称为控制组（cgroups）的技术。 cgroup 将应用程序限制为一组特定的资源。控制组允许 Docker 引擎将可用硬件资源共享给容器，并可选地强制实施限制和约束。例如，您可以限制可用于特定容器的内存。 联合文件系统 联合文件系统（UnionFS）是通过创建图层进行操作的文件系统，这使它们非常轻巧和快速。 Docker 引擎使用 UnionFS 为容器提供构建块。Docker 引擎可以使用多种 UnionFS 变体，包括 AUFS，btrfs，vfs 和 DeviceMapper。 容器格式 Docker 引擎将命名空间，控制组和 UnionFS 组合成一个名为容器格式的包装器。默认的容器格式是libcontainer。将来，Docker 可以通过与诸如 BSD Jails 或 Solaris Zones 等技术集成来支持其他容器格式。 资料 https://docs.docker.com/engine/docker-overview/]]></content>
  </entry>
  <entry>
    <title><![CDATA[计算机网络概述]]></title>
    <url>%2Fblog%2F2019%2F02%2F20%2Fcommunication%2Fnetwork-guide%2F</url>
    <content type="text"><![CDATA[计算机网络概述 计算机网络是指将地理位置不同的具有独立功能的多台计算机及其外部设备，通过通信线路连接起来，在网络操作系统，网络管理软件及网络通信协议的管理和协调下，实现资源共享和信息传递的计算机系统。 💡 指南 学习之前，先看一下入门三问： 一、什么是计算机网络？ 计算机网络是指将地理位置不同的具有独立功能的多台计算机及其外部设备，通过通信线路连接起来，在网络操作系统，网络管理软件及网络通信协议的管理和协调下，实现资源共享和信息传递的计算机系统。 ——摘自百度百科 二、为什么学习计算机网络？ 计算机网络是计算机科学的基础课程，也是计算机专业考研必考科目，可见其重要性。作为一名程序员，了解计算机网络，对于 Web 领域，通信领域的开发有莫大的帮助。 在浏览器中访问网页的原理是什么？Wifi 是如何工作的？防火墙是如何保障网络安全的？什么是安全证书？Cookie 和 Session 是什么东西？。。。 如果你接触过这些技术，如果你想了解这些技术的原理，那么你就有必要学习一下计算机网络了。 三、如何学习计算机网络？ 本人有 2 年通信领域开发经验，从事通信设备上的协议开发。就我个人的学习经验来看，学习计算机网络可以分为以下阶段： 基础阶段——一般性的了解网络协议分层及各层功能 了解计算机网络协议分层（OSI）有哪些层，分层的依据是什么（即每层的功能是什么） 了解每层的主要通信设备有哪些； 了解每层有哪些重要网络协议，这些协议有什么作用，基本原理是什么？ 了解每层的传输数据形式（如：报文、帧等） 进阶阶段——系统学习计算机网络知识，将各层主要协议功能串联起来 学习 TCP/IP 详解 卷 1、卷 2、卷 3（内容详实，但文字较为晦涩，不适合初学者） 专业阶段——根据业务领域，有针对性的学习 网络协议很多，而且专业性非常强。精通所有协议，几乎是不可能的，所以有必要根据自己的业务领域，有针对性的深入学习协议。如果你是做 web 开发，那么你很有必要认真学习一下 HTTP、DNS 协议；如果你是做路由器、交换机领域通信开发，那么你应该更深入学习一下 IP/TCP/UDP 协议。。。 如何深入学习协议，最好的学习方式，就是深入学习 RFC，并结合实际的协议报文去了解。 核心概念 计算机网络 - 计算机网络（computer network），通常也简称网络，是利用通信设备和线路将地理位置不同的、功能独立的多个计算机系统连接起来，以功能完善的网络软件实现网络的硬件、软件及资源共享和信息传递的系统。简单的说即连接两台或多台计算机进行通信的系统。 互联网 - 互联网（Internet），即网络的网络。 拓扑结构 计算机网络的拓扑结构可分为： 网型拓扑网型网（Mesh network） 环型拓扑环型网（Ring network） 星型拓扑星型网（Star network） 树状拓扑树型网（Tree network） 总线拓扑总线网（Bus network） 作用范围 广域网 WAN（Wide Area Network） 城域网 MAN（Metropolitan Area Network） 局域网 LAN（Local Area Network） 个人区域网 PAN（Personal Area Network） 性能指标 速率 - 速率的单位是 bit/s（比特每秒）。 带宽（bandwidth） - 带宽有以下两种不同的意义。 信号的带宽是指该信号所包含的各种不同频率成分所占据的频率范围。这种意义的带宽的单位是赫 （或千赫，兆赫，吉赫等）。 网络的带宽表示在单位时间内从网络中的某一点到另一点所能通过的最高数据率。这种意义的带宽的单位是 bit/s（比特每秒）。 吞吐量（throughput） - 吞吐量表示在单位时间内通过某个网络（或信道、接口）的数据量。例如，对于一个 100 Mbit/s 的以太网，其额定速率是 100 Mbit/s。 时延（delay） 总时延 = 排队时延 + 处理时延 + 传输时延 + 传播时延 体系结构 物理层 - 关键词：调制、解调、数字信号、模拟信号、通信媒介、信道复用 数据链路层 - 关键词：点对点信道、广播信道、PPP、CSMA/CD、局域网、以太网、MAC、适配器、集线器、网桥、交换机 网络层 - 关键词：IP、ICMP、ARP、路由 传输层 - 关键词：UDP、TCP、滑动窗口、拥塞控制、三次握手 应用层 - 关键词：HTTP、DNS、FTP、TELNET、DHCP 物理层（Physical Layer） - 物理层只接收和发送一串比特(bit)流，不考虑信息的意义和信息结构。 数据单元：比特流。 典型设备：光纤、同轴电缆、双绞线、中继器和集线器。 数据链路层（Data Link Layer） - 网络层针对的还是主机之间的数据传输服务，而主机之间可以有很多链路，链路层协议就是为同一链路的主机提供数据传输服务。数据链路层把网络层传下来的分组封装成帧。 主要协议：PPP、CSMA/CD 等。 数据单元：帧（frame）。 典型设备：二层交换机、网桥、网卡。 网络层（network layer） - 为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组或包进行传送。 主要协议：IP。 数据单元：IP 数据报（packet）。 典型设备：网关、路由器。 传输层（transport layer） - 为两台主机中进程间的通信提供通用的数据传输服务。 主要协议：TCP、UDP。 数据单元：报文段（segment）或用户数据报。 会话层（Session Layer） - 会话层不参与具体的传输，它提供包括访问验证和会话管理在内的建立和维护应用之间通信的机制。 表示层（Presentation Layer） - 表示层是为在应用过程之间传送的信息提供表示方法的服务，它关心的只是发出信息的语法与语义。表示层要完成某些特定的功能，主要有不同数据编码格式的转换，提供数据压缩、解压缩服务，对数据进行加密、解密。 应用层（application layer） - 通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程间通信和交互的规则。 主要协议：HTTP、DNS、SMTP、Telnet、FTP、SNMP 等。 数据单元：报文（message）。 📚 学习资源 书 谢希仁, 计算机网络 - 国内很多大学将其作为计算机网络课程的指定教材，通俗易懂，适合作为入门教材。 W·Richard Stevens, TCP/IP 详解 卷 1：协议 - TCP/IP 详解三部曲，适合作为进阶教材 W·Richard Stevens, TCP/IP 详解 卷 2：实现 W·Richard Stevens, TCP/IP 详解 卷 3：TCP 事务协议、HTTP、NNTP 和 UNIX 域协议 站点 https://www.rfc-editor.org/ - 在线查阅、下载 RFC 文档 🚪 传送门 | 回首頁 |]]></content>
      <categories>
        <category>communication</category>
      </categories>
      <tags>
        <tag>communication</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法]]></title>
    <url>%2Fblog%2F2019%2F02%2F19%2Falgorithm%2Falgorithm%2FREADME%2F</url>
    <content type="text"><![CDATA[算法 算法思想 穷举算法思想 穷举算法（ExhaustiveAttackmethod)是最简单的一种算法，其依赖于计算机的强大计算能力来穷尽每一种可能的情况，从而达到求解问题的目的。穷举算法效率并不高，但是适应于一些没有明显规律可循的场合。 基本算法思想 穷举算法的基本思想就是从所有可能的情况中搜索正确的答案，其执行步骤如下： 对于一种可能的情况，计算其结果。 判断结果是否满足要求，如果不满足则执行第（1 ) 步来搜索下一个可能的情况；如果满足要求，则表示寻找到一个正确的答案。 注意：在使用穷举算法时，需要明确问题的答案的范围，这样才可以在指定范围内搜索答案。指定范围之后，就可以使用循环语句和条件判断语句逐步验证候选答案的正确性，从而得到需要的正确答案。 经典例子 《孙子算经》【鸡兔同笼问题】 今有鸡兔同笼，上有三十五头，下有九十四足，问鸡兔各几何？ （在一个笼子里关着若干只鸡和若干只兔，从上面数共有 35 个头；从下面数共有 94 只脚。问笼中鸡和兔的数量各是多少？） int chickenRabbit(int head,int foot,int *c,int r)&#123;int i,j;int tag=0;//标志是否有解for(i=0;i&lt;=head;i++)&#123;//鸡的个数穷举 j=head-i;//兔子的个数 if(i2+j*4==foot)&#123;//判断是否满足条件 tag=1; *c=i; *r=j; &#125;&#125;return tag;&#125;int main()&#123; int c,r; if(chickenRabbit(35,94,&amp;c,&amp;r)==1)&#123;//如果有解输出 printf("chickens=%d,rabbits=%d\n",c,r); &#125;else&#123;//如果无解 printf("无解\n"); &#125; return 0;&#125; 递推算法思想 递推算法是非常常用的算法思想，在数学计算等场合有着广泛的应用。递推算法适合有明显公式规律的场合。 基本算法思想 递推算法是一种理性思维模式的代表，根据已有的数据和关系，逐步推导而得到结果。递推算法的执行过程如下： (1) 根据已知结果和关系，求解中间结果。 (2)判定是否达到要求，如果没有达到，则继续根据已知结果和关系求解中间结果。如果满足要求，则表示寻找到一个正确的答案。 【注意】递推算法需要用户知道答案和问题之间的逻辑关系。在许多数学问题中，都有明确的计算公式可以遵循，因此可以采用递推算法来实现。 经典例子 斐波那契《算盘书》【兔子产仔问题】 如果一对两个月大的兔子以后每一个月都可以生一对小兔子，而一对新生的兔子出生两个月后才可以生小兔子。也就是说，1 月份出生，3 月份才可产仔。那么假定一年内没有产生兔子死亡事件,那 么 1 年后共有多少对兔子呢？ 【规律分析】 第一个月：a(1)=1 对兔子； 第二个月：a(2)=1 对兔子； 第三个月：a(3)=1+1=2 对兔子； 第四个月：a(4)=1+2=3 对兔子； 第五个月：a(5)=2+3=5 对兔子； …… 第 n 个月：a(n)=a(n-1)+a(n-2)对兔子； int Fibonacci(int n)&#123;int tl,t2;if (n==1||n==2)//第1、2个月都只有1对兔子&#123;return 1;&#125;else&#123;//采用递归tl=Fibonacci(n-1);t2=Fibonacci(n-2);return tl+t2;//计算第n个月的兔子对数&#125;&#125;int main()&#123; int n=12; printf("%d个月后兔子对数：%d\n",n,Fibonacci(n)); return 0;&#125; 递归算法思想 递归算法是非常常用的算法思想。使用递归算法，往往可以简化代码编写，提高程序的可读性。但是，不合适的递归会导致程序的执行效率变低。 基本算法思想 递归算法就是在程序中不断反复调用自身来达到求解问题的方法。这里的重点是调用自身，这就要求待求解的问题能够分解为相同问题的一个子问题。这样 ，通过多次递归调用，便可以完成求解。 递归调用是一个函数在它的函数体内调用它自身的函数调用方式，这种函数也称为“递归函数”。在递归函数中，主调函数又是被调函数。执行递归函数将反复调用其自身，每调用一次就进入新的一层。 函数的递归调用分两种情况：直接递归和间接递归。 • 直接递归：即在函数中调用函数本身。 • 间接递归：即间接地调用一个函数，如出如 func_a 调 用 func_b, func_b 又调用 func_a。间接递归用得不多。 【注意】编写递归函数时，必须使用 if 语句强制函数在未执行递归调用前返回。否则，在调用函数后，它永远不会返回，这是一个很容易犯的错误。 经典例子 【阶乘】 n!=n(n-1)(n-2)……211long fact(int n)&#123;if(n&lt;=1)return 1;else return nfact(n-1);&#125;int main()&#123; int n=11; printf("%d的阶乘是%d\n",n,fact(n)); return 0;&#125; 分治算法思想 分治算法是一种化繁为简的算法思想。分治算法往往应用于计算步骤比较复杂的问题，通过将问题简化而逐步得到结果。 基本算法思想 分治算法的基本思想是将一个计算复杂的问题分为规模较小，计算简单的小问题求解，然后综合各个小问题，得到最终问题的答案。分治算法的执行过程如下： (1)对于一个规模为 N 的问题，若该问题可以容易地解决（比如说规模&gt;^较小），则直接解决,否则执行下面的步骤。 (2)将该问题分解为” 个规模较小的子问题，这些子问题互相独立，并且原问题形式相同。 (3)递归地解子问题。 (4)然后，将各子问题的解合并得到原问题的解。 【注意】使用分治算法需要待求解问题能够化简为若干个小规模的相同问题，通过逐步划分，达到一个易于求解的阶段而直接进行求解。然后，程序中可以使用递归算法来进行求解。 经典例子 【寻找假币问题】 一个袋子里有 30 个硬币，其中一枚是假币，并且假币和真币一模- 样，肉眼很难分辨，目前只知道假币比真币重量轻一点。请问如何区分出假币？ int falseCoin(int coin[],int low,int high)&#123;int i,sum1,sum2,sum3;int re;sum1=sum2=sum3=0;//若只有两个硬币if(low+1==high)&#123; if(coin[low]&lt;coin[high])&#123;//第一个硬币是假币 re=low+1; return re;&#125;else&#123;//第二个硬币是假币re=high+1;return re;&#125;&#125;//硬币个数大于2if((high-low+1)%2==0)&#123;//偶数个硬币 for(i=low;i&lt;=low+(high-low)/2;i++)&#123;//前半段重量 sum1=sum1+coin[i]; &#125; for(i=low+(high-low)/2+1;i&lt;=high;i++)&#123;//后半段重量 sum2=sum2+coin[i]; &#125; if(sum1&gt;sum2)&#123;//后半段轻，假币在后半段 re=falseCoin(coin,low+(high-low)/2+1,high); return re; &#125;else if(sum1&lt;sum2)&#123;//前半段轻，假币在前半段 re=falseCoin(coin,low,low+(high-low)/2); return re; &#125;&#125;else&#123;//奇数个硬币for(i=low;i&lt;=low+(high-low)/2-1;i++)&#123;//前半段重量 sum1=sum1+coin[i]; &#125; for(i=low+(high-low)/2+1;i&lt;=high;i++)&#123;//后半段重量 sum2=sum2+coin[i]; &#125; sum3=coin[low+(high-low)/2]; if(sum1&gt;sum2)&#123;//后半段轻，假币在后半段 re=falseCoin(coin,low+(high-low)/2+1,high); return re; &#125;else if(sum1&lt;sum2)&#123;//前半段轻，假币在前半段 re=falseCoin(coin,low,low+(high-low)/2-1); return re; &#125; if(sum1+sum3==sum2+sum3)&#123;//前半段+中间硬币和后半段+中间硬币重量相等，说明中间硬币是假币 re=low+(high-low)/2+1; return re; &#125;&#125;&#125;int main()&#123; int n=11,i; int a[n]; for(i=0;i&lt;n;i++)&#123; a[i]=2; &#125; a[7]=1;//设置第八个为假币 printf("假币是第%d个\n", falseCoin(a,0,n-1)); return 0;&#125; 概率算法思想 概率算法依照概率统计的思路来求解问题，往往不能得到问题的精确解，但却在数值计算领域得到了广泛的应用。因为很多数学问题，往往没有或者很难计算解析解，这时便需要通过数值计算来求解近似值。 基本算法思想 概率算法执行的基本过程如下： (1)将问题转化为相应的几何图形 S, S 的面积是容易计算的，问题的结果往往对应几何图形中某一部分 S1 的面积。 (2)然后，向几何图形中随机撒点。 (3)统计几何图形 S 和 S1 中的点数。根 据 S 的面积和 S1 面积的关系以及各图形中的点数来计算得到结果。 (4) 判断上述结果是否在需要的精度之内，如果未达到精度则执行步骤(2)。如果达到精度,则输出近似结果。 概率算法大致分为如下 4 种形式。 • 数值概率算法。 • 蒙 特 卡 罗 （MonteCarlo)算法。 • 拉 斯 维 加 斯 （Las Vegas)算法。 • 舍 伍 德 （Sherwood)算法。 经典例子 【蒙特卡罗 PI 概率算法问题】 在边长为 1 的正方形内，以 1 为半径画一个 1/4 圆。落入院内的概率为 PI/4？ 算法思想：在某面积范围内撒点足够多，落在固定区域的点的概率就会将近结果。 关键：均匀撒点、区域判断 double MontePI(int n)&#123;double PI;double x,y;int i,sum;sum=0;srand(time(NULL));for(i=1;i&lt;n;i++)&#123; x=(double)rand()/RAND_MAX;//在0-1之间产生一个随机数x y=(double)rand()/RAND_MAX;//在0-1之间产生一个随机数y if((xx+yy)&lt;=1)&#123;//判断点是否在圆内 sum++;//计数 &#125;&#125; PI=4.0*sum/n;//计算PI return PI;&#125;int main()&#123; int n=500000; double PI; printf("蒙特卡罗概率PI=%f\n", MontePI(n)); return 0;&#125;]]></content>
      <categories>
        <category>algorithm</category>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络协议之 DNS]]></title>
    <url>%2Fblog%2F2018%2F10%2F17%2Fcommunication%2Fdns%2F</url>
    <content type="text"><![CDATA[网络协议之 DNS 📓 本文已归档到：「blog」 域名系统（英文：Domain Name System，缩写：DNS）是互联网的一项服务。它作为将域名和 IP 地址相互映射的一个分布式数据库，能够使人更方便地访问互联网。DNS 使用 TCP 和 UDP 端口 53。当前，对于每一级域名长度的限制是 63 个字符，域名总长度则不能超过 253 个字符。 关键词：DNS, 域名解析 简介 什么是 DNS？ 什么是域名？ DNS 的分层 DNS 服务类型 记录类型 域名解析 Linux 上的域名相关命令 hostname nslookup 更多内容 简介 什么是 DNS？ DNS 是一个应用层协议。 域名系统 (DNS) 的作用是将人类可读的域名 (如，www.example.com) 转换为机器可读的 IP 地址 (如，192.0.2.44)。 什么是域名？ 域名是由一串用点分隔符 . 组成的互联网上某一台计算机或计算机组的名称，用于在数据传输时标识计算机的方位。域名可以说是一个 IP 地址的代称，目的是为了便于记忆后者。例如，wikipedia.org 是一个域名，和 IP 地址 208.80.152.2 相对应。人们可以直接访问 wikipedia.org 来代替 IP 地址，然后域名系统（DNS）就会将它转化成便于机器识别的 IP 地址。这样，人们只需要记忆 wikipedia.org 这一串带有特殊含义的字符，而不需要记忆没有含义的数字。 DNS 的分层 域名系统是分层次的。 在域名系统的层次结构中，各种域名都隶属于域名系统根域的下级。域名的第一级是顶级域，它包括通用顶级域，例如 .com、.net 和 .org；以及国家和地区顶级域，例如 .us、.cn 和 .tk。顶级域名下一层是二级域名，一级一级地往下。这些域名向人们提供注册服务，人们可以用它创建公开的互联网资源或运行网站。顶级域名的管理服务由对应的域名注册管理机构（域名注册局）负责，注册服务通常由域名注册商负责。 DNS 服务类型 授权型 DNS - 一种授权型 DNS 服务提供一种更新机制，供开发人员用于管理其公用 DNS 名称。然后，它响应 DNS 查询，将域名转换为 IP 地址，以便计算机可以相互通信。授权型 DNS 对域有最终授权且负责提供递归型 DNS 服务器对 IP 地址信息的响应。Amazon Route 53 是一种授权型 DNS 系统。 递归型 DNS - 客户端通常不会对授权型 DNS 服务直接进行查询。而是通常连接到称为解析程序的其他类型 DNS 服务，或递归型 DNS 服务。递归型 DNS 服务就像是旅馆的门童：尽管没有任何自身的 DNS 记录，但是可充当代表您获得 DNS 信息的中间程序。如果递归型 DNS 拥有已缓存或存储一段时间的 DNS 参考，那么它会通过提供源或 IP 信息来响应 DNS 查询。如果没有，则它会将查询传递到一个或多个授权型 DNS 服务器以查找信息。 记录类型 DNS 中，常见的资源记录类型有： NS 记录（域名服务） ─ 指定解析域名或子域名的 DNS 服务器。 MX 记录（邮件交换） ─ 指定接收信息的邮件服务器。 A 记录（地址） ─ 指定域名对应的 IPv4 地址记录。 AAAA 记录（地址） ─ 指定域名对应的 IPv6 地址记录。 CNAME（规范） ─ 一个域名映射到另一个域名或 CNAME 记录（ example.com 指向 www.example.com ）或映射到一个 A记录。 PTR 记录（反向记录） ─ PTR 记录用于定义与 IP 地址相关联的名称。 PTR 记录是 A 或 AAAA 记录的逆。 PTR 记录是唯一的，因为它们以 .arpa 根开始并被委派给 IP 地址的所有者。 详细可以参考：维基百科 - 域名服务器记录类型列表 域名解析 主机名到 IP 地址的映射有两种方式： 静态映射 - 在本机上配置域名和 IP 的映射，旨在本机上使用。Windows 和 Linux 的 hosts 文件中的内容就属于静态映射。 动态映射 - 建立一套域名解析系统（DNS），只在专门的 DNS 服务器上配置主机到 IP 地址的映射，网络上需要使用主机名通信的设备，首先需要到 DNS 服务器查询主机所对应的 IP 地址。 通过域名去查询域名服务器，得到 IP 地址的过程叫做域名解析。在解析域名时，一般先静态域名解析，再动态解析域名。可以将一些常用的域名放入静态域名解析表中，这样可以大大提高域名解析效率。 上图展示了一个动态域名解析的流程，步骤如下： 用户打开 Web 浏览器，在地址栏中输入 www.example.com，然后按 Enter 键。 www.example.com 的请求被路由到 DNS 解析程序，这一般由用户的 Internet 服务提供商 (ISP) 进行管理，例如有线 Internet 服务提供商、DSL 宽带提供商或公司网络。 ISP 的 DNS 解析程序将 www.example.com 的请求转发到 DNS 根名称服务器。 ISP 的 DNS 解析程序再次转发 www.example.com 的请求，这次转发到 .com 域的一个 TLD 名称服务器。.com 域的名称服务器使用与 example.com 域相关的四个 Amazon Route 53 名称服务器的名称来响应该请求。 ISP 的 DNS 解析程序选择一个 Amazon Route 53 名称服务器，并将 www.example.com 的请求转发到该名称服务器。 Amazon Route 53 名称服务器在 example.com 托管区域中查找 www.example.com 记录，获得相关值，例如，Web 服务器的 IP 地址 (192.0.2.44)，并将 IP 地址返回至 DNS 解析程序。 ISP 的 DNS 解析程序最终获得用户需要的 IP 地址。解析程序将此值返回至 Web 浏览器。DNS 解析程序还会将 example.com 的 IP 地址缓存 (存储) 您指定的时长，以便它能够在下次有人浏览 example.com 时更快地作出响应。有关更多信息，请参阅存活期 (TTL)。 Web 浏览器将 www.example.com 的请求发送到从 DNS 解析程序中获得的 IP 地址。这是您的内容所处位置，例如，在 Amazon EC2 实例中或配置为网站终端节点的 Amazon S3 存储桶中运行的 Web 服务器。 192.0.2.44 上的 Web 服务器或其他资源将 www.example.com 的 Web 页面返回到 Web 浏览器，且 Web 浏览器会显示该页面。 注意：只有配置了域名服务器，才能执行域名解析。 例如，在 Linux 中执行 vim /etc/resolv.conf 命令，在其中添加下面的内容来配置域名服务器地址： &gt; nameserver 218.2.135.1&gt; Linux 上的域名相关命令 hostname hostname 命令用于查看和设置系统的主机名称。环境变量 HOSTNAME 也保存了当前的主机名。在使用 hostname 命令设置主机名后，系统并不会永久保存新的主机名，重新启动机器之后还是原来的主机名。如果需要永久修改主机名，需要同时修改 /etc/hosts 和 /etc/sysconfig/network 的相关内容。 参考：http://man.linuxde.net/hostname 示例： $ hostnameAY1307311912260196fcZ nslookup nslookup 命令是常用域名查询工具，就是查 DNS 信息用的命令。 参考：http://man.linuxde.net/nslookup 示例： [root@localhost ~]# nslookup www.jsdig.comServer: 202.96.104.15Address: 202.96.104.15#53Non-authoritative answer:www.jsdig.com canonical name = host.1.jsdig.com.Name: host.1.jsdig.comAddress: 100.42.212.8 更多内容 维基百科 - 域名 维基百科 - 域名系统 维基百科 - 域名服务器记录类型列表 什么是 DNS？ RFC 1034 RFC 1035]]></content>
      <categories>
        <category>communication</category>
      </categories>
      <tags>
        <tag>communication</tag>
        <tag>network</tag>
        <tag>application</tag>
        <tag>protocol</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码坏味道之变革的障碍]]></title>
    <url>%2Fblog%2F2018%2F10%2F13%2Fdesign%2Frefactor%2F%E4%BB%A3%E7%A0%81%E5%9D%8F%E5%91%B3%E9%81%93%E4%B9%8B%E5%8F%98%E9%9D%A9%E7%9A%84%E9%9A%9C%E7%A2%8D%2F</url>
    <content type="text"><![CDATA[代码坏味道之变革的障碍 📓 本文已归档到：「blog」 翻译自：https://sourcemaking.com/refactoring/smells/change-preventers 变革的障碍(Change Preventers)这组坏味道意味着：当你需要改变一处代码时，却发现不得不改变其他的地方。这使得程序开发变得复杂、代价高昂。 发散式变化 平行继承体系 霰弹式修改 扩展阅读 参考资料 发散式变化 发散式变化(Divergent Change) 类似于 霰弹式修改(Shotgun Surgery) ，但实际上完全不同。发散式变化(Divergent Change) 是指一个类受多种变化的影响。霰弹式修改(Shotgun Surgery) 是指多种变化引发多个类相应的修改。 特征 你发现你想要修改一个函数，却必须要同时修改许多不相关的函数。例如，当你想要添加一个新的产品类型时，你需要同步修改对产品进行查找、显示、排序的函数。 问题原因 通常，这种发散式修改是由于编程结构不合理或者“复制-粘贴式编程”。 解决办法 运用 提炼类(Extract Class) 拆分类的行为。 收益 提高代码组织结构 减少重复代码 重构方法说明 提炼类(Extract Class) 问题 某个类做了不止一件事。 解决 建立一个新类，将相关的字段和函数从旧类搬移到新类。 平行继承体系 平行继承体系(Parallel Inheritance Hierarchies) 其实是 霰弹式修改(Shotgun Surgery) 的特殊情况。 特征 每当你为某个类添加一个子类，必须同时为另一个类相应添加一个子类。这种情况的典型特征是：某个继承体系的类名前缀或类名后缀完全相同。 问题原因 起初的继承体系很小，随着不断添加新类，继承体系越来越大，也越来越难修改。 解决方法 一般策略是：让一个继承体系的实例引用另一个继承体系的实例。如果再接再厉运用 搬移函数(Move Method) 和 搬移字段(Move Field)，就可以消除引用端的继承体系。 收益 更好的代码组织 减少重复代码 何时忽略 有时具有并行类层次结构只是一种为了避免程序体系结构更混乱的方法。如果你发现尝试消除平行继承体系导致代码更加丑陋，那么你应该回滚你的修改。 重构方法说明 搬移函数(Move Method) 问题 你的程序中，有个函数与其所驻类之外的另一个类进行更多交流：调用后者，或被后者调用。 解决 在该函数最常引用的类中建立一个有着类似行为的新函数。将旧函数变成一个单纯的委托函数，或是旧函数完全移除。 搬移字段(Move Field) 问题 在你的程序中，某个字段被其所驻类之外的另一个类更多地用到。 解决 在目标类新建一个字段，修改源字段的所有用户，令他们改用新字段。 霰弹式修改 霰弹式修改(Shotgun Surgery) 类似于 发散式变化(Divergent Change) ，但实际上完全不同。发散式变化(Divergent Change) 是指一个类受多种变化的影响。霰弹式修改(Shotgun Surgery) 是指多种变化引发多个类相应的修改。 特征 任何修改都需要在许多不同类上做小幅度修改。 问题原因 一个单一的职责被拆分成大量的类。 解决方法 运用搬移函数(Move Method) 和 搬移字段(Move Field) 来搬移不同类中相同的行为到一个独立类中。如果没有适合存放搬移函数或字段的类，就创建一个新类。 通常，可以运用 将类内联化(Inline Class) 将一些列相关行为放进同一个类。 收益 更好的代码组织 减少重复代码 更易维护 重构方法说明 搬移函数(Move Method) 问题 你的程序中，有个函数与其所驻类之外的另一个类进行更多交流：调用后者，或被后者调用。 解决 在该函数最常引用的类中建立一个有着类似行为的新函数。将旧函数变成一个单纯的委托函数，或是旧函数完全移除。 搬移字段(Move Field) 问题 在你的程序中，某个字段被其所驻类之外的另一个类更多地用到。 解决 在目标类新建一个字段，修改源字段的所有用户，令他们改用新字段。 将类内联化(Inline Class) 问题 某个类没有做太多事情。 解决 将这个类的所有特性搬移到另一个类中，然后移除原类。 扩展阅读 代码的坏味道和重构 代码坏味道之代码臃肿 代码坏味道之滥用面向对象 代码坏味道之变革的障碍 代码坏味道之非必要的 代码坏味道之耦合 参考资料 重构——改善既有代码的设计 - by Martin Fowler https://sourcemaking.com/refactoring]]></content>
      <categories>
        <category>design</category>
        <category>refactor</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>refactor</tag>
        <tag>code-smell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码坏味道之耦合]]></title>
    <url>%2Fblog%2F2018%2F10%2F13%2Fdesign%2Frefactor%2F%E4%BB%A3%E7%A0%81%E5%9D%8F%E5%91%B3%E9%81%93%E4%B9%8B%E8%80%A6%E5%90%88%2F</url>
    <content type="text"><![CDATA[代码坏味道之耦合 📓 本文已归档到：「blog」 翻译自：https://sourcemaking.com/refactoring/smells/couplers 耦合(Couplers)这组坏味道意味着：不同类之间过度耦合。 不完美的库类 中间人 依恋情结 狎昵关系 过度耦合的消息链 扩展阅读 参考资料 不完美的库类 不完美的库类(Incomplete Library Class) 当一个类库已经不能满足实际需要时，你就不得不改变这个库（如果这个库是只读的，那就没辙了）。 问题原因 许多编程技术都建立在库类的基础上。库类的作者没用未卜先知的能力，不能因此责怪他们。麻烦的是库往往构造的不够好，而且往往不可能让我们修改其中的类以满足我们的需要。 解决方法 如果你只想修改类库的一两个函数，可以运用 引入外加函数(Introduce Foreign Method)； 如果想要添加一大堆额外行为，就得运用 引入本地扩展(Introduce Local Extension) 。 收益 减少代码重复（你不用一言不合就自己动手实现一个库的全部功能，代价太高） 何时忽略 如果扩展库会带来额外的工作量。 重构方法说明 引入外加函数(Introduce Foreign Method) 问题 你需要为提供服务的类增加一个函数，但你无法修改这个类。 class Report &#123; //... void sendReport() &#123; Date nextDay = new Date(previousEnd.getYear(), previousEnd.getMonth(), previousEnd.getDate() + 1); //... &#125;&#125; 解决 在客户类中建立一个函数，并一个第一个参数形式传入一个服务类实例。 class Report &#123; //... void sendReport() &#123; Date newStart = nextDay(previousEnd); //... &#125; private static Date nextDay(Date arg) &#123; return new Date(arg.getYear(), arg.getMonth(), arg.getDate() + 1); &#125;&#125; 引入本地扩展(Introduce Local Extension) 问题 你需要为服务类提供一些额外函数，但你无法修改这个类。 解决 建立一个新类，使它包含这些额外函数，让这个扩展品成为源类的子类或包装类。 中间人 中间人(Middle Man) 如果一个类的作用仅仅是指向另一个类的委托，为什么要存在呢？ 问题原因 对象的基本特征之一就是封装：对外部世界隐藏其内部细节。封装往往伴随委托。但是人们可能过度运用委托。比如，你也许会看到一个类的大部分有用工作都委托给了其他类，类本身成了一个空壳，除了委托之外不做任何事情。 解决方法 应该运用 移除中间人(Remove Middle Man)，直接和真正负责的对象打交道。 收益 减少笨重的代码。 何时忽略 如果是以下情况，不要删除已创建的中间人： 添加中间人是为了避免类之间依赖关系。 一些设计模式有目的地创建中间人（例如代理模式和装饰器模式）。 重构方法说明 移除中间人(Remove Middle Man) 问题 某个类做了过多的简单委托动作。 解决 让客户直接调用委托类。 依恋情结 依恋情结(Feature Envy) 一个函数访问其它对象的数据比访问自己的数据更多。 问题原因 这种气味可能发生在字段移动到数据类之后。如果是这种情况，你可能想将数据类的操作移动到这个类中。 解决方法 As a basic rule, if things change at the same time, you should keep them in the same place. Usually data and functions that use this data are changed together (although exceptions are possible). 有一个基本原则：同时会发生改变的事情应该被放在同一个地方。通常，数据和使用这些数据的函数是一起改变的。 如果一个函数明显应该被移到另一个地方，可运用 搬移函数(Move Method) 。 如果仅仅是函数的部分代码访问另一个对象的数据，运用 提炼函数(Extract Method) 将这部分代码移到独立的函数中。 如果一个方法使用来自其他几个类的函数，首先确定哪个类包含大多数使用的数据。然后，将该方法与其他数据一起放在此类中。或者，使用 提炼函数(Extract Method) 将方法拆分为几个部分，可以放置在不同类中的不同位置。 收益 减少重复代码（如果数据处理的代码放在中心位置）。 更好的代码组织性（处理数据的函数靠近实际数据）。 何时忽略 有时，行为被有意地与保存数据的类分开。这通常的优点是能够动态地改变行为（见策略设计模式，访问者设计模式和其他模式）。 重构方法说明 搬移函数(Move Method) 问题 你的程序中，有个函数与其所驻类之外的另一个类进行更多交流：调用后者，或被后者调用。 解决 在该函数最常引用的类中建立一个有着类似行为的新函数。将旧函数变成一个单纯的委托函数，或是旧函数完全移除。 提炼函数(Extract Method) 问题 你有一段代码可以组织在一起。 void printOwing() &#123; printBanner(); //print details System.out.println("name: " + name); System.out.println("amount: " + getOutstanding());&#125; 解决 移动这段代码到一个新的函数中，使用函数的调用来替代老代码。 void printOwing() &#123; printBanner(); printDetails(getOutstanding());&#125;void printDetails(double outstanding) &#123; System.out.println("name: " + name); System.out.println("amount: " + outstanding);&#125; 狎昵关系 狎昵关系(Inappropriate Intimacy) 一个类大量使用另一个类的内部字段和方法。 问题原因 类和类之间应该尽量少的感知彼此（减少耦合）。这样的类更容易维护和复用。 解决方法 最简单的解决方法是运用 搬移函数(Move Method) 和 搬移字段(Move Field) 来让类之间斩断羁绊。 你也可以看看是否能运用 将双向关联改为单向关联(Change Bidirectional Association to Unidirectional) 让其中一个类对另一个说分手。 如果这两个类实在是情比金坚，难分难舍，可以运用 提炼类(Extract Class) 把二者共同点提炼到一个新类中，让它们产生爱的结晶。或者，可以尝试运用 隐藏委托关系(Hide Delegate) 让另一个类来为它们牵线搭桥。 继承往往造成类之间过分紧密，因为子类对超类的了解总是超过后者的主观愿望，如果你觉得该让这个子类自己闯荡，请运用 以委托取代继承(Replace Inheritance with Delegation) 来让超类和子类分家。 收益 提高代码组织性。 提高代码复用性。 重构方法说明 搬移函数(Move Method) 问题 你的程序中，有个函数与其所驻类之外的另一个类进行更多交流：调用后者，或被后者调用。 解决 在该函数最常引用的类中建立一个有着类似行为的新函数。将旧函数变成一个单纯的委托函数，或是旧函数完全移除。 搬移字段(Move Field) 问题 在你的程序中，某个字段被其所驻类之外的另一个类更多地用到。 解决 在目标类新建一个字段，修改源字段的所有用户，令他们改用新字段。 将双向关联改为单向关联(Change Bidirectional Association to Unidirectional) 问题 两个类之间有双向关联，但其中一个类如今不再需要另一个类的特性。 解决 去除不必要的关联。 提炼类(Extract Class) 问题 某个类做了不止一件事。 解决 建立一个新类，将相关的字段和函数从旧类搬移到新类。 隐藏委托关系(Hide Delegate) 问题 客户通过一个委托类来调用另一个对象。 解决 在服务类上建立客户所需的所有函数，用以隐藏委托关系。 以委托取代继承(Replace Inheritance with Delegation) 问题 某个子类只使用超类接口中的一部分，或是根本不需要继承而来的数据。 解决 在子类中新建一个字段用以保存超类；调整子类函数，令它改而委托超类；然后去掉两者之间的继承关系。 过度耦合的消息链 过度耦合的消息链(Message Chains) 消息链的形式类似于：obj.getA().getB().getC()。 问题原因 如果你看到用户向一个对象请求另一个对象，然后再向后者请求另一个对象，然后再请求另一个对象……这就是消息链。实际代码中你看到的可能是一长串 getThis()或一长串临时变量。采取这种方式，意味客户代码将与查找过程中的导航紧密耦合。一旦对象间关系发生任何变化，客户端就不得不做出相应的修改。 解决方法 可以运用 隐藏委托关系(Hide Delegate) 删除一个消息链。 有时更好的选择是：先观察消息链最终得到的对象是用来干什么的。看看能否以 提炼函数(Extract Method)把使用该对象的代码提炼到一个独立函数中，再运用 搬移函数(Move Method) 把这个函数推入消息链。 收益 能减少链中类之间的依赖。 能减少代码量。 何时忽略 过于侵略性的委托可能会使程序员难以理解功能是如何触发的。 重构方法说明 隐藏委托关系(Hide Delegate) 问题 客户通过一个委托类来调用另一个对象。 解决 在服务类上建立客户所需的所有函数，用以隐藏委托关系。 提炼函数(Extract Method) 问题 你有一段代码可以组织在一起。 void printOwing() &#123; printBanner(); //print details System.out.println("name: " + name); System.out.println("amount: " + getOutstanding());&#125; 解决 移动这段代码到一个新的函数中，使用函数的调用来替代老代码。 void printOwing() &#123; printBanner(); printDetails(getOutstanding());&#125;void printDetails(double outstanding) &#123; System.out.println("name: " + name); System.out.println("amount: " + outstanding);&#125; 搬移函数(Move Method) 问题 你的程序中，有个函数与其所驻类之外的另一个类进行更多交流：调用后者，或被后者调用。 解决 在该函数最常引用的类中建立一个有着类似行为的新函数。将旧函数变成一个单纯的委托函数，或是旧函数完全移除。 扩展阅读 代码的坏味道和重构 代码坏味道之代码臃肿 代码坏味道之滥用面向对象 代码坏味道之变革的障碍 代码坏味道之非必要的 代码坏味道之耦合 参考资料 重构——改善既有代码的设计 - by Martin Fowler https://sourcemaking.com/refactoring]]></content>
      <categories>
        <category>design</category>
        <category>refactor</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>refactor</tag>
        <tag>code-smell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码坏味道之滥用面向对象]]></title>
    <url>%2Fblog%2F2018%2F10%2F13%2Fdesign%2Frefactor%2F%E4%BB%A3%E7%A0%81%E5%9D%8F%E5%91%B3%E9%81%93%E4%B9%8B%E6%BB%A5%E7%94%A8%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[代码坏味道之滥用面向对象 📓 本文已归档到：「blog」 翻译自：https://sourcemaking.com/refactoring/smells/oo-abusers 滥用面向对象(Object-Orientation Abusers)这组坏味道意味着：代码部分或完全地违背了面向对象编程原则。 Switch 声明 临时字段 异曲同工的类 被拒绝的馈赠 扩展阅读 参考资料 Switch 声明 Switch 声明(Switch Statements) 你有一个复杂的 switch 语句或 if 序列语句。 问题原因 面向对象程序的一个最明显特征就是：少用 switch 和 case 语句。从本质上说，switch 语句的问题在于重复（if 序列也同样如此）。你常会发现 switch 语句散布于不同地点。如果要为它添加一个新的 case 子句，就必须找到所有 switch 语句并修改它们。面向对象中的多态概念可为此带来优雅的解决办法。 大多数时候，一看到 switch 语句，就应该考虑以多态来替换它。 解决方法 问题是多态该出现在哪？switch 语句常常根据类型码进行选择，你要的是“与该类型码相关的函数或类”，所以应该运用 提炼函数(Extract Method) 将 switch 语句提炼到一个独立函数中，再以 搬移函数(Move Method) 将它搬移到需要多态性的那个类里。 如果你的 switch 是基于类型码来识别分支，这时可以运用 以子类取代类型码(Replace Type Code with Subclass) 或 以状态/策略模式取代类型码(Replace Type Code with State/Strategy) 。 一旦完成这样的继承结构后，就可以运用 以多态取代条件表达式(Replace Conditional with Polymorphism) 了。 如果条件分支并不多并且它们使用不同参数调用相同的函数，多态就没必要了。在这种情况下，你可以运用 以明确函数取代参数(Replace Parameter with Explicit Methods) 。 如果你的选择条件之一是 null，可以运用 引入 Null 对象(Introduce Null Object) 。 收益 提升代码组织性。 何时忽略 如果一个 switch 操作只是执行简单的行为，就没有重构的必要了。 switch 常被工厂设计模式族（工厂方法模式(Factory Method)和抽象工厂模式(Abstract Factory)）所使用，这种情况下也没必要重构。 重构方法说明 提炼函数(Extract Method) 问题 你有一段代码可以组织在一起。 void printOwing() &#123; printBanner(); //print details System.out.println("name: " + name); System.out.println("amount: " + getOutstanding());&#125; 解决 移动这段代码到一个新的函数中，使用函数的调用来替代老代码。 void printOwing() &#123; printBanner(); printDetails(getOutstanding());&#125;void printDetails(double outstanding) &#123; System.out.println("name: " + name); System.out.println("amount: " + outstanding);&#125; 搬移函数(Move Method) 问题 你的程序中，有个函数与其所驻类之外的另一个类进行更多交流：调用后者，或被后者调用。 解决 在该函数最常引用的类中建立一个有着类似行为的新函数。将旧函数变成一个单纯的委托函数，或是旧函数完全移除。 以子类取代类型码(Replace Type Code with Subclass) 问题 你有一个不可变的类型码，它会影响类的行为。 解决 以子类取代这个类型码。 以状态/策略模式取代类型码(Replace Type Code with State/Strategy) 问题 你有一个类型码，它会影响类的行为，但你无法通过继承消除它。 解决 以状态对象取代类型码。 以多态取代条件表达式(Replace Conditional with Polymorphism) 问题 你手上有个条件表达式，它根据对象类型的不同而选择不同的行为。 class Bird &#123; //... double getSpeed() &#123; switch (type) &#123; case EUROPEAN: return getBaseSpeed(); case AFRICAN: return getBaseSpeed() - getLoadFactor() * numberOfCoconuts; case NORWEGIAN_BLUE: return (isNailed) ? 0 : getBaseSpeed(voltage); &#125; throw new RuntimeException("Should be unreachable"); &#125;&#125; 解决 将这个条件表达式的每个分支放进一个子类内的覆写函数中，然后将原始函数声明为抽象函数。 abstract class Bird &#123; //... abstract double getSpeed();&#125;class European extends Bird &#123; double getSpeed() &#123; return getBaseSpeed(); &#125;&#125;class African extends Bird &#123; double getSpeed() &#123; return getBaseSpeed() - getLoadFactor() * numberOfCoconuts; &#125;&#125;class NorwegianBlue extends Bird &#123; double getSpeed() &#123; return (isNailed) ? 0 : getBaseSpeed(voltage); &#125;&#125;// Somewhere in client codespeed = bird.getSpeed(); 以明确函数取代参数(Replace Parameter with Explicit Methods) 问题 你有一个函数，其中完全取决于参数值而采取不同的行为。 void setValue(String name, int value) &#123; if (name.equals("height")) &#123; height = value; return; &#125; if (name.equals("width")) &#123; width = value; return; &#125; Assert.shouldNeverReachHere();&#125; 解决 针对该参数的每一个可能值，建立一个独立函数。 void setHeight(int arg) &#123; height = arg;&#125;void setWidth(int arg) &#123; width = arg;&#125; 引入 Null 对象(Introduce Null Object) 问题 你需要再三检查某对象是否为 null。 if (customer == null) &#123; plan = BillingPlan.basic();&#125;else &#123; plan = customer.getPlan();&#125; 解决 将 null 值替换为 null 对象。 class NullCustomer extends Customer &#123; Plan getPlan() &#123; return new NullPlan(); &#125; // Some other NULL functionality.&#125;// Replace null values with Null-object.customer = (order.customer != null) ? order.customer : new NullCustomer();// Use Null-object as if it's normal subclass.plan = customer.getPlan(); 临时字段 临时字段(Temporary Field)的值只在特定环境下有意义，离开这个环境，它们就什么也不是了。 问题原因 有时你会看到这样的对象：其内某个实例变量仅为某种特定情况而设。这样的代码让人不易理解，因为你通常认为对象在所有时候都需要它的所有变量。在变量未被使用的情况下猜测当初设置目的，会让你发疯。 通常，临时字段是在某一算法需要大量输入时而创建。因此，为了避免函数有过多参数，程序员决定在类中创建这些数据的临时字段。这些临时字段仅仅在算法中使用，其他时候却毫无用处。 这种代码不好理解。你期望查看对象字段的数据，但是出于某种原因，它们总是为空。 解决方法 可以通过 提炼类(Extract Class) 将临时字段和操作它们的所有代码提炼到一个单独的类中。此外，你可以运用 以函数对象取代函数(Replace Method with Method Object) 来实现同样的目的。 引入 Null 对象(Introduce Null Object) 在“变量不合法”的情况下创建一个 null 对象，从而避免写出条件表达式。 收益 更好的代码清晰度和组织性。 重构方法说明 提炼类(Extract Class) 问题 某个类做了不止一件事。 解决 建立一个新类，将相关的字段和函数从旧类搬移到新类。 以函数对象取代函数(Replace Method with Method Object) 问题 你有一个过长函数，它的局部变量交织在一起，以致于你无法应用提炼函数(Extract Method) 。 class Order &#123; //... public double price() &#123; double primaryBasePrice; double secondaryBasePrice; double tertiaryBasePrice; // long computation. //... &#125;&#125; 解决 将函数移到一个独立的类中，使得局部变量成了这个类的字段。然后，你可以将函数分割成这个类中的多个函数。 class Order &#123; //... public double price() &#123; return new PriceCalculator(this).compute(); &#125;&#125;class PriceCalculator &#123; private double primaryBasePrice; private double secondaryBasePrice; private double tertiaryBasePrice; public PriceCalculator(Order order) &#123; // copy relevant information from order object. //... &#125; public double compute() &#123; // long computation. //... &#125;&#125; 引入 Null 对象(Introduce Null Object) 问题 你需要再三检查某对象是否为 null。 if (customer == null) &#123; plan = BillingPlan.basic();&#125;else &#123; plan = customer.getPlan();&#125; 解决 将 null 值替换为 null 对象。 class NullCustomer extends Customer &#123; Plan getPlan() &#123; return new NullPlan(); &#125; // Some other NULL functionality.&#125;// Replace null values with Null-object.customer = (order.customer != null) ? order.customer : new NullCustomer();// Use Null-object as if it's normal subclass.plan = customer.getPlan(); 异曲同工的类 异曲同工的类(Alternative Classes with Different Interfaces) 两个类中有着不同的函数，却在做着同一件事。 问题原因 这种情况往往是因为：创建这个类的程序员并不知道已经有实现这个功能的类存在了。 解决方法 如果两个函数做同一件事，却有着不同的签名，请运用 函数改名(Rename Method) 根据它们的用途重新命名。 运用 搬移函数(Move Method) 、 添加参数(Add Parameter) 和 令函数携带参数(Parameterize Method) 来使得方法的名称和实现一致。 如果两个类仅有部分功能是重复的，尝试运用 提炼超类(Extract Superclass) 。这种情况下，已存在的类就成了超类。 当最终选择并运用某种方法来重构后，也许你就能删除其中一个类了。 收益 消除了不必要的重复代码，为代码瘦身了。 代码更易读（不再需要猜测为什么要有两个功能相同的类）。 何时忽略 有时合并类是不可能的，或者是如此困难以至于没有意义。例如：两个功能相似的类存在于不同的 lib 库中。 重构方法说明 函数改名(Rename Method) 问题 函数的名称未能恰当的揭示函数的用途。 class Person &#123; public String getsnm();&#125; 解决 修改函数名。 class Person &#123; public String getSecondName();&#125; 搬移函数(Move Method) 问题 你的程序中，有个函数与其所驻类之外的另一个类进行更多交流：调用后者，或被后者调用。 解决 在该函数最常引用的类中建立一个有着类似行为的新函数。将旧函数变成一个单纯的委托函数，或是旧函数完全移除。 添加参数(Add Parameter) 问题 某个函数需要从调用端得到更多信息。 class Customer &#123; public Contact getContact();&#125; 解决 为此函数添加一个对象函数，让改对象带进函数所需信息。 class Customer &#123; public Contact getContact(Date date);&#125; 令函数携带参数(Parameterize Method) 问题 若干函数做了类似的工作，但在函数本体中却包含了不同的值。 **解决** 建立单一函数，以参数表达哪些不同的值。 提炼超类(Extract Superclass) 问题 两个类有相似特性。 解决 为这两个类建立一个超类，将相同特性移至超类。 被拒绝的馈赠 被拒绝的馈赠(Refused Bequest) 子类仅仅使用父类中的部分方法和属性。其他来自父类的馈赠成为了累赘。 问题原因 有些人仅仅是想重用超类中的部分代码而创建了子类。但实际上超类和子类完全不同。 解决方法 如果继承没有意义并且子类和父类之间确实没有共同点，可以运用 以委托取代继承(Replace Inheritance with Delegation) 消除继承。 如果继承是适当的，则去除子类中不需要的字段和方法。运用 提炼超类(Extract Superclass) 将所有超类中对于子类有用的字段和函数提取出来，置入一个新的超类中，然后让两个类都继承自它。 收益 提高代码的清晰度和组织性。 重构方法说明 以委托取代继承(Replace Inheritance with Delegation) 问题 某个子类只使用超类接口中的一部分，或是根本不需要继承而来的数据。 解决 在子类中新建一个字段用以保存超类； 调整子类函数，令它改而委托超类； 然后去掉两者之间的继承关系。 提炼超类(Extract Superclass) 问题 两个类有相似特性。 解决 为这两个类建立一个超类，将相同特性移至超类。 扩展阅读 代码的坏味道和重构 代码坏味道之代码臃肿 代码坏味道之滥用面向对象 代码坏味道之变革的障碍 代码坏味道之非必要的 代码坏味道之耦合 参考资料 重构——改善既有代码的设计 - by Martin Fowler https://sourcemaking.com/refactoring]]></content>
      <categories>
        <category>design</category>
        <category>refactor</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>refactor</tag>
        <tag>code-smell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式原理]]></title>
    <url>%2Fblog%2F2018%2F10%2F13%2Fdesign%2Farchitecture%2F%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[分布式原理 1. 分布式术语 1.1. 异常 1.2. 超时 1.3. 衡量指标 2. 数据分布 2.1. 哈希分布 2.2. 顺序分布 2.3. 负载均衡 3. 分布式理论 3.1. CAP 3.2. BASE 4. 分布式事务问题 4.1. 两阶段提交（2PC） 4.2. 补偿事务（TCC） 4.3. 本地消息表（异步确保） 4.4. MQ 事务消息 5. 共识性问题 5.1. Paxos 5.2. Raft 6. 分布式缓存问题 6.1. 缓存雪崩 6.2. 缓存穿透 6.3. 缓存预热 6.4. 缓存更新 6.5. 缓存降级 7. 参考资料 1. 分布式术语 1.1. 异常 服务器宕机 内存错误、服务器停电等都会导致服务器宕机，此时节点无法正常工作，称为不可用。 服务器宕机会导致节点失去所有内存信息，因此需要将内存信息保存到持久化介质上。 网络异常 有一种特殊的网络异常称为——网络分区 ，即集群的所有节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。 磁盘故障 磁盘故障是一种发生概率很高的异常。 使用冗余机制，将数据存储到多台服务器。 1.2. 超时 在分布式系统中，一个请求除了成功和失败两种状态，还存在着超时状态。 可以将服务器的操作设计为具有 幂等性 ，即执行多次的结果与执行一次的结果相同。如果使用这种方式，当出现超时的时候，可以不断地重新请求直到成功。 1.3. 衡量指标 性能 常见的性能指标有：吞吐量、响应时间。 其中，吞吐量指系统在某一段时间可以处理的请求总数，通常为每秒的读操作数或者写操作数；响应时间指从某个请求发出到接收到返回结果消耗的时间。 这两个指标往往是矛盾的，追求高吞吐的系统，往往很难做到低响应时间，解释如下： 在无并发的系统中，吞吐量为响应时间的倒数，例如响应时间为 10 ms，那么吞吐量为 100 req/s，因此高吞吐也就意味着低响应时间。 但是在并发的系统中，由于一个请求在调用 I/O 资源的时候，需要进行等待。服务器端一般使用的是异步等待方式，即等待的请求被阻塞之后不需要一直占用 CPU 资源。这种方式能大大提高 CPU 资源的利用率，例如上面的例子中，单个请求在无并发的系统中响应时间为 10 ms，如果在并发的系统中，那么吞吐量将大于 100 req/s。因此为了追求高吞吐量，通常会提高并发程度。但是并发程度的增加，会导致请求的平均响应时间也增加，因为请求不能马上被处理，需要和其它请求一起进行并发处理，响应时间自然就会增高。 可用性 可用性指系统在面对各种异常时可以提供正常服务的能力。可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的。 一致性 可以从两个角度理解一致性：从客户端的角度，读写操作是否满足某种特性；从服务器的角度，多个数据副本之间是否一致。 可扩展性 指系统通过扩展集群服务器规模来提高性能的能力。理想的分布式系统需要实现“线性可扩展”，即随着集群规模的增加，系统的整体性能也会线性增加。 2. 数据分布 分布式存储系统的数据分布在多个节点中，常用的数据分布方式有哈希分布和顺序分布。 数据库的水平切分（Sharding）也是一种分布式存储方法，下面的数据分布方法同样适用于 Sharding。 2.1. 哈希分布 哈希分布就是将数据计算哈希值之后，按照哈希值分配到不同的节点上。例如有 N 个节点，数据的主键为 key，则将该数据分配的节点序号为：hash(key)%N。 传统的哈希分布算法存在一个问题：当节点数量变化时，也就是 N 值变化，那么几乎所有的数据都需要重新分布，将导致大量的数据迁移。 一致性哈希 Distributed Hash Table（DHT）：对于哈希空间 [0, 2n-1]，将该哈希空间看成一个哈希环，将每个节点都配置到哈希环上。每个数据对象通过哈希取模得到哈希值之后，存放到哈希环中顺时针方向第一个大于等于该哈希值的节点上。 一致性哈希的优点是在增加或者删除节点时只会影响到哈希环中相邻的节点，例如下图中新增节点 X，只需要将数据对象 C 重新存放到节点 X 上即可，对于节点 A、B、D 都没有影响。 2.2. 顺序分布 哈希分布式破坏了数据的有序性，顺序分布则不会。 顺序分布的数据划分为多个连续的部分，按数据的 ID 或者时间分布到不同节点上。例如下图中，User 表的 ID 范围为 1 ~ 7000，使用顺序分布可以将其划分成多个子表，对应的主键范围为 1 ~ 1000，1001 ~ 2000，…，6001 ~ 7000。 顺序分布的优点是可以充分利用每个节点的空间，而哈希分布很难控制一个节点存储多少数据。 但是顺序分布需要使用一个映射表来存储数据到节点的映射，这个映射表通常使用单独的节点来存储。当数据量非常大时，映射表也随着变大，那么一个节点就可能无法存放下整个映射表。并且单个节点维护着整个映射表的开销很大，查找速度也会变慢。为了解决以上问题，引入了一个中间层，也就是 Meta 表，从而分担映射表的维护工作。 2.3. 负载均衡 衡量负载的因素很多，如 CPU、内存、磁盘等资源使用情况、读写请求数等。 分布式系统存储应当能够自动负载均衡，当某个节点的负载较高，将它的部分数据迁移到其它节点。 每个集群都有一个总控节点，其它节点为工作节点，由总控节点根据全局负载信息进行整体调度，工作节点定时发送心跳包（Heartbeat）将节点负载相关的信息发送给总控节点。 一个新上线的工作节点，由于其负载较低，如果不加控制，总控节点会将大量数据同时迁移到该节点上，造成该节点一段时间内无法工作。因此负载均衡操作需要平滑进行，新加入的节点需要较长的一段时间来达到比较均衡的状态。 3. 分布式理论 3.1. CAP 分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容忍性（P：Partition Tolerance），最多只能同时满足其中两项。 一致性 一致性指的是多个数据副本是否能保持一致的特性。 在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。 对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。 可用性 可用性指分布式系统在面对各种异常时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的。 在可用性条件下，系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。 分区容忍性 网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。 在分区容忍性条件下，分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。 权衡 在分布式系统中，分区容忍性必不可少，因为需要总是假设网络是不可靠的。因此，CAP 理论实际在是要在可用性和一致性之间做权衡。 可用性和一致性往往是冲突的，很难都使它们同时满足。在多个节点之间进行数据同步时， 为了保证一致性（CP），就需要让所有节点下线成为不可用的状态，等待同步完成； 为了保证可用性（AP），在同步过程中允许读取所有节点的数据，但是数据可能不一致。 3.2. BASE BASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）三个短语的缩写。 BASE 理论是对 CAP 中一致性和可用性权衡的结果，它的理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。 基本可用 指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性。 例如，电商在做促销时，为了保证购物系统的稳定性，部分消费者可能会被引导到一个降级的页面。 软状态 指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即允许系统不同节点的数据副本之间进行同步的过程存在延时。 最终一致性 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。 ACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。 在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。 4. 分布式事务问题 4.1. 两阶段提交（2PC） 两阶段提交（Two-phase Commit，2PC） 主要用于实现分布式事务，分布式事务指的是事务操作跨越多个节点，并且要求满足事务的 ACID 特性。 通过引入协调者（Coordinator）来调度参与者的行为，并最终决定这些参与者是否要真正执行事务。 运行过程 准备阶段 协调者询问参与者事务是否执行成功，参与者发回事务执行结果。 提交阶段 如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则，协调者发送通知让参与者回滚事务。 需要注意的是，在准备阶段，参与者执行了事务，但是还未提交。只有在提交阶段接收到协调者发来的通知后，才进行提交或者回滚。 问题 同步阻塞 所有事务参与者在等待其它参与者响应的时候都处于同步阻塞状态，无法进行其它操作。 单点问题 协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响，特别是在阶段二发生故障，所有参与者会一直等待状态，无法完成其它操作。 数据不一致 在阶段二，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。 太过保守 任意一个节点失败就会导致整个事务失败，没有完善的容错机制。 2PC 优缺点 优点：尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（其实也不能 100%保证强一致） 缺点：实现复杂，牺牲了可用性，对性能影响较大，不适合高并发高性能场景。 4.2. 补偿事务（TCC） 补偿事务（TCC）其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段： Try 阶段主要是对业务系统做检测及资源预留。 Confirm 阶段主要是对业务系统做确认提交，Try 阶段执行成功并开始执行 Confirm 阶段时，默认 Confirm 阶段是不会出错的。即：只要 Try 成功，Confirm 一定成功。 Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。 举个例子，假设 Bob 要向 Smith 转账，思路大概是： 首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。 在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。 如果第 2 步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。 TCC 优缺点 优点：跟 2PC 比起来，实现以及流程相对简单了一些，但数据的一致性比 2PC 也要差一些。 缺点：缺点还是比较明显的，在 2,3 步中都有可能失败。TCC 属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用 TCC 不太好定义及处理。 4.3. 本地消息表（异步确保） 本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性。 在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。 之后将本地消息表中的消息转发到 Kafka 等消息队列（MQ）中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。 在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。 这种方案遵循 BASE 理论，采用的是最终一致性。 本地消息表利用了本地事务来实现分布式事务，并且使用了消息队列来保证最终一致性。 本地消息表优缺点 优点：一种非常经典的实现，避免了分布式事务，实现了最终一致性。 缺点：消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。 4.4. MQ 事务消息 有一些第三方的 MQ 是支持事务消息的，比如 RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交。但是市面上一些主流的 MQ 都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持。 以阿里的 RocketMQ 中间件为例，其思路大致为： Prepared 消息，会拿到消息的地址。 执行本地事务。 通过第一阶段拿到的地址去访问消息，并修改状态。 也就是说在业务方法内要想消息队列提交两次请求，一次发送消息和一次确认消息。如果确认消息发送失败了 RocketMQ 会定期扫描消息集群中的事务消息，这时候发现了 Prepared 消息，它会向消息发送者确认，所以生产方需要实现一个 check 接口，RocketMQ 会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。 MQ 事务消息优缺点 优点：实现了最终一致性，不需要依赖本地数据库事务。 缺点：实现难度大，主流 MQ 不支持。 5. 共识性问题 5.1. Paxos 用于达成共识性问题，即对多个节点产生的值，该算法能保证只选出唯一一个值。 主要有三类节点： 提议者（Proposer）：提议一个值； 接受者（Acceptor）：对每个提议进行投票； 告知者（Learner）：被告知投票的结果，不参与投票过程。 算法需要满足 safety 和 liveness 两方面的约束要求（实际上这两个基础属性是大部分分布式算法都该考虑的）： safety：保证决议结果是对的，无歧义的，不会出现错误情况。 决议（value）只有在被 proposers 提出的 proposal 才能被最终批准； 在一次执行实例中，只批准（chosen）一个最终决议，意味着多数接受（accept）的结果能成为决议； liveness：保证决议过程能在有限时间内完成。 决议总会产生，并且 learners 能获得被批准（chosen）的决议。 基本过程包括 proposer 提出提案，先争取大多数 acceptor 的支持，超过一半支持时，则发送结案结果给所有人进行确认。一个潜在的问题是 proposer 在此过程中出现故障，可以通过超时机制来解决。极为凑巧的情况下，每次新的一轮提案的 proposer 都恰好故障，系统则永远无法达成一致（概率很小）。 Paxos 能保证在超过 1/21/21/2 的正常节点存在时，系统能达成共识。 单个提案者+多接收者 如果系统中限定只有某个特定节点是提案者，那么一致性肯定能达成（只有一个方案，要么达成，要么失败）。提案者只要收到了来自多数接收者的投票，即可认为通过，因为系统中不存在其他的提案。 但一旦提案者故障，则系统无法工作。 多个提案者+单个接收者 限定某个节点作为接收者。这种情况下，共识也很容易达成，接收者收到多个提案，选第一个提案作为决议，拒绝掉后续的提案即可。 缺陷也是容易发生单点故障，包括接收者故障或首个提案者节点故障。 以上两种情形其实类似主从模式，虽然不那么可靠，但因为原理简单而被广泛采用。 当提案者和接收者都推广到多个的情形，会出现一些挑战。 多个提案者+多个接收者 既然限定单提案者或单接收者都会出现故障，那么就得允许出现多个提案者和多个接收者。问题一下子变得复杂了。 一种情况是同一时间片段（如一个提案周期）内只有一个提案者，这时可以退化到单提案者的情形。需要设计一种机制来保障提案者的正确产生，例如按照时间、序列、或者大家猜拳（出一个数字来比较）之类。考虑到分布式系统要处理的工作量很大，这个过程要尽量高效，满足这一条件的机制非常难设计。 另一种情况是允许同一时间片段内可以出现多个提案者。那同一个节点可能收到多份提案，怎么对他们进行区分呢？这个时候采用只接受第一个提案而拒绝后续提案的方法也不适用。很自然的，提案需要带上不同的序号。节点需要根据提案序号来判断接受哪个。比如接受其中序号较大（往往意味着是接受新提出的，因为旧提案者故障概率更大）的提案。 如何为提案分配序号呢？一种可能方案是每个节点的提案数字区间彼此隔离开，互相不冲突。为了满足递增的需求可以配合用时间戳作为前缀字段。 此外，提案者即便收到了多数接收者的投票，也不敢说就一定通过。因为在此过程中系统可能还有其它的提案。 5.2. Raft Raft 算法是 Paxos 算法的一种简化实现。 包括三种角色：leader、candidate 和 follower，其基本过程为： Leader 选举 - 每个 candidate 随机经过一定时间都会提出选举方案，最近阶段中得票最多者被选为 leader； 同步 log - leader 会找到系统中 log 最新的记录，并强制所有的 follower 来刷新到这个记录； 注：此处 log 并非是指日志消息，而是各种事件的发生记录。 单个 Candidate 的竞选 有三种节点：Follower、Candidate 和 Leader。Leader 会周期性的发送心跳包给 Follower。每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms~300ms，如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段。 下图表示一个分布式系统的最初阶段，此时只有 Follower，没有 Leader。Follower A 等待一个随机的竞选超时时间之后，没收到 Leader 发来的心跳包，因此进入竞选阶段。 此时 A 发送投票请求给其它所有节点。 其它节点会对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader。 之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。 多个 Candidate 竞选 如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票，例如下图中 Candidate B 和 Candidate D 都获得两票，因此需要重新开始投票。 当重新开始投票时，由于每个节点设置的随机竞选超时时间不同，因此能下一次再次出现多个 Candidate 并获得同样票数的概率很低。 同步日志 来自客户端的修改都会被传入 Leader。注意该修改还未被提交，只是写入日志中。 Leader 会把修改复制到所有 Follower。 Leader 会等待大多数的 Follower 也进行了修改，然后才将修改提交。 此时 Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致。 6. 分布式缓存问题 6.1. 缓存雪崩 缓存雪崩是指：在高并发场景下，由于原有缓存失效，新缓存未到期间(例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期)，所有原本应该访问缓存的请求都去查询数据库了，而对数据库 CPU 和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。 解决方案： 用加锁或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。 还有一个简单的方案，就是将缓存失效时间分散开，不要所有缓存时间长度都设置成 5 分钟或者 10 分钟；比如我们可以在原有的失效时间基础上增加一个随机值，比如 1-5 分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 缓存失效时产生的雪崩效应，将所有请求全部放在数据库上，这样很容易就达到数据库的瓶颈，导致服务无法正常提供。尽量避免这种场景的发生。 6.2. 缓存穿透 缓存穿透是指：用户查询的数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库再查询一遍，然后返回空（相当于进行了两次无用的查询）。这样请求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。 当在流量较大时，出现这样的情况，一直请求 DB，很容易导致服务挂掉。 解决方案： 在封装的缓存 SET 和 GET 部分增加个步骤，如果查询一个 KEY 不存在，就以这个 KEY 为前缀设定一个标识 KEY；以后再查询该 KEY 的时候，先查询标识 KEY，如果标识 KEY 存在，就返回一个协定好的非 false 或者 NULL 值，然后 APP 做相应的处理，这样缓存层就不会被穿透。当然这个验证 KEY 的失效时间不能太长。 如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，一般只有几分钟。 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力。 6.3. 缓存预热 缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决方案： 直接写个缓存刷新页面，上线时手工操作下； 数据量不大，可以在项目启动的时候自动进行加载； 定时刷新缓存； 6.4. 缓存更新 除了缓存服务器自带的缓存失效策略之外（Redis 默认的有 6 中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种： 定时去清理过期的缓存； 当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。 两者各有优劣，第一种的缺点是维护大量缓存的 key 是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。 6.5. 缓存降级 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。 7. 参考资料 杨传辉. 大规模分布式存储系统: 原理解析与架构实战[M]. 机械工业出版社, 2013. 区块链技术指南 NEAT ALGORITHMS - PAXOS Raft: Understandable Distributed Consensus Paxos By Example 聊聊分布式事务，再说说解决方案]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
        <tag>distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码的坏味道和重构]]></title>
    <url>%2Fblog%2F2018%2F10%2F13%2Fdesign%2Frefactor%2FREADME%2F</url>
    <content type="text"><![CDATA[代码的坏味道和重构 📓 本文已归档到：「blog」 症与药 对代码的坏味道的思考 重构的原则 代码的坏味道 代码坏味道之代码臃肿 代码坏味道之滥用面向对象 代码坏味道之变革的障碍 代码坏味道之非必要的 代码坏味道之耦合 扩展阅读 参考资料 第一次读《重构:改善既有代码的设计》时，我曾整理过一个简单的笔记。最近，因为参与一个重构项目，再一次温习了《重构:改善既有代码的设计》。过程中，萌发了认真总结、整理重构方法的冲动，于是有了这系列文字。 代码的坏味道还有几篇没有完稿，后面我会陆续补充。。。 症与药 对代码的坏味道的思考 “有病要早治，不要放弃治疗”。多么朴素的道理 ，人人都懂。 病，就是不健康。 人有病，可以通过打针、吃药、做手术来进行治疗。 如果把代码的坏味道（代码质量问题）比作病症，那么重构就是治疗代码的坏味道的药。 个人认为，在重构这件事上，也可以应用治病的道理： 防病于未然。 —— 春秋战国时期的一代名医扁鹊，曾经有个很著名的医学主张：防病于未然。 我觉得这个道理应用于软件代码的重构亦然。编程前要有合理的设计、编程时要有良好的编程风格，尽量减少问题。从这个层面上说，了解代码的坏味道，不仅仅是为了发现问题、解决问题。更重要的作用是：指导我们在编程过程中有意识的去规避这些问题。 小病不医，易得大病。 —— 刘备说过：“勿以善小而不为，勿以恶小而为之”。发现问题就及时修改，代码质量自然容易进入良性循环；反之，亦然。要重视积累的力量，别总以为代码出现点小问题，那都不是事儿。 对症下药。 —— 程序出现了问题，要分析出问题的根本，有针对性的制定合理的重构方案。大家都知道吃错药的后果，同样的，瞎改还不如不改。 忌猛药 —— 医病用猛药容易产生副作用。换一句俗语：步子大了容易扯着蛋。重构如果大刀阔斧的干，那你就要有随时可能扑街的心理准备。推倒重来不是重构，而是重写。重构应该是循序渐进，步步为营的过程。当你发现重写代码比重构代码更简单，往往说明你早就该重构了。 重构的原则 前面把代码质量问题比作病症，而把重构比作药。这里，我们再进一步讨论一下重构的原则。 何谓重构(What) 重构（Refactoring） 的常见定义是：不改变软件系统外部行为的前提下，改善它的内部结构。 个人觉得这个定义有点生涩。不妨理解为：重构是给代码治病的行为。而代码有病是指代码的质量（可靠性、安全性、可复用性、可维护性）和性能有问题。 重构的目的是为了提高代码的质量和性能。 注：功能不全或者不正确，那是残疾代码。就像治病治不了残疾，重构也解决不了功能问题。 为何重构(Why) 翻翻书，上网搜一下，谈到重构的理由大体相同： 重构改进软件设计 重构使软件更容易理解 重构帮助找到 bug 重构提高编程速度 总之就是，重构可以提高代码质量。 何时重构(When) 关于何时重构，我先引用一下 重构并非难在如何做，而是难在何时开始做 一文的观点。 对于一个高速发展的公司来说，停止业务开发，专门来做重构项目，从来就不是一个可接受的选项，“边开飞机边换引擎”才是这种公司想要的。 我们不妨来衡量一下重构的成本和收益。 重构的成本 重构是有成本的，费时费力（时间、人力）不说，还有可能会使本来正常运行的程序出错。所以，很多人都抱着“不求有功，但求无过”的心理得过且过。 还有一种成本：重构使用较新且较为复杂的技术，学习曲线不平滑，团队成员技术切换困难，短期内开发效率可能不升反降。 但是，如果一直放任代码腐朽下去，技术债务会越来越沉重。当代码最终快要跑不动时，架构师们往往还是不得不使用激进的手段来治疗代码的顽疾。但是，这个过程通常都是非常痛苦的，而且有着很高的失败风险。 重构的收益 重构的收益是提高代码的质量和性能，并提高未来的开发效率。但是，应当看到，重构往往并不能在短期内带来实际的效益，或者很难直观看出效益。而对于一个企业来说，没有什么比效益更重要。换句话说，没有实际效益的事，通常也没有价值。很多领导，尤其是非技术方向的领导，并不关心你应用了什么新技术，让代码变得多么优雅等等。 重构的合适时机 从以上来看，重构实在是个吃力不讨好的事情。 于是，很多人屈服于万恶的 KPI 和要命的 deadline，一边吐槽着以前的代码是垃圾，一边自己也在造垃圾。 但是，**重构本应该是个渐进式的过程，不是只有伤筋动骨的改造才叫重构。**如果非要等到代码已经烂到病入膏肓，再使用激进方式来重构，那必然是困难重重，风险极高。 《重构》书中提到的重构时机应该在添加功能、修复功能、审查代码时，不建议专门抽出时间专门做重构项目。 我认为，其思想就是指：重构应该是在开发过程中实时的、渐进的演化过程。 重构的不恰当时机 但是，这里我也要强调一下：不是所有软件开发过程都一定要重构。 较能凸显重构价值的场景是：代码规模较大、生命周期还较长、承担了较多责任、有一个较大（且较不稳定，人员流动频繁）团队在其上工作的单一代码库。 与之相反，有一些场景的重构价值就很小： 代码库生命周期快要走到尾声，开发逐渐减少，以维护为主。 代码库当前版本马上要发布了，这时重构无疑是给自己找麻烦。 重构代价过于沉重：重构后功能的正确性、稳定性难以保障；技术过于超前，团队成员技术迁移难度太大。 如何重构(How) 重构行为在我看来，也是可以分层级的。由高到低，越高层级难度越大： 服务、数据库 现代软件往往业务复杂、庞大。使用微服务、数据迁移来拆分业务，降低业务复杂度成为了主流。但是，这些技术的测试、部署复杂，技术难度很高。 组件、模块、框架 组件、模块、框架的重构，主要是针对代码的设计问题。解决的是代码的整体结构问题。需要对框架、设计模式、分布式、并发等等有足够的了解。 类、接口、函数、字段 《重构》一书提到了**“代码的坏味道”**以及相关的重构方法。这些都是对类、接口、函数、字段级别代码的重构手段。由于这一级别的重构方法较为简单，所以可操作性较强。具体细节可以阅读《代码的坏味道》篇章。 前两种层级的重构已经涉及到架构层面，影响较大，难度较高，如果功力不够不要轻易变动。由于这两个层级涉及领域较广，这里不做论述。 此处为分割线。下面是代码的坏味道系列。。。 代码的坏味道 《重构:改善既有代码的设计》中介绍了 22 种代码的坏味道以及重构手法。这些坏味道可以进一步归类。我总觉得将事物分类有助于理解和记忆。所以本系列将坏味道按照特性分类，然后逐一讲解。 代码坏味道之代码臃肿 代码臃肿(Bloated)这组坏味道意味着：代码中的类、函数、字段没有经过合理的组织，只是简单的堆砌起来。这一类型的问题通常在代码的初期并不明显，但是随着代码规模的增长而逐渐积累（特别是当没有人努力去根除它们时）。 过长函数 过大的类 基本类型偏执 过长参数列 数据泥团 代码坏味道之滥用面向对象 滥用面向对象(Object-Orientation Abusers)这组坏味道意味着：代码部分或完全地违背了面向对象编程原则。 switch 声明 临时字段 被拒绝的馈赠 异曲同工的类 代码坏味道之变革的障碍 变革的障碍(Change Preventers)这组坏味道意味着：当你需要改变一处代码时，却发现不得不改变其他的地方。这使得程序开发变得复杂、代价高昂。 发散式变化 霰弹式修改 平行继承体系 代码坏味道之非必要的 非必要的(Dispensables)这组坏味道意味着：这样的代码可有可无，它的存在反而影响整体代码的整洁和可读性。 过多的注释 重复代码 冗余类 纯稚的数据类 夸夸其谈未来性 代码坏味道之耦合 耦合(Couplers)这组坏味道意味着：不同类之间过度耦合。 依恋情结 狎昵关系 过度耦合的消息链 中间人 不完美的库类 扩展阅读 代码的坏味道和重构 代码坏味道之代码臃肿 代码坏味道之滥用面向对象 代码坏味道之变革的障碍 代码坏味道之非必要的 代码坏味道之耦合 参考资料 重构——改善既有代码的设计 - by Martin Fowler https://sourcemaking.com/refactoring]]></content>
      <categories>
        <category>design</category>
        <category>refactor</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>refactor</tag>
        <tag>code-smell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡]]></title>
    <url>%2Fblog%2F2018%2F10%2F13%2Fdesign%2Farchitecture%2F%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[负载均衡 1. 负载均衡原理 2. 负载均衡分类 2.1. DNS 负载均衡 2.2. IP 负载均衡 2.3. 链路层负载均衡 2.4. 混合型负载均衡 3. 负载均衡算法 3.1. 轮询 3.2. 随机 3.3. 最少连接 3.4. Hash（源地址散列） 3.5. 加权 4. 硬件负载均衡 5. Ngnix 负载均衡 5.1. Ngnix 特点 5.2. Ngnix 功能 5.3. Ngnix 架构 5.4. Ngnix 均衡策略 5.5. Ngnix 场景 6. LVS 负载均衡 6.1. LVS 功能 6.2. LVS 架构 6.3. LVS 均衡策略 6.4. LVS 场景 7. HaProxy 负载均衡 7.1. HaProxy 特点 7.2. HaProxy 均衡策略 8. 资料 1. 负载均衡原理 系统的扩展可分为纵向（垂直）扩展和横向（水平）扩展。纵向扩展，是从单机的角度通过增加硬件处理能力，比如 CPU 处理能力，内存容量，磁盘等方面，实现服务器处理能力的提升，不能满足大型分布式系统（网站），大流量，高并发，海量数据的问题。因此需要采用横向扩展的方式，通过添加机器来满足大型网站服务的处理能力。比如：一台机器不能满足，则增加两台或者多台机器，共同承担访问压力。这就是典型的集群和负载均衡架构：如下图： 应用集群：将同一应用部署到多台机器上，组成处理集群，接收负载均衡设备分发的请求，进行处理，并返回相应数据。 负载均衡设备：将用户访问的请求，根据负载均衡算法，分发到集群中的一台处理服务器。（一种把网络请求分散到一个服务器集群中的可用服务器上去的设备） 负载均衡的作用（解决的问题）： 解决并发压力，提高应用处理性能（增加吞吐量，加强网络处理能力）； 提供故障转移，实现高可用； 通过添加或减少服务器数量，提供网站伸缩性（扩展性）； 安全防护；（负载均衡设备上做一些过滤，黑白名单等处理） 2. 负载均衡分类 根据实现技术不同，可分为 DNS 负载均衡，HTTP 负载均衡，IP 负载均衡，链路层负载均衡等。 2.1. DNS 负载均衡 最早的负载均衡技术，利用域名解析实现负载均衡，在 DNS 服务器，配置多个 A 记录，这些 A 记录对应的服务器构成集群。大型网站总是部分使用 DNS 解析，作为第一级负载均衡。如下图： 优点 使用简单：负载均衡工作，交给 DNS 服务器处理，省掉了负载均衡服务器维护的麻烦 提高性能：可以支持基于地址的域名解析，解析成距离用户最近的服务器地址，可以加快访问速度，改善性能； 缺点 可用性差：DNS 解析是多级解析，新增/修改 DNS 后，解析时间较长；解析过程中，用户访问网站将失败； 扩展性低：DNS 负载均衡的控制权在域名商那里，无法对其做更多的改善和扩展； 维护性差：也不能反映服务器的当前运行状态；支持的算法少；不能区分服务器的差异（不能根据系统与服务的状态来判断负载） 实践建议 将 DNS 作为第一级负载均衡，A 记录对应着内部负载均衡的 IP 地址，通过内部负载均衡将请求分发到真实的 Web 服务器上。一般用于互联网公司，复杂的业务系统不合适使用。如下图： 2.2. IP 负载均衡 在网络层通过修改请求目标地址进行负载均衡。 用户请求数据包，到达负载均衡服务器后，负载均衡服务器在操作系统内核进程获取网络数据包，根据负载均衡算法得到一台真实服务器地址，然后将请求目的地址修改为，获得的真实 ip 地址，不需要经过用户进程处理。 真实服务器处理完成后，响应数据包回到负载均衡服务器，负载均衡服务器，再将数据包源地址修改为自身的 ip 地址，发送给用户浏览器。如下图： IP 负载均衡，真实物理服务器返回给负载均衡服务器，存在两种方式： 负载均衡服务器在修改目的 ip 地址的同时修改源地址。将数据包源地址设为自身盘，即源地址转换（snat）。 将负载均衡服务器同时作为真实物理服务器集群的网关服务器。 优点：在内核进程完成数据分发，比在应用层分发性能更好； 缺点：所有请求响应都需要经过负载均衡服务器，集群最大吞吐量受限于负载均衡服务器网卡带宽； 2.3. 链路层负载均衡 在通信协议的数据链路层修改 mac 地址，进行负载均衡。 数据分发时，不修改 ip 地址，指修改目标 mac 地址，配置真实物理服务器集群所有机器虚拟 ip 和负载均衡服务器 ip 地址一致，达到不修改数据包的源地址和目标地址，进行数据分发的目的。 实际处理服务器 ip 和数据请求目的 ip 一致，不需要经过负载均衡服务器进行地址转换，可将响应数据包直接返回给用户浏览器，避免负载均衡服务器网卡带宽成为瓶颈。也称为直接路由模式（DR 模式）。如下图： 优点：性能好； 缺点：配置复杂； 实践建议：DR 模式是目前使用最广泛的一种负载均衡方式。 2.4. 混合型负载均衡 由于多个服务器群内硬件设备、各自的规模、提供的服务等的差异，可以考虑给每个服务器群采用最合适的负载均衡方式，然后又在这多个服务器群间再一次负载均衡或群集起来以一个整体向外界提供服务（即把这多个服务器群当做一个新的服务器群），从而达到最佳的性能。将这种方式称之为混合型负载均衡。 此种方式有时也用于单台均衡设备的性能不能满足大量连接请求的情况下。是目前大型互联网公司，普遍使用的方式。 方式一，如下图： 以上模式适合有动静分离的场景，反向代理服务器（集群）可以起到缓存和动态请求分发的作用，当时静态资源缓存在代理服务器时，则直接返回到浏览器。如果动态页面则请求后面的应用负载均衡（应用集群）。 方式二，如下图： 以上模式，适合动态请求场景。 因混合模式，可以根据具体场景，灵活搭配各种方式，以上两种方式仅供参考。 3. 负载均衡算法 常用的负载均衡算法有：轮询、随机、最少连接、源地址散列、加权等方式。 3.1. 轮询 将所有请求，依次分发到每台服务器上，适合服务器硬件同相同的场景。 优点：服务器请求数目相同； 缺点：服务器压力不一样，不适合服务器配置不同的情况； 3.2. 随机 请求随机分配到各个服务器。 优点：使用简单； 缺点：不适合机器配置不同的场景； 3.3. 最少连接 将请求分配到连接数最少的服务器（目前处理请求最少的服务器）。 优点：根据服务器当前的请求处理情况，动态分配； 缺点：算法实现相对复杂，需要监控服务器请求连接数； 3.4. Hash（源地址散列） 根据 IP 地址进行 Hash 计算，得到 IP 地址。 优点：将来自同一 IP 地址的请求，同一会话期内，转发到相同的服务器；实现会话粘滞。 缺点：目标服务器宕机后，会话会丢失； 3.5. 加权 在轮询，随机，最少链接，Hash’等算法的基础上，通过加权的方式，进行负载服务器分配。 优点：根据权重，调节转发服务器的请求数目； 缺点：使用相对复杂； 4. 硬件负载均衡 采用硬件的方式实现负载均衡，一般是单独的负载均衡服务器，价格昂贵，一般土豪级公司可以考虑，业界领先的有两款，F5 和 A10。 使用硬件负载均衡，主要考虑一下几个方面： （1）功能考虑：功能全面支持各层级的负载均衡，支持全面的负载均衡算法，支持全局负载均衡； （2）性能考虑：一般软件负载均衡支持到 5 万级并发已经很困难了，硬件负载均衡可以支持 （3）稳定性：商用硬件负载均衡，经过了良好的严格的测试，从经过大规模使用，在稳定性方面高； （4）安全防护：硬件均衡设备除具备负载均衡功能外，还具备防火墙，防 DDOS 攻击等安全功能； （5）维护角度：提供良好的维护管理界面，售后服务和技术支持； （6）土豪公司：F5 Big Ip 价格：15w~55w 不等；A10 价格：55w-100w 不等； 缺点 （1）价格昂贵； （2）扩展能力差； 小结 （1）一般硬件的负载均衡也要做双机高可用，因此成本会比较高。 （2）互联网公司一般使用开源软件，因此大部分应用采用软件负载均衡；部分采用硬件负载均衡。 比如某互联网公司，目前是使用几台 F5 做全局负载均衡，内部使用 Nginx 等软件负载均衡。 5. Ngnix 负载均衡 Ngnix 是一款轻量级的 Web 服务器/反向代理服务器，工作在七层 Http 协议的负载均衡系统。具有高性能、高并发、低内存使用等特点。是一个轻量级的 Http 和反向代理服务器。Nginx 使用 epoll and kqueue 作为开发模型。能够支持高达 50,000 个并发连接数的响应。 操作系统：Liunx，Windows（Linux、FreeBSD、Solaris、Mac OS X、AIX 以及 Microsoft Windows） 开发语言：C 并发性能：官方支持每秒 5 万并发，实际国内一般到每秒 2 万并发，有优化到每秒 10 万并发的。具体性能看应用场景。 5.1. Ngnix 特点 1.模块化设计：良好的扩展性，可以通过模块方式进行功能扩展。 2.高可靠性：主控进程和 worker 是同步实现的，一个 worker 出现问题，会立刻启动另一个 worker。 3.内存消耗低：一万个长连接（keep-alive）,仅消耗 2.5MB 内存。 4.支持热部署：不用停止服务器，实现更新配置文件，更换日志文件、更新服务器程序版本。 5.并发能力强：官方数据每秒支持 5 万并发； 6.功能丰富：优秀的反向代理功能和灵活的负载均衡策略 5.2. Ngnix 功能 基本功能 支持静态资源的 web 服务器。 http,smtp,pop3 协议的反向代理服务器、缓存、负载均衡； 支持 FASTCGI（fpm） 支持模块化，过滤器（让文本可以实现压缩，节约带宽）,ssl 及图像大小调整。 内置的健康检查功能 基于名称和 ip 的虚拟主机 定制访问日志 支持平滑升级 支持 KEEPALIVE 支持 url rewrite 支持路径别名 支持基于 IP 和用户名的访问控制。 支持传输速率限制，支持并发数限制。 扩展功能 性能 Nginx 的高并发，官方测试支持 5 万并发连接。实际生产环境能到 2-3 万并发连接数。10000 个非活跃的 HTTP keep-alive 连接仅占用约 2.5MB 内存。三万并发连接下，10 个 Nginx 进程，消耗内存 150M。淘宝 tengine 团队测试结果是“24G 内存机器上，处理并发请求可达 200 万”。 5.3. Ngnix 架构 Nginx 的基本工作模式 一个 master 进程，生成一个或者多个 worker 进程。但是这里 master 是使用 root 身份启动的，因为 nginx 要工作在 80 端口。而只有管理员才有权限启动小于低于 1023 的端口。master 主要是负责的作用只是启动 worker，加载配置文件，负责系统的平滑升级。其它的工作是交给 worker。那么当 worker 被启动之后，也只是负责一些 web 最简单的工作，而其他的工作都是有 worker 中调用的模块来实现的。 模块之间是以流水线的方式实现功能的。流水线，指的是一个用户请求，由多个模块组合各自的功能依次实现完成的。比如：第一个模块只负责分析请求首部，第二个模块只负责查找数据，第三个模块只负责压缩数据，依次完成各自工作。来实现整个工作的完成。 他们是如何实现热部署的呢？其实是这样的，我们前面说 master 不负责具体的工作，而是调用 worker 工作，他只是负责读取配置文件，因此当一个模块修改或者配置文件发生变化，是由 master 进行读取，因此此时不会影响到 worker 工作。在 master 进行读取配置文件之后，不会立即的把修改的配置文件告知 worker。而是让被修改的 worker 继续使用老的配置文件工作，当 worker 工作完毕之后，直接当掉这个子进程，更换新的子进程，使用新的规则。 Nginx 支持的 sendfile 机制 Sendfile 机制，用户将请求发给内核，内核根据用户的请求调用相应用户进程，进程在处理时需要资源。此时再把请求发给内核（进程没有直接 IO 的能力），由内核加载数据。内核查找到数据之后，会把数据复制给用户进程，由用户进程对数据进行封装，之后交给内核，内核在进行 tcp/ip 首部的封装，最后再发给客户端。这个功能用户进程只是发生了一个封装报文的过程，却要绕一大圈。因此 nginx 引入了 sendfile 机制，使得内核在接受到数据之后，不再依靠用户进程给予封装，而是自己查找自己封装，减少了一个很长一段时间的浪费，这是一个提升性能的核心点。 以上内容摘自网友发布的文章，简单一句话是资源的处理，直接通过内核层进行数据传递，避免了数据传递到应用层，应用层再传递到内核层的开销。 目前高并发的处理，一般都采用 sendfile 模式。通过直接操作内核层数据，减少应用与内核层数据传递。 Nginx 通信模型（I/O 复用机制） 开发模型：epoll 和 kqueue。 支持的事件机制：kqueue、epoll、rt signals、/dev/poll 、event ports、select 以及 poll。 支持的 kqueue 特性包括 EV_CLEAR、EV_DISABLE、NOTE_LOWAT、EV_EOF，可用数据的数量，错误代码. 支持 sendfile、sendfile64 和 sendfilev;文件 AIO；DIRECTIO;支持 Accept-filters 和 TCP_DEFER_ACCEP. 以上概念较多，大家自行百度或谷歌，知识领域是网络通信（BIO,NIO,AIO）和多线程方面的知识。 5.4. Ngnix 均衡策略 nginx 的负载均衡策略可以划分为两大类：内置策略和扩展策略。内置策略包含加权轮询和 ip hash，在默认情况下这两种策略会编译进 nginx 内核，只需在 nginx 配置中指明参数即可。扩展策略有很多，如 fair、通用 hash、consistent hash 等，默认不编译进 nginx 内核。由于在 nginx 版本升级中负载均衡的代码没有本质性的变化，因此下面将以 nginx1.0.15 稳定版为例，从源码角度分析各个策略。 加权轮询（weighted round robin） 轮询的原理很简单，首先我们介绍一下轮询的基本流程。如下是处理一次请求的流程图： 图中有两点需要注意，第一，如果可以把加权轮询算法分为先深搜索和先广搜索，那么 nginx 采用的是先深搜索算法，即将首先将请求都分给高权重的机器，直到该机器的权值降到了比其他机器低，才开始将请求分给下一个高权重的机器；第二，当所有后端机器都 down 掉时，nginx 会立即将所有机器的标志位清成初始状态，以避免造成所有的机器都处在 timeout 的状态，从而导致整个前端被夯住。 ip hash ip hash 是 nginx 内置的另一个负载均衡的策略，流程和轮询很类似，只是其中的算法和具体的策略有些变化，如下图所示： fair fair 策略是扩展策略，默认不被编译进 nginx 内核。其原理是根据后端服务器的响应时间判断负载情况，从中选出负载最轻的机器进行分流。这种策略具有很强的自适应性，但是实际的网络环境往往不是那么简单，因此要慎用。 通用 hash、一致性 hash 这两种也是扩展策略，在具体的实现上有些差别，通用 hash 比较简单，可以以 nginx 内置的变量为 key 进行 hash，一致性 hash 采用了 nginx 内置的一致性 hash 环，可以支持 memcache。 5.5. Ngnix 场景 Ngnix 一般作为入口负载均衡或内部负载均衡，结合反向代理服务器使用。以下架构示例，仅供参考，具体使用根据场景而定。 入口负载均衡架构 Ngnix 服务器在用户访问的最前端。根据用户请求再转发到具体的应用服务器或二级负载均衡服务器（LVS） 内部负载均衡架构 LVS 作为入口负载均衡，将请求转发到二级 Ngnix 服务器，Ngnix 再根据请求转发到具体的应用服务器。 Ngnix 高可用 分布式系统中，应用只部署一台服务器会存在单点故障，负载均衡同样有类似的问题。一般可采用主备或负载均衡设备集群的方式节约单点故障或高并发请求分流。 Ngnix 高可用，至少包含两个 Ngnix 服务器，一台主服务器，一台备服务器，之间使用 Keepalived 做健康监控和故障检测。开放 VIP 端口，通过防火墙进行外部映射。 DNS 解析公网的 IP 实际为 VIP。 6. LVS 负载均衡 LVS 是一个开源的软件，由毕业于国防科技大学的章文嵩博士于 1998 年 5 月创立，用来实现 Linux 平台下的简单负载均衡。LVS 是 Linux Virtual Server 的缩写，意思是 Linux 虚拟服务器。 基于 IP 层的负载均衡调度技术，它在操作系统核心层上，将来自 IP 层的 TCP/UDP 请求均衡地转移到不同的 服务器，从而将一组服务器构成一个高性能、高可用的虚拟服务器。 操作系统：Liunx 开发语言：C 并发性能：默认 4096，可以修改但需要重新编译。 6.1. LVS 功能 LVS 的主要功能是实现 IP 层（网络层）负载均衡，有 NAT,TUN,DR 三种请求转发模式。 LVS/NAT 方式的负载均衡集群 NAT 是指 Network Address Translation，它的转发流程是：Director 机器收到外界请求，改写数据包的目标地址，按相应的调度算法将其发送到相应 Real Server 上，Real Server 处理完该请求后，将结果数据包返回到其默认网关，即 Director 机器上，Director 机器再改写数据包的源地址，最后将其返回给外界。这样就完成一次负载调度。 构架一个最简单的 LVS/NAT 方式的负载均衡集群 Real Server 可以是任何的操作系统，而且无需做任何特殊的设定，惟一要做的就是将其默认网关指向 Director 机器。Real Server 可以使用局域网的内部 IP(192.168.0.0/24)。Director 要有两块网卡，一块网卡绑定一个外部 IP 地址 (10.0.0.1)，另一块网卡绑定局域网的内部 IP(192.168.0.254)，作为 Real Server 的默认网关。 LVS/NAT 方式实现起来最为简单，而且 Real Server 使用的是内部 IP，可以节省 Real IP 的开销。但因为执行 NAT 需要重写流经 Director 的数据包，在速度上有一定延迟； 当用户的请求非常短，而服务器的回应非常大的情况下，会对 Director 形成很大压力，成为新的瓶颈，从而使整个系统的性能受到限制。 LVS/TUN 方式的负载均衡集群 TUN 是指 IP Tunneling，它的转发流程是：Director 机器收到外界请求，按相应的调度算法,通过 IP 隧道发送到相应 Real Server，Real Server 处理完该请求后，将结果数据包直接返回给客户。至此完成一次负载调度。 最简单的 LVS/TUN 方式的负载均衡集群架构使用 IP Tunneling 技术，在 Director 机器和 Real Server 机器之间架设一个 IP Tunnel，通过 IP Tunnel 将负载分配到 Real Server 机器上。Director 和 Real Server 之间的关系比较松散，可以是在同一个网络中，也可以是在不同的网络中，只要两者能够通过 IP Tunnel 相连就行。收到负载分配的 Real Server 机器处理完后会直接将反馈数据送回给客户，而不必通过 Director 机器。实际应用中，服务器必须拥有正式的 IP 地址用于与客户机直接通信，并且所有服务器必须支持 IP 隧道协议。 该方式中 Director 将客户请求分配到不同的 Real Server，Real Server 处理请求后直接回应给用户，这样 Director 就只处理客户机与服务器的一半连接，极大地提高了 Director 的调度处理能力，使集群系统能容纳更多的节点数。另外 TUN 方式中的 Real Server 可以在任何 LAN 或 WAN 上运行，这样可以构筑跨地域的集群，其应对灾难的能力也更强，但是服务器需要为 IP 封装付出一定的资源开销，而且后端的 Real Server 必须是支持 IP Tunneling 的操作系统。 LVS/TUN 方式的负载均衡集群 DR 是指 Direct Routing，它的转发流程是：Director 机器收到外界请求，按相应的调度算法将其直接发送到相应 Real Server，Real Server 处理完该请求后，将结果数据包直接返回给客户，完成一次负载调度。 构架一个最简单的 LVS/DR 方式的负载均衡集群 Real Server 和 Director 都在同一个物理网段中，Director 的网卡 IP 是 192.168.0.253，再绑定另一个 IP： 192.168.0.254 作为对外界的 virtual IP，外界客户通过该 IP 来访问整个集群系统。Real Server 在 lo 上绑定 IP：192.168.0.254，同时加入相应的路由。 LVS/DR 方式与前面的 LVS/TUN 方式有些类似，前台的 Director 机器也是只需要接收和调度外界的请求，而不需要负责返回这些请求的反馈结果，所以能够负载更多的 Real Server，提高 Director 的调度处理能力，使集群系统容纳更多的 Real Server。但 LVS/DR 需要改写请求报文的 MAC 地址，所以所有服务器必须在同一物理网段内。 6.2. LVS 架构 LVS 架设的服务器集群系统有三个部分组成：最前端的负载均衡层（Loader Balancer），中间的服务器群组层，用 Server Array 表示，最底层的数据共享存储层，用 Shared Storage 表示。在用户看来所有的应用都是透明的，用户只是在使用一个虚拟服务器提供的高性能服务。 LVS 的体系架构如图： LVS 的各个层次的详细介绍： Load Balancer 层：位于整个集群系统的最前端，有一台或者多台负载调度器（Director Server）组成，LVS 模块就安装在 Director Server 上，而 Director 的主要作用类似于一个路由器，它含有完成 LVS 功能所设定的路由表，通过这些路由表把用户的请求分发给 Server Array 层的应用服务器（Real Server）上。同时，在 Director Server 上还要安装对 Real Server 服务的监控模块 Ldirectord，此模块用于监测各个 Real Server 服务的健康状况。在 Real Server 不可用时把它从 LVS 路由表中剔除，恢复时重新加入。 Server Array 层：由一组实际运行应用服务的机器组成，Real Server 可以是 WEB 服务器、MAIL 服务器、FTP 服务器、DNS 服务器、视频服务器中的一个或者多个，每个 Real Server 之间通过高速的 LAN 或分布在各地的 WAN 相连接。在实际的应用中，Director Server 也可以同时兼任 Real Server 的角色。 Shared Storage 层：是为所有 Real Server 提供共享存储空间和内容一致性的存储区域，在物理上，一般有磁盘阵列设备组成，为了提供内容的一致性，一般可以通过 NFS 网络文件系统共享数 据，但是 NFS 在繁忙的业务系统中，性能并不是很好，此时可以采用集群文件系统，例如 Red hat 的 GFS 文件系统，oracle 提供的 OCFS2 文件系统等。 从整个 LVS 结构可以看出，Director Server 是整个 LVS 的核心，目前，用于 Director Server 的操作系统只能是 Linux 和 FreeBSD，linux2.6 内核不用任何设置就可以支持 LVS 功能，而 FreeBSD 作为 Director Server 的应用还不是很多，性能也不是很好。对于 Real Server，几乎可以是所有的系统平台，Linux、windows、Solaris、AIX、BSD 系列都能很好的支持。 6.3. LVS 均衡策略 LVS 默认支持八种负载均衡策略，简述如下： 轮询调度（Round Robin） 调度器通过“轮询”调度算法将外部请求按顺序轮流分配到集群中的真实服务器上，它均等地对待每一台服务器，而不管服务器上实际的连接数和系统负载。 加权轮询（Weighted Round Robin） 调度器通过“加权轮询”调度算法根据真实服务器的不同处理能力来调度访问请求。这样可以保证处理能力强的服务器能处理更多的访问流量。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。 最少链接（Least Connections） 调度器通过“最少连接”调度算法动态地将网络请求调度到已建立的链接数最少的服务器上。如果集群系统的真实服务器具有相近的系统性能，采用“最小连接”调度算法可以较好地均衡负载。 加权最少链接（Weighted Least Connections） 在集群系统中的服务器性能差异较大的情况下，调度器采用“加权最少链接”调度算法优化负载均衡性能，具有较高权值的服务器将承受较大比例的活动连接负载。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。 基于局部性的最少链接（Locality-Based Least Connections） “基于局部性的最少链接”调度算法是针对目标 IP 地址的负载均衡，目前主要用于 Cache 集群系统。该算法根据请求的目标 IP 地址找出该目标 IP 地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则用“最少链接” 的原则选出一个可用的服务器，将请求发送到该服务器。 带复制的基于局部性最少链接（Locality-Based Least Connections with Replication） “带复制的基于局部性最少链接”调度算法也是针对目标 IP 地址的负载均衡，目前主要用于 Cache 集群系统。它与 LBLC 算法的不同之处是它要维护从一个目标 IP 地址到一组服务器的映射，而 LBLC 算法维护从一个目标 IP 地址到一台服务器的映射。该算法根据请求的目标 IP 地址找出该目标 IP 地址对应的服务器组，按“最小连接”原则从服务器组中选出一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按“最小连接”原则从这个集群中选出一台服务器，将该服务器加入到服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。 目标地址散列（Destination Hashing） “目标地址散列”调度算法根据请求的目标 IP 地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。 源地址散列（Source Hashing） “源地址散列”调度算法根据请求的源 IP 地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。 除具备以上负载均衡算法外，还可以自定义均衡策略。 6.4. LVS 场景 一般作为入口负载均衡或内部负载均衡，结合反向代理服务器使用。相关架构可参考 Ngnix 场景架构。 7. HaProxy 负载均衡 HAProxy 也是使用较多的一款负载均衡软件。HAProxy 提供高可用性、负载均衡以及基于 TCP 和 HTTP 应用的代理，支持虚拟主机，是免费、快速并且可靠的一种解决方案。特别适用于那些负载特大的 web 站点。运行模式使得它可以很简单安全的整合到当前的架构中，同时可以保护你的 web 服务器不被暴露到网络上。 7.1. HaProxy 特点 支持两种代理模式：TCP（四层）和 HTTP（七层），支持虚拟主机； 配置简单，支持 url 检测后端服务器状态； 做负载均衡软件使用，在高并发情况下，处理速度高于 nginx； TCP 层多用于 Mysql 从（读）服务器负载均衡。 （对 Mysql 进行负载均衡，对后端的 DB 节点进行检测和负载均衡） 能够补充 Nginx 的一些缺点比如 Session 的保持，Cookie 引导等工作 7.2. HaProxy 均衡策略 支持四种常用算法： 1.roundrobin：轮询，轮流分配到后端服务器； 2.static-rr：根据后端服务器性能分配； 3.leastconn：最小连接者优先处理； 4.source：根据请求源 IP，与 Nginx 的 IP_Hash 类似。 8. 资料 大型网站架构系列：负载均衡详解（1） 大型网站架构系列：负载均衡详解（2） 大型网站架构系列：负载均衡详解（3） 大型网站架构系列：负载均衡详解（4）]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
        <tag>load balance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码坏味道之代码臃肿]]></title>
    <url>%2Fblog%2F2018%2F10%2F13%2Fdesign%2Frefactor%2F%E4%BB%A3%E7%A0%81%E5%9D%8F%E5%91%B3%E9%81%93%E4%B9%8B%E4%BB%A3%E7%A0%81%E8%87%83%E8%82%BF%2F</url>
    <content type="text"><![CDATA[代码坏味道之代码臃肿 📓 本文已归档到：「blog」 翻译自：https://sourcemaking.com/refactoring/smells/bloaters 代码臃肿(Bloated)这组坏味道意味着：代码中的类、函数、字段没有经过合理的组织，只是简单的堆砌起来。这一类型的问题通常在代码的初期并不明显，但是随着代码规模的增长而逐渐积累（特别是当没有人努力去根除它们时）。 基本类型偏执 数据泥团 过大的类 过长函数 过长参数列 扩展阅读 参考资料 基本类型偏执 基本类型偏执(Primitive Obsession) 使用基本类型而不是小对象来实现简单任务（例如货币、范围、电话号码字符串等）。 使用常量编码信息（例如一个用于引用管理员权限的常量USER_ADMIN_ROLE = 1 ）。 使用字符串常量作为字段名在数组中使用。 问题原因 类似其他大部分坏味道，基本类型偏执诞生于类初建的时候。一开始，可能只是不多的字段，随着表示的特性越来越多，基本数据类型字段也越来越多。 基本类型常常被用于表示模型的类型。你有一组数字或字符串用来表示某个实体。 还有一个场景：在模拟场景，大量的字符串常量被用于数组的索引。 解决方法 大多数编程语言都支持基本数据类型和结构类型（类、结构体等）。结构类型允许程序员将基本数据类型组织起来，以代表某一事物的模型。 基本数据类型可以看成是机构类型的积木块。当基本数据类型数量成规模后，将它们有组织地结合起来，可以更方便的管理这些数据。 如果你有大量的基本数据类型字段，就有可能将其中部分存在逻辑联系的字段组织起来，形成一个类。更进一步的是，将与这些数据有关联的方法也一并移入类中。为了实现这个目标，可以尝试 以类取代类型码(Replace Type Code with Class) 。 如果基本数据类型字段的值是用于方法的参数，可以使用 引入参数对象(Introduce Parameter Object) 或 保持对象完整(Preserve Whole Object) 。 如果想要替换的数据值是类型码，而它并不影响行为，则可以运用 以类取代类型码(Replace Type Code with Class) 将它替换掉。如果你有与类型码相关的条件表达式，可运用 以子类取代类型码(Replace Type Code with Subclass) 或 以状态/策略模式取代类型码(Replace Type Code with State/Strategy) 加以处理。 如果你发现自己正从数组中挑选数据，可运用 以对象取代数组(Replace Array with Object) 。 收益 多亏了使用对象替代基本数据类型，使得代码变得更加灵活。 代码变得更加易读和更加有组织。特殊数据可以集中进行操作，而不像之前那样分散。不用再猜测这些陌生的常量的意义以及它们为什么在数组中。 更容易发现重复代码。 重构方法说明 以类取代类型码(Replace Type Code with Class) 问题 类之中有一个数值类型码，但它并不影响类的行为。 解决 以一个新的类替换该数值类型码。 引入参数对象(Introduce Parameter Object) 问题 某些参数总是很自然地同时出现。 解决 以一个对象来取代这些参数。 保持对象完整(Preserve Whole Object) 问题 你从某个对象中取出若干值，将它们作为某一次函数调用时的参数。 int low = daysTempRange.getLow();int high = daysTempRange.getHigh();boolean withinPlan = plan.withinRange(low, high); 解决 改为传递整个对象。 boolean withinPlan = plan.withinRange(daysTempRange); 以子类取代类型码(Replace Type Code with Subclass) 问题 你有一个不可变的类型码，它会影响类的行为。 解决 以子类取代这个类型码。 以状态/策略模式取代类型码(Replace Type Code with State/Strategy) 问题 你有一个类型码，它会影响类的行为，但你无法通过继承消除它。 解决 以状态对象取代类型码。 以对象取代数组(Replace Array with Object) 问题 你有一个数组，其中的元素各自代表不同的东西。 String[] row = new String[3];row[0] = "Liverpool";row[1] = "15"; 解决 以对象替换数组。对于数组中的每个元素，以一个字段来表示。 Performance row = new Performance();row.setName("Liverpool");row.setWins("15"); 数据泥团 数据泥团(Data Clumps) 有时，代码的不同部分包含相同的变量组（例如用于连接到数据库的参数）。这些绑在一起出现的数据应该拥有自己的对象。 问题原因 通常，数据泥团的出现时因为糟糕的编程结构或“复制-粘贴式编程”。 有一个判断是否是数据泥团的好办法：删掉众多数据中的一项。这么做，其他数据有没有因而失去意义？如果它们不再有意义，这就是个明确的信号：你应该为它们产生一个新的对象。 解决方法 首先找出这些数据以字段形式出现的地方，运用 提炼类(Extract Class) 将它们提炼到一个独立对象中。 如果数据泥团在函数的参数列中出现，运用 引入参数对象(Introduce Parameter Object) 将它们组织成一个类。 如果数据泥团的部分数据出现在其他函数中，考虑运用 保持对象完整(Preserve Whole Object) 将整个数据对象传入到函数中。 检视一下使用这些字段的代码，也许，将它们移入一个数据类是个不错的主意。 收益 提高代码易读性和组织性。对于特殊数据的操作，可以集中进行处理，而不像以前那样分散。 减少代码量。 何时忽略 有时为了对象中的部分数据而将整个对象作为参数传递给函数，可能会产生让两个类之间不收欢迎的依赖关系，这中情况下可以不传递整个对象。 重构方法说明 提炼类(Extract Class) 问题 某个类做了不止一件事。 解决 建立一个新类，将相关的字段和函数从旧类搬移到新类。 引入参数对象(Introduce Parameter Object) 问题 某些参数总是很自然地同时出现。 解决 以一个对象来取代这些参数。 保持对象完整(Preserve Whole Object) 问题 你从某个对象中取出若干值，将它们作为某一次函数调用时的参数。 int low = daysTempRange.getLow();int high = daysTempRange.getHigh();boolean withinPlan = plan.withinRange(low, high); 解决 改为传递整个对象。 boolean withinPlan = plan.withinRange(daysTempRange); 过大的类 过大的类(Large Class) 一个类含有过多字段、函数、代码行。 问题原因 类通常一开始很小，但是随着程序的增长而逐渐膨胀。 类似于过长函数，程序员通常觉得在一个现存类中添加新特性比创建一个新的类要容易。 解决方法 设计模式中有一条重要原则：职责单一原则。一个类应该只赋予它一个职责。如果它所承担的职责太多，就该考虑为它减减负。 如果过大类中的部分行为可以提炼到一个独立的组件中，可以使用 提炼类(Extract Class)。 如果过大类中的部分行为可以用不同方式实现或使用于特殊场景，可以使用 提炼子类(Extract Subclass)。 如果有必要为客户端提供一组操作和行为，可以使用 提炼接口(Extract Interface)。 如果你的过大类是个 GUI 类，可能需要把数据和行为移到一个独立的领域对象去。你可能需要两边各保留一些重复数据，并保持两边同步。 复制被监视数据(Duplicate Observed Data) 可以告诉你怎么做。 收益 重构过大的类可以使程序员不必记住一个类中大量的属性。 在大多数情况下，分割过大的类可以避免代码和功能的重复。 重构方法说明 提炼类(Extract Class) 问题 某个类做了不止一件事。 解决 建立一个新类，将相关的字段和函数从旧类搬移到新类。 提炼子类(Extract Subclass) 问题 一个类中有些特性仅用于特定场景。 解决 创建一个子类，并将用于特殊场景的特性置入其中。 提炼接口(Extract Interface) 问题 多个客户端使用一个类部分相同的函数。另一个场景是两个类中的部分函数相同。 解决 移动相同的部分函数到接口中。 复制被监视数据(Duplicate Observed Data) 问题 如果存储在类中的数据是负责 GUI 的。 解决 一个比较好的方法是将负责 GUI 的数据放入一个独立的类，以确保 GUI 数据与域类之间的连接和同步。 过长函数 过长函数(Long Method) 一个函数含有太多行代码。一般来说，任何函数超过 10 行时，你就可以考虑是不是过长了。 函数中的代码行数原则上不要超过 100 行。 问题的原因 通常情况下，创建一个新函数的难度要大于添加功能到一个已存在的函数。大部分人都觉得：“我就添加这么两行代码，为此新建一个函数实在是小题大做了。”于是，张三加两行，李四加两行，王五加两行。。。函数日益庞大，最终烂的像一锅浆糊，再也没人能完全看懂了。于是大家就更不敢轻易动这个函数了，只能恶性循环的往其中添加代码。所以，如果你看到一个超过 200 行的函数，通常都是多个程序员东拼西凑出来的。 解决函数 一个很好的技巧是：寻找注释。添加注释，一般有这么几个原因：代码逻辑较为晦涩或复杂；这段代码功能相对独立；特殊处理。 如果代码前方有一行注释，就是在提醒你：可以将这段代码替换成一个函数，而且可以在注释的基础上给这个函数命名。如果函数有一个描述恰当的名字，就不需要去看内部代码究竟是如何实现的。就算只有一行代码，如果它需要以注释来说明，那也值得将它提炼到独立函数中。 为了给一个函数瘦身，可以使用 提炼函数(Extract Method)。 如果局部变量和参数干扰提炼函数，可以使用 以查询取代临时变量(Replace Temp with Query)，引入参数对象(Introduce Parameter Object) 或 保持对象完整(Preserve Whole Object) 。 如果前面两条没有帮助，可以通过 以函数对象取代函数(Replace Method with Method Object) 尝试移动整个函数到一个独立的对象中。 条件表达式和循环常常也是提炼的信号。对于条件表达式，可以使用 分解条件表达式(Decompose Conditional) 。至于循环，应该使用 提炼函数(Extract Method) 将循环和其内的代码提炼到独立函数中。 收益 在所有类型的面向对象代码中，函数比较短小精悍的类往往生命周期较长。一个函数越长，就越不容易理解和维护。 此外，过长函数中往往含有难以发现的重复代码。 性能 是否像许多人说的那样，增加函数的数量会影响性能？在几乎绝大多数情况下，这种影响是可以忽略不计，所以不用担心。 此外，现在有了清晰和易读的代码，在需要的时候，你将更容易找到真正有效的函数来重组代码和提高性能。 重构方法说明 提炼函数(Extract Method) 问题 你有一段代码可以组织在一起。 void printOwing() &#123; printBanner(); //print details System.out.println("name: " + name); System.out.println("amount: " + getOutstanding());&#125; 解决 移动这段代码到一个新的函数中，使用函数的调用来替代老代码。 void printOwing() &#123; printBanner(); printDetails(getOutstanding());&#125;void printDetails(double outstanding) &#123; System.out.println("name: " + name); System.out.println("amount: " + outstanding);&#125; 以查询取代临时变量(Replace Temp with Query) 问题 将表达式的结果放在局部变量中，然后在代码中使用。 double calculateTotal() &#123; double basePrice = quantity * itemPrice; if (basePrice &gt; 1000) &#123; return basePrice * 0.95; &#125; else &#123; return basePrice * 0.98; &#125;&#125; 解决 将整个表达式移动到一个独立的函数中并返回结果。使用查询函数来替代使用变量。如果需要，可以在其他函数中合并新函数。 double calculateTotal() &#123; if (basePrice() &gt; 1000) &#123; return basePrice() * 0.95; &#125; else &#123; return basePrice() * 0.98; &#125;&#125;double basePrice() &#123; return quantity * itemPrice;&#125; 引入参数对象(Introduce Parameter Object) 问题 某些参数总是很自然地同时出现。 解决 以一个对象来取代这些参数。 保持对象完整(Preserve Whole Object) 问题 你从某个对象中取出若干值，将它们作为某一次函数调用时的参数。 int low = daysTempRange.getLow();int high = daysTempRange.getHigh();boolean withinPlan = plan.withinRange(low, high); 解决 改为传递整个对象。 boolean withinPlan = plan.withinRange(daysTempRange); 以函数对象取代函数(Replace Method with Method Object) 问题 你有一个过长函数，它的局部变量交织在一起，以致于你无法应用提炼函数(Extract Method) 。 class Order &#123; //... public double price() &#123; double primaryBasePrice; double secondaryBasePrice; double tertiaryBasePrice; // long computation. //... &#125;&#125; 解决 将函数移到一个独立的类中，使得局部变量成了这个类的字段。然后，你可以将函数分割成这个类中的多个函数。 class Order &#123; //... public double price() &#123; return new PriceCalculator(this).compute(); &#125;&#125;class PriceCalculator &#123; private double primaryBasePrice; private double secondaryBasePrice; private double tertiaryBasePrice; public PriceCalculator(Order order) &#123; // copy relevant information from order object. //... &#125; public double compute() &#123; // long computation. //... &#125;&#125; 分解条件表达式(Decompose Conditional) 问题 你有复杂的条件表达式。 if (date.before(SUMMER_START) || date.after(SUMMER_END)) &#123; charge = quantity * winterRate + winterServiceCharge;&#125;else &#123; charge = quantity * summerRate;&#125; 解决 根据条件分支将整个条件表达式分解成几个函数。 if (notSummer(date)) &#123; charge = winterCharge(quantity);&#125;else &#123; charge = summerCharge(quantity);&#125; 过长参数列 过长参数列(Long Parameter List) 一个函数有超过 3、4 个入参。 问题原因 过长参数列可能是将多个算法并到一个函数中时发生的。函数中的入参可以用来控制最终选用哪个算法去执行。 过长参数列也可能是解耦类之间依赖关系时的副产品。例如，用于创建函数中所需的特定对象的代码已从函数移动到调用函数的代码处，但创建的对象是作为参数传递到函数中。因此，原始类不再知道对象之间的关系，并且依赖性也已经减少。但是如果创建的这些对象，每一个都将需要它自己的参数，这意味着过长参数列。 太长的参数列难以理解，太多参数会造成前后不一致、不易使用，而且一旦需要更多数据，就不得不修改它。 解决方案 如果向已有的对象发出一条请求就可以取代一个参数，那么你应该使用 以函数取代参数(Replace Parameter with Methods) 。在这里，，“已有的对象”可能是函数所属类里的一个字段，也可能是另一个参数。 你还可以运用 保持对象完整(Preserve Whole Object) 将来自同一对象的一堆数据收集起来，并以该对象替换它们。 如果某些数据缺乏合理的对象归属，可使用 引入参数对象(Introduce Parameter Object) 为它们制造出一个“参数对象”。 收益 更易读，更简短的代码。 重构可能会暴露出之前未注意到的重复代码。 何时忽略 这里有一个重要的例外：有时候你明显不想造成&quot;被调用对象&quot;与&quot;较大对象&quot;间的某种依赖关系。这时候将数据从对象中拆解出来单独作为参数，也很合情理。但是请注意其所引发的代价。如果参数列太长或变化太频繁，就需要重新考虑自己的依赖结构了。 重构方法说明 以函数取代参数(Replace Parameter with Methods) 问题 对象调用某个函数，并将所得结果作为参数，传递给另一个函数。而接受该参数的函数本身也能够调用前一个函数。 int basePrice = quantity * itemPrice;double seasonDiscount = this.getSeasonalDiscount();double fees = this.getFees();double finalPrice = discountedPrice(basePrice, seasonDiscount, fees); 解决 让参数接受者去除该项参数，并直接调用前一个函数。 int basePrice = quantity * itemPrice;double finalPrice = discountedPrice(basePrice); 保持对象完整(Preserve Whole Object) 问题 你从某个对象中取出若干值，将它们作为某一次函数调用时的参数。 int low = daysTempRange.getLow();int high = daysTempRange.getHigh();boolean withinPlan = plan.withinRange(low, high); 解决 改为传递整个对象。 boolean withinPlan = plan.withinRange(daysTempRange); 引入参数对象(Introduce Parameter Object) 问题 某些参数总是很自然地同时出现。 解决 以一个对象来取代这些参数。 扩展阅读 代码的坏味道和重构 代码坏味道之代码臃肿 代码坏味道之滥用面向对象 代码坏味道之变革的障碍 代码坏味道之非必要的 代码坏味道之耦合 参考资料 重构——改善既有代码的设计 - by Martin Fowler https://sourcemaking.com/refactoring]]></content>
      <categories>
        <category>design</category>
        <category>refactor</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>refactor</tag>
        <tag>code-smell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码坏味道之非必要的]]></title>
    <url>%2Fblog%2F2018%2F10%2F13%2Fdesign%2Frefactor%2F%E4%BB%A3%E7%A0%81%E5%9D%8F%E5%91%B3%E9%81%93%E4%B9%8B%E9%9D%9E%E5%BF%85%E8%A6%81%E7%9A%84%2F</url>
    <content type="text"><![CDATA[代码坏味道之非必要的 📓 本文已归档到：「blog」 翻译自：https://sourcemaking.com/refactoring/smells/dispensables 非必要的(Dispensables)这组坏味道意味着：这样的代码可有可无，它的存在反而影响整体代码的整洁和可读性。 冗余类 夸夸其谈未来性 纯稚的数据类 过多的注释 重复代码 扩展阅读 参考资料 冗余类 冗余类(Lazy Class) 理解和维护总是费时费力的。如果一个类不值得你花费精力，它就应该被删除。 问题原因 也许一个类的初始设计是一个功能完全的类，然而随着代码的变迁，变得没什么用了。 又或者类起初的设计是为了支持未来的功能扩展，然而却一直未派上用场。 解决方法 没什么用的类可以运用 将类内联化(Inline Class) 来干掉。 如果子类用处不大，试试 折叠继承体系(Collapse Hierarchy) 。 收益 减少代码量 易于维护 何时忽略 有时，创建冗余类是为了描述未来开发的意图。在这种情况下，尝试在代码中保持清晰和简单之间的平衡。 重构方法说明 将类内联化(Inline Class) 问题 某个类没有做太多事情。 解决 将这个类的所有特性搬移到另一个类中，然后移除原类。 折叠继承体系(Collapse Hierarchy) 问题 超类和子类之间无太大区别。 解决 将它们合为一体。 夸夸其谈未来性 夸夸其谈未来性(Speculative Generality) 存在未被使用的类、函数、字段或参数。 问题原因 有时，代码仅仅为了支持未来的特性而产生，然而却一直未实现。结果，代码变得难以理解和维护。 解决方法 如果你的某个抽象类其实没有太大作用，请运用 折叠继承体系(Collapse Hierarch) 。 不必要的委托可运用 将类内联化(Inline Class) 消除。 无用的函数可运用 内联函数(Inline Method) 消除。 函数中有无用的参数应该运用 移除参数(Remove Parameter) 消除。 无用字段可以直接删除。 收益 减少代码量。 更易维护。 何时忽略 如果你在一个框架上工作，创建框架本身没有使用的功能是非常合理的，只要框架的用户需要这个功能。 删除元素之前，请确保它们不在单元测试中使用。如果测试需要从类中获取某些内部信息或执行特殊的测试相关操作，就会发生这种情况。 重构方法说明 折叠继承体系(Collapse Hierarchy) 问题 超类和子类之间无太大区别。 解决 将它们合为一体。 将类内联化(Inline Class) 问题 某个类没有做太多事情。 解决 将这个类的所有特性搬移到另一个类中，然后移除原类。 内联函数(Inline Method) 问题 一个函数的本体比函数名更清楚易懂。 class PizzaDelivery &#123; //... int getRating() &#123; return moreThanFiveLateDeliveries() ? 2 : 1; &#125; boolean moreThanFiveLateDeliveries() &#123; return numberOfLateDeliveries &gt; 5; &#125;&#125; 解决 在函数调用点插入函数本体，然后移除该函数。 class PizzaDelivery &#123; //... int getRating() &#123; return numberOfLateDeliveries &gt; 5 ? 2 : 1; &#125;&#125; 移除参数(Remove Parameter) 问题 函数本体不再需要某个参数。 解决 将该参数去除。 纯稚的数据类 纯稚的数据类(Data Class) 指的是只包含字段和访问它们的 getter 和 setter 函数的类。这些仅仅是供其他类使用的数据容器。这些类不包含任何附加功能，并且不能对自己拥有的数据进行独立操作。 问题原因 当一个新创建的类只包含几个公共字段（甚至可能几个 getters / setters）是很正常的。但是对象的真正力量在于它们可以包含作用于数据的行为类型或操作。 解决方法 如果一个类有公共字段，你应该运用 封装字段(Encapsulated Field) 来隐藏字段的直接访问方式。 如果这些类含容器类的字段，你应该检查它们是不是得到了恰当的封装；如果没有，就运用 封装集合(Encapsulated Collection) 把它们封装起来。 找出这些 getter/setter 函数被其他类运用的地点。尝试以 搬移函数(Move Method) 把那些调用行为搬移到 纯稚的数据类(Data Class) 来。如果无法搬移这个函数，就运用 提炼函数(Extract Method) 产生一个可搬移的函数。 在类已经充满了深思熟虑的函数之后，你可能想要摆脱旧的数据访问方法，以提供适应面较广的类数据访问接口。为此，可以运用 移除设置函数(Remove Setting Method) 和 隐藏函数(Hide Method) 。 收益 提高代码的可读性和组织性。特定数据的操作现在被集中在一个地方，而不是在分散在代码各处。 帮助你发现客户端代码的重复处。 重构方法说明 封装字段(Encapsulated Field) 问题 你的类中存在 public 字段。 class Person &#123; public String name;&#125; 解决 将它声明为 private，并提供相应的访问函数。 class Person &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String arg) &#123; name = arg; &#125;&#125; 封装集合(Encapsulated Collection) 问题 有个函数返回一个集合。 解决 让该函数返回该集合的一个只读副本，并在这个类中提供添加、移除集合元素的函数。 搬移函数(Move Method) 问题 你的程序中，有个函数与其所驻类之外的另一个类进行更多交流：调用后者，或被后者调用。 解决 在该函数最常引用的类中建立一个有着类似行为的新函数。将旧函数变成一个单纯的委托函数，或是旧函数完全移除。 提炼函数(Extract Method) 问题 你有一段代码可以组织在一起。 void printOwing() &#123; printBanner(); //print details System.out.println("name: " + name); System.out.println("amount: " + getOutstanding());&#125; 解决 移动这段代码到一个新的函数中，使用函数的调用来替代老代码。 void printOwing() &#123; printBanner(); printDetails(getOutstanding());&#125;void printDetails(double outstanding) &#123; System.out.println("name: " + name); System.out.println("amount: " + outstanding);&#125; 移除设置函数(Remove Setting Method) 问题 类中的某个字段应该在对象创建时被设值，然后就不再改变。 解决 去掉该字段的所有设值函数。 隐藏函数(Hide Method) 问题 有一个函数，从来没有被其他任何类用到。 解决 将这个函数修改为 private。 过多的注释 过多的注释(Comments) 注释本身并不是坏事。但是常常有这样的情况：一段代码中出现长长的注释，而它之所以存在，是因为代码很糟糕。 问题原因 注释的作者意识到自己的代码不直观或不明显，所以想使用注释来说明自己的意图。这种情况下，注释就像是烂代码的除臭剂。 最好的注释是为函数或类起一个恰当的名字。 如果你觉得一个代码片段没有注释就无法理解，请先尝试重构，试着让所有注释都变得多余。 解决方法 如果一个注释是为了解释一个复杂的表达式，可以运用 提炼变量(Extract Variable) 将表达式切分为易理解的子表达式。 如果你需要通过注释来解释一段代码做了什么，请试试 提炼函数(Extract Method) 。 如果函数已经被提炼，但仍需要注释函数做了什么，试试运用 函数改名(Rename Method) 来为函数起一个可以自解释的名字。 如果需要对系统某状态进行断言，请运用 引入断言(Introduce Assertion) 。 收益 代码变得更直观和明显。 何时忽略 注释有时候很有用： 当解释为什么某事物要以特殊方式实现时。 当解释某种复杂算法时。 当你实在不知可以做些什么时。 重构方法说明 提炼变量(Extract Variable) 问题 你有个难以理解的表达式。 void renderBanner() &#123; if ((platform.toUpperCase().indexOf("MAC") &gt; -1) &amp;&amp; (browser.toUpperCase().indexOf("IE") &gt; -1) &amp;&amp; wasInitialized() &amp;&amp; resize &gt; 0 ) &#123; // do something &#125;&#125; 解决 将表达式的结果或它的子表达式的结果用不言自明的变量来替代。 void renderBanner() &#123; final boolean isMacOs = platform.toUpperCase().indexOf("MAC") &gt; -1; final boolean isIE = browser.toUpperCase().indexOf("IE") &gt; -1; final boolean wasResized = resize &gt; 0; if (isMacOs &amp;&amp; isIE &amp;&amp; wasInitialized() &amp;&amp; wasResized) &#123; // do something &#125;&#125; 提炼函数(Extract Method) 问题 你有一段代码可以组织在一起。 void printOwing() &#123; printBanner(); //print details System.out.println("name: " + name); System.out.println("amount: " + getOutstanding());&#125; 解决 移动这段代码到一个新的函数中，使用函数的调用来替代老代码。 void printOwing() &#123; printBanner(); printDetails(getOutstanding());&#125;void printDetails(double outstanding) &#123; System.out.println("name: " + name); System.out.println("amount: " + outstanding);&#125; 函数改名(Rename Method) 问题 函数的名称未能恰当的揭示函数的用途。 class Person &#123; public String getsnm();&#125; 解决 修改函数名。 class Person &#123; public String getSecondName();&#125; 引入断言(Introduce Assertion) 问题 某一段代码需要对程序状态做出某种假设。 double getExpenseLimit() &#123; // should have either expense limit or a primary project return (expenseLimit != NULL_EXPENSE) ? expenseLimit: primaryProject.getMemberExpenseLimit();&#125; 解决 以断言明确表现这种假设。 double getExpenseLimit() &#123; Assert.isTrue(expenseLimit != NULL_EXPENSE || primaryProject != null); return (expenseLimit != NULL_EXPENSE) ? expenseLimit: primaryProject.getMemberExpenseLimit();&#125; 注：请不要滥用断言。不要使用它来检查”应该为真“的条件，只能使用它来检查“一定必须为真”的条件。实际上，断言更多是用于自我检测代码的一种手段。在产品真正交付时，往往都会消除所有断言。 重复代码 重复代码(Duplicate Code) 重复代码堪称为代码坏味道之首。消除重复代码总是有利无害的。 问题原因 重复代码通常发生在多个程序员同时在同一程序的不同部分上工作时。由于他们正在处理不同的任务，他们可能不知道他们的同事已经写了类似的代码。 还有一种更隐晦的重复，特定部分的代码看上去不同但实际在做同一件事。这种重复代码往往难以找到和消除。 有时重复是有目的性的。当急于满足 deadline，并且现有代码对于要交付的任务是“几乎正确的”时，新手程序员可能无法抵抗复制和粘贴相关代码的诱惑。在某些情况下，程序员只是太懒惰。 解决方法 同一个类的两个函数含有相同的表达式，这时可以采用 提炼函数(Extract Method) 提炼出重复的代码，然后让这两个地点都调用被提炼出来的那段代码。 如果两个互为兄弟的子类含有重复代码： 首先对两个类都运用 提炼函数(Extract Method) ，然后对被提炼出来的函数运用 函数上移(Pull Up Method) ，将它推入超类。 如果重复代码在构造函数中，运用 构造函数本体上移(Pull Up Constructor Body) 。 如果重复代码只是相似但不是完全相同，运用 塑造模板函数(Form Template Method) 获得一个 模板方法模式(Template Method) 。 如果有些函数以不同的算法做相同的事，你可以选择其中较清晰地一个，并运用 替换算法(Substitute Algorithm) 将其他函数的算法替换掉。 如果两个毫不相关的类中有重复代码： 请尝试运用 提炼超类(Extract Superclass) ，以便为维护所有先前功能的这些类创建一个超类。 如果创建超类十分困难，可以在一个类中运用 提炼类(Extract Class) ，并在另一个类中使用这个新的组件。 如果存在大量的条件表达式，并且它们执行完全相同的代码（仅仅是它们的条件不同），可以运用 合并条件表达式(Consolidate Conditional Expression) 将这些操作合并为单个条件，并运用 提炼函数(Extract Method) 将该条件放入一个名字容易理解的独立函数中。 如果条件表达式的所有分支都有部分相同的代码片段：可以运用 合并重复的条件片段(Consolidate Duplicate Conditional Fragments) 将它们都存在的代码片段置于条件表达式外部。 收益 合并重复代码会简化代码的结构，并减少代码量。 代码更简化、更易维护。 重构方法说明 提炼函数(Extract Method) 问题 你有一段代码可以组织在一起。 void printOwing() &#123; printBanner(); //print details System.out.println("name: " + name); System.out.println("amount: " + getOutstanding());&#125; 解决 移动这段代码到一个新的函数中，使用函数的调用来替代老代码。 void printOwing() &#123; printBanner(); printDetails(getOutstanding());&#125;void printDetails(double outstanding) &#123; System.out.println("name: " + name); System.out.println("amount: " + outstanding);&#125; 函数上移(Pull Up Method) 问题 有些函数，在各个子类中产生完全相同的结果。 解决 将该函数移至超类。 构造函数本体上移(Pull Up Constructor Body) 问题 你在各个子类中拥有一些构造函数，它们的本体几乎完全一致。 class Manager extends Employee &#123; public Manager(String name, String id, int grade) &#123; this.name = name; this.id = id; this.grade = grade; &#125; //...&#125; 解决 在超类中新建一个构造函数，并在子类构造函数中调用它。 class Manager extends Employee &#123; public Manager(String name, String id, int grade) &#123; super(name, id); this.grade = grade; &#125; //...&#125; 塑造模板函数(Form Template Method) 问题 你有一些子类，其中相应的某些函数以相同的顺序执行类似的操作，但各个操作的细节上有所不同。 解决 将这些操作分别放进独立函数中，并保持它们都有相同的签名，于是原函数也就变得相同了。然后将原函数上移至超类。 注：这里只提到具体做法，建议了解一下模板方法设计模式。 替换算法(Substitute Algorithm) 问题 你想要把某个算法替换为另一个更清晰的算法。 String foundPerson(String[] people)&#123; for (int i = 0; i &lt; people.length; i++) &#123; if (people[i].equals("Don"))&#123; return "Don"; &#125; if (people[i].equals("John"))&#123; return "John"; &#125; if (people[i].equals("Kent"))&#123; return "Kent"; &#125; &#125; return "";&#125; 解决 将函数本体替换为另一个算法。 String foundPerson(String[] people)&#123; List candidates = Arrays.asList(new String[] &#123;"Don", "John", "Kent"&#125;); for (int i=0; i &lt; people.length; i++) &#123; if (candidates.contains(people[i])) &#123; return people[i]; &#125; &#125; return "";&#125; 提炼超类(Extract Superclass) 问题 两个类有相似特性。 解决 为这两个类建立一个超类，将相同特性移至超类。 提炼类(Extract Class) 问题 某个类做了不止一件事。 解决 建立一个新类，将相关的字段和函数从旧类搬移到新类。 合并条件表达式(Consolidate Conditional Expression) 问题 你有一系列条件分支，都得到相同结果。 double disabilityAmount() &#123; if (seniority &lt; 2) &#123; return 0; &#125; if (monthsDisabled &gt; 12) &#123; return 0; &#125; if (isPartTime) &#123; return 0; &#125; // compute the disability amount //...&#125; 解决 将这些条件分支合并为一个条件，并将这个条件提炼为一个独立函数。 double disabilityAmount() &#123; if (isNotEligableForDisability()) &#123; return 0; &#125; // compute the disability amount //...&#125; 合并重复的条件片段(Consolidate Duplicate Conditional Fragments) 问题 在条件表达式的每个分支上有着相同的一段代码。 if (isSpecialDeal()) &#123; total = price * 0.95; send();&#125;else &#123; total = price * 0.98; send();&#125; 解决 将这段重复代码搬移到条件表达式之外。 if (isSpecialDeal()) &#123; total = price * 0.95;&#125;else &#123; total = price * 0.98;&#125;send(); 扩展阅读 代码的坏味道和重构 代码坏味道之代码臃肿 代码坏味道之滥用面向对象 代码坏味道之变革的障碍 代码坏味道之非必要的 代码坏味道之耦合 参考资料 重构——改善既有代码的设计 - by Martin Fowler https://sourcemaking.com/refactoring]]></content>
      <categories>
        <category>design</category>
        <category>refactor</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>refactor</tag>
        <tag>code-smell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[samba 使用详解]]></title>
    <url>%2Fblog%2F2018%2F09%2F28%2Fos%2Flinux%2Fops%2Fsamba%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[samba 使用详解 samba 是在 Linux 和 UNIX 系统上实现 SMB 协议的一个免费软件。 samba 提供了在不同计算机（即使操作系统不同）上共享服务的能力。 关键词：samba, selinux 1. 安装配置 samba 1.1. 查看是否已经安装 samba 1.2. 安装 samba 工具 1.3. 配置 samba 1.4. 创建 samba 用户 1.5. 启动 samba 服务 1.6. 为 samba 添加防火墙规则 1.7. 测试 samba 服务 1.8. 访问 samba 服务共享的目录 2. 配置详解 2.1. samba 默认配置 2.2. 全局参数 [global] 2.3. 共享参数 [共享名] 3. 常见问题 3.1. 你可能没有权限访问网络资源 3.2. window 下对 samba 的清理操作 4. 参考资料 1. 安装配置 samba 本文将以一个完整的示例来展示如何配置 samba 来实现 Linux 和 Windows 的文件共享。 目标：假设希望共享 Linux 服务器上的 /share/fs 目录。 1.1. 查看是否已经安装 samba CentOS：rpm -qa | grep samba Ubuntu：dpkg -l | grep samba 1.2. 安装 samba 工具 CentOS：yum install -y samba samba-client samba-common Ubuntu：sudo apt-get install -y samba samba-client 1.3. 配置 samba samba 服务的配置文件是 /etc/samba/smb.conf，如果没有则 samba 无法启动。 执行以下命令，编辑配置文件： vim /etc/samba/smb.conf 修改配置如下： [global] workgroup = SAMBA security = user passdb backend = tdbsam printing = cups printcap name = cups load printers = yes cups options = raw[homes] comment = Home Directories valid users = %S, %D%w%S browseable = No read only = No inherit acls = Yes[printers] comment = All Printers path = /var/tmp printable = Yes create mask = 0600 browseable = No[print$] comment = Printer Drivers path = /var/lib/samba/drivers write list = @printadmin root force group = @printadmin create mask = 0664 directory mask = 0775[fs] comment = share folder path = /share/fs browseable = yes writable = yes read only = no guest ok = yes create mask = 0777 directory mask = 0777 public = yes valid users = root 说明： 我在这里添加了一个 [fs] 标签，这就是共享区域的配置。 这里设置 path 属性为 /share/fs，意味着准备共享 /share/fs 目录，需要根据实际需要设置路径。/share/fs 目录的权限要设置为 777：chmod 777 /share/fs。 browseable、writable 等属性就比较容易理解了，即配置共享目录的访问权限。 valid users 属性指定允许访问的用户，需要注意的是指定的用户必须是 Linux 机器上实际存在的用户。 1.4. 创建 samba 用户 创建的 samba 用户必须是 Linux 机器上实际存在的用户。 $ sudo smbpasswd -a rootNew SMB password:Retype new SMB password:Added user root. 根据提示输入 samba 用户的密码。当 samba 服务成功安装、启动后，通过 Windows 系统访问机器共享目录时，就要输入这里配置的用户名、密码。 查看 samba 服务器中已拥有哪些用户 - pdbedit -L 删除 samba 服务中的某个用户 - smbpasswd -x 用户名 1.5. 启动 samba 服务 CentOS 6 $ sudo service samba restart # 重启 samba$ sudo service smb restart # 重启 samba CentOS 7 $ sudo systemctl start smb.service # 启动 samba$ sudo systemctl restart smb.service # 重启 samba$ sudo systemctl enable smb.service # 设置开机自动启动$ sudo systemctl status smb.service # 查询 samba 状态 Ubuntu 16.04.3 $ sudo service smbd restart 1.6. 为 samba 添加防火墙规则 $ sudo firewall-cmd --permanent --zone=public --add-service=samba$ sudo firewall-cmd --reload 1.7. 测试 samba 服务 $ smbclient //localhost/fs -U root 输入 samba 用户的密码，如果成功，就会进入 smb: \&gt;。 1.8. 访问 samba 服务共享的目录 Windows： 访问：\\&lt;你的ip&gt;\&lt;你的共享路径&gt; ： Mac： 与 Windows 类似，直接在 Finder 中访问 smb://&lt;你的ip&gt;/&lt;你的共享路径&gt; 即可。 2. 配置详解 2.1. samba 默认配置 你可以从 这里 获取到默认配置文件： $ cp /etc/samba/smb.conf /etc/samba/smb.conf.bak$ wget "https://git.samba.org/samba.git/?p=samba.git;a=blob_plain;f=examples/smb.conf.default;hb=HEAD" -O /etc/samba/smb.conf smb.conf 默认内容如下： [global] workgroup = SAMBA security = user passdb backend = tdbsam printing = cups printcap name = cups load printers = yes cups options = raw[homes] comment = Home Directories valid users = %S, %D%w%S browseable = No read only = No inherit acls = Yes[printers] comment = All Printers path = /var/tmp printable = Yes create mask = 0600 browseable = No[print$] comment = Printer Drivers path = /var/lib/samba/drivers write list = root create mask = 0664 directory mask = 0775 2.2. 全局参数 [global] [global]config file = /usr/local/samba/lib/smb.conf.%m说明：config file可以让你使用另一个配置文件来覆盖缺省的配置文件。如果文件 不存在，则该项无效。这个参数很有用，可以使得samba配置更灵活，可以让一台samba服务器模拟多台不同配置的服务器。比如，你想让PC1（主机名）这台电脑在访问Samba Server时使用它自己的配置文件，那么先在/etc/samba/host/下为PC1配置一个名为smb.conf.pc1的文件，然后在smb.conf中加入：config file=/etc/samba/host/smb.conf.%m。这样当PC1请求连接Samba Server时，smb.conf.%m就被替换成smb.conf.pc1。这样，对于PC1来说，它所使用的Samba服务就是由smb.conf.pc1定义的，而其他机器访问Samba Server则还是应用smb.conf。workgroup = WORKGROUP说明：设定 Samba Server 所要加入的工作组或者域。server string = Samba Server Version %v说明：设定 Samba Server 的注释，可以是任何字符串，也可以不填。宏%v表示显示Samba的版本号。netbios name = smbserver说明：设置Samba Server的NetBIOS名称。如果不填，则默认会使用该服务器的DNS名称的第一部分。netbios name和workgroup名字不要设置成一样了。interfaces = lo eth0 192.168.12.2/24 192.168.13.2/24说明：设置Samba Server监听哪些网卡，可以写网卡名，也可以写该网卡的IP地址。hosts allow = 127.192.168.1 192.168.10.1说明：表示允许连接到Samba Server的客户端，多个参数以空格隔开。可以用一个IP表示，也可以用一个网段表示。hosts deny 与hosts allow 刚好相反。例如：# 表示容许来自172.17.2.*.*的主机连接，但排除172.17.2.50hosts allow=172.17.2.EXCEPT172.17.2.50# 表示容许来自172.17.2.0/255.255.0.0子网中的所有主机连接hosts allow=172.17.2.0/255.255.0.0# 表示容许来自M1和M2两台计算机连接hosts allow=M1，M2# 表示容许来自SC域的所有计算机连接hosts allow=@SCmax connections = 0说明：max connections用来指定连接Samba Server的最大连接数目。如果超出连接数目，则新的连接请求将被拒绝。0表示不限制。deadtime = 0说明：deadtime用来设置断掉一个没有打开任何文件的连接的时间。单位是分钟，0代表Samba Server不自动切断任何连接。time server = yes/no说明：time server用来设置让nmdb成为windows客户端的时间服务器。log file = /var/log/samba/log.%m说明：设置Samba Server日志文件的存储位置以及日志文件名称。在文件名后加个宏%m（主机名），表示对每台访问Samba Server的机器都单独记录一个日志文件。如果pc1、pc2访问过Samba Server，就会在/var/log/samba目录下留下log.pc1和log.pc2两个日志文件。max log size = 50说明：设置Samba Server日志文件的最大容量，单位为kB，0代表不限制。security = user说明：设置用户访问Samba Server的验证方式，一共有四种验证方式。1. share：用户访问Samba Server不需要提供用户名和口令, 安全性能较低。2. user：Samba Server共享目录只能被授权的用户访问,由Samba Server负责检查账号和密码的正确性。账号和密码要在本Samba Server中建立。3. server：依靠其他Windows NT/2000或Samba Server来验证用户的账号和密码,是一种代理验证。此种安全模式下,系统管理员可以把所有的Windows用户和口令集中到一个NT系统上,使用Windows NT进行Samba认证, 远程服务器可以自动认证全部用户和口令,如果认证失败,Samba将使用用户级安全模式作为替代的方式。4. domain：域安全级别,使用主域控制器(PDC)来完成认证。passdb backend = tdbsam说明：passdb backend就是用户后台的意思。目前有三种后台：smbpasswd、tdbsam和ldapsam。sam应该是security account manager（安全账户管理）的简写。smbpasswd：该方式是使用smb自己的工具smbpasswd来给系统用户（真实用户或者虚拟用户）设置一个Samba密码，客户端就用这个密码来访问Samba的资源。1. smbpasswd文件默认在/etc/samba目录下，不过有时候要手工建立该文件。2. tdbsam：该方式则是使用一个数据库文件来建立用户数据库。数据库文件叫passdb.tdb，默认在/etc/samba目录下。passdb.tdb用户数据库可以使用smbpasswd –a来建立Samba用户，不过要建立的Samba用户必须先是系统用户。我们也可以使用pdbedit命令来建立Samba账户。pdbedit命令的参数很多，我们列出几个主要的。 pdbedit –a username：新建Samba账户。 pdbedit –x username：删除Samba账户。 pdbedit –L：列出Samba用户列表，读取passdb.tdb数据库文件。 pdbedit –Lv：列出Samba用户列表的详细信息。 pdbedit –c “[D]” –u username：暂停该Samba用户的账号。 pdbedit –c “[]” –u username：恢复该Samba用户的账号。3. ldapsam：该方式则是基于LDAP的账户管理方式来验证用户。首先要建立LDAP服务，然后设置“passdb backend = ldapsam:ldap://LDAP Server”encrypt passwords = yes/no说明：是否将认证密码加密。因为现在windows操作系统都是使用加密密码，所以一般要开启此项。不过配置文件默认已开启。smb passwd file = /etc/samba/smbpasswd说明：用来定义samba用户的密码文件。smbpasswd文件如果没有那就要手工新建。username map = /etc/samba/smbusers说明：用来定义用户名映射，比如可以将root换成administrator、admin等。不过要事先在smbusers文件中定义好。比如：root = administrator admin，这样就可以用administrator或admin这两个用户来代替root登陆Samba Server，更贴近windows用户的习惯。guest account = nobody说明：用来设置guest用户名。socket options = TCP_NODELAY SO_RCVBUF=8192 SO_SNDBUF=8192说明：用来设置服务器和客户端之间会话的Socket选项，可以优化传输速度。domain master = yes/no说明：设置Samba服务器是否要成为网域主浏览器，网域主浏览器可以管理跨子网域的浏览服务。local master = yes/no说明：local master用来指定Samba Server是否试图成为本地网域主浏览器。如果设为no，则永远不会成为本地网域主浏览器。但是即使设置为yes，也不等于该Samba Server就能成为主浏览器，还需要参加选举。preferred master = yes/no说明：设置Samba Server一开机就强迫进行主浏览器选举，可以提高Samba Server成为本地网域主浏览器的机会。如果该参数指定为yes时，最好把domain master也指定为yes。使用该参数时要注意：如果在本Samba Server所在的子网有其他的机器（不论是windows NT还是其他Samba Server）也指定为首要主浏览器时，那么这些机器将会因为争夺主浏览器而在网络上大发广播，影响网络性能。如果同一个区域内有多台Samba Server，将上面三个参数设定在一台即可。os level = 200说明：设置samba服务器的os level。该参数决定Samba Server是否有机会成为本地网域的主浏览器。os level从0到255，winNT的os level是32，win95/98的os level是1。Windows 2000的os level是64。如果设置为0，则意味着Samba Server将失去浏览选择。如果想让Samba Server成为PDC，那么将它的os level值设大些。domain logons = yes/no说明：设置Samba Server是否要做为本地域控制器。主域控制器和备份域控制器都需要开启此项。logon . = %u.bat说明：当使用者用windows客户端登陆，那么Samba将提供一个登陆档。如果设置成%u.bat，那么就要为每个用户提供一个登陆档。如果人比较多，那就比较麻烦。可以设置成一个具体的文件名，比如start.bat，那么用户登陆后都会去执行start.bat，而不用为每个用户设定一个登陆档了。这个文件要放置在[netlogon]的path设置的目录路径下。wins support = yes/no说明：设置samba服务器是否提供wins服务。wins server = wins服务器IP地址说明：设置Samba Server是否使用别的wins服务器提供wins服务。wins proxy = yes/no说明：设置Samba Server是否开启wins代理服务。dns proxy = yes/no说明：设置Samba Server是否开启dns代理服务。load printers = yes/no说明：设置是否在启动Samba时就共享打印机。printcap name = cups说明：设置共享打印机的配置文件。printing = cups说明：设置Samba共享打印机的类型。现在支持的打印系统有：bsd, sysv, plp, lprng, aix, hpux, qnx 2.3. 共享参数 [共享名] [共享名]comment = 任意字符串说明：comment是对该共享的描述，可以是任意字符串。path = 共享目录路径说明：path用来指定共享目录的路径。可以用%u、%m这样的宏来代替路径里的unix用户和客户机的Netbios名，用宏表示主要用于[homes]共享域。例如：如果我们不打算用home段做为客户的共享，而是在/home/share/下为每个Linux用户以他的用户名建个目录，作为他的共享目录，这样path就可以写成：path = /home/share/%u; 。用户在连接到这共享时具体的路径会被他的用户名代替，要注意这个用户名路径一定要存在，否则，客户机在访问时会找不到网络路径。同样，如果我们不是以用户来划分目录，而是以客户机来划分目录，为网络上每台可以访问samba的机器都各自建个以它的netbios名的路径，作为不同机器的共享资源，就可以这样写：path = /home/share/%m 。browseable = yes/no说明：browseable用来指定该共享是否可以浏览。writable = yes/no说明：writable用来指定该共享路径是否可写。available = yes/no说明：available用来指定该共享资源是否可用。admin users = 该共享的管理者说明：admin users用来指定该共享的管理员（对该共享具有完全控制权限）。在samba 3.0中，如果用户验证方式设置成“security=share”时，此项无效。例如：admin users =bobyuan，jane（多个用户中间用逗号隔开）。valid users = 允许访问该共享的用户说明：valid users用来指定允许访问该共享资源的用户。例如：valid users = bobyuan，@bob，@tech（多个用户或者组中间用逗号隔开，如果要加入一个组就用“@+组名”表示。）invalid users = 禁止访问该共享的用户说明：invalid users用来指定不允许访问该共享资源的用户。例如：invalid users = root，@bob（多个用户或者组中间用逗号隔开。）write list = 允许写入该共享的用户说明：write list用来指定可以在该共享下写入文件的用户。例如：write list = bobyuan，@bobpublic = yes/no说明：public用来指定该共享是否允许guest账户访问。guest ok = yes/no说明：意义同“public”。几个特殊共享：[homes]comment = Home Directoriesbrowseable = nowritable = yesvalid users = %S; valid users = MYDOMAIN\%S[printers]comment = All Printerspath = /var/spool/sambabrowseable = noguest ok = nowritable = noprintable = yes[netlogon]comment = Network Logon Servicepath = /var/lib/samba/netlogonguest ok = yeswritable = noshare modes = no[Profiles]path = /var/lib/samba/profilesbrowseable = noguest ok = yes 3. 常见问题 3.1. 你可能没有权限访问网络资源 问题现象： 出现 NT_STATUS_ACCESS_DENIED 错误 Windows 下成功登陆 samba 后，点击共享目录仍然提示——你可能没有权限访问网络资源。 解决步骤： 检查是否配置了防火墙规则 # 一种方法是强行关闭防火墙$ sudo service iptables stop# 另一种方法是配置防火墙规则$ sudo firewall-cmd --permanent --zone=public --add-service=samba$ sudo firewall-cmd --reload 关闭 selinux # 将 /etc/selinux/config 文件中的 SELINUX 设为 disabled$ sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config# 重启生效$ reboot 3.2. window 下对 samba 的清理操作 windows 清除访问 samba 局域网密码缓存 在 dos 窗口中输入 control userpasswords2 或者 control keymgr.dll，然后【高级】/【密码管理】，删掉保存的该机器密码。 windows 清除连接的 linux 的 samba 服务缓存 打开 win 的命令行。 输入 net use，就会打印出当前缓存的连接上列表。 根据列表，一个个删除连接： net use 远程连接名称 /del；或者一次性全部删除：net use * /del。 4. 参考资料 http://blog.51cto.com/yuanbin/115761 https://www.jianshu.com/p/750be209a6f0 https://github.com/judasn/Linux-Tutorial/blob/master/markdown-file/Samba.md https://blog.csdn.net/lan120576664/article/details/50396511]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看 Linux 命令帮助信息]]></title>
    <url>%2Fblog%2F2018%2F09%2F26%2Fos%2Flinux%2Fcli%2F01.%E6%9F%A5%E7%9C%8BLinux%E5%91%BD%E4%BB%A4%E5%B8%AE%E5%8A%A9%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[查看 Linux 命令帮助信息 Linux 中有非常多的命令，想全部背下来是很困难的事。所以，我认为学习 Linux 的第一步，就是了解如何快速检索命令说明。 关键词：help, whatis, info, which, whereis, man 查看 Linux 命令帮助信息的要点 命令常见用法 help whatis info which whereis man 参考资料 查看 Linux 命令帮助信息的要点 查看 Shell 内部命令的帮助信息 - 使用 help 查看命令的简要说明 - 使用 whatis 查看命令的详细说明 - 使用 info 查看命令的位置 - 使用 which 定位指令的二进制程序、源代码文件和 man 手册页等相关文件的路径 - 使用 whereis 查看命令的帮助手册（包含说明、用法等信息） - 使用 man 只记得部分命令关键字 - 使用 man -k 注：推荐一些 Linux 命令中文手册： Linux 命令大全 linux-command 命令常见用法 help help 命令用于查看 Shell 内部命令的帮助信息。而对于外部命令的帮助信息只能使用 man 或者 info 命令查看。 参考：http://man.linuxde.net/help whatis whatis 用于查询一个命令执行什么功能。 参考：http://man.linuxde.net/whatis 示例： # 查看 man 命令的简要说明$ whatis man# 查看以 loca 开拓的命令的简要说明$ whatis -w "loca*" info info 是 Linux 下 info 格式的帮助指令。 参考：http://man.linuxde.net/info 示例： # 查看 man 命令的详细说明$ info man which which 命令用于查找并显示给定命令的绝对路径，环境变量 PATH 中保存了查找命令时需要遍历的目录。which 指令会在环境变量$PATH 设置的目录里查找符合条件的文件。也就是说，使用 which 命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 参考：http://man.linuxde.net/which 示例： which pwd # 查找命令的路径 说明：which 是根据使用者所配置的 PATH 变量内的目录去搜寻可运行档的！所以，不同的 PATH 配置内容所找到的命令当然不一样的！ [root@localhost ~]# which cdcd: shell built-in command cd 这个常用的命令竟然找不到啊！为什么呢？这是因为 cd 是 bash 内建的命令！但是 which 默认是找 PATH 内所规范的目录，所以当然一定找不到的！ whereis whereis 命令用来定位指令的二进制程序、源代码文件和 man 手册页等相关文件的路径。 whereis 命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man 说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 参考：http://man.linuxde.net/whereis 示例： whereis git # 将相关的文件都查找出来 man man 命令是 Linux 下的帮助指令，通过 man 指令可以查看 Linux 中的指令帮助、配置文件帮助和编程帮助等信息。 参考：http://man.linuxde.net/man 示例： $ man date # 查看 date 命令的帮助手册$ man 3 printf # 查看 printf 命令的帮助手册中的第 3 类$ man -k keyword # 根据命令中部分关键字来查询命令 man 要点 在 man 的帮助手册中，可以使用 page up 和 page down 来上下翻页。 man 的帮助手册中，将帮助文档分为了 9 个类别，对于有的关键字可能存在多个类别中， 我们就需要指定特定的类别来查看；（一般我们查询 bash 命令，归类在 1 类中）。 man 页面的分类(常用的是分类 1 和分类 3)： 可执行程序或 shell 命令 系统调用(内核提供的函数) 库调用(程序库中的函数) 特殊文件(通常位于 /dev) 文件格式和规范，如 /etc/passwd 游戏 杂项(包括宏包和规范，如 man(7)，groff(7)) 系统管理命令(通常只针对 root 用户) 内核例程 [非标准] 前面说到使用 whatis 会显示命令所在的具体的文档类别，我们学习如何使用它 $ whatis printfprintf (1) - format and print dataprintf (1p) - write formatted outputprintf (3) - formatted output conversionprintf (3p) - print formatted outputprintf [builtins](1) - bash built-in commands, see bash(1) 我们看到 printf 在分类 1 和分类 3 中都有；分类 1 中的页面是命令操作及可执行文件的帮助；而 3 是常用函数库说明；如果我们想看的是 C 语言中 printf 的用法，可以指定查看分类 3 的帮助： $ man 3 printf 参考资料 https://linuxtools-rst.readthedocs.io/zh_CN/latest/base/01_use_man.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop]]></title>
    <url>%2Fblog%2F2018%2F09%2F04%2Fbigdata%2FSqoop%2F</url>
    <content type="text"><![CDATA[Sqoop Sqoop 是一个主要在 Hadoop 和关系数据库之间进行批量数据迁移的工具。 Sqoop 简介 提供多种 Sqoop 连接器 Sqoop 版本 Sqoop 原理 导入 导出 Sqoop 简介 Sqoop 是一个主要在 Hadoop 和关系数据库之间进行批量数据迁移的工具。 Hadoop：HDFS、Hive、HBase、Inceptor、Hyperbase 面向大数据集的批量导入导出 将输入数据集分为 N 个切片，然后启动 N 个 Map 任务并行传输 支持全量、增量两种传输方式 提供多种 Sqoop 连接器 内置连接器 经过优化的专用 RDBMS 连接器：MySQL、PostgreSQL、Oracle、DB2、SQL Server、Netzza 等 通用的 JDBC 连接器：支持 JDBC 协议的数据库 第三方连接器 数据仓库：Teradata NoSQL 数据库：Couchbase Sqoop 版本 Sqoop 1 优缺点 优点 架构简单 部署简单 功能全面 稳定性较高 速度较快 缺点 访问方式单一 命令行方式容易出错，格式紧耦合 安全机制不够完善，存在密码泄露风险 Sqoop 2 优缺点 优点 访问方式多样 集中管理连接器 安全机制较完善 支持多用户 缺点 架构较复杂 部署较繁琐 稳定性一般 速度一般 Sqoop 原理 导入 导出]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume]]></title>
    <url>%2Fblog%2F2018%2F09%2F04%2Fbigdata%2FFlume%2F</url>
    <content type="text"><![CDATA[Flume Sqoop 是一个主要在 Hadoop 和关系数据库之间进行批量数据迁移的工具。 Flume 简介 什么是 Flume ？ 应用场景 Flume 原理 Flume 基本概念 Flume 基本组件 Flume 数据流 资源 Flume 简介 什么是 Flume ？ Flume 是一个分布式海量数据采集、聚合和传输系统。 特点 基于事件的海量数据采集 数据流模型：Source -&gt; Channel -&gt; Sink 事务机制：支持重读重写，保证消息传递的可靠性 内置丰富插件：轻松与各种外部系统集成 高可用：Agent 主备切换 Java 实现：开源，优秀的系统设计 应用场景 Flume 原理 Flume 基本概念 Event：事件，最小数据传输单元，由 Header 和 Body 组成。 Agent：代理，JVM 进程，最小运行单元，由 Source、Channel、Sink 三个基本组件构成，负责将外部数据源产生的数据以 Event 的形式传输到目的地 Source：负责对接各种外部数据源，将采集到的数据封装成 Event，然后写入 Channel Channel：Event 暂存容器，负责保存 Source 发送的 Event，直至被 Sink 成功读取 Sink：负责从 Channel 读取 Event，然后将其写入外部存储，或传输给下一阶段的 Agent 映射关系：1 个 Source -&gt; 多个 Channel，1 个 Channel -&gt; 多个 Sink，1 个 Sink -&gt; 1 个 Channel Flume 基本组件 Source 组件 对接各种外部数据源，将采集到的数据封装成 Event，然后写入 Channel 一个 Source 可向多个 Channel 发送 Event Flume 内置类型丰富的 Source，同时用户可自定义 Source Channel 组件 Event 中转暂存区，存储 Source 采集但未被 Sink 读取的 Event 为了平衡 Source 采集、Sink 读取的速度，可视为 Flume 内部的消息队列 线程安全并具有事务性，支持 Source 写失败重写和 Sink 读失败重读 Sink 组件 从 Channel 读取 Event，将其写入外部存储，或传输到下一阶段的 Agent 一个 Sink 只能从一个 Channel 中读取 Event Sink 成功读取 Event 后，向 Channel 提交事务，Event 被删除，否则 Channel 会等待 Sink 重新读取 Flume 数据流 单层架构 优点：架构简单，使用方便，占用资源较少 缺点 如果采集的数据源或Agent较多，将Event写入到HDFS会产生很多小文件 外部存储升级维护或发生故障，需对采集层的所有Agent做处理，人力成本较高，系统稳定性较差 系统安全性较差 数据源管理较混乱 资源]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce]]></title>
    <url>%2Fblog%2F2018%2F09%2F03%2Fbigdata%2FMapReduce%2F</url>
    <content type="text"><![CDATA[MapReduce MapReduce 简介 概念 思想 特点 适用场景 不适用场景 MapReduce 原理 Job &amp; Task（作业与任务） Split（切片） Map 阶段（映射） Reduce 阶段（化简） Shuffle 阶段（洗牌） Shuffle 详解 Map 端 Reduce 端 作业运行模式 JobTracker/TaskTracker 模式（Hadoop 1.X） YARN 模式（Hadoop 2.X） MapReduce 简介 概念 MapReduce 是一个面向批处理的分布式计算框架。 编程模型：MapReduce 程序被分为 Map（映射）阶段和 Reduce（化简）阶段。 思想 分而治之，并行计算 移动计算，而非移动数据 特点 计算跟着数据走 良好的扩展性：计算能力随着节点数增加，近似线性递增 高容错 状态监控 适合海量数据的离线批处理 降低了分布式编程的门槛 适用场景 数据统计，如：网站的 PV、UV 统计 搜索引擎构建索引 海量数据查询 不适用场景 OLAP 要求毫秒或秒级返回结果 流计算 流计算的输入数据集是动态的，而 MapReduce 是静态的 DAG 计算 多个作业存在依赖关系，后一个的输入是前一个的输出，构成有向无环图 DAG 每个 MapReduce 作业的输出结果都会落盘，造成大量磁盘 IO，导致性能非常低下 MapReduce 原理 Job &amp; Task（作业与任务） 作业是客户端请求执行的一个工作单元 包括输入数据、MapReduce 程序、配置信息 任务是将作业分解后得到的细分工作单元 分为 Map 任务和 Reduce 任务 Split（切片） 输入数据被划分成等长的小数据块，称为输入切片（Input Split），简称切片 Split 是逻辑概念，仅包含元数据信息，如：数据的起始位置、长度、所在节点等 每个 Split 交给一个 Map 任务处理，Split 的数量决定 Map 任务的数量 Split 的划分方式由程序设定，Split 与 HDFS Block 没有严格的对应关系 Split 的大小默认等于 Block 大小 Split 越小，负载越均衡，但集群的开销越大 Map 阶段（映射） 由若干 Map 任务组成，任务数量由 Split 数量决定 输入：Split 切片（key-value），输出：中间计算结果（key-value） Reduce 阶段（化简） 由若干 Reduce 任务组成，任务数量由程序指定 输入：Map 阶段输出的中间结果（key-value），输出：最终结果（key-value） Shuffle 阶段（洗牌） Map、Reduce 阶段的中间环节，负责执行 Partition（分区）、Sort（排序）、Spill（溢写）、Merge（合并）、抓取（Fetch）等工作 Partition 决定了 Map 任务输出的每条数据放入哪个分区，交给哪个 Reduce 任务处理 Reduce 任务的数量决定了 Partition 数量 Partition 编号 = Reduce 任务编号 =“key hashcode % reduce task number” 避免和减少 Shuffle 是 MapReduce 程序调优的重点 Shuffle 详解 Map 端 Map 任务将中间结果写入专用内存缓冲区 Buffer（默认 100M），同时进行 Partition 和 Sort（先按“key hashcode % reduce task number”对数据进行分区，分区内再按 key 排序） 当 Buffer 的数据量达到阈值（默认 80%）时，将数据溢写（Spill）到磁盘的一个临时文件中，文件内数据先分区后排序 Map 任务结束前，将多个临时文件合并（Merge）为一个 Map 输出文件，文件内数据先分区后排序 Reduce 端 Reduce 任务从多个 Map 输出文件中主动抓取（Fetch）属于自己的分区数据，先写入 Buffer，数据量达到阈值后，溢写到磁盘的一个临时文件中 数据抓取完成后，将多个临时文件合并为一个 Reduce 输入文件，文件内数据按 key 排序 作业运行模式 JobTracker/TaskTracker 模式（Hadoop 1.X） JobTracker 节点（Master） 调度任务在 TaskTracker 上运行 若任务失败，指定新 TaskTracker 重新运行 TaskTracker 节点（Slave） 执行任务，发送进度报告 存在的问题 JobTracker 存在单点故障 JobTracker 负载太重（上限 4000 节点） JobTracker 缺少对资源的全面管理 TaskTracker 对资源的描述过于简单 源码很难理解 YARN 模式（Hadoop 2.X） 提交作业 查看作业 终止作业]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark]]></title>
    <url>%2Fblog%2F2018%2F09%2F03%2Fbigdata%2FSpark%2F</url>
    <content type="text"><![CDATA[Spark Spark 简介 Spark 概念 Spark 特点 Spark 原理 编程模型 Spark 简介 Spark 概念 大规模分布式通用计算引擎 Spark Core：核心计算框架 Spark SQL：结构化数据查询 Spark Streaming：实时流处理 Spark MLib：机器学习 Spark GraphX：图计算 具有高吞吐、低延时、通用易扩展、高容错等特点 采用 Scala 语言开发 提供多种运行模式 Spark 特点 计算高效 利用内存计算、Cache 缓存机制，支持迭代计算和数据共享，减少数据读取的 IO 开销 利用 DAG 引擎，减少中间计算结果写入 HDFS 的开销 利用多线程池模型，减少任务启动开销，避免 Shuffle 中不必要的排序和磁盘 IO 操作 通用易用 适用于批处理、流处理、交互式计算、机器学习算法等场景 提供了丰富的开发 API，支持 Scala、Java、Python、R 等 运行模式多样 Local 模式 Standalone 模式 YARN/Mesos 模式 计算高效 利用内存计算、Cache 缓存机制，支持迭代计算和数据共享，减少数据读取的 IO 开销 利用 DAG 引擎，减少中间计算结果写入 HDFS 的开销 利用多线程池模型，减少任务启动开销，避免 Shuffle 中不必要的排序和磁盘 IO 操作 通用易用 适用于批处理、流处理、交互式计算、机器学习等场景 提供了丰富的开发 API，支持 Scala、Java、Python、R 等 Spark 原理 编程模型 RDD 弹性分布式数据集（Resilient Distributed Datesets） 分布在集群中的只读对象集合 由多个 Partition 组成 通过转换操作构造 失效后自动重构（弹性） 存储在内存或磁盘中 Spark 基于 RDD 进行计算 RDD 操作（Operator） Transformation（转换） 将 Scala 集合或 Hadoop 输入数据构造成一个新 RDD 通过已有的 RDD 产生新 RDD 惰性执行：只记录转换关系，不触发计算 例如：map、filter、flatmap、union、distinct、sortbykey Action（动作） 通过 RDD 计算得到一个值或一组值 真正触发计算 例如：first、count、collect、foreach、saveAsTextFile RDD 依赖（Dependency） 窄依赖（Narrow Dependency） 父 RDD 中的分区最多只能被一个子 RDD 的一个分区使用 子 RDD 如果有部分分区数据丢失或损坏，只需从对应的父 RDD 重新计算恢复 例如：map、filter、union 宽依赖（Shuffle/Wide Dependency ） 子 RDD 分区依赖父 RDD 的所有分区 子 RDD 如果部分或全部分区数据丢失或损坏，必须从所有父 RDD 分区重新计算 相对于窄依赖，宽依赖付出的代价要高很多，尽量避免使用 例如：groupByKey、reduceByKey、sortByKey]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS]]></title>
    <url>%2Fblog%2F2018%2F09%2F03%2Fbigdata%2FHDFS%2F</url>
    <content type="text"><![CDATA[HDFS HDFS 是 Hadoop 分布式文件系统。 关键词：分布式、文件系统 概述 HDFS 的特点 HDFS 的概念 NameNode DataNode Block 数据块 Client Block 副本策略 数据流 HDFS 读文件 HDFS 写文件 HDFS 安全模式 什么是安全模式？ 何时正常离开安全模式 触发安全模式的原因 故障排查 HDFS 高可用 NameNode 的 HA 机制 利用 QJM 实现元数据高可用 资源 概述 HDFS 是 Hadoop 的核心子项目。 HDFS 是 Hadoop Distributed File System 的缩写，即 Hadoop 分布式文件系统。 HDFS 是一种用于存储具有流数据访问模式的超大文件的文件系统，它运行在廉价的机器集群上。 HDFS 的特点 优点 高容错 - 数据冗余多副本，副本丢失后自动恢复 高可用 - NameNode HA、安全模式 高扩展 - 能够处理 10K 节点的规模；处理数据达到 GB、TB、甚至 PB 级别的数据；能够处理百万规模以上的文件数量，数量相当之大。 批处理 - 流式数据访问；数据位置暴露给计算框架 构建在廉价商用机器上 - 提供了容错和恢复机制 缺点 不适合低延迟数据访问 - 适合高吞吐率的场景，就是在某一时间内写入大量的数据。但是它在低延时的情况下是不行的，比如毫秒级以内读取数据，它是很难做到的。 不适合大量小文件存储 存储大量小文件(这里的小文件是指小于 HDFS 系统的 Block 大小的文件（默认 64M）)的话，它会占用 NameNode 大量的内存来存储文件、目录和块信息。这样是不可取的，因为 NameNode 的内存总是有限的。 磁盘寻道时间超过读取时间 不支持并发写入 - 一个文件同时只能有一个写入者 不支持文件随机修改 - 仅支持追加写入 HDFS 的概念 HDFS 采用 Master/Slave 架构。 一个 HDFS 集群是由一个 Namenode 和一定数目的 Datanodes 组成。Namenode 是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的 Datanode 一般是一个节点一个，负责管理它所在节点上的存储。HDFS 暴露了文件系统的名字空间，用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在一组 Datanode 上。Namenode 执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体 Datanode 节点的映射。Datanode 负责处理文件系统客户端的读写请求。在 Namenode 的统一调度下进行数据块的创建、删除和复制。 NameNode NameNode 就是 master 工作节点。 管理命名空间 管理元数据：文件的位置、所有者、权限、数据块等 管理 Block 副本策略：默认 3 个副本 处理客户端读写请求，为 DataNode 分配任务 Active NameNode 和 Standby NameNode NameNode 通过 HA 机制来容错。 Active NameNode - 是正在工作的 NameNode； Standby NameNode - 是备份的 NameNode。 Active NameNode 宕机后，Standby NameNode 快速升级为新的 Active NameNode。 Standby NameNode 周期性同步 edits 编辑日志，定期合并 fsimage 与 edits 到本地磁盘。 Hadoop 3.0 允许配置多个 Standby NameNode。 元数据文件 edits（编辑日志文件） - 保存了自最新检查点（Checkpoint）之后的所有文件更新操作。 fsimage（元数据检查点镜像文件） - 保存了文件系统中所有的目录和文件信息，如：某个目录下有哪些子目录和文件，以及文件名、文件副本数、文件由哪些 Block 组成等。 Active NameNode 内存中有一份最新的元数据（= fsimage + edits）。 Standby NameNode 在检查点定期将内存中的元数据保存到 fsimage 文件中。 DataNode **DataNode 就是 slave 工作节点。**NameNode 下达命令，DataNode 执行实际的操作。 存储 Block 和数据校验和 执行客户端发送的读写操作 通过心跳机制定期（默认 3 秒）向 NameNode 汇报运行状态和 Block 列表信息 集群启动时，DataNode 向 NameNode 提供 Block 列表信息 Block 数据块 HDFS 最小存储单元 文件写入 HDFS 会被切分成若干个 Block Block 大小固定，默认为 128MB，可自定义 若一个 Block 的大小小于设定值，不会占用整个块空间 默认情况下每个 Block 有 3 个副本 Client 将文件切分为 Block 数据块 与 NameNode 交互，获取文件元数据 与 DataNode 交互，读取或写入数据 管理 HDFS Block 副本策略 HDFS 被设计成能够在一个大集群中跨机器可靠地存储超大文件。它将每个文件存储成一系列的数据块，除了最后一个，所有的数据块都是同样大小的。为了容错，文件的所有数据块都会有副本。每个文件的数据块大小和副本系数都是可配置的。应用程序可以指定某个文件的副本数目。副本系数可以在文件创建的时候指定，也可以在之后改变。HDFS 中的文件都是一次性写入的，并且严格要求在任何时候只能有一个写入者。 Namenode 全权管理数据块的复制，它周期性地从集群中的每个 Datanode 接收心跳信号和块状态报告(Blockreport)。接收到心跳信号意味着该 Datanode 节点工作正常。块状态报告包含了一个该 Datanode 上所有数据块的列表。 副本 1：放在 Client 所在节点 对于远程 Client，系统会随机选择节点 副本 2：放在不同的机架节点上 副本 3：放在与第二个副本同一机架的不同节点上 副本 N：随机选择 节点选择：同等条件下优先选择空闲节点 数据流 HDFS 读文件 客户端调用 FileSyste 对象的 open() 方法在分布式文件系统中打开要读取的文件。 分布式文件系统通过使用 RPC（远程过程调用）来调用 namenode，确定文件起始块的位置。 分布式文件系统的 DistributedFileSystem 类返回一个支持文件定位的输入流 FSDataInputStream 对象，FSDataInputStream 对象接着封装 DFSInputStream 对象（存储着文件起始几个块的 datanode 地址），客户端对这个输入流调用 read()方法。 DFSInputStream 连接距离最近的 datanode，通过反复调用 read 方法，将数据从 datanode 传输到客户端。 到达块的末端时，DFSInputStream 关闭与该 datanode 的连接，寻找下一个块的最佳 datanode。 客户端完成读取，对 FSDataInputStream 调用 close()方法关闭连接。 HDFS 写文件 客户端通过对 DistributedFileSystem 对象调用 create() 函数来新建文件。 分布式文件系统对 namenod 创建一个 RPC 调用，在文件系统的命名空间中新建一个文件。 Namenode 对新建文件进行检查无误后，分布式文件系统返回给客户端一个 FSDataOutputStream 对象，FSDataOutputStream 对象封装一个 DFSoutPutstream 对象，负责处理 namenode 和 datanode 之间的通信，客户端开始写入数据。 FSDataOutputStream 将数据分成一个一个的数据包，写入内部队列“数据队列”，DataStreamer 负责将数据包依次流式传输到由一组 namenode 构成的管线中。 DFSOutputStream 维护着确认队列来等待 datanode 收到确认回执，收到管道中所有 datanode 确认后，数据包从确认队列删除。 客户端完成数据的写入，对数据流调用 close() 方法。 namenode 确认完成。 HDFS 安全模式 什么是安全模式？ 安全模式是 HDFS 的一种特殊状态，在这种状态下，HDFS 只接收读数据请求，而不接收写入、删除、修改等变更请求。 安全模式是 HDFS 确保 Block 数据安全的一种保护机制。 Active NameNode 启动时，HDFS 会进入安全模式，DataNode 主动向 NameNode 汇报可用 Block 列表等信息，在系统达到安全标准前，HDFS 一直处于“只读”状态。 何时正常离开安全模式 Block 上报率：DataNode 上报的可用 Block 个数 / NameNode 元数据记录的 Block 个数 当 Block 上报率 &gt;= 阈值时，HDFS 才能离开安全模式，默认阈值为 0.999 不建议手动强制退出安全模式 触发安全模式的原因 NameNode 重启 NameNode 磁盘空间不足 Block 上报率低于阈值 DataNode 无法正常启动 日志中出现严重异常 用户操作不当，如：强制关机（特别注意！） 故障排查 找到 DataNode 不能正常启动的原因，重启 DataNode 清理 NameNode 磁盘 谨慎操作，有问题找星环，以免丢失数据 HDFS 高可用 NameNode 的 HA 机制 Active NameNode 和 Standby NameNode 实现主备。 利用 QJM 实现元数据高可用 基于 Paxos 算法 QJM 机制（Quorum Journal Manager） 只要保证 Quorum（法定人数）数量的操作成功，就认为这是一次最终成功的操作 QJM 共享存储系统 部署奇数（2N+1）个 JournalNode JournalNode 负责存储 edits 编辑日志 写 edits 的时候，只要超过半数（N+1）的 JournalNode 返回成功，就代表本次写入成功 最多可容忍 N 个 JournalNode 宕机 利用 ZooKeeper 实现 Active 节点选举。 资源 HDFS 官方文档 HDFS 知识点总结 《Hadoop: The Definitive Guide, Fourth Edition》 by Tom White http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_design.html]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据简介]]></title>
    <url>%2Fblog%2F2018%2F09%2F03%2Fbigdata%2Foverview%2F</url>
    <content type="text"><![CDATA[大数据简介 简介 什么是大数据 应用场景 Hadoop 编年史 技术体系 HDFS MapReduce Spark YARN Hive HBase ElasticSearch 术语 资源 简介 什么是大数据 大数据是指超出传统数据库工具收集、存储、管理和分析能力的数据集。与此同时，及时采集、存储、聚合、管理数据，以及对数据深度分析的新技术和新能力，正在快速增长，就像预测计算芯片增长速度的摩尔定律一样。 Volume - 数据规模巨大 Velocity - 生成和处理速度极快 Variety - 数据规模巨大 Value - 生成和处理速度极快 应用场景 基于大数据的数据仓库 基于大数据的实时流处理 Hadoop 编年史 时间 事件 2003.01 Google发表了Google File System论文 2004.01 Google发表了MapReduce论文 2006.02 Apache Hadoop项目正式启动，并支持MapReduce和HDFS独立发展 2006.11 Google发表了Bigtable论文 2008.01 Hadoop成为Apache顶级项目 2009.03 Cloudera推出世界上首个Hadoop发行版——CDH，并完全开放源码 2012.03 HDFS NameNode HA加入Hadoop主版本 2014.02 Spark代替MapReduce成为Hadoop的缺省计算引擎，并成为Apache顶级项目 技术体系 HDFS 概念 Hadoop 分布式文件系统（Hadoop Distributed File System） 在开源大数据技术体系中，地位无可替代 特点 高容错：数据多副本，副本丢失后自动恢复 高可用：NameNode HA，安全模式 高扩展：10K 节点规模 简单一致性模型：一次写入多次读取，支持追加，不允许修改 流式数据访问：批量读而非随机读，关注吞吐量而非时间 大规模数据集：典型文件大小 GB~TB 级，百万以上文件数量， PB 以上数据规模 构建成本低且安全可靠：运行在大量的廉价商用机器上，硬件错误是常态，提供容错机制 MapReduce 概念 面向批处理的分布式计算框架 编程模型：将 MapReduce 程序分为 Map、Reduce 两个阶段 核心思想 分而治之，分布式计算 移动计算，而非移动数据 特点 高容错：任务失败，自动调度到其他节点重新执行 高扩展：计算能力随着节点数增加，近似线性递增 适用于海量数据的离线批处理 降低了分布式编程的门槛 Spark 高性能分布式通用计算引擎 Spark Core - 基础计算框架（批处理、交互式分析） Spark SQL - SQL 引擎（海量结构化数据的高性能查询） Spark Streaming - 实时流处理（微批） Spark MLlib - 机器学习 Spark GraphX - 图计算 采用 Scala 语言开发 特点 计算高效 - 内存计算、Cache 缓存机制、DAG 引擎、多线程池模型 通用易用 - 适用于批处理、交互式计算、流处理、机器学习、图计算等多种场景 运行模式多样 - Local、Standalone、YARN/Mesos YARN 概念 Yet Another Resource Negotiator，另一种资源管理器 为了解决 Hadoop 1.x 中 MapReduce 的先天缺陷 分布式通用资源管理系统 负责集群资源的统一管理 从 Hadoop 2.x 开始，YARN 成为 Hadoop 的核心组件 特点 专注于资源管理和作业调度 通用 - 适用各种计算框架，如 - MapReduce、Spark 高可用 - ResourceManager 高可用、HDFS 高可用 高扩展 Hive 概念 Hadoop 数据仓库 - 企业决策支持 SQL 引擎 - 对海量结构化数据进行高性能的 SQL 查询 采用 HDFS 或 HBase 为数据存储 采用 MapReduce 或 Spark 为计算框架 特点 提供类 SQL 查询语言 支持命令行或 JDBC/ODBC 提供灵活的扩展性 提供复杂数据类型、扩展函数、脚本等 HBase 概念 Hadoop Database Google BigTable 的开源实现 分布式 NoSQL 数据库 列式存储 - 主要用于半结构化、非结构化数据 采用 HDFS 为文件存储系统 特点 高性能 - 支持高并发写入和查询 高可用 - HDFS 高可用、Region 高可用 高扩展 - 数据自动切分和分布，可动态扩容，无需停机 海量存储 - 单表可容纳数十亿行，上百万列 ElasticSearch 开源的分布式全文检索引擎 基于 Lucene 实现全文数据的快速存储、搜索和分析 处理大规模数据 - PB 级以上 具有较强的扩展性，集群规模可达上百台 首选的分布式搜索引擎 术语 数据仓库（Data Warehouse） - 数据仓库，是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。它是单个数据存储，出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制。 资源 awesome-bigdata Hadoop HBase Hive Impala Flume Kafka Spark Sqoop ElasticSearch]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN]]></title>
    <url>%2Fblog%2F2018%2F09%2F03%2Fbigdata%2FYARN%2F</url>
    <content type="text"><![CDATA[YARN YARN 的目标是解决 MapReduce 的缺陷。 MapReduce 的缺陷（Hadoop 1.x） YARN 简介 YARN 系统架构 ResourceManager（RM） NodeManager（NM） ApplicationMaster（AM） Container YARN 高可用 YARN 资源调度策略 FIFO Scheduler（先进先出调度器） Capacity Scheduler（容量调度器） Fair Scheduler（公平调度器） 资源 MapReduce 的缺陷（Hadoop 1.x） 身兼两职：计算框架 + 资源管理框架 JobTracker 既做资源管理，又做任务调度 任务太重，开销过大 存在单点故障 资源描述模型过于简单，资源利用率较低 仅把 Task 数量看作资源，没有考虑 CPU 和内存 强制把资源分成 Map Task Slot 和 Reduce Task Slot 扩展性较差，集群规模上限 4K 源码难于理解，升级维护困难 YARN 简介 YARN(Yet Another Resource Negotiator，另一种资源管理器)是一个分布式通用资源管理系统。 设计目标：聚焦资源管理、通用（适用各种计算框架）、高可用、高扩展。 YARN 系统架构 主从结构（master/slave） 将 JobTracker 的资源管理、任务调度功能分离 三种角色： ResourceManager（Master） - 集群资源的统一管理和分配 NodeManager（Slave） - 管理节点资源，以及容器的生命周期 ApplicationMaster（新角色） - 管理应用程序实例，包括任务调度和资源申请 ResourceManager（RM） 主要功能 统一管理集群的所有资源 将资源按照一定策略分配给各个应用（ApplicationMaster） 接收 NodeManager 的资源上报信息 核心组件 用户交互服务（User Service） NodeManager 管理 ApplicationMaster 管理 Application 管理 安全管理 资源管理 NodeManager（NM） 主要功能 管理单个节点的资源 向 ResourceManager 汇报节点资源使用情况 管理 Container 的生命周期 核心组件 NodeStatusUpdater ContainerManager ContainerExecutor NodeHealthCheckerService Security WebServer ApplicationMaster（AM） 主要功能 管理应用程序实例 向 ResourceManager 申请任务执行所需的资源 任务调度和监管 实现方式 需要为每个应用开发一个 AM 组件 YARN 提供 MapReduce 的 ApplicationMaster 实现 采用基于事件驱动的异步编程模型，由中央事件调度器统一管理所有事件 每种组件都是一种事件处理器，在中央事件调度器中注册 Container 概念：Container 封装了节点上进程的相关资源，是 YARN 中资源的抽象 分类：运行 ApplicationMaster 的 Container 、运行应用任务的 Container YARN 高可用 ResourceManager 高可用 1 个 Active RM、多个 Standby RM 宕机后自动实现主备切换 ZooKeeper 的核心作用 Active 节点选举 恢复 Active RM 的原有状态信息 重启 AM，杀死所有运行中的 Container 切换方式：手动、自动 YARN 资源调度策略 FIFO Scheduler（先进先出调度器） 调度策略 将所有任务放入一个队列，先进队列的先获得资源，排在后面的任务只有等待 缺点 资源利用率低，无法交叉运行任务 灵活性差，如：紧急任务无法插队，耗时长的任务拖慢耗时短的任务 Capacity Scheduler（容量调度器） 核心思想 - 提前做预算，在预算指导下分享集群资源。 调度策略 集群资源由多个队列分享 每个队列都要预设资源分配的比例（提前做预算） 空闲资源优先分配给“实际资源/预算资源”比值最低的队列 队列内部采用 FIFO 调度策略 特点 层次化的队列设计：子队列可使用父队列资源 容量保证：每个队列都要预设资源占比，防止资源独占 弹性分配：空闲资源可以分配给任何队列，当多个队列争用时，会按比例进行平衡 支持动态管理：可以动态调整队列的容量、权限等参数，也可动态增加、暂停队列 访问控制：用户只能向自己的队列中提交任务，不能访问其他队列 多租户：多用户共享集群资源 Fair Scheduler（公平调度器） 调度策略 多队列公平共享集群资源 通过平分的方式，动态分配资源，无需预先设定资源分配比例 队列内部可配置调度策略：FIFO、Fair（默认） 资源抢占 终止其他队列的任务，使其让出所占资源，然后将资源分配给占用资源量少于最小资源量限制的队列 队列权重 当队列中有任务等待，并且集群中有空闲资源时，每个队列可以根据权重获得不同比例的空闲资源 资源]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[H2 数据库]]></title>
    <url>%2Fblog%2F2018%2F08%2F22%2Fdatabase%2Fsql%2Fh2%2F</url>
    <content type="text"><![CDATA[H2 数据库 概述 使用说明 Spring 整合 H2 H2 SQL 数据类型 集群 参考资料 概述 H2 是一个开源的嵌入式数据库引擎，采用 java 语言编写，不受平台的限制。同时 H2 提供了一个十分方便的 web 控制台用于操作和管理数据库内容。H2 还提供兼容模式，可以兼容一些主流的数据库，因此采用 H2 作为开发期的数据库非常方便。 使用说明 H2 控制台应用 H2 允许用户通过浏览器接口方式访问 SQL 数据库。 进入官方下载地址，选择合适版本，下载并安装到本地。 启动方式：在 bin 目录下，双击 jar 包；执行 java -jar h2*.jar；执行脚本：h2.bat 或 h2.sh。 在浏览器中访问：http://localhost:8082，应该可以看到下图中的页面： 点击 Connect ，可以进入操作界面： 操作界面十分简单，不一一细说。 嵌入式应用 JDBC API Connection conn = DriverManager. getConnection("jdbc:h2:~/test");conn.close(); 详见：Using the JDBC API 连接池 import org.h2.jdbcx.JdbcConnectionPool;JdbcConnectionPool cp = JdbcConnectionPool.create("jdbc:h2:~/test", "sa", "sa");Connection conn = cp.getConnection();conn.close(); cp.dispose(); 详见：Connection Pool Maven &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;version&gt;1.4.197&lt;/version&gt;&lt;/dependency&gt; 详见：Maven 2 Hibernate hibernate.cfg.xml (or use the HSQLDialect): &lt;property name="dialect"&gt; org.hibernate.dialect.H2Dialect&lt;/property&gt; 详见：Hibernate TopLink 和 Glassfish Datasource class: org.h2.jdbcx.JdbcDataSource oracle.toplink.essentials.platform.database.H2Platform 详见：TopLink and Glassfish 运行方式 嵌入式 数据库持久化存储为单个文件。 连接字符串：\~/.h2/DBName 表示数据库文件的存储位置，如果第一次连接则会自动创建数据库。 jdbc:h2:\~/test - ‘test’ 在用户根目录下 jdbc:h2:/data/test - ‘test’ 在 /data 目录下 jdbc:h2:test - ‘test’ 在当前工作目录 内存式 数据库只在内存中运行，关闭连接后数据库将被清空，适合测试环境 连接字符串：jdbc:h2:mem:DBName;DB_CLOSE_DELAY=-1 如果不指定 DBName，则以私有方式启动，只允许一个连接。 jdbc:h2:mem:test - 一个进程中有多个连接 jdbc:h2:mem: - 未命名的私有库，一个连接 服务模式 H2 支持三种服务模式： web server：此种运行方式支持使用浏览器访问 H2 Console TCP server：支持客户端/服务器端的连接方式 PG server：支持 PostgreSQL 客户端 启动 tcp 服务连接字符串示例： jdbc:h2:tcp://localhost/\~/test - 用户根目录 jdbc:h2:tcp://localhost//data/test - 绝对路径 启动服务 执行 java -cp *.jar org.h2.tools.Server 执行如下命令，获取选项列表及默认值 java -cp h2*.jar org.h2.tools.Server -? 常见的选项如下： -web：启动支持 H2 Console 的服务 -webPort ：服务启动端口，默认为 8082 -browser：启动 H2 Console web 管理页面 -tcp：使用 TCP server 模式启动 -pg：使用 PG server 模式启动 设置 jdbc:h2:..;MODE=MySQL 兼容模式（或 HSQLDB 等） jdbc:h2:..;TRACE_LEVEL_FILE=3 记录到 *.trace.db 连接字符串参数 DB_CLOSE_DELAY - 要求最后一个正在连接的连接断开后，不要关闭数据库 MODE=MySQL - 兼容模式，H2 兼容多种数据库，该值可以为：DB2、Derby、HSQLDB、MSSQLServer、MySQL、Oracle、PostgreSQL AUTO_RECONNECT=TRUE - 连接丢失后自动重新连接 AUTO_SERVER=TRUE - 启动自动混合模式，允许开启多个连接，该参数不支持在内存中运行模式 TRACE_LEVEL_SYSTEM_OUT、TRACE_LEVEL_FILE - 输出跟踪日志到控制台或文件， 取值 0 为 OFF，1 为 ERROR（默认值），2 为 INFO，3 为 DEBUG SET TRACE_MAX_FILE_SIZE mb - 设置跟踪日志文件的大小，默认为 16M maven 方式 此外，使用 maven 也可以启动 H2 服务。添加以下插件 &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;java&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;mainClass&gt;org.h2.tools.Server&lt;/mainClass&gt; &lt;arguments&gt; &lt;argument&gt;-web&lt;/argument&gt; &lt;argument&gt;-webPort&lt;/argument&gt; &lt;argument&gt;8090&lt;/argument&gt; &lt;argument&gt;-browser&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt;&lt;/plugin&gt; 在命令行中执行如下命令启动 H2 Console mvn exec:java 或者建立一个 bat 文件 @echo offcall mvn exec:javapause 此操作相当于执行了如下命令： java -jar h2-1.3.168.jar -web -webPort 8090 -browser Spring 整合 H2 添加依赖 &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;version&gt;1.4.194&lt;/version&gt;&lt;/dependency&gt; spring 配置 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:jdbc="http://www.springframework.org/schema/jdbc" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.2.xsd http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc.xsd"&gt; &lt;!--配置数据源--&gt; &lt;bean id="dataSource" class="org.h2.jdbcx.JdbcConnectionPool" destroy-method="dispose"&gt; &lt;constructor-arg&gt; &lt;bean class="org.h2.jdbcx.JdbcDataSource"&gt; &lt;!-- 内存模式 --&gt; &lt;property name="URL" value="jdbc:h2:mem:test"/&gt; &lt;!-- 文件模式 --&gt; &lt;!-- &lt;property name="URL" value="jdbc:h2:testRestDB" /&gt; --&gt; &lt;property name="user" value="root"/&gt; &lt;property name="password" value="root"/&gt; &lt;/bean&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!-- JDBC模板 --&gt; &lt;bean id="jdbcTemplate" class="org.springframework.jdbc.core.JdbcTemplate"&gt; &lt;constructor-arg ref="dataSource"/&gt; &lt;/bean&gt; &lt;bean id="myJdbcTemplate" class="org.zp.notes.spring.jdbc.MyJdbcTemplate"&gt; &lt;property name="jdbcTemplate" ref="jdbcTemplate"/&gt; &lt;/bean&gt; &lt;!-- 初始化数据表结构 --&gt; &lt;jdbc:initialize-database data-source="dataSource" ignore-failures="ALL"&gt; &lt;jdbc:script location="classpath:sql/h2/create_table_student.sql"/&gt; &lt;/jdbc:initialize-database&gt;&lt;/beans&gt; H2 SQL SELECT INSERT UPDATE DELETE BACKUP EXPLAIN 7、MERGE RUNSCRIPT 运行 sql 脚本文件 SCRIPT 根据数据库创建 sql 脚本 SHOW ALTER ALTER INDEX RENAME ALTER SCHEMA RENAME ALTER SEQUENCE ALTER TABLE 增加约束 修改列 删除列 删除序列 ALTER USER 修改用户名 修改用户密码 ALTER VIEW COMMENT CREATE CONSTANT CREATE INDEX CREATE ROLE CREATE SCHEMA CREATE SEQUENCE CREATE TABLE CREATE TRIGGER CREATE USER CREATE VIEW DROP GRANT RIGHT 给 schema 授权授权 给 schema 授权给 schema 授权 复制角色的权限 REVOKE RIGHT 移除授权 移除角色具有的权限 ROLLBACK 从某个还原点（savepoint）回滚 回滚事务 创建 savepoint 数据类型 INT Type 集群 H2 支持两台服务器运行两个数据库成为集群，两个数据库互为备份，如果一个服务器失效，另一个服务器仍然可以工作。另外只有服务模式支持集群配置。 H2 可以通过 CreateCluster 工具创建集群，示例步骤如下（在在一台服务器上模拟两个数据库组成集群）： 创建目录 创建两个服务器工作的目录 启动 tcp 服务 执行如下命令分别在 9101、9102 端口启动两个使用 tcp 服务模式的数据库 使用 CreateCluster 工具创建集群 如果两个数据库不存在，该命令将会自动创建数据库。如果一个数据库失效，可以先删除坏的数据库文件，重新启动数据库，然后重新运行 CreateCluster 工具 连接数据库现在可以使用如下连接字符串连接集群数据库 监控集群运行状态 可以使用如下命令查看配置的集群服务器是否都在运行 限制 H2 的集群并不支持针对事务的负载均衡，所以很多操作会使两个数据库产生不一致的结果 执行如下操作时请小心： 自动增长列和标识列不支持集群，当插入数据时，序列值需要手动创建不支持 SET AUTOCOMMIT FALSE 语句； 如果需要设置成为不自动提交，可以执行方法 Connection.setAutoCommit(false) 参考资料 h2database 官网 Java 嵌入式数据库 H2 学习总结(一)——H2 数据库入门]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO]]></title>
    <url>%2Fblog%2F2018%2F08%2F06%2Fjava%2Fjavacore%2Fio%2FJavaNIO%2F</url>
    <content type="text"><![CDATA[Java NIO 📓 本文已归档到：「blog」 流与块 通道与缓冲区 通道 缓冲区 缓冲区状态变量 文件 NIO 实例 选择器 创建选择器 将通道注册到选择器上 监听事件 获取到达的事件 事件循环 套接字 NIO 实例 内存映射文件 对比 Java NIO Tutorial Java NIO 浅析 IBM: NIO 入门 新的输入/输出 (NIO) 库是在 JDK 1.4 中引入的，弥补了原来的 I/O 的不足，提供了高速的、面向块的 I/O。 流与块 I/O 与 NIO 最重要的区别是数据打包和传输的方式，I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。 面向流的 I/O 一次处理一个字节数据：一个输入流产生一个字节数据，一个输出流消费一个字节数据。为流式数据创建过滤器非常容易，链接几个过滤器，以便每个过滤器只负责复杂处理机制的一部分。不利的一面是，面向流的 I/O 通常相当慢。 面向块的 I/O 一次处理一个数据块，按块处理数据比按流处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 I/O 包和 NIO 已经很好地集成了，java.io.* 已经以 NIO 为基础重新实现了，所以现在它可以利用 NIO 的一些特性。例如，java.io.* 包中的一些类包含以块的形式读写数据的方法，这使得即使在面向流的系统中，处理速度也会更快。 通道与缓冲区 通道 通道 Channel 是对原 I/O 包中的流的模拟，可以通过它读取和写入数据。 通道与流的不同之处在于，流只能在一个方向上移动(一个流必须是 InputStream 或者 OutputStream 的子类)，而通道是双向的，可以用于读、写或者同时用于读写。 通道包括以下类型： FileChannel：从文件中读写数据； DatagramChannel：通过 UDP 读写网络中数据； SocketChannel：通过 TCP 读写网络中数据； ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel。 缓冲区 发送给一个通道的所有数据都必须首先放到缓冲区中，同样地，从通道中读取的任何数据都要先读到缓冲区中。也就是说，不会直接对通道进行读写数据，而是要先经过缓冲区。 缓冲区实质上是一个数组，但它不仅仅是一个数组。缓冲区提供了对数据的结构化访问，而且还可以跟踪系统的读/写进程。 缓冲区包括以下类型： ByteBuffer CharBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer 缓冲区状态变量 capacity：最大容量； position：当前已经读写的字节数； limit：还可以读写的字节数。 状态变量的改变过程举例： 新建一个大小为 8 个字节的缓冲区，此时 position 为 0，而 limit = capacity = 8。capacity 变量不会改变，下面的讨论会忽略它。 从输入通道中读取 5 个字节数据写入缓冲区中，此时 position 移动设置为 5，limit 保持不变。 在将缓冲区的数据写到输出通道之前，需要先调用 flip() 方法，这个方法将 limit 设置为当前 position，并将 position 设置为 0。 从缓冲区中取 4 个字节到输出缓冲中，此时 position 设为 4。 最后需要调用 clear() 方法来清空缓冲区，此时 position 和 limit 都被设置为最初位置。 文件 NIO 实例 以下展示了使用 NIO 快速复制文件的实例： public static void fastCopy(String src, String dist) throws IOException &#123; /* 获得源文件的输入字节流 */ FileInputStream fin = new FileInputStream(src); /* 获取输入字节流的文件通道 */ FileChannel fcin = fin.getChannel(); /* 获取目标文件的输出字节流 */ FileOutputStream fout = new FileOutputStream(dist); /* 获取输出字节流的通道 */ FileChannel fcout = fout.getChannel(); /* 为缓冲区分配 1024 个字节 */ ByteBuffer buffer = ByteBuffer.allocateDirect(1024); while (true) &#123; /* 从输入通道中读取数据到缓冲区中 */ int r = fcin.read(buffer); /* read() 返回 -1 表示 EOF */ if (r == -1) &#123; break; &#125; /* 切换读写 */ buffer.flip(); /* 把缓冲区的内容写入输出文件中 */ fcout.write(buffer); /* 清空缓冲区 */ buffer.clear(); &#125;&#125; 选择器 NIO 常常被叫做非阻塞 IO，主要是因为 NIO 在网络通信中的非阻塞特性被广泛使用。 NIO 实现了 IO 多路复用中的 Reactor 模型，一个线程 Thread 使用一个选择器 Selector 通过轮询的方式去监听多个通道 Channel 上的事件，从而让一个线程就可以处理多个事件。 通过配置监听的通道 Channel 为非阻塞，那么当 Channel 上的 IO 事件还未到达时，就不会进入阻塞状态一直等待，而是继续轮询其它 Channel，找到 IO 事件已经到达的 Channel 执行。 因为创建和切换线程的开销很大，因此使用一个线程来处理多个事件而不是一个线程处理一个事件具有更好的性能。 应该注意的是，只有套接字 Channel 才能配置为非阻塞，而 FileChannel 不能，为 FileChannel 配置非阻塞也没有意义。 创建选择器 Selector selector = Selector.open(); 将通道注册到选择器上 ServerSocketChannel ssChannel = ServerSocketChannel.open();ssChannel.configureBlocking(false);ssChannel.register(selector, SelectionKey.OP_ACCEPT); 通道必须配置为非阻塞模式，否则使用选择器就没有任何意义了，因为如果通道在某个事件上被阻塞，那么服务器就不能响应其它事件，必须等待这个事件处理完毕才能去处理其它事件，显然这和选择器的作用背道而驰。 在将通道注册到选择器上时，还需要指定要注册的具体事件，主要有以下几类： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 它们在 SelectionKey 的定义如下： public static final int OP_READ = 1 &lt;&lt; 0;public static final int OP_WRITE = 1 &lt;&lt; 2;public static final int OP_CONNECT = 1 &lt;&lt; 3;public static final int OP_ACCEPT = 1 &lt;&lt; 4; 可以看出每个事件可以被当成一个位域，从而组成事件集整数。例如： int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 监听事件 int num = selector.select(); 使用 select() 来监听到达的事件，它会一直阻塞直到有至少一个事件到达。 获取到达的事件 Set&lt;SelectionKey&gt; keys = selector.selectedKeys();Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator();while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; // ... &#125; else if (key.isReadable()) &#123; // ... &#125; keyIterator.remove();&#125; 事件循环 因为一次 select() 调用不能处理完所有的事件，并且服务器端有可能需要一直监听事件，因此服务器端处理事件的代码一般会放在一个死循环内。 while (true) &#123; int num = selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; // ... &#125; else if (key.isReadable()) &#123; // ... &#125; keyIterator.remove(); &#125;&#125; 套接字 NIO 实例 public class NIOServer &#123; public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel ssChannel = ServerSocketChannel.open(); ssChannel.configureBlocking(false); ssChannel.register(selector, SelectionKey.OP_ACCEPT); ServerSocket serverSocket = ssChannel.socket(); InetSocketAddress address = new InetSocketAddress("127.0.0.1", 8888); serverSocket.bind(address); while (true) &#123; selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; ServerSocketChannel ssChannel1 = (ServerSocketChannel) key.channel(); // 服务器会为每个新连接创建一个 SocketChannel SocketChannel sChannel = ssChannel1.accept(); sChannel.configureBlocking(false); // 这个新连接主要用于从客户端读取数据 sChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; SocketChannel sChannel = (SocketChannel) key.channel(); System.out.println(readDataFromSocketChannel(sChannel)); sChannel.close(); &#125; keyIterator.remove(); &#125; &#125; &#125; private static String readDataFromSocketChannel(SocketChannel sChannel) throws IOException &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); StringBuilder data = new StringBuilder(); while (true) &#123; buffer.clear(); int n = sChannel.read(buffer); if (n == -1) &#123; break; &#125; buffer.flip(); int limit = buffer.limit(); char[] dst = new char[limit]; for (int i = 0; i &lt; limit; i++) &#123; dst[i] = (char) buffer.get(i); &#125; data.append(dst); buffer.clear(); &#125; return data.toString(); &#125;&#125; public class NIOClient &#123; public static void main(String[] args) throws IOException &#123; Socket socket = new Socket("127.0.0.1", 8888); OutputStream out = socket.getOutputStream(); String s = "hello world"; out.write(s.getBytes()); out.close(); &#125;&#125; 内存映射文件 内存映射文件 I/O 是一种读和写文件数据的方法，它可以比常规的基于流或者基于通道的 I/O 快得多。 向内存映射文件写入可能是危险的，只是改变数组的单个元素这样的简单操作，就可能会直接修改磁盘上的文件。修改数据与将数据保存到磁盘是没有分开的。 下面代码行将文件的前 1024 个字节映射到内存中，map() 方法返回一个 MappedByteBuffer，它是 ByteBuffer 的子类。因此，可以像使用其他任何 ByteBuffer 一样使用新映射的缓冲区，操作系统会在需要时负责执行映射。 MappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_WRITE, 0, 1024); 对比 NIO 与普通 I/O 的区别主要有以下两点： NIO 是非阻塞的 NIO 面向块，I/O 面向流]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>io</tag>
        <tag>nio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 网络编程]]></title>
    <url>%2Fblog%2F2018%2F08%2F06%2Fjava%2Fjavacore%2Fio%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Java 网络编程 📓 本文已归档到：「blog」 关键词：URL、InetAddress InetAddress URL Sockets Datagram Java 中的网络支持： InetAddress：用于表示网络上的硬件资源，即 IP 地址； URL：统一资源定位符； Sockets：使用 TCP 协议实现网络通信； Datagram：使用 UDP 协议实现网络通信。 InetAddress 没有公有的构造函数，只能通过静态方法来创建实例。 InetAddress.getByName(String host);InetAddress.getByAddress(byte[] address); URL 可以直接从 URL 中读取字节流数据。 public static void main(String[] args) throws IOException &#123; URL url = new URL("http://www.baidu.com"); /* 字节流 */ InputStream is = url.openStream(); /* 字符流 */ InputStreamReader isr = new InputStreamReader(is, "utf-8"); /* 提供缓存功能 */ BufferedReader br = new BufferedReader(isr); String line; while ((line = br.readLine()) != null) &#123; System.out.println(line); &#125; br.close();&#125; Sockets ServerSocket：服务器端类 Socket：客户端类 服务器和客户端通过 InputStream 和 OutputStream 进行输入输出。 Datagram DatagramPacket：数据包类 DatagramSocket：通信类]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 常用 IO 类]]></title>
    <url>%2Fblog%2F2018%2F08%2F06%2Fjava%2Fjavacore%2Fio%2FJava%E5%B8%B8%E7%94%A8IO%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[Java 常用 IO 类 📓 本文已归档到：「blog」 关键词：File、RandomAccessFile、System、Scanner File RandomAccessFile System Scanner File File 类是 java.io 包中唯一对文件本身进行操作的类。它可以对文件、目录进行增删查操作。 常用方法 createNewFille 可以使用 createNewFille() 方法创建一个新文件。 注： Windows 中使用反斜杠表示目录的分隔符 \。 Linux 中使用正斜杠表示目录的分隔符 /。 最好的做法是使用 File.separator 静态常量，可以根据所在操作系统选取对应的分隔符。 示例： File f = new File(filename);boolean flag = f.createNewFile(); mkdir 可以使用 mkdir() 来创建文件夹，但是如果要创建的目录的父路径不存在，则无法创建成功。 如果要解决这个问题，可以使用 mkdirs()，当父路径不存在时，会连同上级目录都一并创建。 示例： File f = new File(filename);boolean flag = f.mkdir(); delete 可以使用 delete() 来删除文件或目录。 需要注意的是，如果删除的是目录，且目录不为空，直接用 delete() 删除会失败。 示例： File f = new File(filename);boolean flag = f.delete(); list 和 listFiles File 中给出了两种列出文件夹内容的方法： list(): 列出全部名称，返回一个字符串数组。 listFiles(): 列出完整的路径，返回一个 File 对象数组。 list() 示例： File f = new File(filename);String str[] = f.list(); listFiles() 示例： File f = new File(filename);File files[] = f.listFiles(); RandomAccessFile 注：RandomAccessFile 类虽然可以实现对文件内容的读写操作，但是比较复杂。所以一般操作文件内容往往会使用字节流或字符流方式。 RandomAccessFile 类是随机读取类，它是一个完全独立的类。 它适用于由大小已知的记录组成的文件，所以我们可以使用 seek() 将记录从一处转移到另一处，然后读取或者修改记录。 文件中记录的大小不一定都相同，只要能够确定哪些记录有多大以及它们在文件中的位置即可。 写操作 当用 rw 方式声明 RandomAccessFile 对象时，如果要写入的文件不存在，系统将自行创建。 r 为只读；w 为只写；rw 为读写。 示例： public class RandomAccessFileDemo01 &#123; public static void main(String args[]) throws IOException &#123; File f = new File("d:" + File.separator + "test.txt"); // 指定要操作的文件 RandomAccessFile rdf = null; // 声明RandomAccessFile类的对象 rdf = new RandomAccessFile(f, "rw");// 读写模式，如果文件不存在，会自动创建 String name = null; int age = 0; name = "zhangsan"; // 字符串长度为8 age = 30; // 数字的长度为4 rdf.writeBytes(name); // 将姓名写入文件之中 rdf.writeInt(age); // 将年龄写入文件之中 name = "lisi "; // 字符串长度为8 age = 31; // 数字的长度为4 rdf.writeBytes(name); // 将姓名写入文件之中 rdf.writeInt(age); // 将年龄写入文件之中 name = "wangwu "; // 字符串长度为8 age = 32; // 数字的长度为4 rdf.writeBytes(name); // 将姓名写入文件之中 rdf.writeInt(age); // 将年龄写入文件之中 rdf.close(); // 关闭 &#125;&#125; 读操作 读取是直接使用 r 的模式即可，以只读的方式打开文件。 读取时所有的字符串只能按照 byte 数组方式读取出来，而且长度必须和写入时的固定大小相匹配。 public class RandomAccessFileDemo02 &#123; public static void main(String args[]) throws IOException &#123; File f = new File("d:" + File.separator + "test.txt"); // 指定要操作的文件 RandomAccessFile rdf = null; // 声明RandomAccessFile类的对象 rdf = new RandomAccessFile(f, "r");// 以只读的方式打开文件 String name = null; int age = 0; byte b[] = new byte[8]; // 开辟byte数组 // 读取第二个人的信息，意味着要空出第一个人的信息 rdf.skipBytes(12); // 跳过第一个人的信息 for (int i = 0; i &lt; b.length; i++) &#123; b[i] = rdf.readByte(); // 读取一个字节 &#125; name = new String(b); // 将读取出来的byte数组变为字符串 age = rdf.readInt(); // 读取数字 System.out.println("第二个人的信息 --&gt; 姓名：" + name + "；年龄：" + age); // 读取第一个人的信息 rdf.seek(0); // 指针回到文件的开头 for (int i = 0; i &lt; b.length; i++) &#123; b[i] = rdf.readByte(); // 读取一个字节 &#125; name = new String(b); // 将读取出来的byte数组变为字符串 age = rdf.readInt(); // 读取数字 System.out.println("第一个人的信息 --&gt; 姓名：" + name + "；年龄：" + age); rdf.skipBytes(12); // 空出第二个人的信息 for (int i = 0; i &lt; b.length; i++) &#123; b[i] = rdf.readByte(); // 读取一个字节 &#125; name = new String(b); // 将读取出来的byte数组变为字符串 age = rdf.readInt(); // 读取数字 System.out.println("第三个人的信息 --&gt; 姓名：" + name + "；年龄：" + age); rdf.close(); // 关闭 &#125;&#125; System System 中提供了三个常用于 IO 的静态成员： System.out System.err System.in 示例：重定向 System.out 输出流 import java.io.*;public class SystemOutDemo &#123; public static void main(String args[]) throws Exception &#123; OutputStream out = new FileOutputStream("d:\\test.txt"); PrintStream ps = new PrintStream(out); System.setOut(ps); System.out.println("人生若只如初见，何事秋风悲画扇"); ps.close(); out.close(); &#125;&#125; 示例：重定向 System.err 输出流 public class SystemErrDemo &#123; public static void main(String args[]) throws IOException &#123; OutputStream bos = new ByteArrayOutputStream(); // 实例化 PrintStream ps = new PrintStream(bos); // 实例化 System.setErr(ps); // 输出重定向 System.err.print("此处有误"); System.out.println(bos); // 输出内存中的数据 &#125;&#125; 示例：接受控制台输入信息 import java.io.*;public class SystemInDemo &#123; public static void main(String args[]) throws IOException &#123; InputStream input = System.in; StringBuffer buf = new StringBuffer(); System.out.print("请输入内容："); int temp = 0; while ((temp = input.read()) != -1) &#123; char c = (char) temp; if (c == '\n') &#123; break; &#125; buf.append(c); &#125; System.out.println("输入的内容为：" + buf); input.close(); &#125;&#125; Scanner Scanner 可以完成输入数据操作，并对数据进行验证。 示例： import java.io.*;public class ScannerDemo &#123; public static void main(String args[]) &#123; Scanner scan = new Scanner(System.in); // 从键盘接收数据 int i = 0; float f = 0.0f; System.out.print("输入整数："); if (scan.hasNextInt()) &#123; // 判断输入的是否是整数 i = scan.nextInt(); // 接收整数 System.out.println("整数数据：" + i); &#125; else &#123; System.out.println("输入的不是整数！"); &#125; System.out.print("输入小数："); if (scan.hasNextFloat()) &#123; // 判断输入的是否是小数 f = scan.nextFloat(); // 接收小数 System.out.println("小数数据：" + f); &#125; else &#123; System.out.println("输入的不是小数！"); &#125; Date date = null; String str = null; System.out.print("输入日期（yyyy-MM-dd）："); if (scan.hasNext("^\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;$")) &#123; // 判断 str = scan.next("^\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;$"); // 接收 try &#123; date = new SimpleDateFormat("yyyy-MM-dd").parse(str); &#125; catch (Exception e) &#123;&#125; &#125; else &#123; System.out.println("输入的日期格式错误！"); &#125; System.out.println(date); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 常见面试问题]]></title>
    <url>%2Fblog%2F2018%2F08%2F02%2Fjava%2Fjavaweb%2Fspring%2Fspring-interview%2F</url>
    <content type="text"><![CDATA[Spring 常见面试问题 📓 本文已归档到：「blog」 1. 一般问题 1.1. 不同版本的 Spring Framework 有哪些主要功能？ 1.2. 什么是 Spring Framework？ 1.3. 列举 Spring Framework 的优点。 1.4. Spring Framework 有哪些不同的功能？ 1.5. Spring Framework 中有多少个模块，它们分别是什么？ 1.6. 什么是 Spring 配置文件？ 1.7. Spring 应用程序有哪些不同组件？ 1.8. 使用 Spring 有哪些方式？ 2. 依赖注入（Ioc） 2.1. 什么是 Spring IOC 容器？ 2.2. 什么是依赖注入？ 2.3. 可以通过多少种方式完成依赖注入？ 2.4. 区分构造函数注入和 setter 注入。 2.5. spring 中有多少种 IOC 容器？ 2.6. 区分 BeanFactory 和 ApplicationContext。 2.7. 列举 IoC 的一些好处。 2.8. Spring IoC 的实现机制。 3. Beans 3.1. 什么是 spring bean？ 3.2. spring 提供了哪些配置方式？ 3.3. spring 支持集中 bean scope？ 3.4. spring bean 容器的生命周期是什么样的？ 3.5. 什么是 spring 的内部 bean？ 3.6. 什么是 spring 装配 3.7. 自动装配有哪些方式？ 3.8. 自动装配有什么局限？ 4. 注解 4.1. 你用过哪些重要的 Spring 注解？ 4.2. 如何在 spring 中启动注解装配？ 4.3. @Component, @Controller, @Repository, @Service 有何区别？ 4.4. @Required 注解有什么用？ 4.5. @Autowired 注解有什么用？ 4.6. @Qualifier 注解有什么用？ 4.7. @RequestMapping 注解有什么用？ 5. 数据访问 5.1. spring DAO 有什么用？ 5.2. 列举 Spring DAO 抛出的异常。 5.3. spring JDBC API 中存在哪些类？ 5.4. 使用 Spring 访问 Hibernate 的方法有哪些？ 5.5. 列举 spring 支持的事务管理类型 5.6. spring 支持哪些 ORM 框架 6. AOP 6.1. 什么是 AOP？ 6.2. AOP 中的 Aspect、Advice、Pointcut、JointPoint 和 Advice 参数分别是什么？ 6.3. 什么是通知（Advice）？ 6.4. 有哪些类型的通知（Advice）？ 6.5. 指出在 spring aop 中 concern 和 cross-cutting concern 的不同之处。 6.6. AOP 有哪些实现方式？ 6.7. Spring AOP and AspectJ AOP 有什么区别？ 6.8. 如何理解 Spring 中的代理？ 6.9. 什么是编织（Weaving）？ 7. MVC 7.1. Spring MVC 框架有什么用？ 7.2. 描述一下 DispatcherServlet 的工作流程 7.3. 介绍一下 WebApplicationContext 8. 资料 1. 一般问题 1.1. 不同版本的 Spring Framework 有哪些主要功能？ Version Feature Spring 2.5 发布于 2007 年。这是第一个支持注解的版本。 Spring 3.0 发布于 2009 年。它完全利用了 Java5 中的改进，并为 JEE6 提供了支持。 Spring 4.0 发布于 2013 年。这是第一个完全支持 JAVA8 的版本。 1.2. 什么是 Spring Framework？ Spring 是一个开源应用框架，旨在降低应用程序开发的复杂度。 它是轻量级、松散耦合的。 它具有分层体系结构，允许用户选择组件，同时还为 J2EE 应用程序开发提供了一个有凝聚力的框架。 它可以集成其他框架，如 Structs、Hibernate、EJB 等，所以又称为框架的框架。 1.3. 列举 Spring Framework 的优点。 由于 Spring Frameworks 的分层架构，用户可以自由选择自己需要的组件。 Spring Framework 支持 POJO(Plain Old Java Object) 编程，从而具备持续集成和可测试性。 由于依赖注入和控制反转，JDBC 得以简化。 它是开源免费的。 1.4. Spring Framework 有哪些不同的功能？ 轻量级 - Spring 在代码量和透明度方面都很轻便。 IOC - 控制反转 AOP - 面向切面编程可以将应用业务逻辑和系统服务分离，以实现高内聚。 容器 - Spring 负责创建和管理对象（Bean）的生命周期和配置。 MVC - 对 web 应用提供了高度可配置性，其他框架的集成也十分方便。 事务管理 - 提供了用于事务管理的通用抽象层。Spring 的事务支持也可用于容器较少的环境。 JDBC 异常 - Spring 的 JDBC 抽象层提供了一个异常层次结构，简化了错误处理策略。 1.5. Spring Framework 中有多少个模块，它们分别是什么？ Spring 核心容器 – 该层基本上是 Spring Framework 的核心。它包含以下模块： Spring Core Spring Bean SpEL (Spring Expression Language) Spring Context 数据访问/集成 – 该层提供与数据库交互的支持。它包含以下模块： JDBC (Java DataBase Connectivity) ORM (Object Relational Mapping) OXM (Object XML Mappers) JMS (Java Messaging Service) Transaction Web – 该层提供了创建 Web 应用程序的支持。它包含以下模块： Web Web – Servlet Web – Socket Web – Portlet AOP – 该层支持面向切面编程 Instrumentation – 该层为类检测和类加载器实现提供支持。 Test – 该层为使用 JUnit 和 TestNG 进行测试提供支持。 几个杂项模块: Messaging – 该模块为 STOMP 提供支持。它还支持注解编程模型，该模型用于从 WebSocket 客户端路由和处理 STOMP 消息。 Aspects – 该模块为与 AspectJ 的集成提供支持。 1.6. 什么是 Spring 配置文件？ Spring 配置文件是 XML 文件。该文件主要包含类信息。它描述了这些类是如何配置以及相互引入的。但是，XML 配置文件冗长且更加干净。如果没有正确规划和编写，那么在大项目中管理变得非常困难。 1.7. Spring 应用程序有哪些不同组件？ Spring 应用一般有以下组件： 接口 - 定义功能。 Bean 类 - 它包含属性，setter 和 getter 方法，函数等。 Spring 面向切面编程（AOP） - 提供面向切面编程的功能。 Bean 配置文件 - 包含类的信息以及如何配置它们。 用户程序 - 它使用接口。 1.8. 使用 Spring 有哪些方式？ 使用 Spring 有以下方式： 作为一个成熟的 Spring Web 应用程序。 作为第三方 Web 框架，使用 Spring Frameworks 中间层。 用于远程使用。 作为企业级 Java Bean，它可以包装现有的 POJO（Plain Old Java Objects）。 2. 依赖注入（Ioc） 2.1. 什么是 Spring IOC 容器？ Spring 框架的核心是 Spring 容器。容器创建对象，将它们装配在一起，配置它们并管理它们的完整生命周期。Spring 容器使用依赖注入来管理组成应用程序的组件。容器通过读取提供的配置元数据来接收对象进行实例化，配置和组装的指令。该元数据可以通过 XML，Java 注解或 Java 代码提供。 2.2. 什么是依赖注入？ 在依赖注入中，您不必创建对象，但必须描述如何创建它们。您不是直接在代码中将组件和服务连接在一起，而是描述配置文件中哪些组件需要哪些服务。由 IoC 容器将它们装配在一起。 2.3. 可以通过多少种方式完成依赖注入？ 通常，依赖注入可以通过三种方式完成，即： 构造函数注入 setter 注入 接口注入 在 Spring Framework 中，仅使用构造函数和 setter 注入。 2.4. 区分构造函数注入和 setter 注入。 构造函数注入 setter 注入 没有部分注入 有部分注入 不会覆盖 setter 属性 会覆盖 setter 属性 任意修改都会创建一个新实例 任意修改不会创建一个新实例 适用于设置很多属性 适用于设置少量属性 2.5. spring 中有多少种 IOC 容器？ BeanFactory - BeanFactory 就像一个包含 bean 集合的工厂类。它会在客户端要求时实例化 bean。 ApplicationContext - ApplicationContext 接口扩展了 BeanFactory 接口。它在 BeanFactory 基础上提供了一些额外的功能。 2.6. 区分 BeanFactory 和 ApplicationContext。 BeanFactory ApplicationContext 它使用懒加载 它使用即时加载 它使用语法显式提供资源对象 它自己创建和管理资源对象 不支持国际化 支持国际化 不支持基于依赖的注解 支持基于依赖的注解 2.7. 列举 IoC 的一些好处。 IoC 的一些好处是： 它将最小化应用程序中的代码量。 它将使您的应用程序易于测试，因为它不需要单元测试用例中的任何单例或 JNDI 查找机制。 它以最小的影响和最少的侵入机制促进松耦合。 它支持即时的实例化和延迟加载服务。 2.8. Spring IoC 的实现机制。 Spring 中的 IoC 的实现原理就是工厂模式加反射机制。 示例： interface Fruit &#123; public abstract void eat();&#125;class Apple implements Fruit &#123; public void eat()&#123; System.out.println("Apple"); &#125;&#125;class Orange implements Fruit &#123; public void eat()&#123; System.out.println("Orange"); &#125;&#125;class Factory &#123; public static Fruit getInstance(String ClassName) &#123; Fruit f=null; try &#123; f=(Fruit)Class.forName(ClassName).newInstance(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return f; &#125;&#125;class Client &#123; public static void main(String[] a) &#123; Fruit f=Factory.getInstance("io.github.dunwu.spring.Apple"); if(f!=null)&#123; f.eat(); &#125; &#125;&#125; 3. Beans 3.1. 什么是 spring bean？ 它们是构成用户应用程序主干的对象。 Bean 由 Spring IoC 容器管理。 它们由 Spring IoC 容器实例化，配置，装配和管理。 Bean 是基于用户提供给容器的配置元数据创建。 3.2. spring 提供了哪些配置方式？ 基于 xml 配置 bean 所需的依赖项和服务在 XML 格式的配置文件中指定。这些配置文件通常包含许多 bean 定义和特定于应用程序的配置选项。它们通常以 bean 标签开头。例如： &lt;bean id="studentbean" class="org.edureka.firstSpring.StudentBean"&gt; &lt;property name="name" value="Edureka"&gt;&lt;/property&gt;&lt;/bean&gt; 基于注解配置 您可以通过在相关的类，方法或字段声明上使用注解，将 bean 配置为组件类本身，而不是使用 XML 来描述 bean 装配。默认情况下，Spring 容器中未打开注解装配。因此，您需要在使用它之前在 Spring 配置文件中启用它。例如： &lt;beans&gt;&lt;context:annotation-config/&gt;&lt;!-- bean definitions go here --&gt;&lt;/beans&gt; 基于 Java API 配置 Spring 的 Java 配置是通过使用 @Bean 和 @Configuration 来实现。 @Bean 注解扮演与 &lt;bean /&gt; 元素相同的角色。 @Configuration 类允许通过简单地调用同一个类中的其他 @Bean 方法来定义 bean 间依赖关系。 例如： @Configurationpublic class StudentConfig &#123; @Bean public StudentBean myStudent() &#123; return new StudentBean(); &#125;&#125; 3.3. spring 支持集中 bean scope？ Spring bean 支持 5 种 scope： Singleton - 每个 Spring IoC 容器仅有一个单实例。 Prototype - 每次请求都会产生一个新的实例。 Request - 每一次 HTTP 请求都会产生一个新的实例，并且该 bean 仅在当前 HTTP 请求内有效。 Session - 每一次 HTTP 请求都会产生一个新的 bean，同时该 bean 仅在当前 HTTP session 内有效。 Global-session - 类似于标准的 HTTP Session 作用域，不过它仅仅在基于 portlet 的 web 应用中才有意义。Portlet 规范定义了全局 Session 的概念，它被所有构成某个 portlet web 应用的各种不同的 portlet 所共享。在 global session 作用域中定义的 bean 被限定于全局 portlet Session 的生命周期范围内。如果你在 web 中使用 global session 作用域来标识 bean，那么 web 会自动当成 session 类型来使用。 仅当用户使用支持 Web 的 ApplicationContext 时，最后三个才可用。 3.4. spring bean 容器的生命周期是什么样的？ spring bean 容器的生命周期流程如下： Spring 容器根据配置中的 bean 定义中实例化 bean。 Spring 使用依赖注入填充所有属性，如 bean 中所定义的配置。 如果 bean 实现 BeanNameAware 接口，则工厂通过传递 bean 的 ID 来调用 setBeanName()。 如果 bean 实现 BeanFactoryAware 接口，工厂通过传递自身的实例来调用 setBeanFactory()。 如果存在与 bean 关联的任何 BeanPostProcessors，则调用 preProcessBeforeInitialization() 方法。 如果为 bean 指定了 init 方法（&lt;bean&gt; 的 init-method 属性），那么将调用它。 最后，如果存在与 bean 关联的任何 BeanPostProcessors，则将调用 postProcessAfterInitialization() 方法。 如果 bean 实现 DisposableBean 接口，当 spring 容器关闭时，会调用 destory()。 如果为 bean 指定了 destroy 方法（&lt;bean&gt; 的 destroy-method 属性），那么将调用它。 3.5. 什么是 spring 的内部 bean？ 只有将 bean 用作另一个 bean 的属性时，才能将 bean 声明为内部 bean。为了定义 bean，Spring 的基于 XML 的配置元数据在 &lt;property&gt; 或 &lt;constructor-arg&gt; 中提供了 &lt;bean&gt; 元素的使用。内部 bean 总是匿名的，它们总是作为原型。 例如，假设我们有一个 Student 类，其中引用了 Person 类。这里我们将只创建一个 Person 类实例并在 Student 中使用它。 Student.java public class Student &#123; private Person person; //Setters and Getters&#125;public class Person &#123; private String name; private String address; //Setters and Getters&#125; bean.xml &lt;bean id=“StudentBean" class="com.edureka.Student"&gt; &lt;property name="person"&gt; &lt;!--This is inner bean --&gt; &lt;bean class="com.edureka.Person"&gt; &lt;property name="name" value=“Scott"&gt;&lt;/property&gt; &lt;property name="address" value=“Bangalore"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 3.6. 什么是 spring 装配 当 bean 在 Spring 容器中组合在一起时，它被称为装配或 bean 装配。 Spring 容器需要知道需要什么 bean 以及容器应该如何使用依赖注入来将 bean 绑定在一起，同时装配 bean。 3.7. 自动装配有哪些方式？ Spring 容器能够自动装配 bean。也就是说，可以通过检查 BeanFactory 的内容让 Spring 自动解析 bean 的协作者。 自动装配的不同模式： no - 这是默认设置，表示没有自动装配。应使用显式 bean 引用进行装配。 byName - 它根据 bean 的名称注入对象依赖项。它匹配并装配其属性与 XML 文件中由相同名称定义的 bean。 byType - 它根据类型注入对象依赖项。如果属性的类型与 XML 文件中的一个 bean 名称匹配，则匹配并装配属性。 构造函数 - 它通过调用类的构造函数来注入依赖项。它有大量的参数。 autodetect - 首先容器尝试通过构造函数使用 autowire 装配，如果不能，则尝试通过 byType 自动装配。 3.8. 自动装配有什么局限？ 覆盖的可能性 - 您始终可以使用 &lt;constructor-arg&gt; 和 &lt;property&gt; 设置指定依赖项，这将覆盖自动装配。 基本元数据类型 - 简单属性（如原数据类型，字符串和类）无法自动装配。 令人困惑的性质 - 总是喜欢使用明确的装配，因为自动装配不太精确。 4. 注解 4.1. 你用过哪些重要的 Spring 注解？ @Controller - 用于 Spring MVC 项目中的控制器类。 @Service - 用于服务类。 @RequestMapping - 用于在控制器处理程序方法中配置 URI 映射。 @ResponseBody - 用于发送 Object 作为响应，通常用于发送 XML 或 JSON 数据作为响应。 @PathVariable - 用于将动态值从 URI 映射到处理程序方法参数。 @Autowired - 用于在 spring bean 中自动装配依赖项。 @Qualifier - 使用 @Autowired 注解，以避免在存在多个 bean 类型实例时出现混淆。 @Scope - 用于配置 spring bean 的范围。 @Configuration，@ComponentScan 和 @Bean - 用于基于 java 的配置。 @Aspect，@Before，@After，@Around，@Pointcut - 用于切面编程（AOP）。 4.2. 如何在 spring 中启动注解装配？ 默认情况下，Spring 容器中未打开注解装配。因此，要使用基于注解装配，我们必须通过配置&lt;context：annotation-config /&gt; 元素在 Spring 配置文件中启用它。 4.3. @Component, @Controller, @Repository, @Service 有何区别？ @Component：这将 java 类标记为 bean。它是任何 Spring 管理组件的通用构造型。spring 的组件扫描机制现在可以将其拾取并将其拉入应用程序环境中。 @Controller：这将一个类标记为 Spring Web MVC 控制器。标有它的 Bean 会自动导入到 IoC 容器中。 @Service：此注解是组件注解的特化。它不会对 @Component 注解提供任何其他行为。您可以在服务层类中使用 @Service 而不是 @Component，因为它以更好的方式指定了意图。 @Repository：这个注解是具有类似用途和功能的 @Component 注解的特化。它为 DAO 提供了额外的好处。它将 DAO 导入 IoC 容器，并使未经检查的异常有资格转换为 Spring DataAccessException。 4.4. @Required 注解有什么用？ @Required 应用于 bean 属性 setter 方法。此注解仅指示必须在配置时使用 bean 定义中的显式属性值或使用自动装配填充受影响的 bean 属性。如果尚未填充受影响的 bean 属性，则容器将抛出 BeanInitializationException。 示例： public class Employee &#123; private String name; @Required public void setName(String name)&#123; this.name=name; &#125; public string getName()&#123; return name; &#125;&#125; 4.5. @Autowired 注解有什么用？ @Autowired 可以更准确地控制应该在何处以及如何进行自动装配。此注解用于在 setter 方法，构造函数，具有任意名称或多个参数的属性或方法上自动装配 bean。默认情况下，它是类型驱动的注入。 public class Employee &#123; private String name; @Autowired public void setName(String name) &#123; this.name=name; &#125; public string getName()&#123; return name; &#125;&#125; 4.6. @Qualifier 注解有什么用？ 当您创建多个相同类型的 bean 并希望仅使用属性装配其中一个 bean 时，您可以使用@Qualifier 注解和 @Autowired 通过指定应该装配哪个确切的 bean 来消除歧义。 例如，这里我们分别有两个类，Employee 和 EmpAccount。在 EmpAccount 中，使用@Qualifier 指定了必须装配 id 为 emp1 的 bean。 Employee.java public class Employee &#123; private String name; @Autowired public void setName(String name) &#123; this.name=name; &#125; public string getName() &#123; return name; &#125;&#125; EmpAccount.java public class EmpAccount &#123; private Employee emp; @Autowired @Qualifier(emp1) public void showName() &#123; System.out.println(“Employee name : ”+emp.getName); &#125;&#125; 4.7. @RequestMapping 注解有什么用？ @RequestMapping 注解用于将特定 HTTP 请求方法映射到将处理相应请求的控制器中的特定类/方法。此注解可应用于两个级别： 类级别：映射请求的 URL 方法级别：映射 URL 以及 HTTP 请求方法 5. 数据访问 5.1. spring DAO 有什么用？ Spring DAO 使得 JDBC，Hibernate 或 JDO 这样的数据访问技术更容易以一种统一的方式工作。这使得用户容易在持久性技术之间切换。它还允许您在编写代码时，无需考虑捕获每种技术不同的异常。 5.2. 列举 Spring DAO 抛出的异常。 5.3. spring JDBC API 中存在哪些类？ JdbcTemplate SimpleJdbcTemplate NamedParameterJdbcTemplate SimpleJdbcInsert SimpleJdbcCall 5.4. 使用 Spring 访问 Hibernate 的方法有哪些？ 我们可以通过两种方式使用 Spring 访问 Hibernate： 使用 Hibernate 模板和回调进行控制反转 扩展 HibernateDAOSupport 并应用 AOP 拦截器节点 5.5. 列举 spring 支持的事务管理类型 Spring 支持两种类型的事务管理： 程序化事务管理：在此过程中，在编程的帮助下管理事务。它为您提供极大的灵活性，但维护起来非常困难。 声明式事务管理：在此，事务管理与业务代码分离。仅使用注解或基于 XML 的配置来管理事务。 5.6. spring 支持哪些 ORM 框架 Hibernate iBatis JPA JDO OJB 6. AOP 6.1. 什么是 AOP？ AOP(Aspect-Oriented Programming), 即 面向切面编程, 它与 OOP( Object-Oriented Programming, 面向对象编程) 相辅相成, 提供了与 OOP 不同的抽象软件结构的视角. 在 OOP 中, 我们以类(class)作为我们的基本单元, 而 AOP 中的基本单元是 Aspect(切面) 6.2. AOP 中的 Aspect、Advice、Pointcut、JointPoint 和 Advice 参数分别是什么？ Aspect - Aspect 是一个实现交叉问题的类，例如事务管理。方面可以是配置的普通类，然后在 Spring Bean 配置文件中配置，或者我们可以使用 Spring AspectJ 支持使用 @Aspect 注解将类声明为 Aspect。 Advice - Advice 是针对特定 JoinPoint 采取的操作。在编程方面，它们是在应用程序中达到具有匹配切入点的特定 JoinPoint 时执行的方法。您可以将 Advice 视为 Spring 拦截器（Interceptor）或 Servlet 过滤器（filter）。 Advice Arguments - 我们可以在 advice 方法中传递参数。我们可以在切入点中使用 args() 表达式来应用于与参数模式匹配的任何方法。如果我们使用它，那么我们需要在确定参数类型的 advice 方法中使用相同的名称。 Pointcut - Pointcut 是与 JoinPoint 匹配的正则表达式，用于确定是否需要执行 Advice。 Pointcut 使用与 JoinPoint 匹配的不同类型的表达式。Spring 框架使用 AspectJ Pointcut 表达式语言来确定将应用通知方法的 JoinPoint。 JoinPoint - JoinPoint 是应用程序中的特定点，例如方法执行，异常处理，更改对象变量值等。在 Spring AOP 中，JoinPoint 始终是方法的执行器。 6.3. 什么是通知（Advice）？ 特定 JoinPoint 处的 Aspect 所采取的动作称为 Advice。Spring AOP 使用一个 Advice 作为拦截器，在 JoinPoint “周围”维护一系列的拦截器。 6.4. 有哪些类型的通知（Advice）？ Before - 这些类型的 Advice 在 joinpoint 方法之前执行，并使用 @Before 注解标记进行配置。 After Returning - 这些类型的 Advice 在连接点方法正常执行后执行，并使用@AfterReturning 注解标记进行配置。 After Throwing - 这些类型的 Advice 仅在 joinpoint 方法通过抛出异常退出并使用 @AfterThrowing 注解标记配置时执行。 After (finally) - 这些类型的 Advice 在连接点方法之后执行，无论方法退出是正常还是异常返回，并使用 @After 注解标记进行配置。 Around - 这些类型的 Advice 在连接点之前和之后执行，并使用 @Around 注解标记进行配置。 6.5. 指出在 spring aop 中 concern 和 cross-cutting concern 的不同之处。 concern 是我们想要在应用程序的特定模块中定义的行为。它可以定义为我们想要实现的功能。 cross-cutting concern 是一个适用于整个应用的行为，这会影响整个应用程序。例如，日志记录，安全性和数据传输是应用程序几乎每个模块都需要关注的问题，因此它们是跨领域的问题。 6.6. AOP 有哪些实现方式？ 实现 AOP 的技术，主要分为两大类： 静态代理 - 指使用 AOP 框架提供的命令进行编译，从而在编译阶段就可生成 AOP 代理类，因此也称为编译时增强； 编译时编织（特殊编译器实现） 类加载时编织（特殊的类加载器实现）。 动态代理 - 在运行时在内存中“临时”生成 AOP 动态代理类，因此也被称为运行时增强。 JDK 动态代理 CGLIB 6.7. Spring AOP and AspectJ AOP 有什么区别？ Spring AOP 基于动态代理方式实现；AspectJ 基于静态代理方式实现。 Spring AOP 仅支持方法级别的 PointCut；提供了完全的 AOP 支持，它还支持属性级别的 PointCut。 6.8. 如何理解 Spring 中的代理？ 将 Advice 应用于目标对象后创建的对象称为代理。在客户端对象的情况下，目标对象和代理对象是相同的。 Advice + Target Object = Proxy 6.9. 什么是编织（Weaving）？ 为了创建一个 advice 对象而链接一个 aspect 和其它应用类型或对象，称为编织（Weaving）。在 Spring AOP 中，编织在运行时执行。请参考下图： 7. MVC 7.1. Spring MVC 框架有什么用？ Spring Web MVC 框架提供 模型-视图-控制器 架构和随时可用的组件，用于开发灵活且松散耦合的 Web 应用程序。 MVC 模式有助于分离应用程序的不同方面，如输入逻辑，业务逻辑和 UI 逻辑，同时在所有这些元素之间提供松散耦合。 7.2. 描述一下 DispatcherServlet 的工作流程 DispatcherServlet 的工作流程可以用一幅图来说明： 向服务器发送 HTTP 请求，请求被前端控制器 DispatcherServlet 捕获。 DispatcherServlet 根据 -servlet.xml 中的配置对请求的 URL 进行解析，得到请求资源标识符（URI）。然后根据该 URI，调用 HandlerMapping 获得该 Handler 配置的所有相关的对象（包括 Handler 对象以及 Handler 对象对应的拦截器），最后以HandlerExecutionChain 对象的形式返回。 DispatcherServlet 根据获得的Handler，选择一个合适的 HandlerAdapter。（附注：如果成功获得HandlerAdapter后，此时将开始执行拦截器的 preHandler(…)方法）。 提取Request中的模型数据，填充Handler入参，开始执行Handler（Controller)。 在填充Handler的入参过程中，根据你的配置，Spring 将帮你做一些额外的工作： HttpMessageConveter： 将请求消息（如 Json、xml 等数据）转换成一个对象，将对象转换为指定的响应信息。 数据转换：对请求消息进行数据转换。如String转换成Integer、Double等。 数据根式化：对请求消息进行数据格式化。 如将字符串转换成格式化数字或格式化日期等。 数据验证： 验证数据的有效性（长度、格式等），验证结果存储到BindingResult或Error中。 Handler(Controller)执行完成后，向 DispatcherServlet 返回一个 ModelAndView 对象； 根据返回的ModelAndView，选择一个适合的 ViewResolver（必须是已经注册到 Spring 容器中的ViewResolver)返回给DispatcherServlet。 ViewResolver 结合Model和View，来渲染视图。 视图负责将渲染结果返回给客户端。 7.3. 介绍一下 WebApplicationContext WebApplicationContext 是 ApplicationContext 的扩展。它具有 Web 应用程序所需的一些额外功能。它与普通的 ApplicationContext 在解析主题和决定与哪个 servlet 关联的能力方面有所不同。 8. 资料 Top 50 Spring Interview Questions You Must Prepare In 2018 Spring Interview Questions and Answers （完） 👉 想学习更多 Spring 内容可以访问我的 Spring 教程：spring-tutorial]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>interview</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 实战篇]]></title>
    <url>%2Fblog%2F2018%2F07%2F17%2Fjava%2Fjavaweb%2Fdistributed%2Fmq%2Frocketmq-basics%2F</url>
    <content type="text"><![CDATA[RocketMQ 实战篇 概述 简介 核心概念 安装 环境要求 下载解压 启动 Name Server 启动 Broker 收发消息 关闭服务器 API Producer Consumer FAQ 资料 概述 简介 RocketMQ 是一款开源的分布式消息队列，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。 RocketMQ 被阿里巴巴捐赠给 Apache，成为 Apache 的孵化项目。 核心概念 RocketMQ 有以下核心概念： Producer - 将业务应用程序系统生成的消息发送给代理。RocketMQ 提供多种发送范例：同步，异步和单向。 Producer Group - 具有相同角色的 Producer 组合在一起。如果原始 Producer 在事务之后崩溃，则代理可以联系同一 Producer 组的不同 Producer 实例以提交或回滚事务。警告：考虑到提供的 Producer 在发送消息方面足够强大，每个 Producer 组只允许一个实例，以避免不必要的生成器实例初始化。 Consumer - Consumer 从 Broker 那里获取消息并将其提供给应用程序。从用户应用的角度来看，提供了两种类型的 Consumer： PullConsumer - PullConsumer 积极地从 Broker 那里获取消息。一旦提取了批量消息，用户应用程序就会启动消费过程。 PushConsumer - PushConsumer 封装消息提取，消费进度并维护其他内部工作，为最终用户留下回调接口，这个借口会在消息到达时被执行。 Consumer Group - 完全相同角色的 Consumer 被组合在一起并命名为 Consumer Group。Consumer Group 是一个很好的概念，在消息消费方面实现负载平衡和容错目标非常容易。警告：Consumer Group 中的 Consumer 实例必须具有完全相同的主题订阅。 Broker - Broker 是 RocketMQ 的主要组成部分。它接收从 Producer 发送的消息，存储它们并准备处理来自 Consumer 的消费请求。它还存储与消息相关的元数据，包括 Consumer Group，消耗进度偏移和主题/队列信息。 Name Server - 充当路由信息提供者。Producer/Consumer 客户查找主题以查找相应的 Broker 列表。 Topic - 是 Producer 传递消息和 Consumer 提取消息的类别。 Message - 是要传递的信息。消息必须有一个主题，可以将其解释为您要发送给的邮件地址。消息还可以具有可选 Tag 和额外的键值对。例如，您可以为消息设置业务密钥，并在代理服务器上查找消息以诊断开发期间的问题。 Message Queue - 主题被划分为一个或多个子主题“消息队列”。 Tag - 即子主题，为用户提供了额外的灵活性。对于 Tag，来自同一业务模块的具有不同目的的消息可以具有相同的主题和不同的 Tag。 安装 环境要求 推荐 64 位操作系统：Linux/Unix/Mac 64bit JDK 1.8+ Maven 3.2.x Git 下载解压 进入官方下载地址：https://rocketmq.apache.org/dowloading/releases/，选择合适版本 建议选择 binary 版本。 解压到本地： &gt; unzip rocketmq-all-4.2.0-source-release.zip&gt; cd rocketmq-all-4.2.0/ 启动 Name Server &gt; nohup sh bin/mqnamesrv &amp;&gt; tail -f ~/logs/rocketmqlogs/namesrv.logThe Name Server boot success... 启动 Broker &gt; nohup sh bin/mqbroker -n localhost:9876 -c conf/broker.conf &amp;&gt; tail -f ~/logs/rocketmqlogs/broker.logThe broker[%s, 172.30.30.233:10911] boot success... 收发消息 执行收发消息操作之前，不许告诉客户端命名服务器的位置。在 RocketMQ 中有多种方法来实现这个目的。这里，我们使用最简单的方法——设置环境变量 NAMESRV_ADDR ： &gt; export NAMESRV_ADDR=localhost:9876&gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.ProducerSendResult [sendStatus=SEND_OK, msgId= ...&gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.ConsumerConsumeMessageThread_%d Receive New Messages: [MessageExt... 关闭服务器 &gt; sh bin/mqshutdown brokerThe mqbroker(36695) is running...Send shutdown request to mqbroker(36695) OK&gt; sh bin/mqshutdown namesrvThe mqnamesrv(36664) is running...Send shutdown request to mqnamesrv(36664) OK API 首先在项目中引入 maven 依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.2.0&lt;/version&gt;&lt;/dependency&gt; Producer Producer 在 RocketMQ 中负责发送消息。 RocketMQ 有三种消息发送方式： 可靠的同步发送 可靠的异步发送 单项发送 可靠的同步发送 可靠的同步传输用于广泛的场景，如重要的通知消息，短信通知，短信营销系统等。 public class SyncProducer &#123; public static void main(String[] args) throws Exception &#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name"); //Launch the instance. producer.start(); for (int i = 0; i &lt; 100; i++) &#123; //Create a message instance, specifying topic, tag and message body. Message msg = new Message("TopicTest" /* Topic */, "TagA" /* Tag */, ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); //Call send message to deliver message to one of brokers. SendResult sendResult = producer.send(msg); System.out.printf("%s%n", sendResult); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 可靠的异步发送 异步传输通常用于响应时间敏感的业务场景。 public class AsyncProducer &#123; public static void main(String[] args) throws Exception &#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer("ExampleProducerGroup"); //Launch the instance. producer.start(); producer.setRetryTimesWhenSendAsyncFailed(0); for (int i = 0; i &lt; 100; i++) &#123; final int index = i; //Create a message instance, specifying topic, tag and message body. Message msg = new Message("TopicTest", "TagA", "OrderID188", "Hello world".getBytes(RemotingHelper.DEFAULT_CHARSET)); producer.send(msg, new SendCallback() &#123; @Override public void onSuccess(SendResult sendResult) &#123; System.out.printf("%-10d OK %s %n", index, sendResult.getMsgId()); &#125; @Override public void onException(Throwable e) &#123; System.out.printf("%-10d Exception %s %n", index, e); e.printStackTrace(); &#125; &#125;); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 单向传输 单向传输用于需要中等可靠性的情况，例如日志收集。 public class OnewayProducer &#123; public static void main(String[] args) throws Exception&#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer("ExampleProducerGroup"); //Launch the instance. producer.start(); for (int i = 0; i &lt; 100; i++) &#123; //Create a message instance, specifying topic, tag and message body. Message msg = new Message("TopicTest" /* Topic */, "TagA" /* Tag */, ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); //Call send message to deliver message to one of brokers. producer.sendOneway(msg); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; Consumer Consumer 在 RocketMQ 中负责接收消息。 public class OrderedConsumer &#123; public static void main(String[] args) throws Exception &#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("example_group_name"); consumer.setNamesrvAddr(RocketConfig.HOST); consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); consumer.subscribe("TopicTest", "TagA || TagC || TagD"); consumer.registerMessageListener(new MessageListenerOrderly() &#123; AtomicLong consumeTimes = new AtomicLong(0); @Override public ConsumeOrderlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context) &#123; context.setAutoCommit(false); System.out.printf(Thread.currentThread().getName() + " Receive New Messages: " + msgs + "%n"); this.consumeTimes.incrementAndGet(); if ((this.consumeTimes.get() % 2) == 0) &#123; return ConsumeOrderlyStatus.SUCCESS; &#125; else if ((this.consumeTimes.get() % 3) == 0) &#123; return ConsumeOrderlyStatus.ROLLBACK; &#125; else if ((this.consumeTimes.get() % 4) == 0) &#123; return ConsumeOrderlyStatus.COMMIT; &#125; else if ((this.consumeTimes.get() % 5) == 0) &#123; context.setSuspendCurrentQueueTimeMillis(3000); return ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT; &#125; return ConsumeOrderlyStatus.SUCCESS; &#125; &#125;); consumer.start(); System.out.printf("Consumer Started.%n"); &#125;&#125; FAQ connect to &lt;172.17.0.1:10909&gt; failed 启动后，Producer 客户端连接 RocketMQ 时报错： org.apache.rocketmq.remoting.exception.RemotingConnectException: connect to &lt;172.17.0.1:10909&gt; failed at org.apache.rocketmq.remoting.netty.NettyRemotingClient.invokeSync(NettyRemotingClient.java:357) at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessageSync(MQClientAPIImpl.java:343) at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessage(MQClientAPIImpl.java:327) at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessage(MQClientAPIImpl.java:290) at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendKernelImpl(DefaultMQProducerImpl.java:688) at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendSelectImpl(DefaultMQProducerImpl.java:901) at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.send(DefaultMQProducerImpl.java:878) at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.send(DefaultMQProducerImpl.java:873) at org.apache.rocketmq.client.producer.DefaultMQProducer.send(DefaultMQProducer.java:369) at com.emrubik.uc.mdm.sync.utils.MdmInit.sendMessage(MdmInit.java:62) at com.emrubik.uc.mdm.sync.utils.MdmInit.main(MdmInit.java:2149) 原因：RocketMQ 部署在虚拟机上，内网 ip 为 10.10.30.63，该虚拟机一个 docker0 网卡，ip 为 172.17.0.1。RocketMQ broker 启动时默认使用了 docker0 网卡，Producer 客户端无法连接 172.17.0.1，造成以上问题。 解决方案 （1）干掉 docker0 网卡或修改网卡名称 （2）停掉 broker，修改 broker 配置文件，重启 broker。 修改 conf/broker.conf，增加两行来指定启动 broker 的 IP： namesrvAddr = 10.10.30.63:9876brokerIP1 = 10.10.30.63 启动时需要指定配置文件 nohup sh bin/mqbroker -n localhost:9876 -c conf/broker.conf &amp; 资料 RocketMQ 官方文档]]></content>
      <categories>
        <category>javaweb</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaweb</tag>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ 原理篇]]></title>
    <url>%2Fblog%2F2018%2F07%2F17%2Fjava%2Fjavaweb%2Fdistributed%2Fmq%2Frocketmq-advanced%2F</url>
    <content type="text"><![CDATA[RocketMQ 原理篇 架构 NameServer Broker Producer Consumer 关键特性以及其实现原理 顺序消息 消息重复 事务消息 资料 架构 RocketMQ 由四部分组成：NameServer、Broker、Producer、Consumer。其中任意一个组成都可以水平扩展为集群模式，以避免单点故障问题。 NameServer NameServer 提供轻量级的服务发现和路由能力。每个 NameServer 节点记录全部的路由信息，支持相应的读写操作，并支持快速存储扩展。 NameServer 是一个功能齐全的服务器，主要包括两个功能： Broker 管理 - NameServer 接受来自 Broker 集群的注册，并提供心跳机制来检查 Broker 节点是否存活。 路由管理 - 每个 NameServer 将保存有关 Broker 集群的完整路由信息和客户端查询的查询队列。 RocketMQ 客户端（Producer/Consumer）将从 NameServer 查询队列路由信息。 将 NameServer 地址列表提供给客户端有四种方法： 编程方式 - 类似：producer.setNamesrvAddr(&quot;ip:port&quot;) Java 选项 - 使用 rocketmq.namesrv.addr 参数 环境变量 - 设置环境变量 NAMESRV_ADDR HTTP 端点 更详细信息可以参考官方文档：here Broker Broker 通过提供轻量级的 TOPIC 和 QUEUE 机制来处理消息存储。它们支持 Push 和 Pull 模型，包含容错机制（2 个副本或 3 个副本），并提供强大的峰值填充和以原始时间顺序累积数千亿条消息的能力。此外，Brokers 还提供灾难恢复，丰富的指标统计和警报机制。 Broker 有几个重要的子模块： Remoting Module - 即代理的条目，处理来自客户端的请求。 Client Manager - 管理客户（生产者/消费者）并维护消费者的主题订阅。 Store 服务 - 提供简单的 API 来存储或查询物理磁盘中的消息。 HA 服务 - 提供主代理和从代理之间的数据同步功能。 Index 服务 - 按指定密钥构建消息索引，并提供快速消息查询。 Producer Producers 支持分布式部署。Distributed Producers 通过多种负载均衡模式向 Broker 集群发送消息。发送过程支持快速故障并具有低延迟。 Consumer Consumer 也支持 Push 和 Pull 模型中的分布式部署。它还支持群集消费和消息广播。它提供实时消息订阅机制，可以满足大多数消费者的需求。 RocketMQ 的网站为感兴趣的用户提供了一个简单的快速入门指南。 关键特性以及其实现原理 分布式消息系统作为实现分布式系统可扩展、可伸缩性的关键组件，需要具有高吞吐量、高可用等特点。而谈到消息系统的设计，就回避不了两个问题： 消息的顺序问题 消息的重复问题 顺序消息 第一种模型 假如生产者产生了 2 条消息：M1、M2，要保证这两条消息的顺序，应该怎样做？你脑中想到的可能是这样： 假定 M1 发送到 S1，M2 发送到 S2，如果要保证 M1 先于 M2 被消费，那么需要 M1 到达消费端被消费后，通知 S2，然后 S2 再将 M2 发送到消费端。 这个模型存在的问题是，如果 M1 和 M2 分别发送到两台 Server 上，就不能保证 M1 先达到 MQ 集群，也不能保证 M1 被先消费。换个角度看，如果 M2 先于 M1 达到 MQ 集群，甚至 M2 被消费后，M1 才达到消费端，这时消息也就乱序了，说明以上模型是不能保证消息的顺序的。 第二种模型 如何才能在 MQ 集群保证消息的顺序？一种简单的方式就是将 M1、M2 发送到同一个 Server 上： 这样可以保证 M1 先于 M2 到达 MQServer（生产者等待 M1 发送成功后再发送 M2），根据先达到先被消费的原则，M1 会先于 M2 被消费，这样就保证了消息的顺序。 这个模型也仅仅是理论上可以保证消息的顺序，在实际场景中可能会遇到下面的问题： 只要将消息从一台服务器发往另一台服务器，就会存在网络延迟问题。如上图所示，如果发送 M1 耗时大于发送 M2 的耗时，那么 M2 就仍将被先消费，仍然不能保证消息的顺序。即使 M1 和 M2 同时到达消费端，由于不清楚消费端 1 和消费端 2 的负载情况，仍然有可能出现 M2 先于 M1 被消费的情况。 如何解决这个问题？将 M1 和 M2 发往同一个消费者，且发送 M1 后，需要消费端响应成功后才能发送 M2。 这可能产生另外的问题：如果 M1 被发送到消费端后，消费端 1 没有响应，那是继续发送 M2 呢，还是重新发送 M1？一般为了保证消息一定被消费，肯定会选择重发 M1 到另外一个消费端 2，就如下图所示。 这样的模型就严格保证消息的顺序，细心的你仍然会发现问题，消费端 1 没有响应 Server 时有两种情况，一种是 M1 确实没有到达(数据在网络传送中丢失)，另外一种消费端已经消费 M1 且已经发送响应消息，只是 MQ Server 端没有收到。如果是第二种情况，重发 M1，就会造成 M1 被重复消费。也就引入了我们要说的第二个问题，消息重复问题，这个后文会详细讲解。 回过头来看消息顺序问题，严格的顺序消息非常容易理解，也可以通过文中所描述的方式来简单处理。总结起来，要实现严格的顺序消息，简单且可行的办法就是： 保证生产者 - MQServer - 消费者是一对一对一的关系。 这样的设计虽然简单易行，但也会存在一些很严重的问题，比如： 并行度就会成为消息系统的瓶颈（吞吐量不够） 更多的异常处理，比如：只要消费端出现问题，就会导致整个处理流程阻塞，我们不得不花费更多的精力来解决阻塞的问题。 RocketMQ 的解决方案：通过合理的设计或者将问题分解来规避。如果硬要把时间花在解决问题本身，实际上不仅效率低下，而且也是一种浪费。从这个角度来看消息的顺序问题，我们可以得出两个结论： 不关注乱序的应用实际大量存在 队列无序并不意味着消息无序 最后我们从源码角度分析 RocketMQ 怎么实现发送顺序消息。 RocketMQ 通过轮询所有队列的方式来确定消息被发送到哪一个队列（负载均衡策略）。比如下面的示例中，订单号相同的消息会被先后发送到同一个队列中： // RocketMQ 通过 MessageQueueSelector 中实现的算法来确定消息发送到哪一个队列上// RocketMQ 默认提供了两种 MessageQueueSelector 实现：随机/Hash// 当然你可以根据业务实现自己的 MessageQueueSelector 来决定消息按照何种策略发送到消息队列中SendResult sendResult = producer.send(msg, new MessageQueueSelector() &#123; @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Integer id = (Integer) arg; int index = id % mqs.size(); return mqs.get(index); &#125;&#125;, orderId); 在获取到路由信息以后，会根据 MessageQueueSelector 实现的算法来选择一个队列，同一个 OrderId 获取到的肯定是同一个队列。 private SendResult send() &#123; // 获取topic路由信息 TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic()); if (topicPublishInfo != null &amp;&amp; topicPublishInfo.ok()) &#123; MessageQueue mq = null; // 根据我们的算法，选择一个发送队列 // 这里的arg = orderId mq = selector.select(topicPublishInfo.getMessageQueueList(), msg, arg); if (mq != null) &#123; return this.sendKernelImpl(msg, mq, communicationMode, sendCallback, timeout); &#125; &#125;&#125; 消息重复 造成消息重复的根本原因是：网络不可达。只要通过网络交换数据，就无法避免这个问题。所以解决这个问题的办法就是绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？ 消费端处理消息的业务逻辑保持幂等性。 保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现。 第 1 条很好理解，只要保持幂等性，不管来多少条重复消息，最后处理的结果都一样。 第 2 条原理就是利用一张日志表来记录已经处理成功的消息的 ID，如果新到的消息 ID 已经在日志表中，那么就不再处理这条消息。 第 1 条解决方案，很明显应该在消费端实现，不属于消息系统要实现的功能。 第 2 条可以消息系统实现，也可以业务端实现。正常情况下出现重复消息的概率其实很小，如果由消息系统来实现的话，肯定会对消息系统的吞吐量和高可用有影响，所以最好还是由业务端自己处理消息重复的问题，这也是 RocketMQ 不解决消息重复的问题的原因。 RocketMQ 不保证消息不重复，如果你的业务需要保证严格的不重复消息，需要你自己在业务端去重。 事务消息 RocketMQ 除了支持普通消息，顺序消息，另外还支持事务消息。 假设这样的场景： 图中执行本地事务（Bob 账户扣款）和发送异步消息应该保证同时成功或者同时失败，也就是扣款成功了，发送消息一定要成功，如果扣款失败了，就不能再发送消息。那问题是：我们是先扣款还是先发送消息呢？ RocketMQ 分布式事务步骤： 发送 Prepared 消息 2222222222222222222，并拿到接受消息的地址。 执行本地事务 通过第 1 步骤拿到的地址去访问消息，并修改消息状态。 资料 RocketMQ 官方文档 分布式开放消息系统(RocketMQ)的原理与实践]]></content>
      <categories>
        <category>javaweb</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaweb</tag>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 反射和动态代理]]></title>
    <url>%2Fblog%2F2018%2F07%2F16%2Fjava%2Fjavacore%2Fbasics%2FJava%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"><![CDATA[深入理解 Java 反射和动态代理 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「javacore」 简介 什么是反射 反射的应用场景 反射的缺点 反射机制 类加载过程 Class 对象 使用反射 java.lang.reflect 包 获得 Class 对象 判断是否为某个类的实例 创建实例 Field Method Constructor Array 动态代理 静态代理 动态代理 InvocationHandler 接口 Proxy 类 动态代理实例 小结 参考资料 简介 什么是反射 反射(Reflection)是 Java 程序开发语言的特征之一，它允许运行中的 Java 程序获取自身的信息，并且可以操作类或对象的内部属性。 通过反射机制，可以在运行时访问 Java 对象的属性，方法，构造方法等。 反射的应用场景 反射的主要应用场景有： 开发通用框架 - 反射最重要的用途就是开发各种通用框架。很多框架（比如 Spring）都是配置化的（比如通过 XML 文件配置 JavaBean、Filter 等），为了保证框架的通用性，它们可能需要根据配置文件加载不同的对象或类，调用不同的方法，这个时候就必须用到反射——运行时动态加载需要加载的对象。 动态代理 - 在切面编程（AOP）中，需要拦截特定的方法，通常，会选择动态代理方式。这时，就需要反射技术来实现了。 注解 - 注解本身仅仅是起到标记作用，它需要利用反射机制，根据注解标记去调用注解解释器，执行行为。如果没有反射机制，注解并不比注释更有用。 可扩展性功能 - 应用程序可以通过使用完全限定名称创建可扩展性对象实例来使用外部的用户定义类。 反射的缺点 性能开销 - 由于反射涉及动态解析的类型，因此无法执行某些 Java 虚拟机优化。因此，反射操作的性能要比非反射操作的性能要差，应该在性能敏感的应用程序中频繁调用的代码段中避免。 破坏封装性 - 反射调用方法时可以忽略权限检查，因此可能会破坏封装性而导致安全问题。 内部曝光 - 由于反射允许代码执行在非反射代码中非法的操作，例如访问私有字段和方法，所以反射的使用可能会导致意想不到的副作用，这可能会导致代码功能失常并可能破坏可移植性。反射代码打破了抽象，因此可能会随着平台的升级而改变行为。 反射机制 类加载过程 类加载的完整过程如下： （1）在编译时，Java 编译器编译好 .java 文件之后，在磁盘中产生 .class 文件。.class 文件是二进制文件，内容是只有 JVM 能够识别的机器码。 （2）JVM 中的类加载器读取字节码文件，取出二进制数据，加载到内存中，解析.class 文件内的信息。类加载器会根据类的全限定名来获取此类的二进制字节流；然后，将字节流所代表的静态存储结构转化为方法区的运行时数据结构；接着，在内存中生成代表这个类的 java.lang.Class 对象。 （3）加载结束后，JVM 开始进行连接阶段（包含验证、准备、初始化）。经过这一系列操作，类的变量会被初始化。 Class 对象 要想使用反射，首先需要获得待操作的类所对应的 Class 对象。Java 中，无论生成某个类的多少个对象，这些对象都会对应于同一个 Class 对象。这个 Class 对象是由 JVM 生成的，通过它能够获悉整个类的结构。所以，java.lang.Class 可以视为所有反射 API 的入口点。 反射的本质就是：在运行时，把 Java 类中的各种成分映射成一个个的 Java 对象。 举例来说，假如定义了以下代码： User user = new User(); 步骤说明： JVM 加载方法的时候，遇到 new User()，JVM 会根据 User 的全限定名去加载 User.class 。 JVM 会去本地磁盘查找 User.class 文件并加载 JVM 内存中。 JVM 通过调用类加载器自动创建这个类对应的 Class 对象，并且存储在 JVM 的方法区。注意：一个类有且只有一个 Class 对象。 使用反射 java.lang.reflect 包 Java 中的 java.lang.reflect 包提供了反射功能。java.lang.reflect 包中的类都没有 public 构造方法。 java.lang.reflect 包的核心接口和类如下： Member 接口 - 反映关于单个成员(字段或方法)或构造函数的标识信息。 Field 类 - 提供一个类的域的信息以及访问类的域的接口。 Method 类 - 提供一个类的方法的信息以及访问类的方法的接口。 Constructor 类 - 提供一个类的构造函数的信息以及访问类的构造函数的接口。 Array 类 - 该类提供动态地生成和访问 JAVA 数组的方法。 Modifier 类 - 提供了 static 方法和常量，对类和成员访问修饰符进行解码。 Proxy 类 - 提供动态地生成代理类和类实例的静态方法。 获得 Class 对象 获得 Class 的三种方法： （1）使用 Class 类的 forName 静态方法 示例： package io.github.dunwu.javacore.reflect;public class ReflectClassDemo01 &#123; public static void main(String[] args) throws ClassNotFoundException &#123; Class c1 = Class.forName("io.github.dunwu.javacore.reflect.ReflectClassDemo01"); System.out.println(c1.getCanonicalName()); Class c2 = Class.forName("[D"); System.out.println(c2.getCanonicalName()); Class c3 = Class.forName("[[Ljava.lang.String;"); System.out.println(c3.getCanonicalName()); &#125;&#125;//Output://io.github.dunwu.javacore.reflect.ReflectClassDemo01//double[]//java.lang.String[][] 使用类的完全限定名来反射对象的类。常见的应用场景为：在 JDBC 开发中常用此方法加载数据库驱动。 （2）直接获取某一个对象的 class 示例： public class ReflectClassDemo02 &#123; public static void main(String[] args) &#123; boolean b; // Class c = b.getClass(); // 编译错误 Class c1 = boolean.class; System.out.println(c1.getCanonicalName()); Class c2 = java.io.PrintStream.class; System.out.println(c2.getCanonicalName()); Class c3 = int[][][].class; System.out.println(c3.getCanonicalName()); &#125;&#125;//Output://boolean//java.io.PrintStream//int[][][] （3）调用 Object 的 getClass 方法，示例： Object 类中有 getClass 方法，因为所有类都继承 Object 类。从而调用 Object 类来获取 示例： package io.github.dunwu.javacore.reflect;import java.util.HashSet;import java.util.Set;public class ReflectClassDemo03 &#123; enum E &#123;A, B&#125; public static void main(String[] args) &#123; Class c = "foo".getClass(); System.out.println(c.getCanonicalName()); Class c2 = ReflectClassDemo03.E.A.getClass(); System.out.println(c2.getCanonicalName()); byte[] bytes = new byte[1024]; Class c3 = bytes.getClass(); System.out.println(c3.getCanonicalName()); Set&lt;String&gt; set = new HashSet&lt;&gt;(); Class c4 = set.getClass(); System.out.println(c4.getCanonicalName()); &#125;&#125;//Output://java.lang.String//io.github.dunwu.javacore.reflect.ReflectClassDemo.E//byte[]//java.util.HashSet 判断是否为某个类的实例 判断是否为某个类的实例有两种方式： 用 instanceof 关键字 用 Class 对象的 isInstance 方法（它是一个 Native 方法） 示例： public class InstanceofDemo &#123; public static void main(String[] args) &#123; ArrayList arrayList = new ArrayList(); if (arrayList instanceof List) &#123; System.out.println("ArrayList is List"); &#125; if (List.class.isInstance(arrayList)) &#123; System.out.println("ArrayList is List"); &#125; &#125;&#125;//Output://ArrayList is List//ArrayList is List 创建实例 通过反射来创建实例对象主要有两种方式： 用 Class 对象的 newInstance 方法。 用 Constructor 对象的 newInstance 方法。 示例： public class NewInstanceDemo &#123; public static void main(String[] args) throws IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException &#123; Class&lt;?&gt; c1 = StringBuilder.class; StringBuilder sb = (StringBuilder) c1.newInstance(); sb.append("aaa"); System.out.println(sb.toString()); //获取String所对应的Class对象 Class&lt;?&gt; c2 = String.class; //获取String类带一个String参数的构造器 Constructor constructor = c2.getConstructor(String.class); //根据构造器创建实例 String str2 = (String) constructor.newInstance("bbb"); System.out.println(str2); &#125;&#125;//Output://aaa//bbb Field Class 对象提供以下方法获取对象的成员（Field）： getFiled - 根据名称获取公有的（public）类成员。 getDeclaredField - 根据名称获取已声明的类成员。但不能得到其父类的类成员。 getFields - 获取所有公有的（public）类成员。 getDeclaredFields - 获取所有已声明的类成员。 示例如下： public class ReflectFieldDemo &#123; class FieldSpy&lt;T&gt; &#123; public boolean[][] b = &#123;&#123;false, false&#125;, &#123;true, true&#125;&#125;; public String name = "Alice"; public List&lt;Integer&gt; list; public T val; &#125; public static void main(String[] args) throws NoSuchFieldException &#123; Field f1 = FieldSpy.class.getField("b"); System.out.format("Type: %s%n", f1.getType()); Field f2 = FieldSpy.class.getField("name"); System.out.format("Type: %s%n", f2.getType()); Field f3 = FieldSpy.class.getField("list"); System.out.format("Type: %s%n", f3.getType()); Field f4 = FieldSpy.class.getField("val"); System.out.format("Type: %s%n", f4.getType()); &#125;&#125;//Output://Type: class [[Z//Type: class java.lang.String//Type: interface java.util.List//Type: class java.lang.Object Method Class 对象提供以下方法获取对象的方法（Method）： getMethod - 返回类或接口的特定方法。其中第一个参数为方法名称，后面的参数为方法参数对应 Class 的对象。 getDeclaredMethod - 返回类或接口的特定声明方法。其中第一个参数为方法名称，后面的参数为方法参数对应 Class 的对象。 getMethods - 返回类或接口的所有 public 方法，包括其父类的 public 方法。 getDeclaredMethods - 返回类或接口声明的所有方法，包括 public、protected、默认（包）访问和 private 方法，但不包括继承的方法。 获取一个 Method 对象后，可以用 invoke 方法来调用这个方法。 invoke 方法的原型为: public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException 示例： public class ReflectMethodDemo &#123; public static void main(String[] args) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException &#123; // 返回所有方法 Method[] methods1 = System.class.getDeclaredMethods(); System.out.println("System getDeclaredMethods 清单（数量 = " + methods1.length + "）："); for (Method m : methods1) &#123; System.out.println(m); &#125; // 返回所有 public 方法 Method[] methods2 = System.class.getMethods(); System.out.println("System getMethods 清单（数量 = " + methods2.length + "）："); for (Method m : methods2) &#123; System.out.println(m); &#125; // 利用 Method 的 invoke 方法调用 System.currentTimeMillis() Method method = System.class.getMethod("currentTimeMillis"); System.out.println(method); System.out.println(method.invoke(null)); &#125;&#125; Constructor Class 对象提供以下方法获取对象的构造方法（Constructor）： getConstructor - 返回类的特定 public 构造方法。参数为方法参数对应 Class 的对象。 getDeclaredConstructor - 返回类的特定构造方法。参数为方法参数对应 Class 的对象。 getConstructors - 返回类的所有 public 构造方法。 getDeclaredConstructors - 返回类的所有构造方法。 获取一个 Constructor 对象后，可以用 newInstance 方法来创建类实例。 示例： public class ReflectMethodConstructorDemo &#123; public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException &#123; Constructor&lt;?&gt;[] constructors1 = String.class.getDeclaredConstructors(); System.out.println("String getDeclaredConstructors 清单（数量 = " + constructors1.length + "）："); for (Constructor c : constructors1) &#123; System.out.println(c); &#125; Constructor&lt;?&gt;[] constructors2 = String.class.getConstructors(); System.out.println("String getConstructors 清单（数量 = " + constructors2.length + "）："); for (Constructor c : constructors2) &#123; System.out.println(c); &#125; System.out.println("===================="); Constructor constructor = String.class.getConstructor(String.class); System.out.println(constructor); String str = (String) constructor.newInstance("bbb"); System.out.println(str); &#125;&#125; Array 数组在 Java 里是比较特殊的一种类型，它可以赋值给一个对象引用。下面我们看一看利用反射创建数组的例子： public class ReflectArrayDemo &#123; public static void main(String[] args) throws ClassNotFoundException &#123; Class&lt;?&gt; cls = Class.forName("java.lang.String"); Object array = Array.newInstance(cls, 25); //往数组里添加内容 Array.set(array, 0, "Scala"); Array.set(array, 1, "Java"); Array.set(array, 2, "Groovy"); Array.set(array, 3, "Scala"); Array.set(array, 4, "Clojure"); //获取某一项的内容 System.out.println(Array.get(array, 3)); &#125;&#125;//Output://Scala 其中的 Array 类为 java.lang.reflect.Array 类。我们通过 Array.newInstance 创建数组对象，它的原型是： public static Object newInstance(Class&lt;?&gt; componentType, int length) throws NegativeArraySizeException &#123; return newArray(componentType, length);&#125; 动态代理 动态代理是反射的一个非常重要的应用场景。动态代理常被用于一些 Java 框架中。例如 Spring 的 AOP ，Dubbo 的 SPI 接口，就是基于 Java 动态代理实现的。 静态代理 静态代理其实就是指设计模式中的代理模式。 代理模式为其他对象提供一种代理以控制对这个对象的访问。 Subject 定义了 RealSubject 和 Proxy 的公共接口，这样就在任何使用 RealSubject 的地方都可以使用 Proxy 。 abstract class Subject &#123; public abstract void Request();&#125; RealSubject 定义 Proxy 所代表的真实实体。 class RealSubject extends Subject &#123; @Override public void Request() &#123; System.out.println("真实的请求"); &#125;&#125; Proxy 保存一个引用使得代理可以访问实体，并提供一个与 Subject 的接口相同的接口，这样代理就可以用来替代实体。 class Proxy extends Subject &#123; private RealSubject real; @Override public void Request() &#123; if (null == real) &#123; real = new RealSubject(); &#125; real.Request(); &#125;&#125; 说明： 静态代理模式固然在访问无法访问的资源，增强现有的接口业务功能方面有很大的优点，但是大量使用这种静态代理，会使我们系统内的类的规模增大，并且不易维护；并且由于 Proxy 和 RealSubject 的功能本质上是相同的，Proxy 只是起到了中介的作用，这种代理在系统中的存在，导致系统结构比较臃肿和松散。 动态代理 为了解决静态代理的问题，就有了创建动态代理的想法： 在运行状态中，需要代理的地方，根据 Subject 和 RealSubject，动态地创建一个 Proxy，用完之后，就会销毁，这样就可以避免了 Proxy 角色的 class 在系统中冗杂的问题了。 Java 动态代理基于经典代理模式，引入了一个 InvocationHandler，InvocationHandler 负责统一管理所有的方法调用。 动态代理步骤： 获取 RealSubject 上的所有接口列表； 确定要生成的代理类的类名，默认为：com.sun.proxy.$ProxyXXXX； 根据需要实现的接口信息，在代码中动态创建 该 Proxy 类的字节码； 将对应的字节码转换为对应的 class 对象； 创建 InvocationHandler 实例 handler，用来处理 Proxy 所有方法调用； Proxy 的 class 对象 以创建的 handler 对象为参数，实例化一个 proxy 对象。 从上面可以看出，JDK 动态代理的实现是基于实现接口的方式，使得 Proxy 和 RealSubject 具有相同的功能。 但其实还有一种思路：通过继承。即：让 Proxy 继承 RealSubject，这样二者同样具有相同的功能，Proxy 还可以通过重写 RealSubject 中的方法，来实现多态。CGLIB 就是基于这种思路设计的。 在 Java 的动态代理机制中，有两个重要的类（接口），一个是 InvocationHandler 接口、另一个则是 Proxy 类，这一个类和一个接口是实现我们动态代理所必须用到的。 InvocationHandler 接口 InvocationHandler 接口定义： public interface InvocationHandler &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;&#125; 每一个动态代理类都必须要实现 InvocationHandler 这个接口，并且每个代理类的实例都关联到了一个 Handler，当我们通过代理对象调用一个方法的时候，这个方法的调用就会被转发为由 InvocationHandler 这个接口的 invoke 方法来进行调用。 我们来看看 InvocationHandler 这个接口的唯一一个方法 invoke 方法： Object invoke(Object proxy, Method method, Object[] args) throws Throwable 参数说明： proxy - 代理的真实对象。 method - 所要调用真实对象的某个方法的 Method 对象 args - 所要调用真实对象某个方法时接受的参数 如果不是很明白，等下通过一个实例会对这几个参数进行更深的讲解。 Proxy 类 Proxy 这个类的作用就是用来动态创建一个代理对象的类，它提供了许多的方法，但是我们用的最多的就是 newProxyInstance 这个方法： public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException 这个方法的作用就是得到一个动态的代理对象。 参数说明： loader - 一个 ClassLoader 对象，定义了由哪个 ClassLoader 对象来对生成的代理对象进行加载。 interfaces - 一个 Interface 对象的数组，表示的是我将要给我需要代理的对象提供一组什么接口，如果我提供了一组接口给它，那么这个代理对象就宣称实现了该接口(多态)，这样我就能调用这组接口中的方法了 h - 一个 InvocationHandler 对象，表示的是当我这个动态代理对象在调用方法的时候，会关联到哪一个 InvocationHandler 对象上 动态代理实例 上面的内容介绍完这两个接口(类)以后，我们来通过一个实例来看看我们的动态代理模式是什么样的： 首先我们定义了一个 Subject 类型的接口，为其声明了两个方法： public interface Subject &#123; void hello(String str); String bye();&#125; 接着，定义了一个类来实现这个接口，这个类就是我们的真实对象，RealSubject 类： public class RealSubject implements Subject &#123; @Override public void hello(String str) &#123; System.out.println("Hello " + str); &#125; @Override public String bye() &#123; System.out.println("Goodbye"); return "Over"; &#125;&#125; 下一步，我们就要定义一个动态代理类了，前面说个，每一个动态代理类都必须要实现 InvocationHandler 这个接口，因此我们这个动态代理类也不例外： public class InvocationHandlerDemo implements InvocationHandler &#123; // 这个就是我们要代理的真实对象 private Object subject; // 构造方法，给我们要代理的真实对象赋初值 public InvocationHandlerDemo(Object subject) &#123; this.subject = subject; &#125; @Override public Object invoke(Object object, Method method, Object[] args) throws Throwable &#123; // 在代理真实对象前我们可以添加一些自己的操作 System.out.println("Before method"); System.out.println("Call Method: " + method); // 当代理对象调用真实对象的方法时，其会自动的跳转到代理对象关联的handler对象的invoke方法来进行调用 Object obj = method.invoke(subject, args); // 在代理真实对象后我们也可以添加一些自己的操作 System.out.println("After method"); System.out.println(); return obj; &#125;&#125; 最后，来看看我们的 Client 类： public class Client &#123; public static void main(String[] args) &#123; // 我们要代理的真实对象 Subject realSubject = new RealSubject(); // 我们要代理哪个真实对象，就将该对象传进去，最后是通过该真实对象来调用其方法的 InvocationHandler handler = new InvocationHandlerDemo(realSubject); /* * 通过Proxy的newProxyInstance方法来创建我们的代理对象，我们来看看其三个参数 * 第一个参数 handler.getClass().getClassLoader() ，我们这里使用handler这个类的ClassLoader对象来加载我们的代理对象 * 第二个参数realSubject.getClass().getInterfaces()，我们这里为代理对象提供的接口是真实对象所实行的接口，表示我要代理的是该真实对象，这样我就能调用这组接口中的方法了 * 第三个参数handler， 我们这里将这个代理对象关联到了上方的 InvocationHandler 这个对象上 */ Subject subject = (Subject)Proxy.newProxyInstance(handler.getClass().getClassLoader(), realSubject .getClass().getInterfaces(), handler); System.out.println(subject.getClass().getName()); subject.hello("World"); String result = subject.bye(); System.out.println("Result is: " + result); &#125;&#125; 我们先来看看控制台的输出： com.sun.proxy.$Proxy0Before methodCall Method: public abstract void io.github.dunwu.javacore.reflect.InvocationHandlerDemo$Subject.hello(java.lang.String)Hello WorldAfter methodBefore methodCall Method: public abstract java.lang.String io.github.dunwu.javacore.reflect.InvocationHandlerDemo$Subject.bye()GoodbyeAfter methodResult is: Over 我们首先来看看 com.sun.proxy.$Proxy0 这东西，我们看到，这个东西是由 System.out.println(subject.getClass().getName()); 这条语句打印出来的，那么为什么我们返回的这个代理对象的类名是这样的呢？ Subject subject = (Subject)Proxy.newProxyInstance(handler.getClass().getClassLoader(), realSubject .getClass().getInterfaces(), handler); 可能我以为返回的这个代理对象会是 Subject 类型的对象，或者是 InvocationHandler 的对象，结果却不是，首先我们解释一下为什么我们这里可以将其转化为 Subject 类型的对象？ 原因就是：在 newProxyInstance 这个方法的第二个参数上，我们给这个代理对象提供了一组什么接口，那么我这个代理对象就会实现了这组接口，这个时候我们当然可以将这个代理对象强制类型转化为这组接口中的任意一个，因为这里的接口是 Subject 类型，所以就可以将其转化为 Subject 类型了。 同时我们一定要记住，通过 Proxy.newProxyInstance 创建的代理对象是在 jvm 运行时动态生成的一个对象，它并不是我们的 InvocationHandler 类型，也不是我们定义的那组接口的类型，而是在运行是动态生成的一个对象，并且命名方式都是这样的形式，以$开头，proxy 为中，最后一个数字表示对象的标号。 接着我们来看看这两句 subject.hello("World");String result = subject.bye(); 这里是通过代理对象来调用实现的那种接口中的方法，这个时候程序就会跳转到由这个代理对象关联到的 handler 中的 invoke 方法去执行，而我们的这个 handler 对象又接受了一个 RealSubject 类型的参数，表示我要代理的就是这个真实对象，所以此时就会调用 handler 中的 invoke 方法去执行。 我们看到，在真正通过代理对象来调用真实对象的方法的时候，我们可以在该方法前后添加自己的一些操作，同时我们看到我们的这个 method 对象是这样的： public abstract void io.github.dunwu.javacore.reflect.InvocationHandlerDemo$Subject.hello(java.lang.String)public abstract java.lang.String io.github.dunwu.javacore.reflect.InvocationHandlerDemo$Subject.bye() 正好就是我们的 Subject 接口中的两个方法，这也就证明了当我通过代理对象来调用方法的时候，起实际就是委托由其关联到的 handler 对象的 invoke 方法中来调用，并不是自己来真实调用，而是通过代理的方式来调用的。 小结 参考资料 Java 编程思想 JAVA 核心技术（卷 1） 深入解析 Java 反射（1） - 基础 Java 基础之—反射（非常重要） 官方 Reflection API 文档 java 的动态代理机制详解 Java 动态代理机制详解（JDK 和 CGLIB，Javassist，ASM）]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>proxy</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
        <tag>reflect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka 实战篇]]></title>
    <url>%2Fblog%2F2018%2F07%2F12%2Fjava%2Fjavaweb%2Fdistributed%2Fmq%2Fkafka-basics%2F</url>
    <content type="text"><![CDATA[Kafka 实战篇 Kafka 是一个分布式的、可水平扩展的、基于发布/订阅模式的、支持容错的消息系统。 | 官网 | 官方文档 | Github | 1. 概述 1.1. 简介 1.2. 系统结构和存储结构 1.3. 小结 2. 安装部署 2.1. 下载解压 2.2. 启动服务器 2.3. 停止服务器 2.4. 创建主题 2.5. 生产者生产消息 2.6. 消费者消费消息 2.7. 集群部署 3. API 4. 生产者（Producer） 4.1. 发送消息流程 4.2. 发送消息方式 5. 消费者（Consumer） 5.1. 消费者和消费者群组 5.2. 消费消息流程 5.3. 提交偏移量 5.4. 从指定偏移量获取数据 6. Broker 6.1. 集群控制器 6.2. 分区 leader 和 follower 6.3. 群组协调器 7. Zookeeper 集群 7.1. 节点信息 7.2. zookeeper 一些总结 8. 资料 1. 概述 1.1. 简介 Kafka 是一个分布式的、可水平扩展的、基于发布/订阅模式的、支持容错的 MQ 中间件。具有如下特点： 伸缩性。随着数据量增长，可以通过对 broker 集群水平扩展来提高系统性能。 高性能。通过横向扩展生产者、消费者(通过消费者群组实现)和 broker（通过扩展实现系统伸缩性）可以轻松处理巨大的消息流。 消息持久化。基于磁盘的数据存储，消息不会丢失。 通过 Partition 来实现多个生产者（同一个 topic 消息根据 key 放置到一个 Partition 中）和多个消费者（一个 Partition 由消费者群组中每一个消费者来负责）。 1.2. 系统结构和存储结构 1.2.1. 系统结构 producer 采用 push 方式向 broker 发送消息；customer 采用 pull 方式从 broker 接受消息。 Producer - 消息生产者，负责发布消息到 Kafka Broker。 Consumer - 消息消费者，从 Kafka Broker 读取消息的客户端。 Consumer Group - 每个 Consumer 属于一个特定的 Consumer Group，若不指定 group name 则属于默认的 group。在同一个 Group 中，每一个 customer 可以消费多个 Partition，但是一个 Partition 只能指定给一个这个 Group 中一个 Customer。 Broker - Kafka 集群包含一个或多个服务器，这种服务器被称为 broker 1.2.2. Topic 的存储结构 Topic - 每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。（物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上，但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处）。 Partition - Partition 是物理上的概念，每个 Topic 包含一个或多个 Partition。为了使得 Kafka 的吞吐率可以线性提高，物理上把 Topic 分成一个或多个 Partition，每个 Partition 在物理上对应一个文件夹，该文件夹下存储这个 Partition 的所有消息和索引文件。 Segment - 是 Partition 目录下的文件，保存存储消息。 索引 - 方便查询 segment 1.2.3. Segment 文件格式 可以把 topic 当做一个数据表，表中每一个记录都是 key,value 形式，如下图。 注意： 必须要有一个 key，如果没有，则默认会生成一个 key，可以把 key 当做一个消息的标识，同一个 key 可能有多条数据。 1.3. 小结 （1）一个主题存在多个分区，每一分区属于哪个 leader broker? 在任意一个 broker 机器都有每一个分区所属 leader 的信息，所以可以通过访问任意一个 broker 获取这些信息。 （2）每个消费者群组对应的分区偏移量的元数据存储在哪里。 最新版本保存在 kafka 中，对应的主题是_consumer_offsets。老版本是在 zookeeper 中。 （3）假设某一个消息处理业务逻辑失败了。是否还可以继续向下执行？如果可以的话，那么此时怎么保证这个消息还会继续被处理呢？ 答案是：正常情况下无法再处理有问题的消息。 这里举一个例子，如 M1-&gt;M2-&gt;M3-&gt;M4，假设第一次 poll 时，得到 M1 和 M2，M1 处理成功，M2 处理失败，我们采用提交方式为处理一个消息就提交一次，此时我们提交偏移量是 offset1，但是当我们第二次执行 poll 时，此时只会获取到 M3 和 M4，因为 poll 的时候是根据本地偏移量来获取的，不是 kafka 中保存的初始偏移量。解决这个问题方法是通过 seek 操作定位到 M2 的位置，此时再执行 poll 时就会获取到 M2 和 M3。 （4）当一个消费者执行了 close 之后，此时会执行再均衡，那么再均衡是在哪里发生的呢？其他同组的消费者如何感知到？ 是通过群组中成为群主的消费者执行再均衡，执行完毕之后,通过群组协调器把每一个消费者负责分区信息发送给消费者，每一个消费者只能知道它负责的分区信息。 （5）如何保证时序性 因为 kafkaf 只保证一个分区内的消息才有时序性，所以只要消息属于同一个 topic 且在同一个分区内，就可以保证 kafka 消费消息是有顺序的了。 2. 安装部署 环境要求：JDK8、ZooKeeper 2.1. 下载解压 进入官方下载地址：http://kafka.apache.org/downloads，选择合适版本。 解压到本地： &gt; tar -xzf kafka_2.11-1.1.0.tgz&gt; cd kafka_2.11-1.1.0 现在您已经在您的机器上下载了最新版本的 Kafka。 2.2. 启动服务器 由于 Kafka 依赖于 ZooKeeper，所以运行前需要先启动 ZooKeeper &gt; bin/zookeeper-server-start.sh config/zookeeper.properties[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)... 然后，启动 Kafka &gt; bin/kafka-server-start.sh config/server.properties[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)... 2.3. 停止服务器 执行所有操作后，可以使用以下命令停止服务器 $ bin/kafka-server-stop.sh config/server.properties 2.4. 创建主题 创建一个名为 test 的 Topic，这个 Topic 只有一个分区以及一个备份： &gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 2.5. 生产者生产消息 运行生产者，然后可以在控制台中输入一些消息，这些消息会发送到服务器： &gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testThis is a messageThis is another message 2.6. 消费者消费消息 启动消费者，然后获得服务器中 Topic 下的消息： &gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginningThis is a messageThis is another message 2.7. 集群部署 复制配置为多份（Windows 使用 copy 命令代理）： &gt; cp config/server.properties config/server-1.properties&gt; cp config/server.properties config/server-2.properties 修改配置： config/server-1.properties: broker.id=1 listeners=PLAINTEXT://:9093 log.dir=/tmp/kafka-logs-1config/server-2.properties: broker.id=2 listeners=PLAINTEXT://:9094 log.dir=/tmp/kafka-logs-2 其中，broker.id 这个参数必须是唯一的。 端口故意配置的不一致，是为了可以在一台机器启动多个应用节点。 根据这两份配置启动三个服务器节点： &gt; bin/kafka-server-start.sh config/server.properties &amp;...&gt; bin/kafka-server-start.sh config/server-1.properties &amp;...&gt; bin/kafka-server-start.sh config/server-2.properties &amp;... 创建一个新的 Topic 使用 三个备份： &gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic 查看主题： &gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topicTopic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 1 Replicas: 1,2,0 Isr: 1,2,0 leader - 负责指定分区的所有读取和写入的节点。每个节点将成为随机选择的分区部分的领导者。 replicas - 是复制此分区日志的节点列表，无论它们是否为领导者，或者即使它们当前处于活动状态。 isr - 是“同步”复制品的集合。这是副本列表的子集，该列表当前处于活跃状态并且已经被领导者捕获。 3. API Stream API 的 maven 依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt; 其他 API 的 maven 依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt; 4. 生产者（Producer） 4.1. 发送消息流程 发送消息流程如下图，需要注意的有： 分区器 Partitioner，分区器决定了一个消息被分配到哪个分区。在我们创建消息时，我们可以选择性指定一个键值 key 或者分区 Partition，如果传入的是 key，则通过图中的分区器 Partitioner 选择一个分区来保存这个消息；如果 key 和 Partition 都没有指定，则会默认生成一个 key。 批次传输。批次，就是一组消息，这些消息属于同一个主题和分区。发送时，会把消息分成批次 Batch 传输，如果每一个消息发送一次，会导致大量的网路开销， 如果消息成功写入 kafka，就返回一个 RecoredMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。 如果消息发送失败，可以进行重试，重试次数可以在配置中指定。 生产者在向 broker 发送消息时是怎么确定向哪一个 broker 发送消息？ 生产者客户端会向任一个 broker 发送一个元数据请求（MetadataRequest），获取到每一个分区对应的 leader 信息，并缓存到本地。 step2:生产者在发送消息时，会指定 Partition 或者通过 key 得到到一个 Partition，然后根据 Partition 从缓存中获取相应的 leader 信息。 4.2. 发送消息方式 4.2.1. 发送并忘记（fire-and-forget） 代码如下，直接通过 send 方法来发送 ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("CustomerCountry", "Precision Products", "France"); try &#123; producer.send(record); &#125; catch (Exception e) &#123; e.printStackTrace();&#125; 4.2.2. 同步发送 代码如下，与“发送并忘记”的方式区别在于多了一个 get()方法，会一直阻塞等待 broker 返回结果： ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("CustomerCountry", "Precision Products", "France"); try &#123; producer.send(record).get(); &#125; catch (Exception e) &#123; e.printStackTrace();&#125; 4.2.3. 异步发送 代码如下，异步方式相对于“发送并忘记”的方式的不同在于，在异步返回时可以执行一些操作，如记录错误或者成功日志。 首先，定义一个 callback private class DemoProducerCallback implements Callback &#123; @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if (e != null) &#123; e.printStackTrace(); &#125; &#125;&#125; 然后，使用这个 callback ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("CustomerCountry", "Biomedical Materials", "USA");producer.send(record, new DemoProducerCallback()); 4.2.4. 发送消息示例 import java.util.Properties;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;/** * Kafka 生产者生产消息示例 生产者配置参考：https://kafka.apache.org/documentation/#producerconfigs */public class ProducerDemo &#123; private static final String HOST = "localhost:9092"; public static void main(String[] args) &#123; // 1. 指定生产者的配置 Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, HOST); properties.put(ProducerConfig.ACKS_CONFIG, "all"); properties.put(ProducerConfig.RETRIES_CONFIG, 0); properties.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384); properties.put(ProducerConfig.LINGER_MS_CONFIG, 1); properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432); properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer"); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer"); // 2. 使用配置初始化 Kafka 生产者 Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(properties); try &#123; // 3. 使用 send 方法发送异步消息 for (int i = 0; i &lt; 100; i++) &#123; String msg = "Message " + i; producer.send(new ProducerRecord&lt;&gt;("HelloWorld", msg)); System.out.println("Sent:" + msg); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 4. 关闭生产者 producer.close(); &#125; &#125;&#125; 5. 消费者（Consumer） 5.1. 消费者和消费者群组 5.1.1. 消费者介绍 消费者以pull 方式从 broker 拉取消息，消费者可以订阅一个或多个主题，然后按照消息生成顺序（kafka 只能保证分区中消息的顺序）读取消息。 一个消息消息只有在所有跟随者节点都进行了同步，才会被消费者获取到。如下图，只能消费 Message0、Message1、Message2： 5.1.2. 消费者分区组 消费者群组可以实现并发的处理消息。一个消费者群组作为消费一个 topic 消息的单元，每一个 Partition 只能隶属于一个消费者群组中一个 customer，如下图 5.1.3. 消费者区组的再均衡 当在群组里面 新增/移除消费者 或者 新增/移除 kafka 集群 broker 节点 时，群组协调器 Broker 会触发再均衡，重新为每一个 Partition 分配消费者。再均衡期间，消费者无法读取消息，造成整个消费者群组一小段时间的不可用。 新增消费者。customer 订阅主题之后，第一次执行 poll 方法 移除消费者。执行 customer.close()操作或者消费客户端宕机，就不再通过 poll 向群组协调器发送心跳了，当群组协调器检测次消费者没有心跳，就会触发再均衡。 新增 broker。如重启 broker 节点 移除 broker。如 kill 掉 broker 节点。 再均衡是是通过消费者群组中的称为“群主”消费者客户端进行的。什么是群主呢？“群主”就是第一个加入群组的消费者。消费者第一次加入群组时，它会向群组协调器发送一个 JoinGroup 的请求，如果是第一个，则此消费者被指定为“群主”（群主是不是和 qq 群很想啊，就是那个第一个进群的人）。 群主分配分区的过程如下： 群主从群组协调器获取群组成员列表，然后给每一个消费者进行分配分区 Partition。 两个分配策略：Range 和 RoundRobin。 Range 策略，就是把若干个连续的分区分配给消费者，如存在分区 1-5，假设有 3 个消费者，则消费者 1 负责分区 1-2,消费者 2 负责分区 3-4，消费者 3 负责分区 5。 RoundRoin 策略，就是把所有分区逐个分给消费者，如存在分区 1-5，假设有 3 个消费者，则分区 1-&gt;消费 1，分区 2-&gt;消费者 2，分区 3&gt;消费者 3，分区 4&gt;消费者 1，分区 5-&gt;消费者 2。 群主分配完成之后，把分配情况发送给群组协调器。 群组协调器再把这些信息发送给消费者。每一个消费者只能看到自己的分配信息，只有群主知道所有消费者的分配信息。 5.2. 消费消息流程 5.2.1. 消费流程 demo 具体步骤如下 step1 创建消费者。 step2 订阅主题。除了订阅主题方式外还有使用指定分组的模式，但是常用方式都是订阅主题方式 stpe3 轮询消息。通过 poll 方法轮询。 stpe4 关闭消费者。在不用消费者之后，会执行 close 操作。close 操作会关闭 socket，并触发当前消费者群组的再均衡。 // 1.构建KafkaCustomerConsumer consumer = buildCustomer();// 2.设置主题consumer.subscribe(Arrays.asList(topic));// 3.接受消息try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(500); System.out.println("customer Message---"); for (ConsumerRecord&lt;String, String&gt; record : records) // print the offset,key and value for the consumer records. System.out.printf("offset = %d, key = %s, value = %s\n", record.offset(), record.key(), record.value()); &#125;&#125; finally &#123; // 4.关闭消息 consumer.close();&#125; 创建消费者的代码如下： public Consumer buildCustomer() &#123; Properties props = new Properties(); // bootstrap.servers是Kafka集群的IP地址。多个时,使用逗号隔开 props.put("bootstrap.servers", "localhost:9092"); // 消费者群组 props.put("group.id", "test"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("session.timeout.ms", "30000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer &lt;String, String&gt;(props); return consumer;&#125; 5.2.2. 消费消息方式 分为订阅主题和指定分组两种方式： 消费者分组模式。通过订阅主题方式时，消费者必须加入到消费者群组中，即消费者必须有一个自己的分组； 独立消费者模式。这种模式就是消费者是独立的不属于任何消费者分组，自己指定消费那些 Partition。 1、订阅主题方式 consumer.subscribe(Arrays.asList(topic)); 2、独立消费者模式 通过 consumer 的 assign(Collection partitions)方法来为消费者指定分区。 public void consumeMessageForIndependentConsumer(String topic)&#123; // 1.构建KafkaCustomer Consumer consumer = buildCustomer(); // 2.指定分区 // 2.1获取可用分区 List&lt;PartitionInfo&gt; partitionInfoList = buildCustomer().partitionsFor(topic); // 2.2指定分区,这里是指定了所有分区,也可以指定个别的分区 if(null != partitionInfoList)&#123; List&lt;TopicPartition&gt; partitions = Lists.newArrayList(); for(PartitionInfo partitionInfo : partitionInfoList)&#123; partitions.add(new TopicPartition(partitionInfo.topic(),partitionInfo.partition())); &#125; consumer.assign(partitions); &#125; // 3.接受消息 while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(500); System.out.println("consume Message---"); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; // print the offset,key and value for the consumer records. System.out.printf("offset = %d, key = %s, value = %s\n", record.offset(), record.key(), record.value()); // 异步提交 consumer.commitAsync(); &#125; &#125;&#125; 5.2.3. 轮询获取消息 通过 poll 来获取消息，但是获取消息时并不是立刻返回结果，需要考虑两个因素： 消费者通过 customer.poll(time)中设置的等待时间 broker 会等待累计一定量数据，然后发送给消费者。这样可以减少网络开销。 poll 处了获取消息外，还有其他作用，如下： 发送心跳信息。消费者通过向被指派为群组协调器的 broker 发送心跳来维护他和群组的从属关系，当机器宕掉后，群组协调器触发再分配 5.3. 提交偏移量 5.3.1. 偏移量和提交 （1）偏移量 偏移量 offeset，是指一个消息在分区中位置。在通过生产者向 kafka 推送消息返回的结果中包含了这个偏移量值，或者在消费者拉取信息时，也会包含消息的偏移量信息。 （2）提交的解释 我们把消息的偏移量提交到 kafka 的操作叫做提交或提交偏移量。 （3）偏移量的应用 目前会有两个位置记录这个偏移量： a. Kafka Broker 保存。消费者通过提交操作，把读取分区中最新消息的偏移量更新到 kafka 服务器端（老版本的 kafka 是保存在 zookeeper 中），即消费者往一个叫做_consumer_offset 的特殊主题发送消息，消息里面包消息的偏移量信息,并且该主题配置清理策略是 compact，即对于每一个 key 只保存最新的值（key 由 groupId、topic 和 partition 组成）。关于提交操作在本节进行讨论。 如果消费者一直处于运行状态，这个偏移量是没有起到作用，只有当加入或者删除一个群组里消费者，然后进行再均衡操作只有，此时为了可以继续之前工作，新的消费者需要知道上一个消费者处理这个分区的位置信息。 b. 消费者客户端保存。消费者客户端会保存 poll()每一次执行后的最后一个消息的偏移量，这样每次执行轮询操作 poll 时，都从这个位置获取信息。这个信息修改可以通过后续小节中三个 seek 方法来修改。 （4）提交时会遇到两个问题 a. 重复处理 当提交的偏移量小于客户端处理的最后一个消息的偏移量时，会出现重复处理消息的问题，如下图 b. 消息丢失 当提交的偏移量大于客户端处理的最后端最后一个消息的偏移量，会出现消息丢失的问题，如下图： （5）提交方式 主要分为：自动提交和手动提交。 a. 自动提交 auto.commit.commit ,默认为 true 自动提交，自动提交时通过轮询方式来做，时间间通过 auto.commit.interval.ms 属性来进行设置。 b. 手动提交 除了自动提交，还可以进行手动提交，手动提交就是通过代码调用函数的方式提交，在使用手动提交时首先需要将 auto.commit.commit 设置为 false，目前有三种方式：同步提交、异步提交、同步和异步结合。 5.3.2. 同步提交 可以通过 commitSync 来进行提交，同步提交会一直提交直到成功。如下 public void customerMessageWithSyncCommit(String topic) &#123; // 1.构建KafkaCustomer Consumer consumer = buildCustomer(); // 2.设置主题 consumer.subscribe(Arrays.asList(topic)); // 3.接受消息 while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(500); System.out.println("customer Message---"); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; // print the offset,key and value for the consumer records. System.out.printf("offset = %d, key = %s, value = %s\n", record.offset(), record.key(), record.value()); // 同步提交 try &#123; consumer.commitSync(); &#125; catch (Exception e) &#123; logger.error("commit error"); &#125; &#125; &#125;&#125; 5.3.3. 异步提交 同步提交一个缺点是，在进行提交 commitAysnc()会阻塞整个下面流程。所以引入了异步提交 commitAsync()，如下代码，这里定义了 OffsetCommitCallback，也可以只进行 commitAsync()，不设置任何参数。 public void customerMessageWithAsyncCommit(String topic) &#123; // 1.构建KafkaCustomer Consumer consumer = buildCustomer(); // 2.设置主题 consumer.subscribe(Arrays.asList(topic)); // 3.接受消息 while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(500); System.out.println("customer Message---"); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; // print the offset,key and value for the consumer records. System.out.printf("offset = %d, key = %s, value = %s\n", record.offset(), record.key(), record.value()); // 异步提交 consumer.commitAsync(new OffsetCommitCallback() &#123; public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception e) &#123; if (e != null) &#123; logger.error("Commit failed for offsets&#123;&#125;", offsets, e); &#125; &#125; &#125;); &#125; &#125;&#125; 5.3.4. 同步和异步提交 代码如下： public void customerMessageWithSyncAndAsyncCommit(String topic) &#123; // 1.构建KafkaCustomer Consumer consumer = buildCustomer(); // 2.设置主题 consumer.subscribe(Arrays.asList(topic)); // 3.接受消息 try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(500); System.out.println("customer Message---"); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; // print the offset,key and value for the consumer records. System.out.printf("offset = %d, key = %s, value = %s\n", record.offset(), record.key(), record.value()); // 异步提交 consumer.commitAsync(); &#125; &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; // 同步提交 try &#123; consumer.commitSync(); &#125; finally &#123; consumer.close(); &#125; &#125;&#125; 5.4. 从指定偏移量获取数据 我们读取消息是通过 poll 方法。它根据消费者客户端本地保存的当前偏移量来获取消息。如果我们需要从指定偏移量位置获取数据，此时就需要修改这个值为我们想要读取消息开始的地方，目前有如下三个方法： seekToBeginning(Collection partitions)。可以修改分区当前偏移量为分区的起始位置、 seekToEnd(Collection partitions)。可以修改分区当前偏移量为分区的末尾位置 seek(TopicPartition partition, long offset); 可以修改分区当前偏移量为分区的起始位置 通过 seek(TopicPartition partition, long offset)可以实现处理消息和提交偏移量在一个事务中完成。思路就是需要在可短建立一个数据表，保证处理消息和和消息偏移量位置写入到这个数据表在一个事务中，此时就可以保证处理消息和记录偏移量要么同时成功，要么同时失败。代码如下： consumer.subscribe(topic);// 1.第一次调用pool,加入消费者群组consumer.poll(0);// 2.获取负责的分区，并从本地数据库读取改分区最新偏移量，并通过seek方法修改poll获取消息的位置for (TopicPartition partition: consumer.assignment()) consumer.seek(partition, getOffsetFromDB(partition));while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; processRecord(record); storeRecordInDB(record); storeOffsetInDB(record.topic(), record.partition(), record.offset()); &#125; commitDBTransaction();&#125; 6. Broker 6.1. 集群控制器 控制器除了具有一般 broker 的功能，还负责分区 leader 的选举。 6.2. 分区 leader 和 follower Kafka 在 0.8 以前的版本中，如果一个 broker 机器宕机了，其上面的 Partition 都不能用了。为了实现 High Availablity，引入了复制功能，即一个 Partition 还会在其他的 broker 上面进行备份。为了实现复制功能，引入了分区 leader 和 follower： （1）Leader 作用 生产者和消费者请求都会经过这个 leader。Producer 和 Concumer 往一个 Partition 写入和读取消息时，都会首先查找这个 Partition 的 leader 保存那些 follower 节点的状态与自己是一致的。 （2）follower 作用： 定时通过类似消费者的 poll 方法从 leader 中获取消息，进行备份 同一个 topic 的不同 Partition 会分布在多个 broker 上，而且一个 Partition 还会在其他的 broker 上面进行备份，Producer 在发布消息到某个 Partition 时，先找到该 Partition 的 Leader，然后向这个 leader 推送消息；每个 Follower 都从 Leader 拉取消息，拉取消息成功之后，向 leader 发送一个 ack 确认。如下一个流程图： 6.3. 群组协调器 群组协调器，顾名思义就是维护消费者群组 ，消费者通过向被指派为群组协调器的 broker（不同的群组可以有不同的协调器）发送心跳来维护它们和群组的从属关系，以及它们对分区的所有权。 1、触发再均衡 只要消费者以正常的时间间隔发送心跳，就会被认为是活跃的。消费者是通过 poll 获取消息时，发送的心跳的，当消费者客户端宕机之后，群组协调器在一段内没有收到心跳，则此时会认为消费者已死亡，然后触发一次再均衡。具体再均衡流程，可以参考上面的“再均衡”小节。 7. Zookeeper 集群 7.1. 节点信息 参考：https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper Zookeeper 保存的就是节点信息和节点状态，不会保存 kafka 的消息信息，节点信息包括： broker。broer 启动时在 zookeeper 注册、并通过 watcher 监听 broker 节点变化；并且还记录 topic 和 Partition 的信息。 consumers,消费者节点信息 admin config controller 和 controloer_epoch 7.1.1. broker （1）Topic 的注册信息 作用：在创建 zookeeper 时，注册 topic 的 Partition 信息，包括每一个分区的复制节点 id。 路径：/brokers/topics/[topic] 数据格式： Schema:&#123; "fields" : [ &#123;"name": "version", "type": "int", "doc": "version id"&#125;, &#123;"name": "partitions", "type": &#123;"type": "map", "values": &#123;"type": "array", "items": "int", "doc": "a list of replica ids"&#125;, "doc": "a map from partition id to replica list"&#125;, &#125; ]&#125;Example:&#123; "version": 1, "partitions": &#123;"0": [0, 1, 3] &#125; &#125; # 分区0的对应的复制节点是0、1、3.&#125; （2）分区信息 作用：记录分区信息，如分区的 leader 信息 路径信息：/brokers/topics/[topic]/partitions/[partitionId]/state 格式： Schema:&#123; "fields": [ &#123;"name": "version", "type": "int", "doc": "version id"&#125;, &#123;"name": "isr", "type": &#123;"type": "array", "items": "int", "doc": "an array of the id of replicas in isr"&#125; &#125;, &#123;"name": "leader", "type": "int", "doc": "id of the leader replica"&#125;, &#123;"name": "controller_epoch", "type": "int", "doc": "epoch of the controller that last updated the leader and isr info"&#125;, &#123;"name": "leader_epoch", "type": "int", "doc": "epoch of the leader"&#125; ]&#125;Example:&#123; "version": 1, "isr": [0,1], "leader": 0, "controller_epoch": 1, "leader_epoch": 0&#125; （3）broker 信息 作用：在 borker 启动时，向 zookeeper 注册节点信息 路径：/brokers/ids/[brokerId] 数据格式： Schema:&#123; "fields": [ &#123;"name": "version", "type": "int", "doc": "version id"&#125;, &#123;"name": "host", "type": "string", "doc": "ip address or host name of the broker"&#125;, &#123;"name": "port", "type": "int", "doc": "port of the broker"&#125;, &#123;"name": "jmx_port", "type": "int", "doc": "port for jmx"&#125; ]&#125;Example:&#123; "version": 1, "host": "192.168.1.148", "port": 9092, "jmx_port": 9999&#125; 7.1.2. controller 和 controller_epoch （1）控制器的 epoch: /controller_epoch -&gt; int (epoch) （2）控制器的注册信息: /controller -&gt; int (broker id of the controller) 7.1.3. consumer （1）消费者注册信息: 路径：/consumers/[groupId]/ids/[consumerId] 数据格式： Schema:&#123; "fields": [ &#123;"name": "version", "type": "int", "doc": "version id"&#125;, &#123;"name": "pattern", "type": "string", "doc": "can be of static, white_list or black_list"&#125;, &#123;"name": "subscription", "type" : &#123;"type": "map", "values": &#123;"type": "int"&#125;, "doc": "a map from a topic or a wildcard pattern to the number of streams"&#125; &#125; ]&#125;Example:A static subscription:&#123; "version": 1, "pattern": "static", "subscription": &#123;"topic1": 1, "topic2": 2&#125;&#125;A whitelist subscription:&#123; "version": 1, "pattern": "white_list", "subscription": &#123;"abc": 1&#125;&#125;A blacklist subscription:&#123; "version": 1, "pattern": "black_list", "subscription": &#123;"abc": 1&#125;&#125; 7.1.4. admin （1）Re-assign partitions 路径：/admin/reassign_partitions 数据格式： &#123; "fields":[ &#123; "name":"version", "type":"int", "doc":"version id" &#125;, &#123; "name":"partitions", "type":&#123; "type":"array", "items":&#123; "fields":[ &#123; "name":"topic", "type":"string", "doc":"topic of the partition to be reassigned" &#125;, &#123; "name":"partition", "type":"int", "doc":"the partition to be reassigned" &#125;, &#123; "name":"replicas", "type":"array", "items":"int", "doc":"a list of replica ids" &#125; ], &#125; "doc":"an array of partitions to be reassigned to new replicas" &#125; &#125; ]&#125;Example:&#123; "version": 1, "partitions": [ &#123; "topic": "Foo", "partition": 1, "replicas": [0, 1, 3] &#125; ]&#125; （2）Preferred replication election 路径：/admin/preferred_replica_election 数据格式： &#123; "fields":[ &#123; "name":"version", "type":"int", "doc":"version id" &#125;, &#123; "name":"partitions", "type":&#123; "type":"array", "items":&#123; "fields":[ &#123; "name":"topic", "type":"string", "doc":"topic of the partition for which preferred replica election should be triggered" &#125;, &#123; "name":"partition", "type":"int", "doc":"the partition for which preferred replica election should be triggered" &#125; ], &#125; "doc":"an array of partitions for which preferred replica election should be triggered" &#125; &#125; ]&#125;Example:&#123; "version": 1, "partitions": [ &#123; "topic": "Foo", "partition": 1 &#125;, &#123; "topic": "Bar", "partition": 0 &#125; ]&#125; （3）Delete topics /admin/delete_topics/[topic_to_be_deleted] (the value of the path in empty) 7.1.5. config Topic Configuration /config/topics/[topic_name] 数据格式： &#123; "version": 1, "config": &#123; "config.a": "x", "config.b": "y", ... &#125;&#125; 7.2. zookeeper 一些总结 离开了 Zookeeper, Kafka 不能对 Topic 进行新增操作, 但是仍然可以 produce 和 consume 消息. 8. 资料 Kafka(03) Kafka 介绍]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaweb</tag>
        <tag>分布式</tag>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 基础篇]]></title>
    <url>%2Fblog%2F2018%2F07%2F12%2Fjava%2Fjavaweb%2Fdistributed%2Frpc%2Fzookeeper-basics%2F</url>
    <content type="text"><![CDATA[ZooKeeper 基础篇 ZooKeeper 是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。 ZooKeeper 的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 本文旨在快速入门 ZooKeeper，侧重于介绍如何使用。 1. 安装 1.1. 下载解压 ZooKeeper 1.2. 创建配置文件 1.3. 启动 ZooKeeper 服务器 1.4. 启动 CLI 1.5. 停止 ZooKeeper 服务器 2. CLI 2.1. 创建 Znodes 2.2. 获取数据 2.3. Watch（监视） 2.4. 设置数据 2.5. 创建子项/子节点 2.6. 列出子项 2.7. 检查状态 2.8. 移除 Znode 3. API 3.1. ZooKeeper API 的基础知识 3.2. Java 绑定 3.3. 连接到 ZooKeeper 集合 3.4. 创建 Znode 3.5. Exists - 检查 Znode 的存在 3.6. getData 方法 3.7. setData 方法 3.8. getChildren 方法 3.9. 删除 Znode 4. 资源 1. 安装 在安装 ZooKeeper 之前，请确保你的系统是在以下任一操作系统上运行： 任意 Linux OS - 支持开发和部署。适合演示应用程序。 Windows OS - 仅支持开发。 Mac OS - 仅支持开发。 环境要求：JDK6+ 安装步骤如下： 1.1. 下载解压 ZooKeeper 进入官方下载地址：http://zookeeper.apache.org/releases.html#download ，选择合适版本。 解压到本地： $ tar -zxf zookeeper-3.4.6.tar.gz$ cd zookeeper-3.4.6 1.2. 创建配置文件 你必须创建 conf/zoo.cfg 文件，否则启动时会提示你没有此文件。 初次尝试，不妨直接使用 Kafka 提供的模板配置文件 conf/zoo_sample.cfg： $ cp conf/zoo_sample.cfg conf/zoo.cfg 1.3. 启动 ZooKeeper 服务器 执行以下命令 $ bin/zkServer.sh start 执行此命令后，你将收到以下响应 $ JMX enabled by default$ Using config: /Users/../zookeeper-3.4.6/bin/../conf/zoo.cfg$ Starting zookeeper ... STARTED 1.4. 启动 CLI 键入以下命令 $ bin/zkCli.sh 键入上述命令后，将连接到 ZooKeeper 服务器，你应该得到以下响应。 Connecting to localhost:2181................................................Welcome to ZooKeeper!................................WATCHER::WatchedEvent state:SyncConnected type: None path:null[zk: localhost:2181(CONNECTED) 0] 1.5. 停止 ZooKeeper 服务器 连接服务器并执行所有操作后，可以使用以下命令停止 zookeeper 服务器。 $ bin/zkServer.sh stop 本节安装内容参考：Zookeeper 安装 2. CLI ZooKeeper 命令行界面（CLI）用于与 ZooKeeper 集合进行交互以进行开发。它有助于调试和解决不同的选项。 要执行 ZooKeeper CLI 操作，首先打开 ZooKeeper 服务器（“bin/zkServer.sh start”），然后打开 ZooKeeper 客户端（“bin/zkCli.sh”）。一旦客户端启动，你可以执行以下操作： 创建 znode 获取数据 监视 znode 的变化 设置数据 创建 znode 的子节点 列出 znode 的子节点 检查状态 移除/删除 znode 现在让我们用一个例子逐个了解上面的命令。 2.1. 创建 Znodes 用给定的路径创建一个 znode。flag 参数指定创建的 znode 是临时的，持久的还是顺序的。默认情况下，所有 znode 都是持久的。 当会话过期或客户端断开连接时，临时节点（flag：-e）将被自动删除。 顺序节点保证 znode 路径将是唯一的。 ZooKeeper 集合将向 znode 路径填充 10 位序列号。例如，znode 路径 /myapp 将转换为 /myapp0000000001，下一个序列号将为 /myapp0000000002。如果没有指定 flag，则 znode 被认为是持久的。 语法： create /path /data 示例： create /FirstZnode “Myfirstzookeeper-app" 输出： [zk: localhost:2181(CONNECTED) 0] create /FirstZnode “Myfirstzookeeper-app"Created /FirstZnode 要创建顺序节点，请添加 flag：-s，如下所示。 语法： create -s /path /data 示例： create -s /FirstZnode second-data 输出： [zk: localhost:2181(CONNECTED) 2] create -s /FirstZnode “second-data"Created /FirstZnode0000000023 要创建临时节点，请添加 flag：-e ，如下所示。 语法： create -e /path /data 示例： create -e /SecondZnode “Ephemeral-data" 输出： [zk: localhost:2181(CONNECTED) 2] create -e /SecondZnode “Ephemeral-data"Created /SecondZnode 记住当客户端断开连接时，临时节点将被删除。你可以通过退出 ZooKeeper CLI，然后重新打开 CLI 来尝试。 2.2. 获取数据 它返回 znode 的关联数据和指定 znode 的元数据。你将获得信息，例如上次修改数据的时间，修改的位置以及数据的相关信息。此 CLI 还用于分配监视器以显示数据相关的通知。 语法： get /path 示例： get /FirstZnode 输出： [zk: localhost:2181(CONNECTED) 1] get /FirstZnode“Myfirstzookeeper-app"cZxid = 0x7fctime = Tue Sep 29 16:15:47 IST 2015mZxid = 0x7fmtime = Tue Sep 29 16:15:47 IST 2015pZxid = 0x7fcversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 22numChildren = 0 要访问顺序节点，必须输入 znode 的完整路径。 示例： get /FirstZnode0000000023 输出： [zk: localhost:2181(CONNECTED) 1] get /FirstZnode0000000023“Second-data"cZxid = 0x80ctime = Tue Sep 29 16:25:47 IST 2015mZxid = 0x80mtime = Tue Sep 29 16:25:47 IST 2015pZxid = 0x80cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 13numChildren = 0 2.3. Watch（监视） 当指定的 znode 或 znode 的子数据更改时，监视器会显示通知。你只能在 get 命令中设置watch。 语法： get /path [watch] 1 示例： get /FirstZnode 1 输出： [zk: localhost:2181(CONNECTED) 1] get /FirstZnode 1“Myfirstzookeeper-app"cZxid = 0x7fctime = Tue Sep 29 16:15:47 IST 2015mZxid = 0x7fmtime = Tue Sep 29 16:15:47 IST 2015pZxid = 0x7fcversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 22numChildren = 0 输出类似于普通的 get 命令，但它会等待后台等待 znode 更改。&lt;从这里开始&gt; 2.4. 设置数据 设置指定 znode 的数据。完成此设置操作后，你可以使用 get CLI 命令检查数据。 语法： set /path /data 示例： set /SecondZnode Data-updated 输出： [zk: localhost:2181(CONNECTED) 1] get /SecondZnode “Data-updated"cZxid = 0x82ctime = Tue Sep 29 16:29:50 IST 2015mZxid = 0x83mtime = Tue Sep 29 16:29:50 IST 2015pZxid = 0x82cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x15018b47db00000dataLength = 14numChildren = 0 如果你在 get 命令中分配了watch选项（如上一个命令），则输出将类似如下所示。 输出： [zk: localhost:2181(CONNECTED) 1] get /FirstZnode “Mysecondzookeeper-app"WATCHER: :WatchedEvent state:SyncConnected type:NodeDataChanged path:/FirstZnodecZxid = 0x7fctime = Tue Sep 29 16:15:47 IST 2015mZxid = 0x84mtime = Tue Sep 29 17:14:47 IST 2015pZxid = 0x7fcversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 23numChildren = 0 2.5. 创建子项/子节点 创建子节点类似于创建新的 znode。唯一的区别是，子 znode 的路径也将具有父路径。 语法： create /parent/path/subnode/path /data 示例： create /FirstZnode/Child1 firstchildren 输出： [zk: localhost:2181(CONNECTED) 16] create /FirstZnode/Child1 “firstchildren"created /FirstZnode/Child1[zk: localhost:2181(CONNECTED) 17] create /FirstZnode/Child2 “secondchildren"created /FirstZnode/Child2 2.6. 列出子项 此命令用于列出和显示 znode 的子项。 语法： ls /path 示例： ls /MyFirstZnode 输出： [zk: localhost:2181(CONNECTED) 2] ls /MyFirstZnode[mysecondsubnode, myfirstsubnode] 2.7. 检查状态 状态描述指定的 znode 的元数据。它包含时间戳，版本号，ACL，数据长度和子 znode 等细项。 语法： stat /path 示例： stat /FirstZnode 输出： [zk: localhost:2181(CONNECTED) 1] stat /FirstZnodecZxid = 0x7fctime = Tue Sep 29 16:15:47 IST 2015mZxid = 0x7fmtime = Tue Sep 29 17:14:24 IST 2015pZxid = 0x7fcversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 23numChildren = 0 2.8. 移除 Znode 移除指定的 znode 并递归其所有子节点。只有在这样的 znode 可用的情况下才会发生。 语法： rmr /path 示例： rmr /FirstZnode 输出： [zk: localhost:2181(CONNECTED) 10] rmr /FirstZnode[zk: localhost:2181(CONNECTED) 11] get /FirstZnodeNode does not exist: /FirstZnode 删除（delete/path）命令类似于 remove 命令，除了它只适用于没有子节点的 znode。 3. API ZooKeeper 有一个绑定 Java 和 C 的官方 API。Zookeeper 社区为大多数语言（.NET，python 等）提供非官方 API。 使用 ZooKeeper API，应用程序可以连接，交互，操作数据，协调，最后断开与 ZooKeeper 集合的连接。 ZooKeeper API 具有丰富的功能，以简单和安全的方式获得 ZooKeeper 集合的所有功能。ZooKeeper API 提供同步和异步方法。 ZooKeeper 集合和 ZooKeeper API 在各个方面都完全相辅相成，对开发人员有很大的帮助。让我们在本章讨论 Java 绑定。 3.1. ZooKeeper API 的基础知识 与 ZooKeeper 集合进行交互的应用程序称为 ZooKeeper 客户端。 Znode 是 ZooKeeper 集合的核心组件，ZooKeeper API 提供了一小组方法使用 ZooKeeper 集合来操纵 znode 的所有细节。 客户端应该遵循以步骤，与 ZooKeeper 集合进行清晰和干净的交互。 连接到 ZooKeeper 集合。ZooKeeper 集合为客户端分配会话 ID。 定期向服务器发送心跳。否则，ZooKeeper 集合将过期会话 ID，客户端需要重新连接。 只要会话 ID 处于活动状态，就可以获取/设置 znode。 所有任务完成后，断开与 ZooKeeper 集合的连接。如果客户端长时间不活动，则 ZooKeeper 集合将自动断开客户端。 3.2. Java 绑定 让我们来了解本章中最重要的一组 ZooKeeper API。ZooKeeper API 的核心部分是ZooKeeper 类。它提供了在其构造函数中连接 ZooKeeper 集合的选项，并具有以下方法： connect - 连接到 ZooKeeper 集合 create- 创建 znode exists- 检查 znode 是否存在及其信息 getData - 从特定的 znode 获取数据 setData - 在特定的 znode 中设置数据 getChildren - 获取特定 znode 中的所有子节点 delete - 删除特定的 znode 及其所有子项 close - 关闭连接 3.3. 连接到 ZooKeeper 集合 ZooKeeper 类通过其构造函数提供 connect 功能。构造函数的签名如下 : ZooKeeper(String connectionString, int sessionTimeout, Watcher watcher) connectionString - ZooKeeper 集合主机。 sessionTimeout - 会话超时（以毫秒为单位）。 watcher - 实现“监视器”界面的对象。ZooKeeper 集合通过监视器对象返回连接状态。 让我们创建一个新的帮助类 ZooKeeperConnection ，并添加一个方法 connect 。 connect 方法创建一个 ZooKeeper 对象，连接到 ZooKeeper 集合，然后返回对象。 这里 CountDownLatch 用于停止（等待）主进程，直到客户端与 ZooKeeper 集合连接。 ZooKeeper 集合通过监视器回调来回复连接状态。一旦客户端与 ZooKeeper 集合连接，监视器回调就会被调用，并且监视器回调函数调用CountDownLatch的countDown方法来释放锁，在主进程中await。 以下是与 ZooKeeper 集合连接的完整代码。 示例： // import java classesimport java.io.IOException;import java.util.concurrent.CountDownLatch;// import zookeeper classesimport org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.KeeperState;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.AsyncCallback.StatCallback;import org.apache.zookeeper.KeeperException.Code;import org.apache.zookeeper.data.Stat;public class ZooKeeperConnection &#123; // declare zookeeper instance to access ZooKeeper ensemble private ZooKeeper zoo; final CountDownLatch connectedSignal = new CountDownLatch(1); // Method to connect zookeeper ensemble. public ZooKeeper connect(String host) throws IOException,InterruptedException &#123; zoo = new ZooKeeper(host,5000,new Watcher() &#123; public void process(WatchedEvent we) &#123; if (we.getState() == KeeperState.SyncConnected) &#123; connectedSignal.countDown(); &#125; &#125; &#125;); connectedSignal.await(); return zoo; &#125; // Method to disconnect from zookeeper server public void close() throws InterruptedException &#123; zoo.close(); &#125;&#125; 保存上面的代码，它将在下一节中用于连接 ZooKeeper 集合。 3.4. 创建 Znode ZooKeeper 类提供了在 ZooKeeper 集合中创建一个新的 znode 的create方法。 create 方法的签名如下： create(String path, byte[] data, List&lt;ACL&gt; acl, CreateMode createMode) path - Znode 路径。例如，/myapp1，/myapp2，/myapp1/mydata1，myapp2/mydata1/myanothersubdata data - 要存储在指定 znode 路径中的数据 acl - 要创建的节点的访问控制列表。ZooKeeper API 提供了一个静态接口 ZooDefs.Ids 来获取一些基本的 acl 列表。例如，ZooDefs.Ids.OPEN_ACL_UNSAFE 返回打开 znode 的 acl 列表。 createMode - 节点的类型，即临时，顺序或两者。这是一个枚举。 让我们创建一个新的 Java 应用程序来检查 ZooKeeper API 的 create 功能。创建文件 ZKCreate.java 。在 main 方法中，创建一个类型为 ZooKeeperConnection 的对象，并调用 connect 方法连接到 ZooKeeper 集合。 connect 方法将返回 ZooKeeper 对象 zk 。现在，请使用自定义path和data调用 zk 对象的 create 方法。 创建 znode 的完整程序代码如下： 示例： import java.io.IOException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.KeeperState;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.ZooDefs;public class ZKCreate &#123; // create static instance for zookeeper class. private static ZooKeeper zk; // create static instance for ZooKeeperConnection class. private static ZooKeeperConnection conn; // Method to create znode in zookeeper ensemble public static void create(String path, byte[] data) throws KeeperException,InterruptedException &#123; zk.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; public static void main(String[] args) &#123; // znode path String path = "/MyFirstZnode"; // Assign path to znode // data in byte array byte[] data = "My first zookeeper app".getBytes(); // Declare data try &#123; conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); create(path, data); // Create the data to the specified path conn.close(); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); //Catch error message &#125; &#125;&#125; 一旦编译和执行应用程序，将在 ZooKeeper 集合中创建具有指定数据的 znode。你可以使用 ZooKeeper CLI zkCli.sh 进行检查。 cd /path/to/zookeeperbin/zkCli.sh&gt;&gt;&gt; get /MyFirstZnode 3.5. Exists - 检查 Znode 的存在 ZooKeeper 类提供了 exists 方法来检查 znode 的存在。如果指定的 znode 存在，则返回一个 znode 的元数据。exists方法的签名如下： exists(String path, boolean watcher) path- Znode 路径 watcher - 布尔值，用于指定是否监视指定的 znode 让我们创建一个新的 Java 应用程序来检查 ZooKeeper API 的“exists”功能。创建文件“ZKExists.java”。在 main 方法中，使用“ZooKeeperConnection”对象创建 ZooKeeper 对象“zk”。然后，使用自定义“path”调用“zk”对象的“exists”方法。完整的列表如下： 示例： import java.io.IOException;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.KeeperState;import org.apache.zookeeper.data.Stat;public class ZKExists &#123; private static ZooKeeper zk; private static ZooKeeperConnection conn; // Method to check existence of znode and its status, if znode is available. public static Stat znode_exists(String path) throws KeeperException,InterruptedException &#123; return zk.exists(path, true); &#125; public static void main(String[] args) throws InterruptedException,KeeperException &#123; String path = "/MyFirstZnode"; // Assign znode to the specified path try &#123; conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); Stat stat = znode_exists(path); // Stat checks the path of the znode if(stat != null) &#123; System.out.println("Node exists and the node version is " + stat.getVersion()); &#125; else &#123; System.out.println("Node does not exists"); &#125; &#125; catch(Exception e) &#123; System.out.println(e.getMessage()); // Catches error messages &#125; &#125;&#125; 一旦编译和执行应用程序，你将获得以下输出。 Node exists and the node version is 1. 3.6. getData 方法 ZooKeeper 类提供 getData 方法来获取附加在指定 znode 中的数据及其状态。 getData 方法的签名如下： getData(String path, Watcher watcher, Stat stat) path - Znode 路径。 watcher - 监视器类型的回调函数。当指定的 znode 的数据改变时，ZooKeeper 集合将通过监视器回调进行通知。这是一次性通知。 stat - 返回 znode 的元数据。 让我们创建一个新的 Java 应用程序来了解 ZooKeeper API 的 getData 功能。创建文件 ZKGetData.java 。在 main 方法中，使用 ZooKeeperConnection 对象创建一个 ZooKeeper 对象 zk 。然后，使用自定义路径调用 zk 对象的 getData 方法。 下面是从指定节点获取数据的完整程序代码： 示例： import java.io.IOException;import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.KeeperState;import org.apache.zookeeper.data.Stat;public class ZKGetData &#123; private static ZooKeeper zk; private static ZooKeeperConnection conn; public static Stat znode_exists(String path) throws KeeperException,InterruptedException &#123; return zk.exists(path,true); &#125; public static void main(String[] args) throws InterruptedException, KeeperException &#123; String path = "/MyFirstZnode"; final CountDownLatch connectedSignal = new CountDownLatch(1); try &#123; conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); Stat stat = znode_exists(path); if(stat != null) &#123; byte[] b = zk.getData(path, new Watcher() &#123; public void process(WatchedEvent we) &#123; if (we.getType() == Event.EventType.None) &#123; switch(we.getState()) &#123; case Expired: connectedSignal.countDown(); break; &#125; &#125; else &#123; String path = "/MyFirstZnode"; try &#123; byte[] bn = zk.getData(path, false, null); String data = new String(bn, "UTF-8"); System.out.println(data); connectedSignal.countDown(); &#125; catch(Exception ex) &#123; System.out.println(ex.getMessage()); &#125; &#125; &#125; &#125;, null); String data = new String(b, "UTF-8"); System.out.println(data); connectedSignal.await(); &#125; else &#123; System.out.println("Node does not exists"); &#125; &#125; catch(Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125;&#125; 一旦编译和执行应用程序，你将获得以下输出 My first zookeeper app 应用程序将等待 ZooKeeper 集合的进一步通知。使用 ZooKeeper CLI zkCli.sh 更改指定 znode 的数据。 cd /path/to/zookeeperbin/zkCli.sh&gt;&gt;&gt; set /MyFirstZnode Hello 现在，应用程序将打印以下输出并退出。 Hello 3.7. setData 方法 ZooKeeper 类提供 setData 方法来修改指定 znode 中附加的数据。 setData 方法的签名如下： setData(String path, byte[] data, int version) path- Znode 路径 data - 要存储在指定 znode 路径中的数据。 version- znode 的当前版本。每当数据更改时，ZooKeeper 会更新 znode 的版本号。 现在让我们创建一个新的 Java 应用程序来了解 ZooKeeper API 的 setData 功能。创建文件 ZKSetData.java 。在 main 方法中，使用 ZooKeeperConnection 对象创建一个 ZooKeeper 对象 zk 。然后，使用指定的路径，新数据和节点版本调用 zk 对象的 setData 方法。 以下是修改附加在指定 znode 中的数据的完整程序代码。 示例： import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.KeeperState;import java.io.IOException;public class ZKSetData &#123; private static ZooKeeper zk; private static ZooKeeperConnection conn; // Method to update the data in a znode. Similar to getData but without watcher. public static void update(String path, byte[] data) throws KeeperException,InterruptedException &#123; zk.setData(path, data, zk.exists(path,true).getVersion()); &#125; public static void main(String[] args) throws InterruptedException,KeeperException &#123; String path= "/MyFirstZnode"; byte[] data = "Success".getBytes(); //Assign data which is to be updated. try &#123; conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); update(path, data); // Update znode data to the specified path &#125; catch(Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125;&#125; 编译并执行应用程序后，指定的 znode 的数据将被改变，并且可以使用 ZooKeeper CLI zkCli.sh 进行检查。 cd /path/to/zookeeperbin/zkCli.sh&gt;&gt;&gt; get /MyFirstZnode 3.8. getChildren 方法 ZooKeeper 类提供 getChildren 方法来获取特定 znode 的所有子节点。 getChildren 方法的签名如下： getChildren(String path, Watcher watcher) path - Znode 路径。 watcher - 监视器类型的回调函数。当指定的 znode 被删除或 znode 下的子节点被创建/删除时，ZooKeeper 集合将进行通知。这是一次性通知。 示例： import java.io.IOException;import java.util.*;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.Watcher.Event.KeeperState;import org.apache.zookeeper.data.Stat;public class ZKGetChildren &#123; private static ZooKeeper zk; private static ZooKeeperConnection conn; // Method to check existence of znode and its status, if znode is available. public static Stat znode_exists(String path) throws KeeperException,InterruptedException &#123; return zk.exists(path,true); &#125; public static void main(String[] args) throws InterruptedException,KeeperException &#123; String path = "/MyFirstZnode"; // Assign path to the znode try &#123; conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); Stat stat = znode_exists(path); // Stat checks the path if(stat!= null) &#123; // getChildren method - get all the children of znode.It has two args, path and watch List &lt;String&gt; children = zk.getChildren(path, false); for(int i = 0; i &lt; children.size(); i++) System.out.println(children.get(i)); //Print children's &#125; else &#123; System.out.println("Node does not exists"); &#125; &#125; catch(Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125;&#125; 在运行程序之前，让我们使用 ZooKeeper CLI zkCli.sh 为 /MyFirstZnode 创建两个子节点。 cd /path/to/zookeeperbin/zkCli.sh&gt;&gt;&gt; create /MyFirstZnode/myfirstsubnode Hi&gt;&gt;&gt; create /MyFirstZnode/mysecondsubmode Hi 现在，编译和运行程序将输出上面创建的 znode。 myfirstsubnodemysecondsubnode 3.9. 删除 Znode ZooKeeper 类提供了 delete 方法来删除指定的 znode。 delete 方法的签名如下： delete(String path, int version) path - Znode 路径。 version - znode 的当前版本。 让我们创建一个新的 Java 应用程序来了解 ZooKeeper API 的 delete 功能。创建文件 ZKDelete.java 。在 main 方法中，使用 ZooKeeperConnection 对象创建一个 ZooKeeper 对象 zk 。然后，使用指定的路径和版本号调用 zk 对象的 delete 方法。 删除 znode 的完整程序代码如下： 示例： import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.KeeperException;public class ZKDelete &#123; private static ZooKeeper zk; private static ZooKeeperConnection conn; // Method to check existence of znode and its status, if znode is available. public static void delete(String path) throws KeeperException,InterruptedException &#123; zk.delete(path,zk.exists(path,true).getVersion()); &#125; public static void main(String[] args) throws InterruptedException,KeeperException &#123; String path = "/MyFirstZnode"; //Assign path to the znode try &#123; conn = new ZooKeeperConnection(); zk = conn.connect("localhost"); delete(path); //delete the node with the specified path &#125; catch(Exception e) &#123; System.out.println(e.getMessage()); // catches error messages &#125; &#125;&#125; 4. 资源 | 官网 | 官网文档 | Github |]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>rpc</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZooKeeper 高级篇]]></title>
    <url>%2Fblog%2F2018%2F07%2F10%2Fjava%2Fjavaweb%2Fdistributed%2Frpc%2Fzookeeper-advanced%2F</url>
    <content type="text"><![CDATA[ZooKeeper 高级篇 ZooKeeper 是一个分布式应用协调系统，已经用到了许多分布式项目中，用来完成统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等工作。 本文侧重于总结 ZooKeeper 工作原理。 1. 概述 1.1. ZooKeeper 是什么？ 1.2. ZooKeeper 提供了什么？ 1.3. Zookeeper 的特性 1.4. 工作原理 1.5. Server 工作状态 2. 文件系统 2.1. znode 类型 3. 通知机制 4. 应用场景 4.1. 统一命名服务（Name Service） 4.2. 配置管理（Configuration Management） 4.3. 集群管理（Group Membership） 4.4. 分布式锁 4.5. 队列管理 5. 复制 6. 选举流程 7. 同步流程 8. 资源 8.1. 官方资源 8.2. 文章 1. 概述 1.1. ZooKeeper 是什么？ ZooKeeper 作为一个分布式的服务框架，主要用来解决分布式集群中应用系统的一致性问题，它能提供基于类似于文件系统的目录节点树方式的数据存储，但是 ZooKeeper 并不是用来专门存储数据的，它的作用主要是用来维护和监控你存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理。 1.2. ZooKeeper 提供了什么？ 文件系统 通知机制 1.3. Zookeeper 的特性 最终一致性：client 不论连接到哪个 Server，展示给它都是同一个视图，这是 zookeeper 最重要的性能。 可靠性：具有简单、健壮、良好的性能，如果消息被到一台服务器接受，那么它将被所有的服务器接受。 实时性：Zookeeper 保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。但由于网络延时等原因，Zookeeper 不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用 sync()接口。 等待无关（wait-free）：慢的或者失效的 client 不得干预快速的 client 的请求，使得每个 client 都能有效的等待。 原子性：更新只能成功或者失败，没有中间状态。 顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息 a 在消息 b 前发布，则在所有 Server 上消息 a 都将在消息 b 前被发布；偏序是指如果一个消息 b 在消息 a 后被同一个发送者发布，a 必将排在 b 前面。 1.4. 工作原理 ZooKeeper 的核心是原子广播，这个机制保证了各个 Server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多数 Server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 Server 具有相同的系统状态。 为了保证事务的顺序一致性，ZooKeeper 采用了递增的事务 id 号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上了 zxid。实现中 zxid 是一个 64 位的数字，它高 32 位是 epoch 用来标识 leader 关系是否改变，每次一个 leader 被选出来，它都会有一个新的 epoch，标识当前属于那个 leader 的统治时期。低 32 位用于递增计数。 1.5. Server 工作状态 每个 Server 在工作过程中有三种状态： LOOKING - 当前 Server 不知道 leader 是谁，正在搜寻 LEADING - 当前 Server 即为选举出来的 leader FOLLOWING - leader 已经选举出来，当前 Server 与之同步 2. 文件系统 ZooKeeper 会维护一个具有层次关系的数据结构，它非常类似于一个标准的文件系统，如下图所示： ZooKeeper 这种数据结构有如下这些特点： 每个子目录项如 NameService 都被称作为 znode，这个 znode 是被它所在的路径唯一标识，如 Server1 这个 znode 的标识为 /NameService/Server1 znode 可以有子节点目录，并且每个 znode 可以存储数据，注意 EPHEMERAL 类型的目录节点不能有子节点目录 znode 是有版本的，每个 znode 中存储的数据可以有多个版本，也就是一个访问路径中可以存储多份数据 znode 可以是临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除，ZooKeeper 的客户端和服务器通信采用长连接方式，每个客户端和服务器通过心跳来保持连接，这个连接状态称为 session，如果 znode 是临时节点，这个 session 失效，znode 也就删除了 znode 的目录名可以自动编号，如 App1 已经存在，再创建的话，将会自动命名为 App2 znode 可以被监控，包括这个目录节点中存储的数据的修改，子节点目录的变化等，一旦变化可以通知设置监控的客户端，这个是 ZooKeeper 的核心特性，ZooKeeper 的很多功能都是基于这个特性实现的，后面在典型的应用场景中会有实例介绍 2.1. znode 类型 PERSISTENT(持久化目录节点) - 客户端与 zookeeper 断开连接后，该节点依旧存在 PERSISTENT_SEQUENTIAL(持久化顺序编号目录节点) - 客户端与 zookeeper 断开连接后，该节点依旧存在，只是 Zookeeper 给该节点名称进行顺序编号 EPHEMERAL(临时目录节点) - 客户端与 zookeeper 断开连接后，该节点被删除 EPHEMERAL_SEQUENTIAL(临时顺序编号目录节点) - 客户端与 zookeeper 断开连接后，该节点被删除，只是 Zookeeper 给该节点名称进行顺序编号 3. 通知机制 客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper 会通知客户端。 4. 应用场景 4.1. 统一命名服务（Name Service） 分布式应用中，通常需要有一套完整的命名规则，既能够产生唯一的名称又便于人识别和记住，通常情况下用树形的名称结构是一个理想的选择，树形的名称结构是一个有层次的目录结构，既对人友好又不会重复。说到这里你可能想到了 JNDI，没错 ZooKeeper 的 Name Service 与 JNDI 能够完成的功能是差不多的，它们都是将有层次的目录结构关联到一定资源上，但是 ZooKeeper 的 Name Service 更加是广泛意义上的关联，也许你并不需要将名称关联到特定资源上，你可能只需要一个不会重复名称，就像数据库中产生一个唯一的数字主键一样。 Name Service 已经是 ZooKeeper 内置的功能，你只要调用 ZooKeeper 的 API 就能实现。如调用 create 接口就可以很容易创建一个目录节点。 4.2. 配置管理（Configuration Management） 配置的管理在分布式应用环境中很常见，例如同一个应用系统需要多台 PC Server 运行，但是它们运行的应用系统的某些配置项是相同的，如果要修改这些相同的配置项，那么就必须同时修改每台运行这个应用系统的 PC Server，这样非常麻烦而且容易出错。 像这样的配置信息完全可以交给 ZooKeeper 来管理，将配置信息保存在 ZooKeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 ZooKeeper 的通知，然后从 ZooKeeper 获取新的配置信息应用到系统中。 4.3. 集群管理（Group Membership） ZooKeeper 能够很容易的实现集群管理的功能，如有多台 Server 组成一个服务集群，那么必须要一个“总管”知道当前集群中每台机器的服务状态，一旦有机器不能提供服务，集群中其它集群必须知道，从而做出调整重新分配服务策略。同样当增加集群的服务能力时，就会增加一台或多台 Server，同样也必须让“总管”知道。 ZooKeeper 不仅能够帮你维护当前的集群中机器的服务状态，而且能够帮你选出一个“总管”，让这个总管来管理集群，这就是 ZooKeeper 的另一个功能 Leader Election。 它们的实现方式都是在 ZooKeeper 上创建一个 EPHEMERAL 类型的目录节点，然后每个 Server 在它们创建目录节点的父目录节点上调用 getChildren(String path, boolean watch) 方法并设置 watch 为 true，由于是 EPHEMERAL 目录节点，当创建它的 Server 死去，这个目录节点也随之被删除，所以 Children 将会变化，这时 getChildren 上的 Watch 将会被调用，所以其它 Server 就知道已经有某台 Server 死去了。新增 Server 也是同样的原理。 ZooKeeper 如何实现 Leader Election，也就是选出一个 Master Server。和前面的一样每台 Server 创建一个 EPHEMERAL 目录节点，不同的是它还是一个 SEQUENTIAL 目录节点，所以它是个 EPHEMERAL_SEQUENTIAL 目录节点。之所以它是 EPHEMERAL_SEQUENTIAL 目录节点，是因为我们可以给每台 Server 编号，我们可以选择当前是最小编号的 Server 为 Master，假如这个最小编号的 Server 死去，由于是 EPHEMERAL 节点，死去的 Server 对应的节点也被删除，所以当前的节点列表中又出现一个最小编号的节点，我们就选择这个节点为当前 Master。这样就实现了动态选择 Master，避免了传统意义上单 Master 容易出现单点故障的问题。 4.4. 分布式锁 ZooKeeper 实现分布式锁的步骤： 创建一个目录 mylock； 线程 A 想获取锁就在 mylock 目录下创建临时顺序节点； 获取 mylock 目录下所有的子节点，然后获取比自己小的兄弟节点，如果不存在，则说明当前线程顺序号最小，获得锁； 线程 B 获取所有节点，判断自己不是最小节点，设置监听比自己次小的节点； 线程 A 处理完，删除自己的节点，线程 B 监听到变更事件，判断自己是不是最小的节点，如果是则获得锁。 ZooKeeper 版本的分布式锁问题相对比较来说少。 锁的占用时间限制：redis 就有占用时间限制，而 ZooKeeper 则没有，最主要的原因是 redis 目前没有办法知道已经获取锁的客户端的状态，是已经挂了呢还是正在执行耗时较长的业务逻辑。而 ZooKeeper 通过临时节点就能清晰知道，如果临时节点存在说明还在执行业务逻辑，如果临时节点不存在说明已经执行完毕释放锁或者是挂了。由此看来 redis 如果能像 ZooKeeper 一样添加一些与客户端绑定的临时键，也是一大好事。 是否单点故障：redis 本身有很多中玩法，如客户端一致性 hash，服务器端 sentinel 方案或者 cluster 方案，很难做到一种分布式锁方式能应对所有这些方案。而 ZooKeeper 只有一种玩法，多台机器的节点数据是一致的，没有 redis 的那么多的麻烦因素要考虑。 总体上来说 ZooKeeper 实现分布式锁更加的简单，可靠性更高。但 ZooKeeper 因为需要频繁的创建和删除节点，性能上不如 Redis 方式。 4.5. 队列管理 ZooKeeper 可以处理两种类型的队列： 当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，这种是同步队列。 队列按照 FIFO 方式进行入队和出队操作，例如实现生产者和消费者模型。 同步队列用 ZooKeeper 实现的实现思路如下： 创建一个父目录 /synchronizing，每个成员都监控标志（Set Watch）位目录 /synchronizing/start 是否存在，然后每个成员都加入这个队列，加入队列的方式就是创建 /synchronizing/member_i 的临时目录节点，然后每个成员获取 / synchronizing 目录的所有目录节点，也就是 member_i。判断 i 的值是否已经是成员的个数，如果小于成员个数等待 /synchronizing/start 的出现，如果已经相等就创建 /synchronizing/start。 5. 复制 ZooKeeper 作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。 从客户端读写访问的透明度来看，数据复制集群系统分下面两种： 写主(WriteMaster) ：对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离； 写任意(Write Any)：对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。 对 ZooKeeper 来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，随着机器的增多吞吐能力肯定下降（这也是它建立 observer 的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。 6. 选举流程 选举状态： LOOKING，竞选状态。 FOLLOWING，随从状态，同步 leader 状态，参与投票。 OBSERVING，观察状态,同步 leader 状态，不参与投票。 LEADING，领导者状态。 ZooKeeper 选举流程基于 Paxos 算法。 选举线程由当前 Server 发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的 Server； 选举线程首先向所有 Server 发起一次询问(包括自己)； 选举线程收到回复后，验证是否是自己发起的询问(验证 zxid 是否一致)，然后获取对方的 id(myid)，并存储到当前询问对象列表中，最后获取对方提议的 leader 相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中； 收到所有 Server 回复以后，就计算出 zxid 最大的那个 Server，并将这个 Server 相关信息设置成下一次要投票的 Server； 线程将当前 zxid 最大的 Server 设置为当前 Server 要推荐的 Leader，如果此时获胜的 Server 获得 n/2 + 1 的 Server 票数，设置当前推荐的 leader 为获胜的 Server，将根据获胜的 Server 相关信息设置自己的状态，否则，继续这个过程，直到 leader 被选举出来。 通过流程分析我们可以得出：要使 Leader 获得多数 Server 的支持，则 Server 总数必须是奇数 2n+1，且存活的 Server 的数目不得少于 n+1. 每个 Server 启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的 server 还会从磁盘快照中恢复数据和会话信息，zk 会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。 述 Leader 选择过程中的状态变化，这是假设全部实例中均没有数据，假设服务器启动顺序分别为：A,B,C。 7. 同步流程 选完 Leader 以后，zk 就进入状态同步过程。 Leader 等待 server 连接； Follower 连接 leader，将最大的 zxid 发送给 leader； Leader 根据 follower 的 zxid 确定同步点； 完成同步后通知 follower 已经成为 uptodate 状态； Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了。 8. 资源 8.1. 官方资源 | 官网 | 官网文档 | Github | 8.2. 文章 分布式服务框架 ZooKeeper – 管理分布式环境中的数据 ZooKeeper 的功能以及工作原理]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>rpc</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式技术实现]]></title>
    <url>%2Fblog%2F2018%2F07%2F09%2Fdesign%2Farchitecture%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[分布式技术实现 1. 分布式事务 2. 分布式锁 2.1. 基于数据库实现分布式锁 2.2. 基于 Redis 实现分布式锁 2.3. 基于 ZooKeeper 实现分布式锁 3. 分布式 Session 3.1. Sticky Sessions 3.2. Session Replication 3.3. Session Server 4. 分布式存储 5. 分布式缓存 6. 分布式计算 7. 负载均衡 7.1. 算法 7.2. 实现 8. 资料 1. 分布式事务 参考：分布式原理#4-分布式事务问题 2. 分布式锁 Java 原生 API 虽然有并发锁，但并没有提供分布式锁的能力，所以针对分布式场景中的锁需要解决的方案。 分布式锁的解决方案大致有以下几种： 基于数据库实现 基于缓存（redis，memcached 等）实现 基于 Zookeeper 实现 2.1. 基于数据库实现分布式锁 实现 1. 创建表 CREATE TABLE `methodLock` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键', `method_name` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的方法名', `desc` varchar(1024) NOT NULL DEFAULT '备注信息', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成', PRIMARY KEY (`id`), UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁定中的方法'; 2. 获取锁 想要锁住某个方法时，执行以下 SQL： insert into methodLock(method_name,desc) values (‘method_name’,‘desc’) 因为我们对 method_name 做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，可以执行方法体内容。 成功插入则获取锁。 3. 释放锁 当方法执行完毕之后，想要释放锁的话，需要执行以下 Sql: delete from methodLock where method_name ='method_name' 问题 这把锁强依赖数据库的可用性。如果数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。 这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。 这把锁只能是非阻塞的，因为数据的 insert 操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。 这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。 解决办法 单点问题可以用多数据库实例，同时塞 N 个表，N/2+1 个成功就任务锁定成功 写一个定时任务，隔一段时间清除一次过期的数据。 写一个 while 循环，不断的重试插入，直到成功。 在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。 小结 优点: 直接借助数据库，容易理解。 缺点: 会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。操作数据库需要一定的开销，性能问题需要考虑。 2.2. 基于 Redis 实现分布式锁 相比于用数据库来实现分布式锁，基于缓存实现的分布式锁的性能会更好一些。目前有很多成熟的分布式产品，包括 Redis、memcache、Tair 等。这里以 Redis 举例。 Redis 命令 setnx - setnx key val：当且仅当 key 不存在时，set 一个 key 为 val 的字符串，返回 1；若 key 存在，则什么都不做，返回 0。 expire - expire key timeout：为 key 设置一个超时时间，单位为 second，超过这个时间锁会自动释放，避免死锁。 delete - delete key：删除 key 实现 单点实现步骤： 获取锁的使用，使用 setnx 加锁，锁的 value 值为一个随机生成的 UUID，再使用 expire 设置一个过期值。 获取锁的时候还设置一个获取的超时时间，若超过这个时间则放弃获取锁。 释放锁的时候，通过 UUID 判断是不是该锁，若是该锁，则执行 delete 进行锁释放。 问题 单点问题。如果单机 redis 挂掉了，那么程序会跟着出错。 如果转移使用 slave 节点，复制不是同步复制，会出现多个程序获取锁的情况 小结 可以考虑使用 redisson 的解决方案。 2.3. 基于 ZooKeeper 实现分布式锁 实现 这也是 ZooKeeper 客户端 curator 的分布式锁实现。 创建一个目录 mylock； 线程 A 想获取锁就在 mylock 目录下创建临时顺序节点； 获取 mylock 目录下所有的子节点，然后获取比自己小的兄弟节点，如果不存在，则说明当前线程顺序号最小，获得锁； 线程 B 获取所有节点，判断自己不是最小节点，设置监听比自己次小的节点； 线程 A 处理完，删除自己的节点，线程 B 监听到变更事件，判断自己是不是最小的节点，如果是则获得锁。 小结 ZooKeeper 版本的分布式锁问题相对比较来说少。 锁的占用时间限制：redis 就有占用时间限制，而 ZooKeeper 则没有，最主要的原因是 redis 目前没有办法知道已经获取锁的客户端的状态，是已经挂了呢还是正在执行耗时较长的业务逻辑。而 ZooKeeper 通过临时节点就能清晰知道，如果临时节点存在说明还在执行业务逻辑，如果临时节点不存在说明已经执行完毕释放锁或者是挂了。由此看来 redis 如果能像 ZooKeeper 一样添加一些与客户端绑定的临时键，也是一大好事。 是否单点故障：redis 本身有很多中玩法，如客户端一致性 hash，服务器端 sentinel 方案或者 cluster 方案，很难做到一种分布式锁方式能应对所有这些方案。而 ZooKeeper 只有一种玩法，多台机器的节点数据是一致的，没有 redis 的那么多的麻烦因素要考虑。 总体上来说 ZooKeeper 实现分布式锁更加的简单，可靠性更高。但 ZooKeeper 因为需要频繁的创建和删除节点，性能上不如 Redis 方式。 3. 分布式 Session 在分布式场景下，一个用户的 Session 如果只存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器上，该服务器没有用户的 Session，就可能导致用户需要重新进行登录等操作。 分布式 Session 的几种实现策略： 粘性 session 应用服务器间的 session 复制共享 基于 cache DB 缓存的 session 共享 3.1. Sticky Sessions 需要配置负载均衡器，使得一个用户的所有请求都路由到一个服务器节点上，这样就可以把用户的 Session 存放在该服务器节点中。 缺点：当服务器节点宕机时，将丢失该服务器节点上的所有 Session。 3.2. Session Replication 在服务器节点之间进行 Session 同步操作，这样的话用户可以访问任何一个服务器节点。 缺点：占用过多内存；同步过程占用网络带宽以及服务器处理器时间。 3.3. Session Server 使用一个单独的服务器存储 Session 数据，可以存在 MySQL 数据库上，也可以存在 Redis 或者 Memcached 这种内存型数据库。 缺点：需要去实现存取 Session 的代码。 4. 分布式存储 通常有两种解决方案： 数据分布：就是把数据分块存在不同的服务器上（分库分表）。 数据复制：让所有的服务器都有相同的数据，提供相当的服务。 参考：分布式原理.md#2-数据分布 5. 分布式缓存 使用缓存的好处： 提升数据读取速度 提升系统扩展能力，通过扩展缓存，提升系统承载能力 降低存储成本，Cache+DB 的方式可以承担原有需要多台 DB 才能承担的请求量，节省机器成本 根据业务场景，通常缓存有以下几种使用方式 懒汉式(读时触发)：写入 DB 后, 然后把相关的数据也写入 Cache 饥饿式(写时触发)：先查询 DB 里的数据, 然后把相关的数据写入 Cache 定期刷新：适合周期性的跑数据的任务，或者列表型的数据，而且不要求绝对实时性 缓存分类： 应用内缓存：如：EHCache 分布式缓存：如：Memached、Redis 参考：分布式原理.md#6-分布式缓存问题 6. 分布式计算 7. 负载均衡 7.1. 算法 轮询（Round Robin） 轮询算法把每个请求轮流发送到每个服务器上。下图中，一共有 6 个客户端产生了 6 个请求，这 6 个请求按 (1, 2, 3, 4, 5, 6) 的顺序发送。最后，(1, 3, 5) 的请求会被发送到服务器 1，(2, 4, 6) 的请求会被发送到服务器 2。 该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载（下图的 Server 2）。 加权轮询（Weighted Round Robbin） 加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值。例如下图中，服务器 1 被赋予的权值为 5，服务器 2 被赋予的权值为 1，那么 (1, 2, 3, 4, 5) 请求会被发送到服务器 1，(6) 请求会被发送到服务器 2。 最少连接（least Connections） 由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。例如下图中，(1, 3, 5) 请求会被发送到服务器 1，但是 (1, 3) 很快就断开连接，此时只有 (5) 请求连接服务器 1；(2, 4, 6) 请求被发送到服务器 2，只有 (2) 的连接断开。该系统继续运行时，服务器 2 会承担过大的负载。 最少连接算法就是将请求发送给当前最少连接数的服务器上。例如下图中，服务器 1 当前连接数最小，那么新到来的请求 6 就会被发送到服务器 1 上。 加权最少连接（Weighted Least Connection） 在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。 随机算法（Random） 把请求随机发送到服务器上。和轮询算法类似，该算法比较适合服务器性能差不多的场景。 源地址哈希法 (IP Hash) 源地址哈希通过对客户端 IP 哈希计算得到的一个数值，用该数值对服务器数量进行取模运算，取模结果便是目标服务器的序号。 优点：保证同一 IP 的客户端都会被 hash 到同一台服务器上。 缺点：不利于集群扩展，后台服务器数量变更都会影响 hash 结果。可以采用一致性 Hash 改进。 7.2. 实现 HTTP 重定向 HTTP 重定向负载均衡服务器收到 HTTP 请求之后会返回服务器的地址，并将该地址写入 HTTP 重定向响应中返回给浏览器，浏览器收到后需要再次发送请求。 缺点： 用户访问的延迟会增加； 如果负载均衡器宕机，就无法访问该站点。 DNS 重定向 使用 DNS 作为负载均衡器，根据负载情况返回不同服务器的 IP 地址。大型网站基本使用了这种方式做为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。 缺点： DNS 查找表可能会被客户端缓存起来，那么之后的所有请求都会被重定向到同一个服务器。 修改 MAC 地址 使用 LVS（Linux Virtual Server）这种链路层负载均衡器，根据负载情况修改请求的 MAC 地址。 修改 IP 地址 在网络层修改请求的目的 IP 地址。 代理自动配置 正向代理与反向代理的区别： 正向代理：发生在客户端，是由用户主动发起的。比如翻墙，客户端通过主动访问代理服务器，让代理服务器获得需要的外网数据，然后转发回客户端。 反向代理：发生在服务器端，用户不知道代理的存在。 PAC 服务器是用来判断一个请求是否要经过代理。 8. 资料 https://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html https://github.com/CyC2018/Interview-Notebook/blob/master/notes/分布式问题分析.md https://www.jianshu.com/p/453c6e7ff81c https://juejin.im/post/5a20cd8bf265da43163cdd9a https://github.com/redisson/redisson/wiki/8.-分布式锁和同步器 https://github.com/L316476844/distributed-session 分布式缓存架构基础 阿里 P8 技术专家细究分布式缓存问题]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
        <tag>distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网站典型故障]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fdesign%2Farchitecture%2F%E7%BD%91%E7%AB%99%E5%85%B8%E5%9E%8B%E6%95%85%E9%9A%9C%2F</url>
    <content type="text"><![CDATA[网站典型故障 📓 本文已归档到：「blog」 1. 海量日志耗尽磁盘空间引发的故障 2. 高并发访问数据库引发的故障 3. 高并发情况下锁引发的故障 4. 缓存引发的故障 5. 应用启动不同步引发的故障 6. 大文件读写独占磁盘引发的故障 7. 资料 1. 海量日志耗尽磁盘空间引发的故障 现象：应用发布后，硬盘空间低于警戒值，服务器宕机。 分析：应用部署在服务器上，不断打印日志，但不定期清理日志，导致磁盘空间最终被耗尽。 总结： 应用自身的日志和第三方组件日志应分别配置。 日志输出级别不要设置太低，导致打印很多无关痛痒的信息。 2. 高并发访问数据库引发的故障 现象：数据库负载居高不下。 分析：某条 sql 执行频率非常高，追查发现，被网站首页调用。 总结：首页不应该访问数据库。 3. 高并发情况下锁引发的故障 现象：应用不定时地因为响应超时而报警，但是很快又超时接触，恢复正常。 分析：某个单例对象中多处使用了 synchronized，由于 this 对象只有一个，所有的并发请求都要排队获得者唯一的一把锁。 总结：使用锁操作要谨慎。 4. 缓存引发的故障 现象：没有新应用发布，但是数据库突然负载飙升，并很快失去响应。 分析：缓存服务器管理失当。 总结：当网站架构对缓存依赖性很强时，应该重视对缓存服务的管理。 5. 应用启动不同步引发的故障 现象：某应用发布后，服务器立即崩溃。 分析：后台服务还没准备好，前台应用就开始接受请求，导致故障。 总结：发布脚本中不断用 curl 命令访问后台应用特定页面，直到收到 OK，再启动前台应用。 6. 大文件读写独占磁盘引发的故障 现象：上传图片非常慢。 分析：文件存储最有可能出错的地方是存储服务器。检查发现，大部分文件大小比较小，少数几个文件非常大，读写大文件比较耗时，在这个读写时间内，磁盘基本被大文件操作独占，导致其他用户的文件操作缓慢。 总结：文件存储下需要根据不同文件类型和用途进行管理。 7. 资料 大型网站技术架构]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大型分布式网站架构]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fdesign%2Farchitecture%2F%E5%A4%A7%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BD%91%E7%AB%99%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[大型分布式网站架构 📓 本文已归档到：「blog」 1. 大型分布式网站架构概述 1.1. 大型网站的特点 1.2. 大型网站架构目标 1.3. 大型网站架构模式 1.4. 高性能架构 1.5. 高可用架构 1.6. 可伸缩架构 1.7. 可扩展架构 1.8. 安全架构 1.9. 敏捷性 1.10. 大型架构举例 2. 电商网站架构案例 2.1. 网站初级架构 2.2. 系统容量预估 2.3. 网站架构分析 2.4. 网站架构优化 2.5. 架构总结 3. 资料 1. 大型分布式网站架构概述 1.1. 大型网站的特点 用户多，分布广泛 大流量，高并发 海量数据，服务高可用 安全环境恶劣，易受网络攻击 功能多，变更快，频繁发布 从小到大，渐进发展 以用户为中心 免费服务，付费体验 1.2. 大型网站架构目标 高性能：提供快速的访问体验。 高可用：网站服务一直可以正常访问。 可伸缩：通过硬件增加/减少，提高/降低处理能力。 安全性：提供网站安全访问和数据加密，安全存储等策略。 扩展性：方便的通过新增/移除方式，增加/减少新的功能/模块。 敏捷性：随需应变，快速响应； 1.3. 大型网站架构模式 分层：一般可分为，应用层，服务层，数据层，管理层，分析层； 分割：一般按照业务/模块/功能特点进行划分，比如应用层分为首页，用户中心。 分布式：将应用分开部署（比如多台物理机），通过远程调用协同工作。 集群：一个应用/模块/功能部署多份（如：多台物理机），通过负载均衡共同提供对外访问。 缓存：将数据放在距离应用或用户最近的位置，加快访问速度。 异步：将同步的操作异步化。客户端发出请求，不等待服务端响应，等服务端处理完毕后，使用通知或轮询的方式告知请求方。一般指：请求——响应——通知 模式。 冗余：增加副本，提高可用性，安全性，性能。 安全：对已知问题有有效的解决方案，对未知/潜在问题建立发现和防御机制。 自动化：将重复的，不需要人工参与的事情，通过工具的方式，使用机器完成。 敏捷性：积极接受需求变更，快速响应业务发展需求。 1.4. 高性能架构 以用户为中心，提供快速的网页访问体验。主要参数有较短的响应时间，较大的并发处理能力，较高的吞吐量，稳定的性能参数。 可分为前端优化，应用层优化，代码层优化，存储层优化。 前端优化：网站业务逻辑之前的部分； 浏览器优化：减少 Http 请求数，使用浏览器缓存，启用压缩，Css Js 位置，Js 异步，减少 Cookie 传输； CDN 加速，反向代理； 应用层优化：处理网站业务的服务器。使用缓存，异步，集群 代码优化：合理的架构，多线程，资源复用（对象池，线程池等），良好的数据结构，JVM 调优，单例，Cache 等； 存储优化：缓存，固态硬盘，光纤传输，优化读写，磁盘冗余，分布式存储（HDFS），NOSQL 等； 1.5. 高可用架构 大型网站应该在任何时候都可以正常访问。正常提供对外服务。因为大型网站的复杂性，分布式，廉价服务器，开源数据库，操作系统等特点。要保证高可用是很困难的，也就是说网站的故障是不可避免的。 如何提高可用性，就是需要迫切解决的问题。首先，需要从架构级别，在规划的时候，就考虑可用性。行业内一般用几个 9 表示可用性指标。比如四个 9（99.99），一年内允许的不可用时间是 53 分钟。 不同层级使用的策略不同，一般采用冗余备份和失效转移解决高可用问题。 应用层：一般设计为无状态的，对于每次请求，使用哪一台服务器处理是没有影响的。一般使用负载均衡技术（需要解决 Session 同步问题），实现高可用。 服务层：负载均衡，分级管理，快速失败（超时设置），异步调用，服务降级，幂等设计等。 数据层：冗余备份（冷，热备[同步，异步]，温备），失效转移（确认，转移，恢复）。数据高可用方面著名的理论基础是 CAP 理论（持久性，可用性，数据一致性[强一致，用户一致，最终一致]） 1.6. 可伸缩架构 伸缩性是指在不改变原有架构设计的基础上，通过添加/减少硬件（服务器）的方式，提高/降低系统的处理能力。 应用层：对应用进行垂直或水平切分。然后针对单一功能进行负载均衡（DNS,HTTP[反向代理],IP,链路层）。 服务层：与应用层类似； 数据层：分库，分表，NOSQL 等；常用算法 Hash，一致性 Hash。 1.7. 可扩展架构 可以方便的进行功能模块的新增/移除，提供代码/模块级别良好的可扩展性。 模块化，组件化：高内聚，内耦合，提高复用性，扩展性。 稳定接口：定义稳定的接口，在接口不变的情况下，内部结构可以“随意”变化。 设计模式：应用面向对象思想，原则，使用设计模式，进行代码层面的设计。 消息队列：模块化的系统，通过消息队列进行交互，使模块之间的依赖解耦。 分布式服务：公用模块服务化，提供其他系统使用，提高可重用性，扩展性。 1.8. 安全架构 对已知问题有有效的解决方案，对未知/潜在问题建立发现和防御机制。对于安全问题，首先要提高安全意识，建立一个安全的有效机制，从政策层面，组织层面进行保障。比如服务器密码不能泄露，密码每月更新，并且三次内不能重复；每周安全扫描等。以制度化的方式，加强安全体系的建设。同时，需要注意与安全有关的各个环节。安全问题不容忽视。包括基础设施安全，应用系统安全，数据保密安全等。 基础设施安全：硬件采购，操作系统，网络环境方面的安全。一般采用，正规渠道购买高质量的产品，选择安全的操作系统，及时修补漏洞，安装杀毒软件防火墙。防范病毒，后门。设置防火墙策略，建立 DDOS 防御系统，使用攻击检测系统，进行 子网隔离等手段。 ​ 应用系统安全：在程序开发时，对已知常用问题，使用正确的方式，在代码层面解决掉。防止跨站脚本攻击（XSS），注入攻击，跨站请求伪造（CSRF），错误信息，HTML 注释，文件上传，路径遍历等。还可以使用 Web 应用防火墙（比如：ModSecurity），进行安全漏洞扫描等措施，加强应用级别的安全。 ​ 数据保密安全：存储安全（存在在可靠的设备，实时，定时备份），保存安全（重要的信息加密保存，选择合适的人员复杂保存和检测等），传输安全（防止数据窃取和数据篡改）； ​ 常用的加解密算法（单项散列加密[MD5,SHA]，对称加密[DES,3DES,RC]），非对称加密[RSA]等。 1.9. 敏捷性 网站的架构设计，运维管理要适应变化，提供高伸缩性，高扩展性。方便的应对快速的业务发展，突增高流量访问等要求。 除上面介绍的架构要素外，还需要引入敏捷管理，敏捷开发的思想。使业务，产品，技术，运维统一起来，随需应变，快速响应。 1.10. 大型架构举例 以上采用七层逻辑架构，第一层客户层，第二层前端优化层，第三层应用层，第四层服务层，第五层数据存储层，第六层大数据存储层，第七层大数据处理层。 客户层：支持 PC 浏览器和手机 APP。差别是手机 APP 可以直接访问通过 IP 访问，反向代理服务器。 前端层：使用 DNS 负载均衡，CDN 本地加速以及反向代理服务； 应用层：网站应用集群；按照业务进行垂直拆分，比如商品应用，会员中心等； 服务层：提供公用服务，比如用户服务，订单服务，支付服务等； 数据层：支持关系型数据库集群（支持读写分离），NOSQL 集群，分布式文件系统集群；以及分布式 Cache； 大数据存储层：支持应用层和服务层的日志数据收集，关系数据库和 NOSQL 数据库的结构化和半结构化数据收集； 大数据处理层：通过 Mapreduce 进行离线数据分析或 Storm 实时数据分析，并将处理后的数据存入关系型数据库。（实际使用中，离线数据和实时数据会按照业务要求进行分类处理，并存入不同的数据库中，供应用层或服务层使用）。 2. 电商网站架构案例 2.1. 网站初级架构 一般网站，刚开始的做法，是三台服务器，一台部署应用，一台部署数据库，一台部署 NFS 文件系统。 这是前几年比较传统的做法，之前见到一个网站 10 万多会员，垂直服装设计门户，N 多图片。使用了一台服务器部署了应用，数据库以及图片存储。出现了很多性能问题。 如下图： 但是，目前主流的网站架构已经发生了翻天覆地的变化。一般都会采用集群的方式，进行高可用设计。至少是下面这个样子。 （1） 使用集群对应用服务器进行冗余，实现高可用；（负载均衡设备可与应用一块部署） 使用数据库主备模式，实现数据备份和高可用； 2.2. 系统容量预估 预估步骤： （1） 注册用户数-日均 UV 量-每日的 PV 量-每天的并发量； （2） 峰值预估：平常量的 2~3 倍； （3） 根据并发量（并发，事务数），存储容量计算系统容量。 客户需求：3~5 年用户数达到 1000 万注册用户； 每秒并发数预估： （1） 每天的 UV 为 200 万（二八原则）； （2） 每日每天点击浏览 30 次； （3） PV 量：200*30=6000 万； （4） 集中访问量：240.2=4.8 小时会有 6000 万0.8=4800 万（二八原则）； （5） 每分并发量：4.8*60=288 分钟，每分钟访问 4800/288=16.7 万（约等于）； （6） 每秒并发量：16.7 万/60=2780（约等于）； （7） 假设：高峰期为平常值的三倍，则每秒的并发数可以达到 8340 次。 （8） 1 毫秒=1.3 次访问； 没好好学数学后悔了吧？！（不知道以上算是否有错误，呵呵~~） 服务器预估：（以 tomcat 服务器举例） （1） 按一台 web 服务器，支持每秒 300 个并发计算。平常需要 10 台服务器（约等于）；[tomcat 默认配置是 150] （2） 高峰期：需要 30 台服务器； 容量预估：70/90 原则 系统 CPU 一般维持在 70%左右的水平，高峰期达到 90%的水平，是不浪费资源，并比较稳定的。内存，IO 类似。 以上预估仅供参考，因为服务器配置，业务逻辑复杂度等都有影响。在此 CPU，硬盘，网络等不再进行评估。 2.3. 网站架构分析 根据以上预估，有几个问题： 需要部署大量的服务器，高峰期计算，可能要部署 30 台 Web 服务器。并且这三十台服务器，只有秒杀，活动时才会用到，存在大量的浪费。 所有的应用部署在同一台服务器，应用之间耦合严重。需要进行垂直切分和水平切分。 大量应用存在冗余代码 服务器 SESSION 同步耗费大量内存和网络带宽 数据需要频繁访问数据库，数据库访问压力巨大。 大型网站一般需要做以下架构优化（优化是架构设计时，就要考虑的，一般从架构/代码级别解决，调优主要是简单参数的调整，比如 JVM 调优；如果调优涉及大量代码改造，就不是调优了，属于重构）： 业务拆分 应用集群部署（分布式部署，集群部署和负载均衡） 多级缓存 单点登录（分布式 Session） 数据库集群（读写分离，分库分表） 服务化 消息队列 其他技术 2.4. 网站架构优化 业务拆分 根据业务属性进行垂直切分，划分为产品子系统，购物子系统，支付子系统，评论子系统，客服子系统，接口子系统（对接如进销存，短信等外部系统）。 根据业务子系统进行等级定义，可分为核心系统和非核心系统。核心系统：产品子系统，购物子系统，支付子系统；非核心：评论子系统，客服子系统，接口子系统。 业务拆分作用：提升为子系统可由专门的团队和部门负责，专业的人做专业的事，解决模块之间耦合以及扩展性问题；每个子系统单独部署，避免集中部署导致一个应用挂了，全部应用不可用的问题。 等级定义作用：用于流量突发时，对关键应用进行保护，实现优雅降级；保护关键应用不受到影响。 拆分后的架构图： 参考部署方案 2 （1） 如上图每个应用单独部署 （2） 核心系统和非核心系统组合部署 应用集群部署（分布式，集群，负载均衡） ​ 分布式部署：将业务拆分后的应用单独部署，应用直接通过 RPC 进行远程通信； ​ 集群部署：电商网站的高可用要求，每个应用至少部署两台服务器进行集群部署； ​ 负载均衡：是高可用系统必须的，一般应用通过负载均衡实现高可用，分布式服务通过内置的负载均衡实现高可用，关系型数据库通过主备方式实现高可用。 集群部署后架构图： 多级缓存 缓存按照存放的位置一般可分为两类：本地缓存和分布式缓存。本案例采用二级缓存的方式，进行缓存的设计。一级缓存为本地缓存，二级缓存为分布式缓存。（还有页面缓存，片段缓存等，那是更细粒度的划分） 一级缓存，缓存数据字典，和常用热点数据等基本不可变/有规则变化的信息，二级缓存缓存需要的所有缓存。当一级缓存过期或不可用时，访问二级缓存的数据。如果二级缓存也没有，则访问数据库。 缓存的比例，一般 1:4，即可考虑使用缓存。（理论上是 1:2 即可）。 ​ 根据业务特性可使用以下缓存过期策略： （1） 缓存自动过期； （2） 缓存触发过期； 单点登录（分布式 Session） 系统分割为多个子系统，独立部署后，不可避免的会遇到会话管理的问题。一般可采用 Session 同步，Cookies，分布式 Session 方式。电商网站一般采用分布式 Session 实现。 再进一步可以根据分布式 Session，建立完善的单点登录或账户管理系统。 ​ 流程说明 （1） 用户第一次登录时，将会话信息（用户 Id 和用户信息），比如以用户 Id 为 Key，写入分布式 Session； （2） 用户再次登录时，获取分布式 Session，是否有会话信息，如果没有则调到登录页； （3） 一般采用 Cache 中间件实现，建议使用 Redis，因为它有持久化功能，方便分布式 Session 宕机后，可以从持久化存储中加载会话信息； （4） 存入会话时，可以设置会话保持的时间，比如 15 分钟，超过后自动超时； 结合 Cache 中间件，实现的分布式 Session，可以很好的模拟 Session 会话。 数据库集群（读写分离，分库分表） 大型网站需要存储海量的数据，为达到海量数据存储，高可用，高性能一般采用冗余的方式进行系统设计。一般有两种方式读写分离和分库分表。 读写分离：一般解决读比例远大于写比例的场景，可采用一主一备，一主多备或多主多备方式。 本案例在业务拆分的基础上，结合分库分表和读写分离。如下图： （1） 业务拆分后：每个子系统需要单独的库； （2） 如果单独的库太大，可以根据业务特性，进行再次分库，比如商品分类库，产品库； （3） 分库后，如果表中有数据量很大的，则进行分表，一般可以按照 Id，时间等进行分表；（高级的用法是一致性 Hash） （4） 在分库，分表的基础上，进行读写分离； 相关中间件可参考 Cobar（阿里，目前已不在维护），TDDL（阿里），Atlas（奇虎 360），MyCat（在 Cobar 基础上，国内很多牛人，号称国内第一开源项目）。 分库分表后序列的问题，JOIN，事务的问题，会在分库分表主题分享中，介绍。 服务化 ​将多个子系统公用的功能/模块，进行抽取，作为公用服务使用。比如本案例的会员子系统就可以抽取为公用的服务。 消息队列 ​ 消息队列可以解决子系统/模块之间的耦合，实现异步，高可用，高性能的系统。是分布式系统的标准配置。本案例中，消息队列主要应用在购物，配送环节。 （1） 用户下单后，写入消息队列，后直接返回客户端； （2） 库存子系统：读取消息队列信息，完成减库存； （3） 配送子系统：读取消息队列信息，进行配送； 目前使用较多的 MQ 有 Active MQ,Rabbit MQ,Zero MQ，MS MQ 等，需要根据具体的业务场景进行选择。建议可以研究下 Rabbit MQ。 其他架构（技术） 除了以上介绍的业务拆分，应用集群，多级缓存，单点登录，数据库集群，服务化，消息队列外。还有 CDN，反向代理，分布式文件系统，大数据处理等系统。 此处不详细介绍，大家可以问度娘/Google，有机会的话也可以分享给大家。 2.5. 架构总结 以上是本次分享的架构总结，其中细节可参考前面分享的内容。其中还有很多可以优化和细化的地方，因为是案例分享，主要针对重要部分做了介绍，工作中需要大家根据具体的业务场景进行架构设计。 以上是电商网站架构案例的分享一共有三篇，从电商网站的需求，到单机架构，逐步演变为常用的，可供参考的分布式架构的原型。除具备功能需求外，还具备一定的高性能，高可用，可伸缩，可扩展等非功能质量需求（架构目标）。 3. 资料 大型分布式网站架构技术总结 大型网站架构系列：电商网站架构案例(1) 大型网站架构系列：电商网站架构案例(2) 大型网站架构系列：电商网站架构案例(3)]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
        <tag>distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网站的高可用架构]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fdesign%2Farchitecture%2F%E7%BD%91%E7%AB%99%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[网站的高可用架构 📓 本文已归档到：「blog」 1. 网站可用性的度量 2. 高可用的网站架构 3. 高可用的应用 3.1. 通过负载均衡进行无状态服务的失效转移 3.2. 应用服务器集群的 Session 管理 4. 高可用的服务 5. 高可用的数据 5.1. CAP 原理 5.2. 数据备份 5.3. 失效转移 6. 高可用网站的软件质量保证 7. 网站监控 8. 资料 1. 网站可用性的度量 网站不可用也被称作网站故障，业界通常用多个 9 来衡量网站的可用性。如 QQ 的可用性为 4 个 9，即 99.99% 可用。 网站不可用时间 = 故障修复时间点 - 故障发现时间点网站年度可用性指标 = (1 - 网站不可用时间/年度总时间) * 100% 2. 高可用的网站架构 大型网站的分层架构及服务器的分布式部署使得位于不同层次的服务器具有不同的可用性特点。关闭服务或服务器宕机时产生的影响也不相同，高可用的解决方案也差异甚大。 3. 高可用的应用 3.1. 通过负载均衡进行无状态服务的失效转移 应用层主要处理网站应用的业务逻辑，一个显著的特点是应用的 无状态 性。 所谓的 无状态 的应用是指应用服务器不保存业务的上下文信息，而仅根据每次请求提交的数据进行相应的业务逻辑处理，多个服务实例之间完全对等，请求提交到任意服务器，处理结果都是完全一样的。 负载均衡，顾名思义，主要使用在业务量和数据量较高的情况下，当单台服务器不足以承担所有的负载压力时，通过负载均衡手段，将流量和数据分摊到一个集群组成的多台服务器上，以提高整体的负载处理能力。 3.2. 应用服务器集群的 Session 管理 应用服务器的高可用架构设计主要基于服务无状态这一特性。事实上，业务总是有状态的，如购物车记录用户的购买信息；用户的登录状态；最新发布的消息等等。 Web 应用中将这些多次请求修改使用的上下文对象称作会话。单机情况下，Session 可由部署在服务器上的 Web 容器管理。 而在集群环境下，Session 管理有以下手段： Session 复制 Session 复制是指应用服务器开发 Web 容器的 Session 复制功能，在集群中的几台服务器之间同步 Session 对象。 这种方案很简单，但当集群规模较大时，集群服务间需要大量的通信来进行 Session 复制。 Session 绑定 可以利用负载均衡的源地址 Hash 算法实现，总是将来源于同一 IP 的请求分发到同一台服务器上。这样在整个会话期间，用户所有的请求都在同一台服务器上处理，即 Session 绑定到某台特定服务器上。这种方法又被称作会话粘滞。 但是这种策略不符合高可用的需求，因为一旦某台服务器宕机，那么该机器上的 Session 也就不复存在了。 利用 Cookie 记录 Session 可以将 Session 记录在客户端（浏览器 Cookie），每次请求服务器时，将 Session 放在请求中发送给服务器，服务器处理完请求后再将修改过的 Session 响应给客户端。 这种策略的缺点是：Cookie 有大小限制，能记录的信息有限；每次请求响应都需要传输 Cookie，影响性能；如果用户关闭 Cookie，访问就不能工作。 Session 服务器 利用独立部署的 Session 服务器（集群）统一管理 Session，应用服务器每次读写 Session 时，都访问 Session 服务器。 实现 Session 服务器的一种简单方法时：利用分布式缓存、数据库等，在此基础上进行包装，使其符合 Session 的存储和访问要求。 4. 高可用的服务 高可用的服务策略： 分级管理 - 将服务根据业务重要性进行分级管理，并在服务部署上进行隔离。 超时设置 - 由于服务器宕机、线程死锁等原因，可能导致应用程序对服务端的调用失去响应。所以有必要引入超时机制，一旦调用超时，服务化框架抛出异常，应用程序根据服务调度策略，选择重试或请求转移到其他机器上。 异步调用 - 对于需要即时响应的业务，应用在调用服务时可以通过消息队列等异步方式完成，避免长时间等待服务响应结果。 服务降级 - 网站访问高峰期，服务可能因为大量并发调用而性能下降，严重时可能会导致宕机。为了保证核心功能的正常运行，需要对服务进行降级。降级有两种手段：拒绝服务和关闭服务。 幂等性设计 - 为了避免服务重复调用，可以通过设置编号的方式进行服务调用有效性校验，有效的操作才能继续执行。 5. 高可用的数据 5.1. CAP 原理 分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容忍性（P：Partition Tolerance），最多只能同时满足其中两项。 可用性 可用性指分布式系统在面对各种异常时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的。 在可用性条件下，系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。 分区容忍性 网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。 在分区容忍性条件下，分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。 一致性 一致性指的是多个数据副本是否能保持一致的特性。 在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。 数据一致性又可以分为以下几点： 强一致性 - 数据更新操作结果和操作响应总是一致的，即操作响应通知更新失败，那么数据一定没有被更新，而不是处于不确定状态。 最终一致性 - 即物理存储的数据可能是不一致的，终端用户访问到的数据可能也是不一致的，但系统经过一段时间的自我修复和修正，数据最终会达到一致。 权衡 在分布式系统中，分区容忍性必不可少，因为需要总是假设网络是不可靠的。因此，CAP 理论实际在是要在可用性和一致性之间做权衡。 可用性和一致性往往是冲突的，很难都使它们同时满足。在多个节点之间进行数据同步时， 为了保证一致性（CP），就需要让所有节点下线成为不可用的状态，等待同步完成； 为了保证可用性（AP），在同步过程中允许读取所有节点的数据，但是数据可能不一致。 5.2. 数据备份 冷备份：定期将数据复制到某种存储介质。 热备份 异步热备方式 - 异步方式是指多份数据副本的写入操作异步完成，应用程序收到数据服务系统的写操作成功响应时，只写成功了一份，存储系统将会异步地写其他副本。 同步热备方式 - 同步方式是指多份数据副本的写入操作同步完成，即应用程序收到数据服务系统的写成功响应时，多份数据都已经写操作成功。但是当应用程序收到数据写操作失败的响应式，可能有部分副本或者全部副本都已经写入成功了（因为网络或者系统故障，无法返回操作成功的响应）。 5.3. 失效转移 失效确认 判断服务器宕机的手段有两种：心跳检测和访问失败报告。 对于应用程序的访问失败报告，控制中心还需要再一次发送心跳检测进行确认，以免错误判断服务器宕机。因为一旦进行数据访问的失效转移，意味着数据存储多份副本不一致，需要进行后续一系列的复杂动作。 访问转移 确认某台数据服务器宕机后，就需要将数据读写访问重新路由到其他服务器上。对于完全对等存储的服务器，当其中一台宕机后，应用程序根据配置直接切换到对等服务器上。如果存储不对等，就需要重新计算路由，选择存储服务器。 数据恢复 因为某台服务器宕机，所以数据存储的副本数目会减少，必须将副本的数目恢复到系统设定的值，否则，再有服务器宕机时，就可能出现无法访问转移，数据永久丢失的情况。因此系统需要从健康的服务器复制数据，将数据副本数目恢复到设定值。 6. 高可用网站的软件质量保证 高可用网站的软件质量保证的手段： 自动化发布 自动化测试 预发布验证 代码控制 灰度发布 7. 网站监控 监控数据采集 用户行为日志收集 服务器性能监控 运行数据报告 监控管理 系统报警 失效转移 自动优雅降级 8. 资料 大型网站技术架构]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网站的可扩展架构]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fdesign%2Farchitecture%2F%E7%BD%91%E7%AB%99%E7%9A%84%E5%8F%AF%E6%89%A9%E5%B1%95%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[网站的可扩展架构 📓 本文已归档到：「blog」 1. 构建可扩展的网站架构 2. 利用分布式消息队列降低系统耦合性 2.1. 事件驱动架构 2.2. 分布式消息队列 3. 利用分布式服务打造可复用的业务平台 4. 可扩展的数据结构 5. 资料 扩展性（Extensibility） - 指对现有系统影响最小的情况下，系统功能可持续扩展或提升的能力。表现在系统基础设施稳定不需要经常变更，应用之间较少依赖和耦合，对需求变更可以敏捷响应。它是系统架构设计层面的开闭原则（对扩展开放、对修改关闭），架构设计考虑未来功能扩展，当系统增加新功能时，不需要对现有系统的结构和代码进行修改。 伸缩性（Scalability） - 指系统能够通过增加减少自身资源规模的方式增减自己计算处理事务的能力。如果这种增减是成比例的，就被称作线性伸缩性。在网站架构中 ，通常指利用集群的方式增加服务器数量、提高系统的整体事务吞吐能力。 1. 构建可扩展的网站架构 低耦合的系统更容易扩展、复用。 设计网站可扩展架构的核心思想是模块化，并在此基础上，降低模块间的耦合性，提高模块的复用性。 分层和分割不仅可以进行架构伸缩，也是模块化设计的重要手段，利用分层和分割的方式将软件分割为若干个低耦合的独立的组件模块，这些组件模块以消息传递及依赖调用的方式聚合成一个完整的系统。 在大型网站中，这些模块通过分布式部署的方式，独立的模块部署在独立的服务器上，从物理上分离模块间的耦合关系，进一步降低耦合性提高复用性。 2. 利用分布式消息队列降低系统耦合性 2.1. 事件驱动架构 事件驱动架构通过在低耦合的模块间传输事件消息，以保持模块的松散耦合，并借助事件消息的通信完成模块间合作。典型的事件驱动架构就是操作系统中常见的生产者消费者模式。在大型网站中，最常见的实现手段就是分布式消息队列。 2.2. 分布式消息队列 消息生产者应用程序通过远程访问接口将消息推送给消息队列服务器，消息队列服务器将消息写入本地内存队列后立即返回成功响应给消息生产者。消息队列服务器根据消息订阅列表查找订阅该消息的消息消费者应用程序，将消息队列中的消息按照先进先出（FIFO）的原则将消息通过远程通信接口发送给消息消费者程序。 在伸缩性方面，由于消息队列服务器上的数据可以看作是即时处理的，因此类似于无状态的服务器，伸缩性设计比较简单。将新服务器加入分布式消息队列集群中，通知生产者服务器更改消息队列服务器列表即可。 在可用性方面，为了避免消费者进程处理缓慢，分布式消息队列服务器内存空间不足造成的问题，如果内存队列已满，会将消息写入磁盘，消息推送模块在将内存队列消息处理完成以后，将磁盘内容加载到内存队列继续处理。 3. 利用分布式服务打造可复用的业务平台 分布式服务则通过接口分解系统耦合性，不同子系统通过相同的接口描述进行服务调用。 大型网站分布式服务的需求与特点： 负载均衡 失效转移 高效的远程通信 整合异构系统 对应用最少侵入 版本管理 实时监控 4. 可扩展的数据结构 传统的关系型数据库为了保证关系运算的正确性，在设计数据库表结构的时候，就需要指定表的 schema ——字段名称，数据类型等，并要遵循特定的设计范式。这些规范带来一个问题：难以面对需求变更带来的挑战，所以有人通过预先设计一些冗余字段来应对。 许多 NoSql 数据库使用 ColumnFamily 设计来设计可扩展的数据结构。 5. 资料 大型网站技术架构]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式架构]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fdesign%2Farchitecture%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[分布式架构 📓 本文已归档到：「blog」 分布式架构的演进 分布式架构的问题 分布式架构的关键技术 消息队列 服务化 服务总线 分布式架构的通信模式 request/response 模式（同步模式） Callback（异步模式） Future 模式 Oneway 模式 Reliable 模式 资料 分布式架构的演进 分布式架构的问题 当服务越来越多时，服务 URL 配置管理变得非常困难，F5 硬件负载均衡器的单点压力也越来越大。 当进一步发展，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。 接着，服务的调用量越来越大，服务的容量问题就暴露出来，这个服务需要多少机器支撑？什么时候该加机器？ 服务多了，沟通成本也开始上升，调某个服务失败该找谁？服务的参数都有什么约定？ 一个服务有多个业务消费者，如何确保服务质量？ 随着服务的不停升级，总有些意想不到的事发生，比如 cache 写错了导致内存溢出，故障不可避免，每次核心服务一挂，影响一大片，人心慌慌，如何控制故障的影响面？服务是否可以功能降级？或者资源劣化？ 分布式架构的关键技术 消息队列 消息队列通过消息对象分解系统耦合性，不同子系统处理同一个消息。 消息队列框架 消息队列原理 服务化 服务框架通过接口分解系统耦合性，不同子系统通过相同的接口描述进行服务启用。 服务框架是一个点对点模型。 服务框架面向同构系统。 适合：移动应用、互联网应用、外部系统。 服务化框架 服务化原理 服务治理 服务治理是服务框架/服务总线的核心功能。所谓服务治理，是指服务的提供方和消费方达成一致的约定，保证服务的高质量。服务治理功能可以解决将某些特定流量引入某一批机器，以及限制某些非法消费者的恶意访问，并在提供者处理量达到一定程度是，拒绝接受新的访问。 当前比较流行的服务治理框架：Dubbo。 服务总线 服务总线同服务框架一样，均是通过接口分解系统耦合性，不同子系统通过相同的接口描述进行服务启用。 服务总线是一个总线式的模型。 服务总线面向同构、异构系统。 适合：内部系统。 服务总线框架 服务总线原理 分布式架构的通信模式 request/response 模式（同步模式） 客户端发起请求一直阻塞到服务端返回请求为止。 Callback（异步模式） 客户端发送一个 RPC 请求给服务器，服务端处理后再发送一个消息给消息发送端提供的 callback 端点，此类情况非常合适以下场景：A 组件发送 RPC 请求给 B，B 处理完成后，需要通知 A 组件做后续处理。 Future 模式 客户端发送完请求后，继续做自己的事情，返回一个包含消息结果的 Future 对象。客户端需要使用返回结果时，使用 Future 对象的.get(),如果此时没有结果返回的话，会一直阻塞到有结果返回为止。 Oneway 模式 客户端调用完继续执行，不管接收端是否成功。 Reliable 模式 为保证通信可靠，将借助于消息中心来实现消息的可靠送达，请求将做持久化存储，在接收方在线时做送达，并由消息中心保证异常重试。 资料 https://www.zhihu.com/question/22764869/answer/31277656]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
        <tag>distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大型网站架构概述]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fdesign%2Farchitecture%2F%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[大型网站架构概述 📓 本文已归档到：「blog」 1. 大型网站系统的特点 2. 大型网站架构演化历程 2.1. 初始阶段架构 2.2. 应用服务和数据服务分离 2.3. 使用缓存改善性能 2.4. 使用应用服务器集群 2.5. 数据库读写分离 2.6. 反向代理和 CDN 加速 2.7. 分布式文件系统和分布式数据库 2.8. 使用 NoSQL 和搜索引擎 2.9. 业务拆分 2.10. 分布式服务 3. 大型网站架构模式 3.1. 分层 3.2. 分割 3.3. 分布式 3.4. 集群 3.5. 缓存 3.6. 异步 3.7. 冗余 3.8. 自动化 3.9. 安全 4. 大型网站核心架构要素 4.1. 性能 4.2. 可用性 4.3. 伸缩性 4.4. 扩展性 4.5. 安全性 5. 资料 1. 大型网站系统的特点 高并发、大流量 高可用 海量数据 用户分布广泛，网络情况复杂 安全环境恶劣 需求快速变更，迭代频繁 渐进式发展 2. 大型网站架构演化历程 2.1. 初始阶段架构 问题：网站运营初期，访问用户少，一台服务器绰绰有余。 特征：应用程序、数据库、文件等所有的资源都在一台服务器上。 描述：通常服务器操作系统使用 linux，应用程序使用 PHP 开发，然后部署在 Apache 上，数据库使用 Mysql，通俗称为 LAMP。汇集各种免费开源软件以及一台廉价服务器就可以开始系统的发展之路了。 2.2. 应用服务和数据服务分离 问题：越来越多的用户访问导致性能越来越差，越来越多的数据导致存储空间不足，一台服务器已不足以支撑。 特征：应用服务器、数据库服务器、文件服务器分别独立部署。 描述：三台服务器对性能要求各不相同：应用服务器要处理大量业务逻辑，因此需要更快更强大的 CPU；数据库服务器需要快速磁盘检索和数据缓存，因此需要更快的硬盘和更大的内存；文件服务器需要存储大量文件，因此需要更大容量的硬盘。 2.3. 使用缓存改善性能 问题：随着用户逐渐增多，数据库压力太大导致访问延迟。 特征：由于网站访问和财富分配一样遵循二八定律：80% 的业务访问集中在 20% 的数据上。将数据库中访问较集中的少部分数据缓存在内存中，可以减少数据库的访问次数，降低数据库的访问压力。 描述：缓存分为两种：应用服务器上的本地缓存和分布式缓存服务器上的远程缓存，本地缓存访问速度更快，但缓存数据量有限，同时存在与应用程序争用内存的情况。分布式缓存可以采用集群方式，理论上可以做到不受内存容量限制的缓存服务。 2.4. 使用应用服务器集群 问题：使用缓存后，数据库访问压力得到有效缓解。但是单一应用服务器能够处理的请求连接有限，在访问高峰期，成为瓶颈。 特征：多台服务器通过负载均衡同时向外部提供服务，解决单一服务器处理能力和存储空间不足的问题。 描述：使用集群是系统解决高并发、海量数据问题的常用手段。通过向集群中追加资源，提升系统的并发处理能力，使得服务器的负载压力不再成为整个系统的瓶颈。 2.5. 数据库读写分离 问题：网站使用缓存后，使绝大部分数据读操作访问都可以不通过数据库就能完成，但是仍有一部分读操作和全部的写操作需要访问数据库，在网站的用户达到一定规模后，数据库因为负载压力过高而成为网站的瓶颈。 特征：目前大部分的主流数据库都提供主从热备功能，通过配置两台数据库主从关系，可以将一台数据库服务器的数据更新同步到一台服务器上。网站利用数据库的主从热备功能，实现数据库读写分离，从而改善数据库负载压力。 描述：应用服务器在写操作的时候，访问主数据库，主数据库通过主从复制机制将数据更新同步到从数据库。这样当应用服务器在读操作的时候，访问从数据库获得数据。为了便于应用程序访问读写分离后的数据库，通常在应用服务器端使用专门的数据访问模块，使数据库读写分离的对应用透明。 2.6. 反向代理和 CDN 加速 问题：中国网络环境复杂，不同地区的用户访问网站时，速度差别也极大。 特征：采用 CDN 和反向代理加快系统的静态资源访问速度。 描述：CDN 和反向代理的基本原理都是缓存，区别在于 CDN 部署在网络提供商的机房，使用户在请求网站服务时，可以从距离自己最近的网络提供商机房获取数据；而反向代理则部署在网站的中心机房，当用户请求到达中心机房后，首先访问的服务器时反向代理服务器，如果反向代理服务器中缓存着用户请求的资源，就将其直接返回给用户。 2.7. 分布式文件系统和分布式数据库 问题：随着大型网站业务持续增长，数据库经过读写分离，从一台服务器拆分为两台服务器，依然不能满足需求。 特征：数据库采用分布式数据库，文件系统采用分布式文件系统。 描述：分布式数据库是数据库拆分的最后方法，只有在单表数据规模非常庞大的时候才使用。不到不得已时，更常用的数据库拆分手段是业务分库，将不同的业务数据库部署在不同的物理服务器上。 2.8. 使用 NoSQL 和搜索引擎 问题：随着网站业务越来越复杂，对数据存储和检索的需求也越来越复杂。 特征：系统引入 NoSQL 数据库及搜索引擎。 描述：NoSQL 数据库及搜索引擎对可伸缩的分布式特性具有更好的支持。应用服务器通过统一数据访问模块访问各种数据，减轻应用程序管理诸多数据源的麻烦。 2.9. 业务拆分 问题：大型网站的业务场景日益复杂，分为多个产品线。 特征：采用分而治之的手段将整个网站业务分成不同的产品线。系统上按照业务进行拆分改造，应用服务器按照业务区分进行分别部署。 描述：应用之间可以通过超链接建立关系，也可以通过消息队列进行数据分发，当然更多的还是通过访问同一个数据存储系统来构成一个关联的完整系统。 纵向拆分：将一个大应用拆分为多个小应用，如果新业务较为独立，那么就直接将其设计部署为一个独立的 Web 应用系统。纵向拆分相对较为简单，通过梳理业务，将较少相关的业务剥离即可。 横向拆分：将复用的业务拆分出来，独立部署为分布式服务，新增业务只需要调用这些分布式服务横向拆分需要识别可复用的业务，设计服务接口，规范服务依赖关系。 2.10. 分布式服务 问题：随着业务越拆越小，存储系统越来越庞大，应用系统整体复杂程度呈指数级上升，部署维护越来越困难。由于所有应用要和所有数据库系统连接，最终导致数据库连接资源不足，拒绝服务。 特征：公共业务提取出来，独立部署。由这些可复用的业务连接数据库，通过分布式服务提供共用业务服务。 3. 大型网站架构模式 3.1. 分层 大型网站架构中常采用分层结构，将软件系统分为应用层、服务层、数据层： 应用层 - 负责具体业务和视图展示。如网站首页及搜索输入和结果展示。 服务层 - 为应用层提供服务支持。如用户管理服务、购物车服务等。 应用层 - 提供数据存储访问服务。如数据库、缓存、文件、搜索引擎等。 分层架构的约束：禁止跨层次的调用（应用层直接调用数据层）及逆向调用（数据层调用服务层，或者服务层调用应用层）。 分层结构内部还可以继续分层，如应用可以再细分为视图层和业务逻辑层；服务层也可以细分为数据接口层和逻辑处理层。 3.2. 分割 将不同的功能和服务分割开来，包装成高内聚低耦合的模块单元。这有助于软件的开发和维护，便于不同模块的分布式部署，提高网站的并发处理能力和功能扩展能力。 3.3. 分布式 大于大型网站，分层和分割的一个主要目的是为了切分后的模块便于分布式部署，即将不同模块部署在不同的服务器上，通过远程调用协同工作。 分布式意味可以用更多的机器工作，那么 CPU、内存、存储资源也就更丰富，能够处理的并发访问和数据量就越大，进而能够为更多的用户提供服务。 分布式也引入了一些问题： 服务调用必须通过网络，网络延迟会影响性能 服务器越多，宕机概率也越大，是可用性降低 数据一致性非常困难，分布式事务也难以保证 网站依赖错综复杂，开发管理维护困难 常用的分布式方案： 分布式应用和服务 分布式静态资源 分布式数据和存储 分布式计算 3.4. 集群 集群即多台服务器部署相同应用构成一个集群，通过负载均衡设备共同对外提供服务。 集群需要具备伸缩性和故障转移机制：伸缩性是指可以根据用户访问量向集群添加或减少机器；故障转移是指，当某台机器出现故障时，负载均衡设备或失效转移机制将请求转发到集群中的其他机器上，从而不影响用户使用。 3.5. 缓存 缓存就是将数据存放在距离最近的位置以加快处理速度。缓存是改善软件性能的第一手段。 网站应用中，缓存除了可以加快数据访问速度以外，还可以减轻后端应用和数据存储的负载压力。 常见缓存手段： CDN 反向代理 本地缓存 分布式缓存 使用缓存有两个前提： 数据访问热点不均匀，频繁访问的数据应该放在缓存中 数据在某个时间段有效，不过很快过期，否则缓存数据会因已经失效而产生脏读 3.6. 异步 软件发展的一个重要目标和驱动力是降低软件耦合性。事物之间直接关系越少，彼此影响就越小，也就更容易独立发展。 大型网站架构中，系统解耦的手段除了分层、分割、分布式等，还有一个重要手段——异步。 业务间的消息传递不是同步调用，而是将一个业务操作拆分成多阶段，每个阶段间通过共享数据的方式异步执行进行协作。 在单一服务器内部可通过多线程共享内存队列的方式实现异步，处在业务操作前面的线程将操作输出到队列，后面的线程从队列中读取数据进行处理； 在分布式系统中，多个服务器集群通过分布式消息队列实现异步。 异步架构是典型的生产者消费模式，二者不存在直接调用。异步消息队列还有如下特性： 提高系统可用性 加快响应速度 消除并发访问高峰 3.7. 冗余 大型网站，出现服务器宕机是必然事件。要保证部分服务器宕机的情况下网站依然可以继续服务，不丢失数据，就需要一定程度的服务器冗余运行，数据冗余备份。这样当某台服务器宕机是，可以将其上的服务和数据访问转移到其他机器上。 访问和负载很小的服务也必须部署 至少两台服务器构成一个集群，目的就是通过冗余实现服务高可用。数据除了定期备份，存档保存，实现 冷备份 外；为了保证在线业务高可用，还需要对数据库进行主从分离，实时同步实现 热备份。 为了抵御地震、海啸等不可抗因素导致的网站完全瘫痪，某些大型网站会对整个数据中心进行备份，全球范围内部署 灾备数据中心。网站程序和数据实时同步到多个灾备数据中心。 3.8. 自动化 大型网站架构的自动化架构设计主要集中在发布运维方面： 发布过程自动化 自动化代码管理 自动化测试 自动化安全监测 自动化部署 运维自动化 自动化监控 自动化报警 自动化失效转移 自动化失效恢复 自动化降级 自动化分配资源 3.9. 安全 密码 和 手机校验码 进行身份认证 登录、交易等重要操作需要对网络通信进行 加密，存储的敏感数据如用户信息等也进行加密处理 防止机器人程序攻击网站，使用 验证码 进行识别 对常见用于 攻击 网站的 XSS 攻击、SQL 注入、进行编码转换等相应处理 对垃圾信息、敏感信息进行 过滤 对交易转账等重要操作根据交易模式和交易信息进行 风险控制 4. 大型网站核心架构要素 架构 的一种通俗说法是：最高层次的规划，难以改变的决定。 除了系统功能需求外，架构还需要关注以下架构要素： 4.1. 性能 性能问题无处不在，所以网站性能优化手段也十分繁多： 前端 浏览器缓存 静态资源压缩 合理布局页面 减少 cookie 传输 CDN 应用服务器 本地缓存 分布式缓存 异步消息队列 集群 代码层面：使用多线程、改善内存管理 数据库 索引 数据库缓存 SQL 优化 4.2. 可用性 可用性指部分服务器出现故障时，还能否对用户提供服务 冗余 通过负载均衡设备建立集群共同对外提供服务 数据存储在多台服务器，互相备份 自动化：通过预发布验证、自动化测试、自动化发布、灰度发布等手段，减少将故障引入线上环境的可能 4.3. 伸缩性 衡量伸缩的标准就是是否可以用多台服务器构建集群，是否容易向集群中增删服务器节点。增删服务器节点后是否可以提供和之前无差别的服务。集群中可容纳的总服务器数是否有限制。 应用服务器集群 - 只要服务器上保存数据，则所有服务器都是对等的，通过负载均衡设备向集群中不断加入服务器即可 缓存服务器集群 - 加入新的服务器可能会导致缓存路由失效，进而导致集群中的大部分缓存数据都无法访问。虽然缓存数据可以通过数据库重新加载，但是如果应用严重依赖缓存，可能会导致网站崩溃。需要改进缓存路由算法保证缓存数据的可访问性。 关系型数据库集群 - 关系型数据库虽然支持数据复制，主从热备等机制，但是很难做到大规模集群的可伸缩性，因此关系型数据库的集群伸缩性方案必须在数据库之外实现，通过路由分区等手段将部署有多个数据库的服务器组成一个集群。 NOSql 数据库集群 - 由于先天就是为了应对海量数据而产生，因此对伸缩性的支持通常都非常好。 4.4. 扩展性 衡量扩展性的标准就是增加新的业务产品时，是否可以实现对现有产品透明无影响，不需要任何改动或很少改动，既有功能就可以上线新产品。主要手段有：事件驱动架构和分布式服务。 4.5. 安全性 安全性保护网站不受恶意攻击，保护网站重要数据不被窃取。 5. 资料 大型网站技术架构]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网站的伸缩性架构]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fdesign%2Farchitecture%2F%E7%BD%91%E7%AB%99%E7%9A%84%E4%BC%B8%E7%BC%A9%E6%80%A7%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[网站的伸缩性架构 📓 本文已归档到：「blog」 1. 网站架构的伸缩性设计 1.1. 不同功能进行物理分离实现伸缩 1.2. 单一功能通过集群规模实现伸缩 2. 应用服务器集群的伸缩性设计 2.1. HTTP 重定向负载均衡 2.2. DNS 域名解析负载均衡 2.3. 反向代理负载均衡 2.4. IP 负载均衡 2.5. 数据链路层负载均衡 2.6. 负载均衡算法 3. 分布式缓存集群的伸缩性设计 4. 数据存储服务器集群的伸缩性设计 4.1. 关系型数据库的伸缩性设计 4.2. NoSql 数据库的伸缩性设计 5. 资料 1. 网站架构的伸缩性设计 1.1. 不同功能进行物理分离实现伸缩 纵向分离（分层后分离）：将业务处理流程上的不同部分分离部署，实现系统伸缩性。 横向分离（业务分割后分离）：将不同的业务模块分离部署，实现系统伸缩性。 1.2. 单一功能通过集群规模实现伸缩 将不同功能分离部署可以实现一定程度的伸缩性，但是随着网站的访问量逐步增加，即使分离到最小粒度的独立部署，单一的服务器也不能满足业务规模的要求。因此必须使用服务器集群，即将相同服务部署在多态服务器上构成一个集群整体对外提供服务。 2. 应用服务器集群的伸缩性设计 2.1. HTTP 重定向负载均衡 利用 HTTP 重定向协议实现负载均衡。 这种负载均衡方案的优点是比较简单。缺点是浏览器需要两次请求服务器才能完成一次访问，性能较差：重定向服务器自身的处理能力有可能成为瓶颈，整个集群的伸缩性规模有限；使用 HTTP 302 响应码重定向，可能使搜索引擎判断为 SEO 作弊，降低搜索排名。 2.2. DNS 域名解析负载均衡 利用 DNS 处理域名解析请求的同时进行负载均衡处理的一种方案。 在 DNS 服务器中配置多个 A 记录，如： 114.100.40.1 www.mysite.com114.100.40.2 www.mysite.com114.100.40.3 www.mysite.com 每次域名解析请求都会根据负载均衡算法计算一个不同的 IP 地址返回，这样 A 记录中配置的多个服务器就构成一个集群，并可以实现负载均衡。 DNS 域名解析负载均衡的优点： 将负载均衡的工作转交给了 DNS，省掉了网站管理维护的麻烦。 同时，许多 DNS 服务器还支持基于地理位置的域名解析，即将域名解析成距离用户地理最近的一个服务器地址，这样可以加快用户访问速度，改善性能。 DNS 域名解析负载均衡的缺点： DNS 是多级解析，每一级 DNS 都可能缓存 A 记录，当某台服务器下线后，即使修改了 DNS 的 A 记录，要使其生效也需要较长时间。这段时间，依然会域名解析到已经下线的服务器，导致用户访问失败。 DNS 的负载均衡的控制权在域名服务商那里，网站无法对其做更多改善和更强大的管理。 2.3. 反向代理负载均衡 大多数反向代理服务器同时提供反向代理和负载均衡的功能。 反向代理服务器的优点是部署简单。缺点是反向代理服务器时所有请求和响应的中转站，其性能可能会成为瓶颈。 2.4. IP 负载均衡 在网络层通过修改请求目标地址进行负载均衡。负载均衡服务器（网关服务器）在操作系统内核获取网络数据包，根据负载均衡算法计算得到一台真实 Web 服务器 10.0.0.1，然后将目的 IP 地址修改为 10.0.0.1，不需要通过用户进程。真实 Web 服务器处理完成后，响应数据包回到负载均衡服务器，负载均衡服务器再将数据包原地址修改为自身的 IP 地址（114.100.80.10）发送给浏览器。 IP 负载均衡在内核完成数据分发，所以处理性能优于反向代理负载均衡。但是因为所有请求响应都要经过负载均衡服务器，集群的最大响应数据吞吐量受制于负载均衡服务器网卡带宽。 2.5. 数据链路层负载均衡 数据链路层负载均衡是指在通信协议的数据链路层修改 mac 地址进行负载均衡。 这种方式又称作三角传输方式，负载均衡数据分发过程中不修改 IP 地址，只修改目的 mac 地址，通过配置真实物理服务器集群所有机器虚拟 IP 和负载均衡服务器 IP 地址一致，从而达到不修改数据包的源地址和目的地址就可以进行数据分发的目的，由于实际处理请求的真实物理服务器 IP 和数据请求目的 IP 一致，不需要通过负载均衡服务器进行地址转换，可将响应数据包直接返回给用户浏览器，避免负载均衡服务器网卡带宽成为瓶颈。这种负载方式又称作直接路由方式。 在 Linux 平台上最好的链路层负载均衡开源产品是 LVS(Linux Virtual Server)。 2.6. 负载均衡算法 负载均衡服务器的实现可以分为两个部分： 根据负载均衡算法和 Web 服务器列表计算得到集群中一台 Web 服务器的地址。 将请求数据发送到该地址对应的 Web 服务器上。 负载均衡算法通常有以下几种： 轮询（Round Robin） - 所有请求被依次分发到每台应用服务器上，即每台服务器需要处理的请求数据都相同，适合于所有服务器硬件都相同的场景。 加权轮询（Weighted Round Robin） - 根据服务器硬件性能情况，在轮询的基础上，按照配置权重将请求分发到每个服务器，高性能服务器能分配更多请求。 随机（Random） - 请求被随机分配到各个应用服务器，在许多场合下，这种方案都很简单实用，因为好的随机数本身就很平均，即使应用服务器硬件配置不同，也可以使用加权随机算法。 最少连接（Least Connection） - 记录每个应用服务器正在处理的连接数，将新到的请求分发到最少连接的服务器上，应该说，这是最符合负载均衡定义的算法。 源地址 Hash（Source Hash） - 根据请求来源的 IP 地址进行 Hash 计算，得到应用服务器，这样来自同一个 IP 地址的请求总在同一个服务器上处理，该请求的上下文信息可以存储在这台服务器上，在一个会话周期内重复使用，从而实现会话粘滞。 3. 分布式缓存集群的伸缩性设计 一致性 HASH 算法 4. 数据存储服务器集群的伸缩性设计 4.1. 关系型数据库的伸缩性设计 主从复制 - 主流关系型数据库一般都支持主从复制。 分库 - 根据业务对数据库进行分割。制约条件是跨库的表不能进行 Join 操作。 分表 - 使用数据库分片中间件，如 Cobar 等。 4.2. NoSql 数据库的伸缩性设计 一般而言，Nosql 不支持 SQL 和 ACID，但是强化了对于高可用和伸缩性的支持。 5. 资料 大型网站技术架构]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网站的安全架构]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fdesign%2Farchitecture%2F%E7%BD%91%E7%AB%99%E7%9A%84%E5%AE%89%E5%85%A8%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[网站的安全架构 📓 本文已归档到：「blog」 关键词：XSS、CSRF、SQL 注入、DoS、消息摘要、加密算法、证书 1. 网站安全的攻与防 1.1. 跨站脚本攻击（XSS） 1.2. 跨站请求伪造（CSRF） 1.3. SQL 注入攻击 1.4. 拒绝服务攻击（DoS） 2. 加密技术及密钥安全管理 2.1. 消息摘要 2.2. 加密算法 2.3. 证书 3. 资料 1. 网站安全的攻与防 互联网环境鱼龙混杂，网站被攻击是常见现象，所以了解一些常见的网站攻击手段十分必要。下面列举比较常见的 4 种攻击手段： 1.1. 跨站脚本攻击（XSS） 概念 跨站脚本攻击（Cross-Site Scripting, XSS），是一种网站应用程序的安全漏洞攻击，是代码注入的一种。它允许恶意用户将代码注入到网页上，其他用户在观看网页时就会受到影响。这类攻击通常包含了 HTML 以及用户端脚本语言。 XSS 攻击示例： 假如有下面一个 textbox &lt;input type="text" name="address1" value="value1from"&gt; value1from 是来自用户的输入，如果用户不是输入 value1from,而是输入 &quot;/&gt;&lt;script&gt;alert(document.cookie)&lt;/script&gt;&lt;!- 那么就会变成： &lt;input type="text" name="address1" value=""/&gt;&lt;script&gt;alert(document.cookie)&lt;/script&gt;&lt;!- "&gt; 嵌入的 JavaScript 代码将会被执行。攻击的威力，取决于用户输入了什么样的脚本。 攻击手段和目的 常用的 XSS 攻击手段和目的有： 盗用 cookie，获取敏感信息。 利用植入 Flash，通过 crossdomain 权限设置进一步获取更高权限；或者利用 Java 等得到类似的操作。 利用 iframe、frame、XMLHttpRequest 或上述 Flash 等方式，以（被攻击）用户的身份执行一些管理动作，或执行一些一般的如发微博、加好友、发私信等操作。 利用可被攻击的域受到其他域信任的特点，以受信任来源的身份请求一些平时不允许的操作，如进行不当的投票活动。 在访问量极大的一些页面上的 XSS 可以攻击一些小型网站，实现 DDoS 攻击的效果。 应对手段 过滤特殊字符 - 将用户所提供的内容进行过滤，从而避免 HTML 和 Jascript 代码的运行。如 &gt; 转义为 &amp;gt、&lt; 转义为 &amp;lt 等，就可以防止大部分攻击。为了避免对不必要的内容错误转移，如 3&lt;5 中的 &lt; 需要进行文本匹配后再转移，如：&lt;img src= 这样的上下文中的 &lt; 才转义。 设置 Cookie 为 HttpOnly - 设置了 HttpOnly 的 Cookie 可以防止 JavaScript 脚本调用，就无法通过 document.cookie 获取用户 Cookie 信息。 👉 参考阅读： Wiki 词条 - 跨站脚本 Web 安全测试之 XSS 1.2. 跨站请求伪造（CSRF） 概念 跨站请求伪造（Cross-site request forgery，CSRF），也被称为 one-click attack 或者 session riding，通常缩写为 CSRF 或者 XSRF。它 是一种挟制用户在当前已登录的 Web 应用程序上执行非本意的操作的攻击方法。和跨站脚本（XSS）相比，XSS 利用的是用户对指定网站的信任，CSRF 利用的是网站对用户网页浏览器的信任。 攻击手段和目的 可以如此理解 CSRF：攻击者盗用了你的身份，以你的名义发送恶意请求。 CSRF 能做的事太多： 以你名义发送邮件，发消息 用你的账号购买商品 用你的名义完成虚拟货币转账 泄露个人隐私 … 应对手段 表单 Token - CSRF 是一个伪造用户请求的操作，所以需要构造用户请求的所有参数才可以。表单 Token 通过在请求参数中添加随机数的办法来阻止攻击者获得所有请求参数。 验证码 - 请求提交是，需要用户输入验证码，以避免用户在不知情的情况下被攻击者伪造请求。 Referer check - HTTP 请求头的 Referer 域中记录着请求资源，可通过检查请求来源，验证其是否合法。 👉 参考阅读： Wiki 词条 - 跨站请求伪造 浅谈 CSRF 攻击方式 「每日一题」CSRF 是什么？「每日一题」CSRF 是什么？ WEB 安全之-CSRF（跨站请求伪造） 1.3. SQL 注入攻击 概念 SQL 注入攻击（SQL injection），是发生于应用程序之数据层的安全漏洞。简而言之，是在输入的字符串之中注入 SQL 指令，在设计不良的程序当中忽略了检查，那么这些注入进去的指令就会被数据库服务器误认为是正常的 SQL 指令而运行，因此遭到破坏或是入侵。 攻击示例： 考虑以下简单的登录表单： &lt;form action="/login" method="POST"&gt;&lt;p&gt;Username: &lt;input type="text" name="username" /&gt;&lt;/p&gt;&lt;p&gt;Password: &lt;input type="password" name="password" /&gt;&lt;/p&gt;&lt;p&gt;&lt;input type="submit" value="登陆" /&gt;&lt;/p&gt;&lt;/form&gt; 我们的处理里面的 SQL 可能是这样的： username:=r.Form.Get("username")password:=r.Form.Get("password")sql:="SELECT * FROM user WHERE username='"+username+"' AND password='"+password+"'" 如果用户的输入的用户名如下，密码任意 myuser' or 'foo' = 'foo' -- 那么我们的 SQL 变成了如下所示： SELECT * FROM user WHERE username='myuser' or 'foo' = 'foo' --'' AND password='xxx' 在 SQL 里面 -- 是注释标记，所以查询语句会在此中断。这就让攻击者在不知道任何合法用户名和密码的情况下成功登录了。 对于 MSSQL 还有更加危险的一种 SQL 注入，就是控制系统，下面这个可怕的例子将演示如何在某些版本的 MSSQL 数据库上执行系统命令。 sql:="SELECT * FROM products WHERE name LIKE '%"+prod+"%'"Db.Exec(sql) 如果攻击提交 a%' exec master..xp_cmdshell 'net user test testpass /ADD' -- 作为变量 prod 的值，那么 sql 将会变成 sql:="SELECT * FROM products WHERE name LIKE '%a%' exec master..xp_cmdshell 'net user test testpass /ADD'--%'" MSSQL 服务器会执行这条 SQL 语句，包括它后面那个用于向系统添加新用户的命令。如果这个程序是以 sa 运行而 MSSQLSERVER 服务又有足够的权限的话，攻击者就可以获得一个系统帐号来访问主机了。 虽然以上的例子是针对某一特定的数据库系统的，但是这并不代表不能对其它数据库系统实施类似的攻击。针对这种安全漏洞，只要使用不同方法，各种数据库都有可能遭殃。 攻击手段和目的 数据表中的数据外泄，例如个人机密数据，账户数据，密码等。 数据结构被黑客探知，得以做进一步攻击（例如 SELECT * FROM sys.tables）。 数据库服务器被攻击，系统管理员账户被窜改（例如 ALTER LOGIN sa WITH PASSWORD='xxxxxx'）。 获取系统较高权限后，有可能得以在网页加入恶意链接、恶意代码以及 XSS 等。 经由数据库服务器提供的操作系统支持，让黑客得以修改或控制操作系统（例如 xp_cmdshell &quot;net stop iisadmin&quot;可停止服务器的 IIS 服务）。 破坏硬盘数据，瘫痪全系统（例如 xp_cmdshell “FORMAT C:”）。 应对手段 使用参数化查询 - 建议使用数据库提供的参数化查询接口，参数化的语句使用参数而不是将用户输入变量嵌入到 SQL 语句中，即不要直接拼接 SQL 语句。例如使用 database/sql 里面的查询函数 Prepare 和 Query ，或者 Exec(query string, args ...interface{})。 单引号转换 - 在组合 SQL 字符串时，先针对所传入的参数作字符取代（将单引号字符取代为连续 2 个单引号字符）。 👉 参考阅读： Wiki 词条 - SQL 注入攻击 避免 SQL 注入 实例讲解 SQL 注入攻击 1.4. 拒绝服务攻击（DoS） 拒绝服务攻击（denial-of-service attack, DoS）亦称洪水攻击，是一种网络攻击手法，其目的在于使目标电脑的网络或系统资源耗尽，使服务暂时中断或停止，导致其正常用户无法访问。 当黑客使用网络上两个或以上被攻陷的电脑作为“僵尸”向特定的目标发动“拒绝服务”式攻击时，称为分布式拒绝服务攻击（distributed denial-of-service attack，缩写：DDoS attack、DDoS）。 攻击方式 带宽消耗型攻击 资源消耗型攻击 应对手段 防火墙 - 允许或拒绝特定通讯协议，端口或 IP 地址。当攻击从少数不正常的 IP 地址发出时，可以简单的使用拒绝规则阻止一切从攻击源 IP 发出的通信。 路由器、交换机 - 具有速度限制和访问控制能力。 流量清洗 - 通过采用抗 DDoS 软件处理，将正常流量和恶意流量区分开。 👉 参考阅读： 拒绝服务攻击 2. 加密技术及密钥安全管理 对于网站来说，用户信息、账户等等敏感数据一旦泄漏，后果严重，所以为了保护数据，应对这些信息进行加密处理。 信息加密技术一般分为： 消息摘要 加密算法 对称加密 非对称加密 证书 2.1. 消息摘要 常用数字签名算法：MD5、SHA 等。 应用场景：将用户密码以消息摘要形式保存到数据库中。 👉 参考阅读： 消息摘要 2.2. 加密算法 对称加密 对称加密指加密和解密所使用的密钥是同一个密钥。 常用对称加密算法：DES 等。 应用场景：Cookie 加密、通信机密等。 非对称加密 非对称加密指加密和解密所使用的不是同一个密钥，而是一个公私钥对。用公钥加密的信息必须用私钥才能解开；反之，用私钥加密的信息只有用公钥才能解开。 常用非对称加密算法：RSA 等。 应用场景：HTTPS 传输中浏览器使用的数字证书实质上是经过权威机构认证的非对称加密公钥。 👉 参考阅读： 加密 2.3. 密钥安全管理 保证密钥安全的方法： 把密钥和算法放在一个独立的服务器上，对外提供加密和解密服务，应用系统通过调用这个服务，实现数据的加解密。 把加解密算法放在应用系统中，密钥则放在独立服务器中，为了提高密钥的安全性，实际存储时，密钥被切分成数片，加密后分别保存在不同存储介质中。 2.3. 证书 证书可以称为信息安全加密的终极手段。公开密钥认证（英语：Public key certificate），又称公开密钥证书、公钥证书、数字证书（digital certificate）、数字认证、身份证书（identity certificate）、电子证书或安全证书，是用于公开密钥基础建设的电子文件，用来证明公开密钥拥有者的身份。此文件包含了公钥信息、拥有者身份信息（主体）、以及数字证书认证机构（发行者）对这份文件的数字签名，以保证这个文件的整体内容正确无误。 透过信任权威数字证书认证机构的根证书、及其使用公开密钥加密作数字签名核发的公开密钥认证，形成信任链架构，已在 TLS 实现并在万维网的 HTTP 以 HTTPS、在电子邮件的 SMTP 以 STARTTLS 引入并广泛应用。 众所周知，常见的应用层协议 HTTP、FTP、Telnet 本身不保证信息安全。但是加入了 SSL/TLS 加密数据包机制的 HTTPS、FTPS、Telnets 是信息安全的。 概念 传输层安全性协议（Transport Layer Security, TLS），及其前身安全套接层（Secure Sockets Layer, SSL）是一种安全协议，目的是为互联网通信，提供安全及数据完整性保障。 证书原理 SSL/TLS 协议的基本思路是采用公钥加密法，也就是说，客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。 这里有两个问题： （1）如何保证公钥不被篡改？ 解决方法：将公钥放在数字证书中。只要证书是可信的，公钥就是可信的。 （2）公钥加密计算量太大，如何减少耗用的时间？ 解决方法：每一次对话（session），客户端和服务器端都生成一个&quot;对话密钥&quot;（session key），用它来加密信息。由于&quot;对话密钥&quot;是对称加密，所以运算速度非常快，而服务器公钥只用于加密&quot;对话密钥&quot;本身，这样就减少了加密运算的消耗时间。 SSL/TLS 协议的基本过程是这样的： 客户端向服务器端索要并验证公钥。 双方协商生成&quot;对话密钥&quot;。 双方采用&quot;对话密钥&quot;进行加密通信。 👉 参考阅读： 传输层安全性协议 公开密钥认证 SSL/TLS 协议运行机制的概述 3. 资料 大型网站技术架构 Wiki 词条 - 跨站脚本 Web 安全测试之 XSS Wiki 词条 - 跨站请求伪造 浅谈 CSRF 攻击方式 「每日一题」CSRF 是什么？「每日一题」CSRF 是什么？ WEB 安全之-CSRF（跨站请求伪造） Wiki 词条 - SQL 注入攻击 避免 SQL 注入 实例讲解 SQL 注入攻击 拒绝服务攻击 传输层安全性协议 公开密钥认证 SSL/TLS 协议运行机制的概述]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网站的高性能架构]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fdesign%2Farchitecture%2F%E7%BD%91%E7%AB%99%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[网站的高性能架构 📓 本文已归档到：「blog」 1. 性能测试 1.1. 性能指标 1.2. 性能测试方法 1.3. 性能测试报告 1.4. 性能优化策略 2. 前端性能优化 2.1. 浏览器访问优化 2.2. CDN 2.3. 反向代理 3. 应用服务性能优化 3.1. 分布式缓存 3.2. 异步操作 3.3. 使用集群 3.4. 代码优化 4. 存储性能优化 4.1. 机械键盘和固态硬盘 4.2. B+数和 LSM 树 4.3. RAID 和 HDFS 5. 资料 1. 性能测试 1.1. 性能指标 网站性能测试的主要指标有： 响应时间 - 响应时间(RT)是指从客户端发一个请求开始计时，到客户端接收到从服务器端返回的响应结果结束所经历的时间，响应时间由请求发送时间、网络传输时间和服务器处理时间三部分组成。 并发数 - 系统同时处理的请求、事务数。 吞吐量 - TPS(每秒事务数)、HPS(每秒 HTTP 请求数)、QPS(每秒查询数)。 性能计数器 - 系统负载、对象与线程数、内存使用、CPU 使用、磁盘与网络 IO 等。这些指标也是系统监控的重要参数。 1.2. 性能测试方法 性能测试 负载测试 压力测试 稳定性测试 1.3. 性能测试报告 性能测试报告示例： 1.4. 性能优化策略 性能分析 - 如果请求响应慢，存在性能问题。需要对请求经历的各个环节逐一分析，排查可能出现性能瓶颈的地方，定位问题。检查监控数据，分析影响性能的主要因素：内存、磁盘、网络、CPU，可能是代码或架构设计不合理，又或者是系统资源确实不足。 性能优化 - 性能优化根据网站分层架构，大致可分为前端性能优化、应用服务性能优化、存储服务性能优化。 2. 前端性能优化 2.1. 浏览器访问优化 减少 HTTP 请求 - HTTP 请求需要建立通信链路，进行数据传输，开销高昂，所以减少 HTTP 请求数可以有效提高访问性能。减少 HTTP 的主要手段是合并 Css、JavaScript、图片。 使用浏览器缓存 - 因为静态资源文件更新频率低，可以缓存浏览器中以提高性能。设置 HTTP 头中的 Cache-Control 和 Expires 属性，可以设定浏览器缓存。 启用压缩 - 在服务器端压缩静态资源文件，在浏览器端解压缩，可以有效减少传输的数据量。由于文本文件压缩率可达 80% 以上，所以可以对静态资源，如 Html、Css、JavaScrip 进行压缩。 CSS 放在页面最上面，JavaScript 放在页面最下面 - 浏览器会在下载完全部的 Css 后才对整个页面进行渲染，所以最好的做法是将 Css 放在页面最上面，让浏览器尽快下载 Css；JavaScript 则相反，浏览器加载 JavaScript 后立即执行，可能会阻塞整个页面，造成页面显示缓慢，因此 JavaScript 最好放在页面最下面。 减少 Cookie 传输 - Cookie 包含在 HTTP 每次的请求和响应中，太大的 Cookie 会严重影响数据传输。 2.2. CDN CDN 一般缓存的是静态资源。 CDN 的本质仍然是一个缓存，而且将数据缓存在离用户最近的地方，使用户已最快速度获取数据，即所谓网络访问第一跳。 2.3. 反向代理 传统代理服务器位于浏览器一侧，代理浏览器将 HTTP 请求发送到互联网上，而反向代理服务器位于网站机房一侧，代理网站服务器接收 HTTP 请求。 反向代理服务器可以配置缓存功能加速 Web 请求，当用户第一次访问静态内容时，静态内容就会被缓存在反向代理服务器上。 反向代理还可以实现负载均衡，通过负载均衡构建的集群可以提高系统总体处理能力。 因为所有请求都必须先经过反向代理服务器，所以可以屏蔽一些攻击 IP，达到保护网站安全的作用。 3. 应用服务性能优化 3.1. 分布式缓存 网站性能优化第一定律：优先考虑使用缓存优化性能。 缓存原理 缓存指将数据存储在相对较高访问速度的存储介质中，以供系统处理。一方面缓存访问速度快，可以减少数据访问的时间，另一方面如果缓存的数据是经过计算处理得到的，那么被缓存的数据无需重复计算即可直接使用，因此缓存还起到减少计算时间的作用。 缓存的本质是一个内存 HASH 表。 缓存主要用来存放那些读写比很高、很少变化的数据，如商品的类目信息，热门词的搜索列表信息、热门商品信息等。 合理使用缓存 缓存数据的选择： 不要存储频繁修改的数据 不要存储非热点数据 数据不一致和脏读： 缓存有有效期，所以存在一定时间的数据不一致和脏读问题。如果不能接受，可以考虑使用数据更新立即更新缓存策略 需要考虑缓存问题：缓存雪崩、缓存穿透、缓存预热 3.2. 异步操作 异步处理一般是通过分布式消息队列的方式。 异步处理可以解决一下问题： 异步处理 应用解耦 流量削锋 日志处理 消息通讯 3.3. 使用集群 在高并发场景下，使用负载均衡技术为一个应用构建一个由多台服务器组成的服务器集群，将并发访问请求分发到多台服务器上处理，避免单一服务器因负载压力过大而响应缓慢，使用户请求具有更好的响应延迟特性。 3.4. 代码优化 多线程 从资源利用的角度看，使用多线程的原因主要有两个：IO 阻塞和多 CPU。 线程数并非越多越好，那么启动多少线程合适呢？ 有个参考公式： 启动线程数 = (任务执行时间 / (任务执行时间 - IO 等待时间)) * CPU 内核数 最佳启动线程数和 CPU 内核数成正比，和 IO 阻塞时间成反比。如果任务都是 CPU 计算型任务，那么线程数最多不要超过 CPU 内核数，因为启动再多线程，CPU 也来不及调度；相反如果是任务需要等待磁盘操作，网络响应，那么多启动线程有助于任务并罚赌，提高系统吞吐量。 线程安全问题 将对象设计为无状态对象 使用局部对象 并发访问资源时使用锁 资源复用 应该尽量减少那些开销很大的系统资源的创建和销毁，如数据库连接、网络通信连接、线程、复杂对象等。从编程角度，资源复用主要有两种模式：单例模式和对象池。 数据结构 根据具体场景，选择合适的数据结构。 垃圾回收 如果 Web 应用运行在 JVM 等具有垃圾回收功能的环境中，那么垃圾回收可能会对系统的性能特性产生巨大影响。立即垃圾回收机制有助于程序优化和参数调优，以及编写内存安全的代码。 4. 存储性能优化 4.1. 机械键盘和固态硬盘 考虑使用固态硬盘替代机械键盘，因为它的读写速度更快。 4.2. B+数和 LSM 树 传统关系数据库的数据库索引一般都使用两级索引的 B+ 树结构，树的层次最多三层。因此可能需要 5 次磁盘访问才能更新一条记录（三次磁盘访问获得数据索引及行 ID，然后再进行一次数据文件读操作及一次数据文件写操作）。 由于磁盘访问是随机的，传统机械键盘在数据随机访问时性能较差，每次数据访问都需要多次访问磁盘影响数据访问性能。 许多 Nosql 数据库中的索引采用 LSM 树作为主要数据结构。LSM 树可视为一个 N 阶合并树。数据写操作都在内存中进行。在 LSM 树上进行一次数据更新不需要磁盘访问，速度远快于 B+ 树。 4.3. RAID 和 HDFS HDFS(分布式文件系统) 更被大型网站所青睐。它可以配合 MapReduce 并发计算任务框架进行大数据处理，可以在整个集群上并发访问所有磁盘，无需 RAID 支持。 5. 资料 大型网站技术架构]]></content>
      <categories>
        <category>design</category>
        <category>architecture</category>
      </categories>
      <tags>
        <tag>design</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式消息队列]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fjava%2Fjavaweb%2Fdistributed%2Fmq%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[分布式消息队列 消息队列是分布式系统中重要的组件，主要解决应用耦合，异步消息，流量削锋等问题。实现高性能，高可用，可伸缩和最终一致性架构。是大型分布式系统不可缺少的中间件。 1. 消息队列应用场景 1.1. 异步处理 1.2. 应用解耦 1.3. 流量削锋 1.4. 日志处理 1.5. 消息通讯 2. JMS 消息服务 2.1. 消息模型 2.1.1. P2P 模式 2.1.2. Pub/sub 模式 2.2. 消息消费 2.3. JMS 编程模型 3. 常用 MQ 中间件 3.1. ActiveMQ 3.2. RabbitMQ 3.3. ZeroMQ 3.4. Kafka 4. MQ 示例 4.1. 电商系统 4.2. 日志收集系统 5. 资料 1. 消息队列应用场景 1.1. 异步处理 场景说明：用户注册后，需要发注册邮件和注册短信。传统的做法有两种 1.串行的方式；2.并行方式。 （1）串行方式：将注册信息写入数据库成功后，发送注册邮件，再发送注册短信。以上三个任务全部完成后，返回给客户端。 （2）并行方式：将注册信息写入数据库成功后，发送注册邮件的同时，发送注册短信。以上三个任务完成后，返回给客户端。与串行的差别是，并行的方式可以提高处理的时间。 假设三个业务节点每个使用 50 毫秒钟，不考虑网络等其他开销，则串行方式的时间是 150 毫秒，并行的时间可能是 100 毫秒。 因为 CPU 在单位时间内处理的请求数是一定的，假设 CPU1 秒内吞吐量是 100 次。则串行方式 1 秒内 CPU 可处理的请求量是 7 次（1000/150）。并行方式处理的请求量是 10 次（1000/100）。 小结：如以上案例描述，传统的方式系统的性能（并发量，吞吐量，响应时间）会有瓶颈。如何解决这个问题呢？ 引入消息队列，将不是必须的业务逻辑，异步处理。改造后的架构如下： 按照以上约定，用户的响应时间相当于是注册信息写入数据库的时间，也就是 50 毫秒。注册邮件，发送短信写入消息队列后，直接返回，因此写入消息队列的速度很快，基本可以忽略，因此用户的响应时间可能是 50 毫秒。因此架构改变后，系统的吞吐量提高到每秒 20 QPS。比串行提高了 3 倍，比并行提高了两倍。 1.2. 应用解耦 场景说明：用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口。如下图： 传统模式的缺点： 1） 假如库存系统无法访问，则订单减库存将失败，从而导致订单失败； 2） 订单系统与库存系统耦合； 如何解决以上问题呢？引入应用消息队列后的方案，如下图： 订单系统：用户下单后，订单系统完成持久化处理，将消息写入消息队列，返回用户订单下单成功。 库存系统：订阅下单的消息，采用拉/推的方式，获取下单信息，库存系统根据下单信息，进行库存操作。 假如：在下单时库存系统不能正常使用。也不影响正常下单，因为下单后，订单系统写入消息队列就不再关心其他的后续操作了。实现订单系统与库存系统的应用解耦。 1.3. 流量削锋 流量削锋也是消息队列中的常用场景，一般在秒杀或团抢活动中使用广泛。 应用场景：秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。为解决这个问题，一般需要在应用前端加入消息队列。 可以控制活动的人数； 可以缓解短时间内高流量压垮应用； 用户的请求，服务器接收后，首先写入消息队列。假如消息队列长度超过最大数量，则直接抛弃用户请求或跳转到错误页面； 秒杀业务根据消息队列中的请求信息，再做后续处理。 1.4. 日志处理 日志处理是指将消息队列用在日志处理中，比如 Kafka 的应用，解决大量日志传输的问题。架构简化如下： 日志采集客户端，负责日志数据采集，定时写入 Kafka 队列； Kafka 消息队列，负责日志数据的接收，存储和转发； 日志处理应用：订阅并消费 kafka 队列中的日志数据； 以下是新浪 kafka 日志处理应用案例： 转自（http://cloud.51cto.com/art/201507/484338.htm） Kafka - 接收用户日志的消息队列。 Logstash - 负责日志传输和解析，统一成 JSON 输出给 Elasticsearch。 Elasticsearch - 实时日志分析服务的核心技术，一个 schemaless，实时的数据存储服务，通过 index 组织数据，兼具强大的搜索和统计功能。 Kibana - 基于 Elasticsearch 的数据可视化组件，超强的数据可视化能力是众多公司选择 ELK stack 的重要原因。 1.5. 消息通讯 消息通讯是指，消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯。比如实现点对点消息队列，或者聊天室等。 点对点通讯： 客户端 A 和客户端 B 使用同一队列，进行消息通讯。 聊天室通讯： 客户端 A，客户端 B，客户端 N 订阅同一主题，进行消息发布和接收。实现类似聊天室效果。 以上实际是消息队列的两种消息模式，点对点或发布订阅模式。模型为示意图，供参考。 2. JMS 消息服务 讲消息队列就不得不提 JMS 。JMS（JAVA Message Service，java 消息服务）API 是一个消息服务的标准/规范，允许应用程序组件基于 JavaEE 平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。 在 EJB 架构中，有消息 bean 可以无缝的与 JM 消息服务集成。在 J2EE 架构模式中，有消息服务者模式，用于实现消息与应用直接的解耦。 2.1. 消息模型 在 JMS 标准中，有两种消息模型： P2P(Point to Point) Pub/Sub(Publish/Subscribe) 2.1.1. P2P 模式 P2P 模式包含三个角色：消息队列（Queue），发送者(Sender)，接收者(Receiver)。每个消息都被发送到一个特定的队列，接收者从队列中获取消息。队列保留着消息，直到他们被消费或超时。 P2P 的特点 每个消息只有一个消费者（Consumer）(即一旦被消费，消息就不再在消息队列中) 发送者和接收者之间在时间上没有依赖性，也就是说当发送者发送了消息之后，不管接收者有没有正在运行，它不会影响到消息被发送到队列 接收者在成功接收消息之后需向队列应答成功 如果希望发送的每个消息都会被成功处理的话，那么需要 P2P 模式。 2.1.2. Pub/sub 模式 包含三个角色主题（Topic），发布者（Publisher），订阅者（Subscriber） 。多个发布者将消息发送到 Topic,系统将这些消息传递给多个订阅者。 Pub/Sub 的特点 每个消息可以有多个消费者 发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息。 为了消费消息，订阅者必须保持运行的状态。 为了缓和这样严格的时间相关性，JMS 允许订阅者创建一个可持久化的订阅。这样，即使订阅者没有被激活（运行），它也能接收到发布者的消息。 如果希望发送的消息可以不被做任何处理、或者只被一个消息者处理、或者可以被多个消费者处理的话，那么可以采用 Pub/Sub 模型。 2.2. 消息消费 在 JMS 中，消息的产生和消费都是异步的。对于消费来说，JMS 的消息者可以通过两种方式来消费消息。 （1）同步 订阅者或接收者通过 receive 方法来接收消息，receive 方法在接收到消息之前（或超时之前）将一直阻塞； （2）异步 订阅者或接收者可以注册为一个消息监听器。当消息到达之后，系统自动调用监听器的 onMessage 方法。 JNDI：Java 命名和目录接口,是一种标准的 Java 命名系统接口。可以在网络上查找和访问服务。通过指定一个资源名称，该名称对应于数据库或命名服务中的一个记录，同时返回资源连接建立所必须的信息。 JNDI 在 JMS 中起到查找和访问发送目标或消息来源的作用。 2.3. JMS 编程模型 (1) ConnectionFactory 创建 Connection 对象的工厂，针对两种不同的 jms 消息模型，分别有 QueueConnectionFactory 和 TopicConnectionFactory 两种。可以通过 JNDI 来查找 ConnectionFactory 对象。 (2) Destination Destination 的意思是消息生产者的消息发送目标或者说消息消费者的消息来源。对于消息生产者来说，它的 Destination 是某个队列（Queue）或某个主题（Topic）;对于消息消费者来说，它的 Destination 也是某个队列或主题（即消息来源）。 所以，Destination 实际上就是两种类型的对象：Queue、Topic。可以通过 JNDI 来查找 Destination。 (3) Connection Connection 表示在客户端和 JMS 系统之间建立的链接（对 TCP/IP socket 的包装）。Connection 可以产生一个或多个 Session。跟 ConnectionFactory 一样，Connection 也有两种类型：QueueConnection 和 TopicConnection。 (4) Session Session 是操作消息的接口。可以通过 session 创建生产者、消费者、消息等。Session 提供了事务的功能。当需要使用 session 发送/接收多个消息时，可以将这些发送/接收动作放到一个事务中。同样，也分 QueueSession 和 TopicSession。 (5) 消息的生产者 消息生产者由 Session 创建，并用于将消息发送到 Destination。同样，消息生产者分两种类型：QueueSender 和 TopicPublisher。可以调用消息生产者的方法（send 或 publish 方法）发送消息。 (6) 消息消费者 消息消费者由 Session 创建，用于接收被发送到 Destination 的消息。两种类型：QueueReceiver 和 TopicSubscriber。可分别通过 session 的 createReceiver(Queue)或 createSubscriber(Topic)来创建。当然，也可以 session 的 creatDurableSubscriber 方法来创建持久化的订阅者。 (7) MessageListener 消息监听器。如果注册了消息监听器，一旦消息到达，将自动调用监听器的 onMessage 方法。EJB 中的 MDB（Message-Driven Bean）就是一种 MessageListener。 深入学习 JMS 对掌握 JAVA 架构，EJB 架构有很好的帮助，消息中间件也是大型分布式系统必须的组件。本次分享主要做全局性介绍，具体的深入需要大家学习，实践，总结，领会。 3. 常用 MQ 中间件 一般商用的容器，比如 WebLogic，JBoss，都支持 JMS 标准，开发上很方便。但免费的比如 Tomcat，Jetty 等则需要使用第三方的消息中间件。本部分内容介绍常用的消息中间件（ActiveMQ、RabbitMQ、RocketMQ、Kafka）以及他们的特点。 3.1. ActiveMQ ActiveMQ 是 Apache 出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持 JMS1.1 和 J2EE 1.4 规范的 JMS Provider 实现，尽管 JMS 规范出台已经是很久的事情了，但是 JMS 在当今的 J2EE 应用中间仍然扮演着特殊的地位。 ActiveMQ 特性如下： ⒈ 多种语言和协议编写客户端。语言: Java,C,C++,C#,Ruby,Perl,Python,PHP。应用协议： OpenWire,Stomp REST,WS Notification,XMPP,AMQP ⒉ 完全支持 JMS1.1 和 J2EE 1.4 规范 （持久化，XA 消息，事务) ⒊ 对 Spring 的支持，ActiveMQ 可以很容易内嵌到使用 Spring 的系统里面去，而且也支持 Spring2.0 的特性 ⒋ 通过了常见 J2EE 服务器（如 Geronimo,JBoss 4,GlassFish,WebLogic)的测试，其中通过 JCA 1.5 resource adaptors 的配置，可以让 ActiveMQ 可以自动的部署到任何兼容 J2EE 1.4 商业服务器上 ⒌ 支持多种传送协议：in-VM,TCP,SSL,NIO,UDP,JGroups,JXTA ⒍ 支持通过 JDBC 和 journal 提供高速的消息持久化 ⒎ 从设计上保证了高性能的集群，客户端-服务器，点对点 ⒏ 支持 Ajax ⒐ 支持与 Axis 的整合 ⒑ 可以很容易得调用内嵌 JMS provider，进行测试 3.2. RabbitMQ RabbitMQ 是流行的开源消息队列系统，用 erlang 语言开发。RabbitMQ 是 AMQP（高级消息队列协议）的标准实现。支持多种客户端，如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP 等，支持 AJAX，持久化。用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。 结构图如下： 几个重要概念： Broker：简单来说就是消息队列服务器实体。 Exchange：消息交换机，它指定消息按什么规则，路由到哪个队列。 Queue：消息队列载体，每个消息都会被投入到一个或多个队列。 Binding：绑定，它的作用就是把 exchange 和 queue 按照路由规则绑定起来。 Routing Key：路由关键字，exchange 根据这个关键字进行消息投递。 vhost：虚拟主机，一个 broker 里可以开设多个 vhost，用作不同用户的权限分离。 producer：消息生产者，就是投递消息的程序。 consumer：消息消费者，就是接受消息的程序。 channel：消息通道，在客户端的每个连接里，可建立多个 channel，每个 channel 代表一个会话任务。 消息队列的使用过程，如下： （1）客户端连接到消息队列服务器，打开一个 channel。 （2）客户端声明一个 exchange，并设置相关属性。 （3）客户端声明一个 queue，并设置相关属性。 （4）客户端使用 routing key，在 exchange 和 queue 之间建立好绑定关系。 （5）客户端投递消息到 exchange。 exchange 接收到消息后，就根据消息的 key 和已经设置的 binding，进行消息路由，将消息投递到一个或多个队列里。 3.3. ZeroMQ 号称史上最快的消息队列，它实际类似于 Socket 的一系列接口，他跟 Socket 的区别是：普通的 socket 是端到端的（1:1 的关系），而 ZMQ 却是可以 N：M 的关系，人们对 BSD 套接字的了解较多的是点对点的连接，点对点连接需要显式地建立连接、销毁连接、选择协议（TCP/UDP）和处理错误等，而 ZMQ 屏蔽了这些细节，让你的网络编程更为简单。ZMQ 用于 node 与 node 间的通信，node 可以是主机或者是进程。 引用官方的说法： “ZMQ(以下 ZeroMQ 简称 ZMQ)是一个简单好用的传输层，像框架一样的一个 socket library，他使得 Socket 编程更加简单、简洁和性能更高。是一个消息处理队列库，可在多个线程、内核和主机盒之间弹性伸缩。ZMQ 的明确目标是“成为标准网络协议栈的一部分，之后进入 Linux 内核”。现在还未看到它们的成功。但是，它无疑是极具前景的、并且是人们更加需要的“传统”BSD 套接字之上的一 层封装。ZMQ 让编写高性能网络应用程序极为简单和有趣。” 特点是： 高性能，非持久化； 跨平台：支持 Linux、Windows、OS X 等。 多语言支持； C、C++、Java、.NET、Python 等 30 多种开发语言。 可单独部署或集成到应用中使用； 可作为 Socket 通信库使用。 与 RabbitMQ 相比，ZMQ 并不像是一个传统意义上的消息队列服务器，事实上，它也根本不是一个服务器，更像一个底层的网络通讯库，在 Socket API 之上做了一层封装，将网络通讯、进程通讯和线程通讯抽象为统一的 API 接口。支持“Request-Reply “，”Publisher-Subscriber“，”Parallel Pipeline”三种基本模型和扩展模型。 ZeroMQ 高性能设计要点： 1、无锁的队列模型 对于跨线程间的交互（用户端和 session）之间的数据交换通道 pipe，采用无锁的队列算法 CAS；在 pipe 两端注册有异步事件，在读或者写消息到 pipe 的时，会自动触发读写事件。 2、批量处理的算法 对于传统的消息处理，每个消息在发送和接收的时候，都需要系统的调用，这样对于大量的消息，系统的开销比较大，zeroMQ 对于批量的消息，进行了适应性的优化，可以批量的接收和发送消息。 3、多核下的线程绑定，无须 CPU 切换 区别于传统的多线程并发模式，信号量或者临界区， zeroMQ 充分利用多核的优势，每个核绑定运行一个工作者线程，避免多线程之间的 CPU 切换开销。 3.4. Kafka Kafka 是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像 Hadoop 的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka 的目的是通过 Hadoop 的并行加载机制来统一线上和离线的消息处理，也是为了通过集群机来提供实时的消费。 Kafka 是一种高吞吐量的分布式发布订阅消息系统，有如下特性： 通过 O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以 TB 的消息存储也能够保持长时间的稳定性能。（文件追加的方式写入数据，过期的数据定期删除） 高吞吐量：即使是非常普通的硬件 Kafka 也可以支持每秒数百万的消息。 支持通过 Kafka 服务器和消费机集群来分区消息。 支持 Hadoop 并行数据加载。 Kafka 相关概念 Broker Kafka 集群包含一个或多个服务器，这种服务器被称为 broker[5] Topic 每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。（物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处） Partition Parition 是物理上的概念，每个 Topic 包含一个或多个 Partition. Producer 负责发布消息到 Kafka broker Consumer 消息消费者，向 Kafka broker 读取消息的客户端。 Consumer Group 每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 group name，若不指定 group name 则属于默认的 group）。 一般应用在大数据日志处理或对实时性（少量延迟），可靠性（少量丢数据）要求稍低的场景使用。 4. MQ 示例 4.1. 电商系统 消息队列采用高可用，可持久化的消息中间件。比如 Active MQ，Rabbit MQ，Rocket Mq。 （1）应用将主干逻辑处理完成后，写入消息队列。消息发送是否成功可以开启消息的确认模式。（消息队列返回消息接收成功状态后，应用再返回，这样保障消息的完整性） （2）扩展流程（发短信，配送处理）订阅队列消息。采用推或拉的方式获取消息并处理。 （3）消息将应用解耦的同时，带来了数据一致性问题，可以采用最终一致性方式解决。比如主数据写入数据库，扩展应用根据消息队列，并结合数据库方式实现基于消息队列的后续处理。 4.2. 日志收集系统 分为 Zookeeper 注册中心，日志收集客户端，Kafka 集群和 Storm 集群（OtherApp）四部分组成。 Zookeeper 注册中心，提出负载均衡和地址查找服务； 日志收集客户端，用于采集应用系统的日志，并将数据推送到 kafka 队列； Kafka 集群：接收，路由，存储，转发等消息处理； Storm 集群：与 OtherApp 处于同一级别，采用拉的方式消费队列中的数据； 5. 资料 大型网站架构系列：分布式消息队列（一） 大型网站架构系列：消息队列（二） 分布式开放消息系统(RocketMQ)的原理与实践 阿里 RocketMQ 优势对比]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式缓存]]></title>
    <url>%2Fblog%2F2018%2F07%2F05%2Fjava%2Fjavaweb%2Fdistributed%2Fcache%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[分布式缓存 1. 缓存概述 1.1. 缓存的原理 1.2. 缓存分类 1.3. 缓存媒介 1.4. 缓存设计 2. CDN 缓存 2.1. CDN 原理 2.2. CDN 优缺点 2.3. CND 架构参考 2.4. CND 技术实践 3. 反向代理缓存 3.1. 缓存原理 3.2. Squid 示例 3.3. 代理缓存比较 4. 分布式缓存 4.1. Memcache 4.2. Memcache 工作原理 4.3. Memcache 集群 4.4. Redis 4.5. Redis 常用数据类型 4.6. Redis 集群 4.7. Memcache 与 Redis 的比较 5. 本地缓存 5.1. 硬盘缓存 5.2. 内存缓存 6. 缓存架构示例 7. 数据一致性 7.1. 场景介绍 7.2. 解决方法 7.3. 其他方法 8. 缓存高可用 8.1. 解决方法 8.2. 其他方法 9. 缓存问题 9.1. 缓存雪崩 9.2. 缓存穿透 9.3. 缓存预热 9.4. 缓存更新 9.5. 缓存降级 1. 缓存概述 缓存是分布式系统中的重要组件，主要解决高并发，大数据场景下，热点数据访问的性能问题。提供高性能的数据快速访问。 1.1. 缓存的原理 将数据写入/读取速度更快的存储（设备）； 将数据缓存到离应用最近的位置； 将数据缓存到离用户最近的位置。 1.2. 缓存分类 在分布式系统中，缓存的应用非常广泛，从部署角度有以下几个方面的缓存应用。 CDN 缓存； 反向代理缓存； 分布式 Cache； 本地应用缓存； 1.3. 缓存媒介 常用中间件：Varnish，Ngnix，Squid，Memcache，Redis，Ehcache 等； 缓存的内容：文件，数据，对象； 缓存的介质：CPU，内存（本地，分布式），磁盘（本地，分布式） 1.4. 缓存设计 缓存设计需要解决以下几个问题： （1）缓存什么？ 哪些数据需要缓存：热点数据、静态资源 （2）缓存的位置？ CDN，反向代理，分布式缓存服务器，本机（内存，硬盘） （3）如何缓存的问题？ 过期策略 固定时间：比如指定缓存的时间是 30 分钟； 相对时间：比如最近 10 分钟内没有访问的数据； 同步机制 实时写入；（推） 异步刷新；（推拉） 2. CDN 缓存 CDN 主要解决将数据缓存到离用户最近的位置，一般缓存静态资源文件（页面，脚本，图片，视频，文件等）。国内网络异常复杂，跨运营商的网络访问会很慢。为了解决跨运营商或各地用户访问问题，可以在重要的城市，部署 CDN 应用。使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。 2.1. CDN 原理 CDN 的基本原理是广泛采用各种缓存服务器，将这些缓存服务器分布到用户访问相对集中的地区或网络中，在用户访问网站时，利用全局负载技术将用户的访问指向距离最近的工作正常的缓存服务器上，由缓存服务器直接响应用户请求。 （1）未部署 CDN 应用前 网络路径： 请求：本机网络（局域网）——》运营商网络——》应用服务器机房 响应：应用服务器机房——》运营商网络——》本机网络（局域网） 在不考虑复杂网络的情况下，从请求到响应需要经过 3 个节点，6 个步骤完成一次用户访问操作。 （2）部署 CDN 应用后 网络路径： 请求：本机网络（局域网）——》运营商网络 响应：运营商网络——》本机网络（局域网） 在不考虑复杂网络的情况下，从请求到响应需要经过 2 个节点，2 个步骤完成一次用户访问操作。 与不部署 CDN 服务相比，减少了 1 个节点，4 个步骤的访问。极大的提高的系统的响应速度。 2.2. CDN 优缺点 （1）优点（摘自百度百科） 本地 Cache 加速：提升访问速度，尤其含有大量图片和静态页面站点； 镜像服务：消除了不同运营商之间互联的瓶颈造成的影响，实现了跨运营商的网络加速，保证不同网络中的用户都能得到良好的访问质量； 远程加速：远程访问用户根据 DNS 负载均衡技术智能自动选择 Cache 服务器，选择最快的 Cache 服务器，加快远程访问的速度； 带宽优化：自动生成服务器的远程 Mirror（镜像）cache 服务器，远程用户访问时从 cache 服务器上读取数据，减少远程访问的带宽、分担网络流量、减轻原站点 WEB 服务器负载等功能。 集群抗攻击：广泛分布的 CDN 节点加上节点之间的智能冗余机制，可以有效地预防黑客入侵以及降低各种 D.D.o.S 攻击对网站的影响，同时保证较好的服务质量。 （2）缺点 a. 动态资源缓存，需要注意实时性； 解决：主要缓存静态资源，动态资源建立多级缓存或准实时同步； b. 如何保证数据的一致性和实时性需要权衡考虑； 解决： 设置缓存失效时间（1 个小时，最终一致性）； 数据版本号； 2.3. CND 架构参考 摘自《云宙视频 CDN 系统》 2.4. CND 技术实践 ​ 目前，中小型互联网公司，综合成本考虑，一般租用第三方 CDN 服务，大型互联网公司，采用自建或第三方结合的方式。比如淘宝刚开始使用第三方的，当流量很大后，第三方公司无法支撑其 CDN 流量，淘宝最后采用自建 CDN 的方式实现。 淘宝 CDN，如下图（来自网络）： 3. 反向代理缓存 反向代理是指在网站服务器机房部署代理服务器，实现负载均衡，数据缓存，安全控制等功能。 3.1. 缓存原理 反向代理位于应用服务器机房，处理所有对 WEB 服务器的请求。如果用户请求的页面在代理服务器上有缓冲的话，代理服务器直接将缓冲内容发送给用户。如果没有缓冲则先向 WEB 服务器发出请求，取回数据，本地缓存后再发送给用户。通过降低向 WEB 服务器的请求数，从而降低了 WEB 服务器的负载。 ​ 反向代理一般缓存静态资源，动态资源转发到应用服务器处理。常用的缓存应用服务器有 Varnish，Ngnix，Squid。 3.2. Squid 示例 Squid 反向代理一般只缓存静态资源，动态程序默认不缓存。根据从 WEB 服务器返回的 HTTP 头标记来缓冲静态页面。有四个最重要 HTTP 头标记： Last-Modified: 告诉反向代理页面什么时间被修改 Expires: 告诉反向代理页面什么时间应该从缓冲区中删除 Cache-Control: 告诉反向代理页面是否应该被缓冲 Pragma: 用来包含实现特定的指令，最常用的是 Pragma:no-cache Squid 反向代理加速网站实例 通过 DNS 的轮询技术，将客户端的请求分发给其中一台 Squid 反向代理服务器处理； 如果这台 Squid 缓存了用户的请求资源，则将请求的资源直接返回给用户； 否则这台 Squid 将没有缓存的请求根据配置的规则发送给邻居 Squid 和后台的 WEB 服务器处理； 这样既减轻后台 WEB 服务器的负载，又提高整个网站的性能和安全性。 3.3. 代理缓存比较 常用的代理缓存有 Varnish，Squid，Ngnix，简单比较如下： Varnish 和 squid 是专业的 cache 服务，nginx 需要第三方模块支持； Varnish 采用内存型缓存，避免了频繁在内存、磁盘中交换文件，性能比 Squid 高； Varnish 由于是内存 cache，所以对小文件如 css,js,小图片啥的支持很棒，后端的持久化缓存可以采用的是 Squid 或 ATS； Squid 功能全而大，适合于各种静态的文件缓存，一般会在前端挂一个 HAProxy 或 nginx 做负载均衡跑多个实例； Nginx 采用第三方模块 ncache 做的缓冲，性能基本达到 varnish，一般作为反向代理使用，可以实现简单的缓存。 4. 分布式缓存 CDN、反向代理缓存，主要解决静态文件，或用户请求资源的缓存，数据源一般为静态文件或动态生成的文件（有缓存头标识）。 分布式缓存，主要指缓存用户经常访问数据的缓存，数据源为数据库。一般起到热点数据访问和减轻数据库压力的作用。 目前分布式缓存设计，在大型网站架构中是必备的架构要素。常用的中间件有 Memcache，Redis。 4.1. Memcache Memcache 是一个高性能，分布式内存对象缓存系统，通过在内存里维护一个统一的巨大的 hash 表，它能够用来存储各种格式的数据，包括图像、视频、文件以及数据库检索的结果等。简单的说就是将数据调用到内存中，然后从内存中读取，从而大大提高读取速度。 Memcache 特性： 使用物理内存作为缓存区，可独立运行在服务器上。每个进程最大 2G，如果想缓存更多的数据，可以开辟更多的 Memcache 进程（不同端口）或者使用分布式 Memcache 进行缓存，将数据缓存到不同的物理机或者虚拟机上。 使用 key-value 的方式来存储数据，这是一种单索引的结构化数据组织形式，可使数据项查询时间复杂度为 O(1)。 协议简单：基于文本行的协议，直接通过 telnet 在 Memcached 服务器上可进行存取数据操作，简单，方便多种缓存参考此协议； 基于 libevent 高性能通信：Libevent 是一套利用 C 开发的程序库，它将 BSD 系统的 kqueue,Linux 系统的 epoll 等事件处理功能封装成一个接口，与传统的 select 相比，提高了性能。 内置的内存管理方式：所有数据都保存在内存中，存取数据比硬盘快，当内存满后，通过 LRU 算法自动删除不使用的缓存，但没有考虑数据的容灾问题，重启服务，所有数据会丢失。 分布式：各个 Memcached 服务器之间互不通信，各自独立存取数据，不共享任何信息。服务器并不具有分布式功能，分布式部署取决于 Memcache 客户端。 缓存策略：Memcached 的缓存策略是 LRU（最近最少使用）到期失效策略。在 Memcached 内存储数据项时，可以指定它在缓存的失效时间，默认为永久。当 Memcached 服务器用完分配的内时，失效的数据被首先替换，然后也是最近未使用的数据。在 LRU 中，Memcached 使用的是一种 Lazy Expiration 策略，自己不会监控存入的 key/vlue 对是否过期，而是在获取 key 值时查看记录的时间戳，检查 key/value 对空间是否过期，这样可减轻服务器的负载。 4.2. Memcache 工作原理 Memcache 的工作流程如下： 先检查客户端的请求数据是否在 Memcached 中，如有，直接把请求数据返回，不再对数据库进行任何操作； 如果请求的数据不在 Memcached 中，就去查数据库，把从数据库中获取的数据返回给客户端，同时把数据缓存一份到 Memcached 中（Memcached 客户端不负责，需要程序实现）； 每次更新数据库的同时更新 Memcached 中的数据，保证一致性； 当分配给 Memcached 内存空间用完之后，会使用 LRU（Least Recently Used，最近最少使用）策略加上到期失效策略，失效数据首先被替换，然后再替换掉最近未使用的数据。 4.3. Memcache 集群 Memcached 虽然称为 “ 分布式 ” 缓存服务器，但服务器端并没有 “ 分布式 ” 功能。每个服务器都是完全独立和隔离的服务。 Memcached 的分布式，是由客户端程序实现的。 当向 Memcached 集群存入/取出 key value 时，Memcached 客户端程序根据一定的算法计算存入哪台服务器，然后再把 key value 值存到此服务器中。 存取数据分二步走，第一步，选择服务器，第二步存取数据。 分布式算法(Consistent Hashing)： 选择服务器算法有两种，一种是根据余数来计算分布，另一种是根据散列算法来计算分布。 余数算法： 先求得键的整数散列值，再除以服务器台数，根据余数确定存取服务器。 优点：计算简单，高效； 缺点：在 Memcached 服务器增加或减少时，几乎所有的缓存都会失效。 散列算法：（一致性 Hash） 先算出 Memcached 服务器的散列值，并将其分布到 0 到 2 的 32 次方的圆上，然后用同样的方法算出存储数据的键的散列值并映射至圆上，最后从数据映射到的位置开始顺时针查找，将数据保存到查找到的第一个服务器上，如果超过 2 的 32 次方，依然找不到服务器，就将数据保存到第一台 Memcached 服务器上。 如果添加了一台 Memcached 服务器，只在圆上增加服务器的逆时针方向的第一台服务器上的键会受到影响。 一致性 Hash 算法：解决了余数算法增加节点命中大幅额度降低的问题，理论上，插入一个实体节点，平均会影响到：虚拟节点数 /2 的节点数据的命中。 4.4. Redis Redis 是一个开源（BSD 许可）的，基于内存的，多数据结构存储系统。可以用作数据库、缓存和消息中间件。 支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 内置了 复制（replication），LUA 脚本（Lua scripting）， LRU 驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis 哨兵（Sentinel）和自动分区（Cluster）提供高可用性（high availability）。 4.5. Redis 常用数据类型 1、String 常用命令：set,get,decr,incr,mget 。 应用场景：String 是最常用的一种数据类型，与 Memcache 的 key value 存储方式类似。 实现方式：String 在 Redis 内部存储默认就是一个字符串，被 RedisObject 所引用，当遇到 incr,decr 等操作时会转成数值型进行计算，此时 RedisObject 的 encoding 字段为 int。 2、Hash 常用命令：hget,hset,hgetall 。 应用场景：以存储一个用户信息对象数据，为例： 实现方式： Redis Hash 对应的 Value，内部实际就是一个 HashMap，实际这里会有 2 种不同实现。 （1） Hash 的成员比较少时 Redis 为了节省内存会采用类似一维数 组的方式来紧凑存储，而不会采用真正的 HashMap 结构，对应的 value RedisObject 的 encoding 为 zipmap； （2） 当成员数量增大时会自动转成真正的 HashMap,此时 encoding 为 ht。 3、List 常用命令：lpush,rpush,lpop,rpop,lrange。 应用场景： Redis list 的应用场景非常多，也是 Redis 最重要的数据结构之一，比如 twitter 的关注列表，粉丝列表等都可以用 Redis 的 list 结构来实现。 实现方式： Redis list 的实现为一个双向链表，可以支持反向查找和遍历，方便操作。不过带来了部分额外的内存开销，Redis 内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。 4、Set 常用命令：sadd,spop,smembers,sunion。 应用场景： Redis set 对外提供的功能与 list 类似是一个列表的功能，特殊之处在于 set 是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set 是一个很好的选择，并且 set 提供了判断某个成员是否在一个 set 集合内的重要接口，这个也是 list 所不能提供的。 实现方式： set 的内部实现是一个 value 永远为 null 的 HashMap，实际就是通过计算 hash 的方式来快速排重的，这也是 set 能提供判断一个成员是否在集合内的原因。 5、Sorted set 常用命令：zadd,zrange,zrem,zcard； 使用场景： Redis sorted set 的使用场景与 set 类似，区别是 set 不是自动有序的，而 sorted set 可以通过用户额外提供一个优先级(score)的参数来为成员排序，并且是插入有序的，即自动排序。当你需要一个有序的并且不重复的集合列表，可以选择 sorted set 数据结构，比如 twitter 的 public timeline 可以以发表时间作为 score 来存储，这样获取时就是自动按时间排好序的。 实现方式： Redis sorted set 的内部使用 HashMap 和跳跃表(SkipList)来保证数据的存储和有序，HashMap 里放的是成员到 score 的映射，而跳跃表里存放的 是所有的成员，排序依据是 HashMap 里存的 score,使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单。 4.6. Redis 集群 （1）通过 keepalived 实现的高可用方案 切换流程： 当 Master 挂了后，VIP 漂移到 Slave；Slave 上 keepalived 通知 Redis 执行：slaveof no one ,开始提供业务 当 Master 起来后，VIP 地址不变，Master 的 keepalived 通知 Redis 执行 slaveof slave IP host ，开始作为从同步数据 依次类推 主从同时 Down 机情况： 非计划性，不做考虑，一般也不会存在这种问题 计划性重启，重启之前通过运维手段 SAVE DUMP 主库数据；需要注意顺序： 关闭其中一台机器上所有 Redis，使得 master 全部切到另外一台机器（多实例部署，单机上既有主又有从的情况）；并关闭机器。 依次 dump 主上 Redis 服务 关闭主 启动主，并等待数据 load 完毕 启动从 删除 DUMP 文件（避免重启加载慢） （2）使用 Twemproxy 实现集群方案 由 twitter 开源的 c 版本 proxy，同时支持 Memcache 和 Redis，目前最新版本为：0.2.4，持续开发中;https://github.com/twitter/twemproxy .twitter 用它主要减少前端与缓存服务间网络连接数。 特点：快、轻量级、减少后端 Cache Server 连接数、易配置、支持 ketama、modula、random、常用 hash 分片算法。 这里使用 keepalived 实现高可用主备方案，解决 proxy 单点问题； 优点： 对于客户端而言，Redis 集群是透明的，客户端简单，遍于动态扩容 Proxy 为单点、处理一致性 hash 时，集群节点可用性检测不存在脑裂问题 高性能，CPU 密集型，而 Redis 节点集群多 CPU 资源冗余，可部署在 Redis 节点集群上，不需要额外设备 4.7. Memcache 与 Redis 的比较 数据结构：Memcache 只支持 key value 存储方式，Redis 支持更多的数据类型，比如 Key value，hash，list，set，zset； 多线程：Memcache 支持多线程，Redis 支持单线程；CPU 利用方面 Memcache 优于 Redis； 持久化：Memcache 不支持持久化，Redis 支持持久化； 内存利用率：Memcache 高，Redis 低（采用压缩的情况下比 Memcache 高）； 过期策略：Memcache 过期后，不删除缓存，会导致下次取数据数据的问题，Redis 有专门线程，清除缓存数据； 5. 本地缓存 本地缓存是指应用内部的缓存，标准的分布式系统，一般有多级缓存构成。本地缓存是离应用最近的缓存，一般可以将数据缓存到硬盘或内存。 5.1. 硬盘缓存 ​ 将数据缓存到硬盘到，读取时从硬盘读取。原理是直接读取本机文件，减少了网络传输消耗，比通过网络读取数据库速度更快。可以应用在对速度要求不是很高，但需要大量缓存存储的场景。 5.2. 内存缓存 直接将数据存储到本机内存中，通过程序直接维护缓存对象，是访问速度最快的方式。 6. 缓存架构示例 职责划分： CDN：存放 HTML,CSS,JS 等静态资源； 反向代理：动静分离，只缓存用户请求的静态资源； 分布式缓存：缓存数据库中的热点数据； 本地缓存：缓存应用字典等常用数据； 请求过程： 浏览器向客户端发起请求，如果 CDN 有缓存则直接返回； 如果 CDN 无缓存，则访问反向代理服务器； 如果反向代理服务器有缓存则直接返回； 如果反向代理服务器无缓存或动态请求，则访问应用服务器； 应用服务器访问本地缓存；如果有缓存，则返回代理服务器，并缓存数据；（动态请求不缓存） 如果本地缓存无数据，则读取分布式缓存；并返回应用服务器；应用服务器将数据缓存到本地缓存（部分）； 如果分布式缓存无数据，则应用程序读取数据库数据，并放入分布式缓存； 7. 数据一致性 缓存是在数据持久化之前的一个节点，主要是将热点数据放到离用户最近或访问速度更快的介质中，加快数据的访问，减小响应时间。 因为缓存属于持久化数据的一个副本，因此不可避免的会出现数据不一致问题。导致脏读或读不到数据的情况。数据不一致，一般是因为网络不稳定或节点故障导致。根据数据的操作顺序，主要有以下几种情况。 7.1. 场景介绍 （1）先写缓存，再写数据库 ​ 如下图： 假如缓存写成功，但写数据库失败或响应延迟，则下次读取（并发读）缓存时，就出现脏读； （2）先写数据库，再写缓存 ​ 如下图： ​ 假如写数据库成功，但写缓存失败，则下次读取（并发读）缓存时，则读不到数据； （3）缓存异步刷新 ​ 指数据库操作和写缓存不在一个操作步骤中，比如在分布式场景下，无法做到同时写缓存或需要异步刷新（补救措施）时候。 ​ 此种情况，主要考虑数据写入和缓存刷新的时效性。比如多久内刷新缓存，不影响用户对数据的访问。 7.2. 解决方法 第一个场景： 这个写缓存的方式，本身就是错误的，需要改为先写持久化介质，再写缓存的方式。 第二个场景： （1）根据写入缓存的响应来进行判断，如果缓存写入失败，则回滚数据库操作；此种方法增加了程序的复杂度，不建议采用； （2）缓存使用时，假如读缓存失败，先读数据库，再回写缓存的方式实现。 第三个场景： （1）首先确定，哪些数据适合此类场景； （2）根据经验值确定合理的数据不一致时间，用户数据刷新的时间间隔； 7.3. 其他方法 超时：设置合理的超时时间； 刷新：定时刷新一定范围内（根据时间，版本号）的数据； ​ 以上是简化数据读写场景，实际中会分为： 缓存与数据库之间的一致性； 多级缓存之间的一致性； 缓存副本之间的一致性。 8. 缓存高可用 业界有两种理论，第一套缓存就是缓存，临时存储数据的，不需要高可用。第二种缓存逐步演化为重要的存储介质，需要做高可用。 本人的看法是，缓存是否高可用，需要根据实际的场景而定。临界点是：是否对后端的数据库造成影响。 具体的决策依据需要根据，集群的规模（数据，缓存），成本（服务器，运维），系统性能（并发量，吞吐量，响应时间）等方面综合评价。 8.1. 解决方法 ​ 缓存的高可用，一般通过分布式和复制实现。分布式实现数据的海量缓存，复制实现缓存数据节点的高可用。架构图如下： ​ 其中，分布式采用一致性 Hash 算法，复制采用异步复制。 8.2. 其他方法 复制双写：缓存节点的复制，由异步改为双写，只有两份都写成功，才算成功。 虚拟层：一致性 Hash 存在，假如其中一个 HASH 环不可用，数据会写入临近的环，当 HASH 可用时，数据又写入正常的 HASH 环，会导致数据偏移问题。这种情况，可以考虑在 HASH 环前面加一个虚拟层实现。 多级缓存：比如一级使用本地缓存，二级采用分布式 Cahce，三级采用分布式 Cache+本地持久化； ​ 方式很多，需要根据业务场景灵活选择。 9. 缓存问题 9.1. 缓存雪崩 缓存雪崩是指：在高并发场景下，由于原有缓存失效，新缓存未到期间(例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期)，所有原本应该访问缓存的请求都去查询数据库了，而对数据库 CPU 和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。 解决方案： 合理规划缓存的失效时间； 合理评估数据库的负载压力； 对数据库进行过载保护或应用层限流； 多级缓存设计，缓存高可用； 缓存失效时产生的雪崩效应，将所有请求全部放在数据库上，这样很容易就达到数据库的瓶颈，导致服务无法正常提供。尽量避免这种场景的发生。 9.2. 缓存穿透 ​ 缓存一般是 Key，value 方式存在，当某一个 Key 不存在时会查询数据库，假如这个 Key，一直不存在，则会频繁的请求数据库，对数据库造成访问压力。 当在流量较大时，出现这样的情况，一直请求 DB，很容易导致服务挂掉。 解决方法： 对结果为空的数据也进行缓存，当此 key 有数据后，清理缓存； 一定不存在的 key，采用布隆过滤器，建立一个大的 Bitmap 中，查询时通过该 bitmap 过滤； 9.3. 缓存预热 缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决方案： 直接写个缓存刷新页面，上线时手工操作下； 数据量不大，可以在项目启动的时候自动进行加载； 定时刷新缓存； 9.4. 缓存更新 除了缓存服务器自带的缓存失效策略之外（Redis 默认的有 6 中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种： 定时去清理过期的缓存； 当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。 两者各有优劣，第一种的缺点是维护大量缓存的 key 是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。 9.5. 缓存降级 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 流操作]]></title>
    <url>%2Fblog%2F2018%2F07%2F01%2Fjava%2Fjavacore%2Fio%2FJava%E6%B5%81%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Java 流操作 📓 本文已归档到：「blog」 关键词：InputStream、OutputStream、Reader、Writer 字符流和字节流 FileReader 和 FileWriter InputStreamReader 和 OutputStreamWriter BufferedReader PrintStream FileInputStream 和 FileOutputStream ByteArrayInputStream 和 ByteArrayOutputStream PipedInputStream 和 PipedOutputStream DataInputStream 和 DataOutputStream ZipInputStream 和 ZipOutputStream ObjectInputStream 和 ObjectOutputStream 字符流和字节流 JAVA IO 中的流操作分为两类： 字节流主要操作字节类型数据（byte）。主要类是 InputStream（输入） 和 OutputStream（输出）。 字符流主要操作字符类型数据，一个字符占两个字节。主要类是 Reader（输入） 和 Writer（输出）。 JAVA IO 中的流操作类，常常是以输入、输出两种形式成对提供。 在 JAVA IO 中，流操作的一般流程如下： 使用 File 类绑定一个文件。 把 File 对象绑定到流对象上。 进行读或写操作。 关闭流 字符流和字节流的区别 字节流主要操作字节类型数据（byte）；字符流主要操作字符类型数据，一个字符占两个字节。 字节流在操作时本身不会用到缓冲区（内存），而是对文件本身直接操作的；字符流在操作时使用了缓冲区，通过缓冲区再操作文件。 FileReader 和 FileWriter FileReader 和 FileWriter 用于输入输出文本文件。 import java.io.*;public class ReaderAndWriterDemo &#123; public static void output(String filepath) throws IOException &#123; // 1.使用 File 类绑定一个文件 File f = new File(filepath); // 2.把 File 对象绑定到流对象上 Writer out = new FileWriter(f); // Writer out = new FileWriter(f, true); // 追加内容方式 // 3.进行读或写操作 String str = "Hello World!!!\r\n"; out.write(str); // 4.关闭流 // 字符流操作时使用了缓冲区，并在关闭字符流时会强制将缓冲区内容输出 // 如果不关闭流，则缓冲区的内容是无法输出的 // 如果想在不关闭流时，将缓冲区内容输出，可以使用 flush 强制清空缓冲区 out.flush(); out.close(); &#125; public static char[] input(String filepath) throws IOException &#123; // 1.使用 File 类绑定一个文件 File f = new File(filepath); // 2.把 File 对象绑定到流对象上 Reader input = new FileReader(f); // 3.进行读或写操作 int temp = 0; // 接收每一个内容 int len = 0; // 读取内容 char[] c = new char[1024]; while ((temp = input.read()) != -1) &#123; // 如果不是-1就表示还有内容，可以继续读取 c[len] = (char) temp; len++; &#125; System.out.println("文件字符数为：" + len); // 4.关闭流 input.close(); return c; &#125; public static void main(String[] args) throws IOException &#123; String filepath = "d:\\test.txt"; output(filepath); System.out.println("内容为：" + new String(input(filepath))); &#125;&#125; InputStreamReader 和 OutputStreamWriter InputStreamReader 和 OutputStreamWriter 可以将 InputStream 和 OutputStream 分别转换为 Reader 和 Writer。 示例： import java.io.*;public class OutputStreamWriterDemo &#123; public static void main(String args[]) throws IOException &#123; File f = new File("d:" + File.separator + "test.txt"); Writer out = new OutputStreamWriter(new FileOutputStream(f)); out.write("hello world!!"); out.close(); &#125;&#125;public class InputStreamReaderDemo &#123; public static void main(String args[]) throws IOException &#123; File f = new File("d:" + File.separator + "test.txt"); Reader reader = new InputStreamReader(new FileInputStream(f)); char c[] = new char[1024]; int len = reader.read(c); reader.close(); System.out.println(new String(c, 0, len)); &#125;&#125; BufferedReader BufferedReader 类用于从缓冲区中读取内容，所有的输入字节数据都放在缓冲区中。 示例： import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;public class BufferedReaderDemo &#123; public static void main(String args[]) throws IOException &#123; BufferedReader buf = new BufferedReader(new InputStreamReader(System.in)); while (true) &#123; System.out.print("请输入内容："); String str = buf.readLine(); if (str.equalsIgnoreCase("exit")) &#123; System.out.print("退出"); break; &#125; System.out.println("输入的内容为：" + str); &#125; &#125;&#125; PrintStream PrintStream 提供了非常方便的打印功能。 事实上，我们常用的 System 中提供的静态成员 System.out 和 System.err 就是 PrintStream 对象。 示例： import java.io.*;public class PrintStreamDemo &#123; public static void main(String arg[]) throws Exception &#123; final String filepath = "d:\\test.txt"; // 如果现在是使用 FileOuputStream 实例化，意味着所有的数据都会输出到文件中 OutputStream os = new FileOutputStream(new File(filepath)); PrintStream ps = new PrintStream(os); ps.print("Hello "); ps.println("World!!!"); ps.printf("姓名：%s；年龄：%d", "张三", 18); ps.close(); &#125;&#125; FileInputStream 和 FileOutputStream FileInputStream 和 FileOutputStream 用于输入、输出文件。 示例： import java.io.*;public class FileStreamDemo &#123; private static final String FILEPATH = "d:\\test.txt"; public static void output(String filepath) throws IOException &#123; // 第1步、使用File类找到一个文件 File f = new File(filepath); // 第2步、通过子类实例化父类对象 OutputStream out = new FileOutputStream(f); // 实例化时，默认为覆盖原文件内容方式；如果添加true参数，则变为对原文件追加内容的方式。 // OutputStream out = new FileOutputStream(f, true); // 第3步、进行写操作 String str = "Hello World\r\n"; byte[] bytes = str.getBytes(); out.write(bytes); // 第4步、关闭输出流 out.close(); &#125; public static void input(String filepath) throws IOException &#123; // 第1步、使用File类找到一个文件 File f = new File(filepath); // 第2步、通过子类实例化父类对象 InputStream input = new FileInputStream(f); // 第3步、进行读操作 // 有三种读取方式，体会其差异 byte[] bytes = new byte[(int) f.length()]; int len = input.read(bytes); // 读取内容 System.out.println("读入数据的长度：" + len); // 第4步、关闭输入流 input.close(); System.out.println("内容为：\n" + new String(bytes)); &#125; public static void main(String args[]) throws Exception &#123; output(FILEPATH); input(FILEPATH); &#125;&#125; ByteArrayInputStream 和 ByteArrayOutputStream ByteArrayInputStream 和 ByteArrayOutputStream 用于在内存中输入、输出数据。 示例： import java.io.*;public class ByteArrayStreamDemo &#123; public static void main(String args[]) &#123; String str = "HELLOWORLD"; // 定义一个字符串，全部由大写字母组成 ByteArrayInputStream bis = new ByteArrayInputStream(str.getBytes()); ByteArrayOutputStream bos = new ByteArrayOutputStream(); // 准备从内存ByteArrayInputStream中读取内容 int temp = 0; while ((temp = bis.read()) != -1) &#123; char c = (char) temp; // 读取的数字变为字符 bos.write(Character.toLowerCase(c)); // 将字符变为小写 &#125; // 所有的数据就全部都在ByteArrayOutputStream中 String newStr = bos.toString(); // 取出内容 try &#123; bis.close(); bos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; System.out.println(newStr); &#125;&#125; PipedInputStream 和 PipedOutputStream PipedInputStream 和 PipedOutputStream 可以在两个线程间进行通信。 示例： import java.io.*;public class PipedStreamDemo &#123; static class Send implements Runnable &#123; private PipedOutputStream pos = null; Send() &#123; pos = new PipedOutputStream(); // 实例化输出流 &#125; @Override public void run() &#123; String str = "Hello World!!!"; try &#123; pos.write(str.getBytes()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; pos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 得到此线程的管道输出流 */ PipedOutputStream getPos() &#123; return pos; &#125; &#125; static class Receive implements Runnable &#123; private PipedInputStream pis = null; Receive() &#123; pis = new PipedInputStream(); &#125; @Override public void run() &#123; byte b[] = new byte[1024]; int len = 0; try &#123; len = pis.read(b); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; pis.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; System.out.println("接收的内容为：" + new String(b, 0, len)); &#125; /** * 得到此线程的管道输入流 */ PipedInputStream getPis() &#123; return pis; &#125; &#125; public static void main(String args[]) &#123; Send s = new Send(); Receive r = new Receive(); try &#123; s.getPos().connect(r.getPis()); // 连接管道 &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; new Thread(s).start(); // 启动线程 new Thread(r).start(); // 启动线程 &#125;&#125; DataInputStream 和 DataOutputStream DataInputStream 和 DataOutputStream 会一定格式将数据输入、输出。 示例： import java.io.*;public class DataStreamDemo &#123; public static final String FILEPATH = "d:\\order.txt"; private static void output(String filepath) throws IOException &#123; // 1.使用 File 类绑定一个文件 File f = new File(filepath); // 2.把 File 对象绑定到流对象上 DataOutputStream dos = new DataOutputStream(new FileOutputStream(f)); // 3.进行读或写操作 String names[] = &#123;"衬衣", "手套", "围巾"&#125;; float prices[] = &#123;98.3f, 30.3f, 50.5f&#125;; int nums[] = &#123;3, 2, 1&#125;; for (int i = 0; i &lt; names.length; i++) &#123; dos.writeChars(names[i]); dos.writeChar('\t'); dos.writeFloat(prices[i]); dos.writeChar('\t'); dos.writeInt(nums[i]); dos.writeChar('\n'); &#125; // 4.关闭流 dos.close(); &#125; private static void input(String filepath) throws IOException &#123; // 1.使用 File 类绑定一个文件 File f = new File(filepath); // 2.把 File 对象绑定到流对象上 DataInputStream dis = new DataInputStream(new FileInputStream(f)); // 3.进行读或写操作 String name = null; // 接收名称 float price = 0.0f; // 接收价格 int num = 0; // 接收数量 char temp[] = null; // 接收商品名称 int len = 0; // 保存读取数据的个数 char c = 0; // '\u0000' try &#123; while (true) &#123; temp = new char[200]; // 开辟空间 len = 0; while ((c = dis.readChar()) != '\t') &#123; // 接收内容 temp[len] = c; len++; // 读取长度加1 &#125; name = new String(temp, 0, len); // 将字符数组变为String price = dis.readFloat(); // 读取价格 dis.readChar(); // 读取\t num = dis.readInt(); // 读取int dis.readChar(); // 读取\n System.out.printf("名称：%s；价格：%5.2f；数量：%d\n", name, price, num); &#125; &#125; catch (EOFException e) &#123; System.out.println("结束"); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; // 4.关闭流 dis.close(); &#125; public static void main(String args[]) throws IOException &#123; output(FILEPATH); input(FILEPATH); &#125;&#125; ZipInputStream 和 ZipOutputStream 示例： import java.io.*;import java.util.zip.*;public class ZipStreamDemo &#123; public static final String ZIP_FILE_PATH = "d:\\zipdemo.zip"; public static void demo01(String zipfilepath) throws IOException &#123; File file = new File(zipfilepath); ZipFile zipFile = new ZipFile(file); ZipEntry entry = zipFile.getEntry("mldn.txt"); System.out.println("压缩文件的名称：" + zipFile.getName()); File outputFile = new File("d:" + File.separator + "mldn_unzip.txt"); OutputStream out = new FileOutputStream(outputFile); // 实例化输出流 InputStream input = zipFile.getInputStream(entry); // 得到一个压缩实体的输入流 int temp = 0; while ((temp = input.read()) != -1) &#123; out.write(temp); &#125; input.close(); // 关闭输入流 out.close(); // 关闭输出流 &#125; /** * 压缩一个文件 */ public static void output1(String filepath, String zipfilepath) throws Exception &#123; // 1.使用 File 类绑定一个文件 // 定义要压缩的文件 File file = new File(filepath); // 定义压缩文件名称 File zipFile = new File(zipfilepath); // 2.把 File 对象绑定到流对象上 InputStream input = new FileInputStream(file); ZipOutputStream zipOut = new ZipOutputStream(new FileOutputStream(zipFile)); // 3.进行读或写操作 zipOut.putNextEntry(new ZipEntry(file.getName())); zipOut.setComment("This is a zip file."); int temp = 0; while ((temp = input.read()) != -1) &#123; // 读取内容 zipOut.write(temp); // 压缩输出 &#125; // 4.关闭流 input.close(); zipOut.close(); &#125; /** * 读取实体为一个文件的压缩包 */ public static void input1(String zipfilepath, String filepath) throws Exception &#123; // 1.使用 File 类绑定一个文件 File zipFile = new File(zipfilepath); // 2.把 File 对象绑定到流对象上 ZipInputStream input = new ZipInputStream(new FileInputStream(zipFile)); // 3.进行读或写操作 ZipEntry entry = input.getNextEntry(); // 得到一个压缩实体 System.out.println("压缩实体名称：" + entry.getName()); // 4.关闭流 input.close(); &#125; /** * 压缩一个目录 */ public static void output2(String dirpath, String zipfilepath) throws Exception &#123; // 1.使用 File 类绑定一个文件 // 定义要压缩的文件夹 File file = new File(dirpath); // 定义压缩文件名称 File zipFile = new File(zipfilepath); // 2.把 File 对象绑定到流对象上 ZipOutputStream zipOut = new ZipOutputStream(new FileOutputStream(zipFile)); zipOut.setComment("This is zip folder."); // 3.进行读或写操作 int temp = 0; if (file.isDirectory()) &#123; // 判断是否是文件夹 File lists[] = file.listFiles(); // 列出全部文件 for (int i = 0; i &lt; lists.length; i++) &#123; InputStream input = new FileInputStream(lists[i]); // 设置ZipEntry对象 zipOut.putNextEntry(new ZipEntry(file.getName() + File.separator + lists[i].getName())); while ((temp = input.read()) != -1) &#123; zipOut.write(temp); &#125; input.close(); &#125; &#125; // 4.关闭流 zipOut.close(); &#125; /** * 解压实体为一个目录的压缩包 */ public static void input2(String zipfilepath, String dirpath) throws Exception &#123; // 1.使用 File 类绑定一个文件 File file = new File(zipfilepath); ZipFile zipFile = new ZipFile(file); // 2.把 File 对象绑定到流对象上 ZipInputStream zis = new ZipInputStream(new FileInputStream(file)); // 3.进行读或写操作 ZipEntry entry = null; while ((entry = zis.getNextEntry()) != null) &#123; // 得到一个压缩实体 System.out.println("解压缩" + entry.getName() + "文件。"); // 定义输出的文件路径 File outFile = new File(dirpath + File.separator + entry.getName()); if (!outFile.getParentFile().exists()) &#123; // 如果输出文件夹不存在 outFile.getParentFile().mkdirs(); // 创建文件夹 &#125; if (!outFile.exists()) &#123; // 判断输出文件是否存在 outFile.createNewFile(); // 创建文件 &#125; InputStream input = zipFile.getInputStream(entry); // 得到每一个实体的输入流 OutputStream out = new FileOutputStream(outFile); // 实例化文件输出流 int temp = 0; while ((temp = input.read()) != -1) &#123; out.write(temp); &#125; input.close(); // 关闭输入流 out.close(); // 关闭输出流 &#125; // 4.关闭流 zis.close(); &#125; public static void main(String[] args) throws Exception &#123; final String filepath = "d:\\demo.txt"; final String zipfilepath = "d:\\demo.zip"; final String dirpath = "d:\\demo2"; final String dirpath2 = "d:\\new"; final String zipfilepath2 = "d:\\demo2.zip"; // demo01(ZIP_FILE_PATH); output1(filepath, zipfilepath); input1(zipfilepath, filepath); output2(dirpath, zipfilepath2); input2(zipfilepath2, dirpath2); &#125;&#125; ObjectInputStream 和 ObjectOutputStream ObjectInputStream 和 ObjectOutputStream 是对象输入输出流，一般用于对象序列化。 示例： import java.io.*;public class ObjectStream &#123; public static class Person implements Serializable &#123; private String name; private int age; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; @Override public String toString() &#123; return "姓名：" + this.name + "；年龄：" + this.age; &#125; &#125; public static void writeObject(String filepath, Object obj[]) throws Exception &#123; // 1.使用 File 类绑定一个文件 File f = new File(filepath); // 2.把 File 对象绑定到流对象上 OutputStream out = new FileOutputStream(f); ObjectOutputStream oos = new ObjectOutputStream(out); // 3.进行读或写操作 oos.writeObject(obj); // 4.关闭流 oos.close(); &#125; public static Object[] readObject(String filepath) throws Exception &#123; // 1.使用 File 类绑定一个文件 File f = new File(filepath); // 2.把 File 对象绑定到流对象上 InputStream input = new FileInputStream(f); ObjectInputStream ois = new ObjectInputStream(input); // 3.进行读或写操作 Object[] objects = (Object[]) ois.readObject(); // 4.关闭流 ois.close(); return objects; &#125; public static void main(String args[]) throws Exception &#123; final String filepath = "d:\\object.txt"; Person per[] = &#123;new Person("张三", 30), new Person("李四", 31), new Person("王五", 32)&#125;; writeObject(filepath, per); Object o[] = readObject(filepath); for (int i = 0; i &lt; o.length; i++) &#123; Person p = (Person) o[i]; System.out.println(p); &#125; &#125;&#125;]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>io</tag>
        <tag>stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 容器概述]]></title>
    <url>%2Fblog%2F2018%2F06%2F29%2Fjava%2Fjavacore%2Fcontainer%2Fjava-container%2F</url>
    <content type="text"><![CDATA[Java 容器概述 📓 本文已归档到：「blog」 容器简介 数组与容器 容器框架 容器基本机制 泛型 Iterable Comparable 和 Comparator Cloneable fail-fast 同步容器和并发容器 资料 容器简介 数组与容器 Java 中常用的存储容器就是数组和容器，二者有以下区别： 数组是固定长度的；容器的长度是可变的。 数组可以存储基本数据类型，也可以存储引用数据类型；容器只能存储引用数据类型，基本数据类型的变量要转换成对应的包装类才能放入容器类中。 容器框架 Java 容器框架主要分为 Collection 和 Map 两种。其中，Collection 又分为 List、Set 以及 Queue。 Collection - 一个独立元素的序列，这些元素都服从一条或者多条规则。 List - 必须按照插入的顺序保存元素。 Set - 不能有重复的元素。 Queue - 按照排队规则来确定对象产生的顺序（通常与它们被插入的顺序相同）。 Map - 一组成对的“键值对”对象，允许你使用键来查找值。 容器基本机制 泛型 Java5 引入了泛型技术。 Java 容器通过泛型技术来保证其数据的类型安全。 举例来说：如果有一个 List 容器，Java 编译器在编译时不会对原始类型进行类型安全检查，却会对带参数的类型进行检查，通过使用 Object 作为类型，可以告知编译器该方法可以接受任何类型的对象，比如 String 或 Integer。 List&lt;Object&gt; list = new ArrayList&lt;Object&gt;();list.add("123");list.add(123); 如果没有泛型技术，如示例中的代码那样，容器中就可能存储任意数据类型，这是很危险的行为。 想了解泛型技术的细节可以参考我的另一篇博文：Java 泛型 Iterable Collection 继承了 Iterable 接口。 迭代其实我们可以简单地理解为遍历，是一个标准化遍历各类容器里面的所有对象的方法类。它是一个经典的设计模式——迭代器模式（Iterator）。 迭代器模式 目标：提供一种方法顺序访问一个聚合对象中各个元素, 而又无须暴露该对象的内部表示。 Comparable 和 Comparator Comparable 是排序接口。若一个类实现了 Comparable 接口，就意味着该类支持排序。实现了 Comparable 接口的类的对象的列表或数组可以通过 Collections.sort 或 Arrays.sort 进行自动排序。 Comparator 是比较接口，我们如果需要控制某个类的次序，而该类本身不支持排序(即没有实现 Comparable 接口)，那么我们就可以建立一个“该类的比较器”来进行排序，这个“比较器”只需要实现 Comparator 接口即可。也就是说，我们可以通过实现 Comparator 来新建一个比较器，然后通过这个比较器对类进行排序。 在 Java 容器中，一些可以排序的容器，如 TreeMap、TreeSet，都可以通过传入 Comparator，来定义内部元素的排序规则。 Cloneable Java 中 一个类要实现 clone 功能 必须实现 Cloneable 接口，否则在调用 clone() 时会报 CloneNotSupportedException 异常。 Java 中所有类都默认继承 java.lang.Object 类，在 java.lang.Object 类中有一个方法 clone()，这个方法将返回 Object 对象的一个拷贝。Object 类里的 clone()方法仅仅用于浅拷贝（拷贝基本成员属性，对于引用类型仅返回指向改地址的引用）。 如果 Java 类需要深拷贝，需要覆写 clone() 方法。 fail-fast Java 容器（如：ArrayList、HashMap、TreeSet 等待）的 javadoc 中常常提到类似的描述： 注意，迭代器的快速失败行为无法得到保证，因为一般来说，不可能对是否出现不同步并发修改做出任何硬性保证。快速失败（fail-fast）迭代器会尽最大努力抛出 ConcurrentModificationException。因此，为提高这类迭代器的正确性而编写一个依赖于此异常的程序是错误的做法：迭代器的快速失败行为应该仅用于检测 bug。 什么是 fail-fast 那么，我们不禁要问，什么是 fail-fast，为什么要有 fail-fast 机制？ fail-fast 是 Java 容器的一种错误检测机制。当多个线程对容器进行结构上的改变的操作时，就可能触发 fail-fast 机制。记住是有可能，而不是一定。 例如：假设存在两个线程（线程 1、线程 2），线程 1 通过 Iterator 在遍历容器 A 中的元素，在某个时候线程 2 修改了容器 A 的结构（是结构上面的修改，而不是简单的修改容器元素的内容），那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生 fail-fast 机制。 fail-fast 原理 容器在迭代操作中改变元素个数都可能会导致 fail-fast。 解决 fail-fast 在遍历过程中所有涉及到改变容器个数的地方全部加上 synchronized 或者直接使用 Collections.synchronizedList，这样就可以解决。但是不推荐，因为增删造成的同步锁可能会阻塞遍历操作。 使用并发容器，如：CopyOnWriterArrayList。 同步容器和并发容器 为了在并发环境下安全地使用容器，Java 提供了同步容器和并发容器。 同步容器和并发容器详情请参考：同步容器和并发容器 资料 Java 编程思想（第 4 版） https://www.jianshu.com/p/589d58033841 https://www.jianshu.com/p/9081017a2d67 java 提高篇（三十）-----Iterator Java 提高篇（三四）-----fail-fast 机制]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>container</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 容器之 Queue]]></title>
    <url>%2Fblog%2F2018%2F06%2F29%2Fjava%2Fjavacore%2Fcontainer%2Fjava-container-queue%2F</url>
    <content type="text"><![CDATA[Java 容器之 Queue 📓 本文已归档到：「blog」 Queue 架构 Queue 接口 BlockingQueue 接口 AbstractQueue 抽象类 PriorityQueue 类 PriorityBlockingQueue 类 LinkedBlockingQueue 类 ArrayBlockingQueue 类 SynchronousQueue 资料 Queue 架构 Queue 接口 Queue 接口定义如下： public interface Queue&lt;E&gt; extends Collection&lt;E&gt; &#123;&#125; BlockingQueue 接口 BlockingQueue 接口定义如下： public interface BlockingQueue&lt;E&gt; extends Queue&lt;E&gt; &#123;&#125; BlockingQueue 顾名思义，是一个阻塞队列。 在 BlockingQueue 中，如果获取队列元素但是队列为空时，会阻塞，等待队列中有元素再返回；如果添加元素时，如果队列已满，那么等到队列可以放入新元素时再放入。 BlockingQueue 对插入操作、移除操作、获取元素操作提供了四种不同的方法用于不同的场景中使用： 抛出异常； 返回特殊值（null 或 true/false，取决于具体的操作）； 阻塞等待此操作，直到这个操作成功； 阻塞等待此操作，直到成功或者超时指定时间。 总结如下： Throws exception Special value Blocks Times out Insert add(e) offer(e) put(e) offer(e, time, unit) Remove remove() poll() take() poll(time, unit) Examine element() peek() not applicable not applicable BlockingQueue 的各个实现类都遵循了这些规则。 BlockingQueue 不接受 null 值元素。 AbstractQueue 抽象类 AbstractQueue 抽象类定义如下： public abstract class AbstractQueue&lt;E&gt; extends AbstractCollection&lt;E&gt; implements Queue&lt;E&gt; &#123;&#125; AbstractQueue 类提供 Queue 接口的骨干实现，以最大限度地减少实现 Queue 接口所需的工作。 PriorityQueue 类 PriorityQueue 类定义如下： public class PriorityQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements java.io.Serializable &#123;&#125; PriorityQueue 要点 PriorityQueue 实现了 Serializable，支持序列化。 PriorityQueue 类是基于优先级堆实现的无界优先级队列。 PriorityQueue 中的元素根据自然顺序或 Comparator 提供的顺序排序。 PriorityQueue 不接受 null 值元素。 PriorityQueue 不是线程安全的。 PriorityBlockingQueue 类 PriorityBlockingQueue 类定义如下： public class PriorityBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123;&#125; PriorityBlockingQueue 要点 PriorityBlockingQueue 实现了 BlockingQueue，也是一个阻塞队列。 PriorityBlockingQueue 实现了 Serializable，支持序列化。 PriorityBlockingQueue 可以视为 PriorityQueue 的线程安全版本。 PriorityBlockingQueue 不接受 null 值元素。 PriorityBlockingQueue 的插入操作 put 方法不会 block，因为它是无界队列（take 方法在队列为空的时候会阻塞）。 PriorityBlockingQueue 原理 PriorityBlockingQueue 有两个重要成员： private transient Object[] queue;private final ReentrantLock lock; queue 是一个 Object 数组，用于保存 PriorityBlockingQueue 的元素。 而可重入锁 lock 则用于在执行插入、删除操作时，保证这个方法在当前线程释放锁之前，其他线程不能访问。 PriorityBlockingQueue 的容量虽然有初始化大小，但是不限制大小，如果当前容量已满，插入新元素时会自动扩容。 LinkedBlockingQueue 类 LinkedBlockingQueue 类定义如下： public class LinkedBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123;&#125; LinkedBlockingQueue 要点 LinkedBlockingQueue 实现了 BlockingQueue，也是一个阻塞队列。 LinkedBlockingQueue 实现了 Serializable，支持序列化。 LinkedBlockingQueue 是基于单链表实现的阻塞队列，可以当做无界队列也可以当做有界队列来使用。 LinkedBlockingQueue 中元素按照插入顺序保存（FIFO）。 LinkedBlockingQueue 原理 // 队列容量private final int capacity;// 队列中的元素数量private final AtomicInteger count = new AtomicInteger(0);// 队头private transient Node&lt;E&gt; head;// 队尾private transient Node&lt;E&gt; last;// take, poll, peek 等读操作的方法需要获取到这个锁private final ReentrantLock takeLock = new ReentrantLock();// 如果读操作的时候队列是空的，那么等待 notEmpty 条件private final Condition notEmpty = takeLock.newCondition();// put, offer 等写操作的方法需要获取到这个锁private final ReentrantLock putLock = new ReentrantLock();// 如果写操作的时候队列是满的，那么等待 notFull 条件private final Condition notFull = putLock.newCondition(); 这里用了两个锁，两个 Condition，简单介绍如下： takeLock 和 notEmpty 搭配：如果要获取（take）一个元素，需要获取 takeLock 锁，但是获取了锁还不够，如果队列此时为空，还需要队列不为空（notEmpty）这个条件（Condition）。 putLock 需要和 notFull 搭配：如果要插入（put）一个元素，需要获取 putLock 锁，但是获取了锁还不够，如果队列此时已满，还需要队列不是满的（notFull）这个条件（Condition）。 ArrayBlockingQueue 类 ArrayBlockingQueue 类定义如下： public class ArrayBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123;&#125; ArrayBlockingQueue 要点 ArrayBlockingQueue 实现了 BlockingQueue，也是一个阻塞队列。 ArrayBlockingQueue 实现了 Serializable，支持序列化。 ArrayBlockingQueue 是基于数组实现的无界阻塞队列。 ArrayBlockingQueue 原理 ArrayBlockingQueue 的重要成员如下： // 用于存放元素的数组final Object[] items;// 下一次读取操作的位置int takeIndex;// 下一次写入操作的位置int putIndex;// 队列中的元素数量int count;// 以下几个就是控制并发用的同步器final ReentrantLock lock;private final Condition notEmpty;private final Condition notFull; ArrayBlockingQueue 实现并发同步的原理就是，读操作和写操作都需要获取到 AQS 独占锁才能进行操作。 如果队列为空，这个时候读操作的线程进入到读线程队列排队，等待写线程写入新的元素，然后唤醒读线程队列的第一个等待线程。 如果队列已满，这个时候写操作的线程进入到写线程队列排队，等待读线程将队列元素移除，然后唤醒写线程队列的第一个等待线程。 对于 ArrayBlockingQueue，我们可以在构造的时候指定以下三个参数： 队列容量，其限制了队列中最多允许的元素个数； 指定独占锁是公平锁还是非公平锁。非公平锁的吞吐量比较高，公平锁可以保证每次都是等待最久的线程获取到锁； 可以指定用一个集合来初始化，将此集合中的元素在构造方法期间就先添加到队列中。 SynchronousQueue SynchronousQueue 定义如下： public class SynchronousQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123;&#125; SynchronousQueue 这个类，不过它在线程池的实现类 ScheduledThreadPoolExecutor 中得到了应用。 SynchronousQueue 的队列其实是虚的，其不提供任何空间（一个都没有）来存储元素。数据必须从某个写线程交给某个读线程，而不是写到某个队列中等待被消费。 SynchronousQueue 中不能使用 peek 方法（在这里这个方法直接返回 null），peek 方法的语义是只读取不移除，显然，这个方法的语义是不符合 SynchronousQueue 的特征的。 SynchronousQueue 也不能被迭代，因为根本就没有元素可以拿来迭代的。 虽然 SynchronousQueue 间接地实现了 Collection 接口，但是如果你将其当做 Collection 来用的话，那么集合是空的。 当然，SynchronousQueue 也不允许传递 null 值的（并发包中的容器类好像都不支持插入 null 值，因为 null 值往往用作其他用途，比如用于方法的返回值代表操作失败）。 资料 解读 Java 并发队列 BlockingQueue]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>container</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一篇文章让你彻底掌握 python 语言]]></title>
    <url>%2Fblog%2F2018%2F06%2F28%2Fprogramming%2Fpython%2F</url>
    <content type="text"><![CDATA[一篇文章让你彻底掌握 python 语言 📓 本文已归档到：「blog」 解释器 注释 数据类型 操作符 算术运算符 比较运算符 赋值运算符 位运算符 逻辑运算符 成员运算符 身份运算符 运算符优先级 控制语句 条件语句 循环语句 函数 函数变量作用域 关键字参数 可变参数列表 返回值 异常 异常处理 抛出异常 自定义异常 面向对象 面向对象技术简介 类定义 类对象 类的方法 继承 多继承 方法重写 类属性与方法 标准库概览 操作系统接口 文件通配符 命令行参数 错误输出重定向和程序终止 字符串正则匹配 数学 Python 编程 解释器 Linux/Unix 的系统上，Python 解释器通常被安装在 /usr/local/bin/python3.4 这样的有效路径（目录）里。 我们可以将路径 /usr/local/bin 添加到您的 Linux/Unix 操作系统的环境变量中，这样您就可以通过 shell 终端输入下面的命令来启动 Python 。 在 Linux/Unix 系统中，你可以在脚本顶部添加以下命令让 Python 脚本可以像 SHELL 脚本一样可直接执行： #! /usr/bin/env python3.4 注释 Python 中的注释有三种形式： 以 # 开头 以 ''' 开始，以 ''' 结尾 以 &quot;&quot;&quot; 开始，以 &quot;&quot;&quot; 结尾 # 单行注释'''这是多行注释，用三个单引号这是多行注释，用三个单引号这是多行注释，用三个单引号'''"""这是多行注释，用三个双引号这是多行注释，用三个双引号这是多行注释，用三个双引号""" 数据类型 Python3 中有六个标准的数据类型： Numbers（数字） String（字符串） List（列表） Tuple（元组） Sets（集合） Dictionaries（字典） 操作符 Python 语言支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 算术运算符 运算符 描述 实例 + 加 - 两个对象相加 a + b 输出结果 31 - 减 - 得到负数或是一个数减去另一个数 a - b 输出结果 -11 * 乘 - 两个数相乘或是返回一个被重复若干次的字符串 a * b 输出结果 210 / 除 - x 除以 y b / a 输出结果 2.1 % 取模 - 返回除法的余数 b % a 输出结果 1 ** 幂 - 返回 x 的 y 次幂 a**b 为 10 的 21 次方 // 取整除 - 返回商的整数部分 9//2 输出结果 4 , 9.0//2.0 输出结果 4.0 比较运算符 运算符 描述 实例 == 等于 - 比较对象是否相等 (a == b) 返回 False。 != 不等于 - 比较两个对象是否不相等 (a != b) 返回 True. &gt; 大于 - 返回 x 是否大于 y (a &gt; b) 返回 False。 &lt; 小于 - 返回 x 是否小于 y。所有比较运算符返回 1 表示真，返回 0 表示假。这分别与特殊的变量 True 和 False 等价。注意，这些变量名的大写。 (a &lt; b) 返回 True。 &gt;= 大于等于 - 返回 x 是否大于等于 y。 (a &gt;= b) 返回 False。 &lt;= 小于等于 - 返回 x 是否小于等于 y。 (a &lt;= b) 返回 True。 赋值运算符 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a *= 乘法赋值运算符 c _= a 等效于 c = c _ a /= 除法赋值运算符 c /= a 等效于 c = c / a %= 取模赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c **= a 等效于 c = c ** a //= 取整除赋值运算符 c //= a 等效于 c = c // a 位运算符 运算符 描述 实例 &amp; 按位与运算符：参与运算的两个值,如果两个相应位都为 1,则该位的结果为 1,否则为 0 (a &amp; b) 输出结果 12 ，二进制解释： 0000 1100 | 按位或运算符：只要对应的二个二进位有一个为 1 时，结果位就为 1。 (a | b) 输出结果 61 ，二进制解释： 0011 1101 ^ 按位异或运算符：当两对应的二进位相异时，结果为 1 (a ^ b) 输出结果 49 ，二进制解释： 0011 0001 ~ 按位取反运算符：对数据的每个二进制位取反,即把 1 变为 0,把 0 变为 1 (~a ) 输出结果 -61 ，二进制解释： 1100 0011， 在一个有符号二进制数的补码形式。 &lt;&lt; 左移动运算符：运算数的各二进位全部左移若干位，由&quot;&lt;&lt;&quot;右边的数指定移动的位数，高位丢弃，低位补 0。 a &lt;&lt; 2 输出结果 240 ，二进制解释： 1111 0000 &gt;&gt; 右移动运算符：把&quot;&gt;&gt;“左边的运算数的各二进位全部右移若干位，”&gt;&gt;&quot;右边的数指定移动的位数 a &gt;&gt; 2 输出结果 15 ，二进制解释： 0000 1111 逻辑运算符 运算符 逻辑表达式 描述 实例 and x and y 布尔&quot;与&quot; - 如果 x 为 False，x and y 返回 False，否则它返回 y 的计算值。 (a and b) 返回 20。 or x or y 布尔&quot;或&quot; - 如果 x 是 True，它返回 x 的值，否则它返回 y 的计算值。 (a or b) 返回 10。 not not x 布尔&quot;非&quot; - 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。 not(a and b) 返回 False 成员运算符 运算符 描述 实例 in 如果在指定的序列中找到值返回 True，否则返回 False。 x 在 y 序列中 , 如果 x 在 y 序列中返回 True。 not in 如果在指定的序列中没有找到值返回 True，否则返回 False。 x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 身份运算符 运算符 描述 实例 is is 是判断两个标识符是不是引用自一个对象 x is y, 如果 id(x) 等于 id(y) , is 返回结果 1 is not is not 是判断两个标识符是不是引用自不同对象 x is not y, 如果 id(x) 不等于 id(y). is not 返回结果 1 运算符优先级 运算符 描述 ** 指数 (最高优先级) ~ + - 按位翻转, 一元加号和减号 (最后两个的方法名为 +@ 和 -@) * / % // 乘，除，取模和取整除 + - 加法减法 &gt;&gt; &lt;&lt; 右移，左移运算符 &amp; 位 ‘AND’ ^ | 位运算符 &lt;= &lt; &gt; &gt;= 比较运算符 &lt;&gt; == != 等于运算符 = %= /= //= -= += *= **= 赋值运算符 is is not 身份运算符 in not in 成员运算符 not or and 逻辑运算符 控制语句 条件语句 if condition_1: statement_block_1elif condition_2: statement_block_2else: statement_block_3 循环语句 while while 判断条件： statements for for &lt;variable&gt; in &lt;sequence&gt;: &lt;statements&gt; range() for i in range(0, 10, 3) : print(i) break 和 continue break 语句可以跳出 for 和 while 的循环体。 continue 语句被用来告诉 Python 跳过当前循环块中的剩余语句，然后继续进行下一轮循环。 pass pass 语句什么都不做。它只在语法上需要一条语句但程序不需要任何操作时使用.例如: while True: pass # 等待键盘中断 (Ctrl+C) 函数 Python 定义函数使用 def 关键字，一般格式如下： def 函数名（参数列表）： 函数体 函数变量作用域 #!/usr/bin/env python3a = 4 # 全局变量def print_func1(): a = 17 # 局部变量 print("in print_func a = ", a)def print_func2(): print("in print_func a = ", a)print_func1()print_func2()print("a = ", a) 以上实例运行结果如下： in print_func a = 17in print_func a = 4a = 4 关键字参数 函数也可以使用 kwarg=value 的关键字参数形式被调用.例如,以下函数: def parrot(voltage, state='a stiff', action='voom', type='Norwegian Blue'): print("-- This parrot wouldn't", action, end=' ') print("if you put", voltage, "volts through it.") print("-- Lovely plumage, the", type) print("-- It's", state, "!") 可以以下几种方式被调用: parrot(1000) # 1 positional argumentparrot(voltage=1000) # 1 keyword argumentparrot(voltage=1000000, action='VOOOOOM') # 2 keyword argumentsparrot(action='VOOOOOM', voltage=1000000) # 2 keyword argumentsparrot('a million', 'bereft of life', 'jump') # 3 positional argumentsparrot('a thousand', state='pushing up the daisies') # 1 positional, 1 keyword 以下为错误调用方法： parrot() # required argument missingparrot(voltage=5.0, 'dead') # non-keyword argument after a keyword argumentparrot(110, voltage=220) # duplicate value for the same argumentparrot(actor='John Cleese') # unknown keyword argument 可变参数列表 最后,一个最不常用的选择是可以让函数调用可变个数的参数.这些参数被包装进一个元组(查看元组和序列).在这些可变个数的参数之前,可以有零到多个普通的参数: def arithmetic_mean(*args): sum = 0 for x in args: sum += x return sum 返回值 Python 的函数的返回值使用 return 语句，可以将函数作为一个值赋值给指定变量： def return_sum(x,y): c = x + y return c 异常 异常处理 try 语句按照如下方式工作； 首先，执行 try 子句（在关键字 try 和关键字 except 之间的语句） 如果没有异常发生，忽略 except 子句，try 子句执行后结束。 如果在执行 try 子句的过程中发生了异常，那么 try 子句余下的部分将被忽略。如果异常的类型和 except 之后的名称相符，那么对应的 except 子句将被执行。最后执行 try 语句之后的代码。 如果一个异常没有与任何的 except 匹配，那么这个异常将会传递给上层的 try 中。 不管 try 子句里面有没有发生异常，finally 子句都会执行。 import systry: f = open('myfile.txt') s = f.readline() i = int(s.strip())except OSError as err: print("OS error: &#123;0&#125;".format(err))except ValueError: print("Could not convert data to an integer.")except: print("Unexpected error:", sys.exc_info()[0]) raisefinally: # 清理行为 抛出异常 Python 使用 raise 语句抛出一个指定的异常。例如: &gt;&gt;&gt; raise NameError('HiThere')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in ?NameError: HiThere 自定义异常 可以通过创建一个新的 exception 类来拥有自己的异常。异常应该继承自 Exception 类，或者直接继承，或者间接继承。 当创建一个模块有可能抛出多种不同的异常时，一种通常的做法是为这个包建立一个基础异常类，然后基于这个基础类为不同的错误情况创建不同的子类： class Error(Exception): """Base class for exceptions in this module.""" passclass InputError(Error): """Exception raised for errors in the input. Attributes: expression -- input expression in which the error occurred message -- explanation of the error """ def __init__(self, expression, message): self.expression = expression self.message = messageclass TransitionError(Error): """Raised when an operation attempts a state transition that's not allowed. Attributes: previous -- state at beginning of transition next -- attempted new state message -- explanation of why the specific transition is not allowed """ def __init__(self, previous, next, message): self.previous = previous self.next = next self.message = message 大多数的异常的名字都以&quot;Error&quot;结尾，就跟标准的异常命名一样。 面向对象 面向对象技术简介 类(Class): 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。 **类变量：**类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。 **数据成员：**类变量或者实例变量用于处理类及其实例对象的相关的数据。 **方法重写：**如果从父类继承的方法不能满足子类的需求，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。 **实例变量：**定义在方法中的变量，只作用于当前实例的类。 **继承：**即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计：一个 Dog 类型的对象派生自 Animal 类，这是模拟&quot;是一个（is-a）&quot;关系（例图，Dog 是一个 Animal）。 **实例化：**创建一个类的实例，类的具体对象。 **方法：**类中定义的函数。 **对象：**通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。 类定义 语法格式如下： class ClassName: &lt;statement-1&gt; . . . &lt;statement-N&gt; 类实例化后，可以使用其属性，实际上，创建一个类之后，可以通过类名访问其属性。 类对象 类对象支持两种操作：属性引用和实例化。 属性引用使用和 Python 中所有的属性引用一样的标准语法：obj.name。 类对象创建后，类命名空间中所有的命名都是有效属性名。所以如果类定义是这样: #!/usr/bin/python3class MyClass: """一个简单的类实例""" i = 12345 def f(self): return 'hello world'# 实例化类x = MyClass()# 访问类的属性和方法print("MyClass 类的属性 i 为：", x.i)print("MyClass 类的方法 f 输出为：", x.f()) 实例化类： # 实例化类x = MyClass()# 访问类的属性和方法 以上创建了一个新的类实例并将该对象赋给局部变量 x，x 为空的对象。 执行以上程序输出结果为： MyClass 类的属性 i 为： 12345MyClass 类的方法 f 输出为： hello world 很多类都倾向于将对象创建为有初始状态的。因此类可能会定义一个名为 init() 的特殊方法（构造方法），像下面这样： def __init__(self): self.data = [] 类定义了 init() 方法的话，类的实例化操作会自动调用 init() 方法。所以在下例中，可以这样创建一个新的实例: x = MyClass() 当然， init() 方法可以有参数，参数通过 init() 传递到类的实例化操作上。例如: &gt;&gt;&gt; class Complex:... def __init__(self, realpart, imagpart):... self.r = realpart... self.i = imagpart...&gt;&gt;&gt; x = Complex(3.0, -4.5)&gt;&gt;&gt; x.r, x.i(3.0, -4.5) 类的方法 在类地内部，使用 def 关键字可以为类定义一个方法，与一般函数定义不同，类方法必须包含参数 self,且为第一个参数: #!/usr/bin/python3#类定义class people: #定义基本属性 name = '' age = 0 #定义私有属性,私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print("%s 说: 我 %d 岁。" %(self.name,self.age))# 实例化类p = people('W3Cschool',10,30)p.speak() 执行以上程序输出结果为： W3Cschool 说: 我 10 岁。 继承 Python 同样支持类的继承，如果一种语言不支持继承就，类就没有什么意义。派生类的定义如下所示: class DerivedClassName(BaseClassName1): &lt;statement-1&gt; . . . &lt;statement-N&gt; 需要注意圆括号中基类的顺序，若是基类中有相同的方法名，而在子类使用时未指定，python 从左至右搜索 即方法在子类中未找到时，从左到右查找基类中是否包含方法。 BaseClassName（示例中的基类名）必须与派生类定义在一个作用域内。除了类，还可以用表达式，基类定义在另一个模块中时这一点非常有用: class DerivedClassName(modname.BaseClassName): 实例 #!/usr/bin/python3#类定义class people: #定义基本属性 name = '' age = 0 #定义私有属性,私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print("%s 说: 我 %d 岁。" %(self.name,self.age))#单继承示例class student(people): grade = '' def __init__(self,n,a,w,g): #调用父类的构函 people.__init__(self,n,a,w) self.grade = g #覆写父类的方法 def speak(self): print("%s 说: 我 %d 岁了，我在读 %d 年级"%(self.name,self.age,self.grade))s = student('ken',10,60,3)s.speak() 执行以上程序输出结果为： ken 说: 我 10 岁了，我在读 3 年级 多继承 Python 同样有限的支持多继承形式。多继承的类定义形如下例: class DerivedClassName(Base1, Base2, Base3): &lt;statement-1&gt; . . . &lt;statement-N&gt; 需要注意圆括号中父类的顺序，若是父类中有相同的方法名，而在子类使用时未指定，python 从左至右搜索 即方法在子类中未找到时，从左到右查找父类中是否包含方法。 #!/usr/bin/python3#类定义class people: #定义基本属性 name = '' age = 0 #定义私有属性,私有属性在类外部无法直接进行访问 __weight = 0 #定义构造方法 def __init__(self,n,a,w): self.name = n self.age = a self.__weight = w def speak(self): print("%s 说: 我 %d 岁。" %(self.name,self.age))#单继承示例class student(people): grade = '' def __init__(self,n,a,w,g): #调用父类的构函 people.__init__(self,n,a,w) self.grade = g #覆写父类的方法 def speak(self): print("%s 说: 我 %d 岁了，我在读 %d 年级"%(self.name,self.age,self.grade))#另一个类，多重继承之前的准备class speaker(): topic = '' name = '' def __init__(self,n,t): self.name = n self.topic = t def speak(self): print("我叫 %s，我是一个演说家，我演讲的主题是 %s"%(self.name,self.topic))#多重继承class sample(speaker,student): a ='' def __init__(self,n,a,w,g,t): student.__init__(self,n,a,w,g) speaker.__init__(self,n,t)test = sample("Tim",25,80,4,"Python")test.speak() #方法名同，默认调用的是在括号中排前地父类的方法 执行以上程序输出结果为： 我叫 Tim，我是一个演说家，我演讲的主题是 Python 方法重写 如果你的父类方法的功能不能满足你的需求，你可以在子类重写你父类的方法，实例如下： #!/usr/bin/python3class Parent: # 定义父类 def myMethod(self): print ('调用父类方法')class Child(Parent): # 定义子类 def myMethod(self): print ('调用子类方法')c = Child() # 子类实例c.myMethod() # 子类调用重写方法 执行以上程序输出结果为： 调用子类方法 类属性与方法 类的私有属性 __private_attrs：两个下划线开头，声明该属性为私有，不能在类地外部被使用或直接访问。在类内部的方法中使用时self.__private_attrs。 类的方法 在类地内部，使用 def 关键字可以为类定义一个方法，与一般函数定义不同，类方法必须包含参数 self,且为第一个参数 类的私有方法 __private_method：两个下划线开头，声明该方法为私有方法，不能在类地外部调用。在类的内部调用 slef.__private_methods。 实例如下： #!/usr/bin/python3class JustCounter: __secretCount = 0 # 私有变量 publicCount = 0 # 公开变量 def count(self): self.__secretCount += 1 self.publicCount += 1 print (self.__secretCount)counter = JustCounter()counter.count()counter.count()print (counter.publicCount)print (counter.__secretCount) # 报错，实例不能访问私有变量 执行以上程序输出结果为： 122Traceback (most recent call last): File "test.py", line 16, in &lt;module&gt; print (counter.__secretCount) # 报错，实例不能访问私有变量AttributeError: 'JustCounter' object has no attribute '__secretCount' 类的专有方法： **init 😗* 构造函数，在生成对象时调用 **del 😗* 析构函数，释放对象时使用 **repr 😗* 打印，转换 **setitem 😗* 按照索引赋值 **getitem😗* 按照索引获取值 **len😗* 获得长度 **cmp😗* 比较运算 **call😗* 函数调用 **add😗* 加运算 **sub😗* 减运算 **mul😗* 乘运算 **div😗* 除运算 **mod😗* 求余运算 **pow😗* 乘方 运算符重载 Python 同样支持运算符重载，我么可以对类的专有方法进行重载，实例如下： #!/usr/bin/python3class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return 'Vector (%d, %d)' % (self.a, self.b) def __add__(self,other): return Vector(self.a + other.a, self.b + other.b)v1 = Vector(2,10)v2 = Vector(5,-2)print (v1 + v2) 以上代码执行结果如下所示: Vector(7,8) 标准库概览 操作系统接口 os 模块提供了不少与操作系统相关联的函数。 &gt;&gt;&gt; import os&gt;&gt;&gt; os.getcwd() # 返回当前的工作目录'C:\\Python34'&gt;&gt;&gt; os.chdir('/server/accesslogs') # 修改当前的工作目录&gt;&gt;&gt; os.system('mkdir today') # 执行系统命令 mkdir0 文件通配符 glob 模块提供了一个函数用于从目录通配符搜索中生成文件列表: &gt;&gt;&gt; import glob&gt;&gt;&gt; glob.glob('*.py')['primes.py', 'random.py', 'quote.py'] 命令行参数 通用工具脚本经常调用命令行参数。这些命令行参数以链表形式存储于 sys 模块的 argv 变量。例如在命令行中执行 python demo.py one two three 后可以得到以下输出结果: &gt;&gt;&gt; import sys&gt;&gt;&gt; print(sys.argv)['demo.py', 'one', 'two', 'three'] 错误输出重定向和程序终止 sys 还有 stdin，stdout 和 stderr 属性，即使在 stdout 被重定向时，后者也可以用于显示警告和错误信息。 &gt;&gt;&gt; sys.stderr.write('Warning, log file not found starting a new one\n')Warning, log file not found starting a new one 字符串正则匹配 re 模块为高级字符串处理提供了正则表达式工具。对于复杂的匹配和处理，正则表达式提供了简洁、优化的解决方案: &gt;&gt;&gt; import re&gt;&gt;&gt; re.findall(r'\bf[a-z]*', 'which foot or hand fell fastest')['foot', 'fell', 'fastest']&gt;&gt;&gt; re.sub(r'(\b[a-z]+) \1', r'\1', 'cat in the the hat')'cat in the hat' 数学 math 模块为浮点运算提供了对底层 C 函数库的访问: &gt;&gt;&gt; import math&gt;&gt;&gt; math.cos(math.pi / 4)0.70710678118654757&gt;&gt;&gt; math.log(1024, 2)10.0 资料 https://github.com/vinta/awesome-python - 资源大全 https://github.com/jobbole/awesome-python-cn - 资源大全 https://github.com/scrapy/scrapy - python 爬虫框架 https://github.com/faif/python-patterns - python 设计模式 https://github.com/kennethreitz/python-guide - python 最佳实践]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>programming</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 容器之 List]]></title>
    <url>%2Fblog%2F2018%2F06%2F27%2Fjava%2Fjavacore%2Fcontainer%2FJava%E5%AE%B9%E5%99%A8%E4%B9%8BList%2F</url>
    <content type="text"><![CDATA[Java 容器之 List 📓 本文已归档到：「blog」 List 是 Collection 的子接口，其中可以保存各个重复的内容。 List 概述 ArrayList LinkedList 小结 资料 List 概述 List 接口定义： public interface List&lt;E&gt; extends Collection&lt;E&gt; List 主要方法： List 常见子类： ArrayList - 动态数组。 LinkedList - 双链表。 ArrayList ArrayList 要点 ArrayList 是一个数组队列，相当于动态数组。与 Java 中的数组相比，ArrayList 的容量可以动态增长。 ArrayList 定义： public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 从 ArrayList 的定义，不难看出 ArrayList 的一些基本特性： ArrayList 实现 List 接口，能对它进行队列操作。 ArrayList 实现了 RandmoAccess 接口，即提供了随机访问功能。RandmoAccess 是 java 中用来被 List 实现，为 List 提供快速访问功能的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象；这就是快速随机访问。 ArrayList 实现了 Cloneable 接口，即覆盖了函数 clone()，能被克隆。 ArrayList 实现 java.io.Serializable 接口，这意味着 ArrayList 支持序列化，能通过序列化去传输。 ArrayList 是非线程安全的。 ArrayList 原理 1. 概览 ArrayList 包含了两个重要的元素：elementData 和 size。 transient Object[] elementData;private int size; elementData 它保存了添加到 ArrayList 中的元素。这个数组的默认大小为 10。 size 则是动态数组的实际大小。 ArrayList 实现了 RandomAccess 接口，因此支持随机访问。这是理所当然的，因为 ArrayList 是基于数组实现的。 2. 序列化 ArrayList 具有动态扩容特性，因此保存元素的数组不一定都会被使用，那么就没必要全部进行序列化。ArrayList 重写了 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。 3. 扩容 添加元素时使用 ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity &gt;&gt; 1)，也就是旧容量的 1.5 倍。 扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。 public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 4. 删除元素 需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，复制的代价很高。 public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 5. Fail-Fast modCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。 在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 ConcurrentModificationException。 private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125; LinkedList LinkedList 要点 LinkedList 基于双向链表实现。由于是双向链表，那么它的顺序访问会非常高效，而随机访问效率比较低。 LinkedList 定义： public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable 从 LinkedList 的定义，可以得出 LinkedList 的一些基本特性： LinkedList 是一个继承于 AbstractSequentialList 的双向链表。它也可以被当作堆栈、队列或双端队列进行操作。 LinkedList 实现 List 接口，能对它进行队列操作。 LinkedList 实现 Deque 接口，即能将 LinkedList 当作双端队列使用。 LinkedList 实现了 Cloneable 接口，即覆盖了函数 clone()，能被克隆。 LinkedList 实现 java.io.Serializable 接口，这意味着 LinkedList 支持序列化。 LinkedList 是非线程安全的。 LinkedList 原理 1. 概览 LinkedList 包含两个重要的成员：first 和 last。 // 链表长度transient int size = 0;// 链表头节点transient Node&lt;E&gt; first;// 链表尾节点transient Node&lt;E&gt; last; size 表示双链表中节点的个数，初始为 0。 first 和 last 分别是双链表的头节点和尾节点。 Node 则表示链表中的实例。Node 中包含三个元素：prev, next, item。其中，prev 是该节点的上一个节点，next 是该节点的下一个节点，item 是该节点所包含的值。 private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; ...&#125; 小结 ArrayList 基于动态数组实现，LinkedList 基于双向链表实现； ArrayList 支持随机访问，所以访问速度更快；LinkedList 在任意位置添加删除元素更快； ArrayList 基于数组实现，存在容量限制，当元素数超过最大容量时，会自动扩容；LinkedList 基于双链表实现，不存在容量限制； ArrayList 和 LinkedList 都不是线程安全的。 资料 Java 编程思想（第 4 版） https://www.cnblogs.com/skywang12345/p/3308556.html http://www.cnblogs.com/skywang12345/p/3308807.html]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>container</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 复制]]></title>
    <url>%2Fblog%2F2018%2F06%2F19%2Fdatabase%2Fnosql%2Fredis%2FRedis%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Redis 复制 Redis 复制要点 当主服务器不进行持久化时复制的安全性 Redis 主从复制是如何工作的 部分重新同步 无磁盘复制 配置 只读从服务器 设置从服务器到主服务器验证 限制有 N 个以上从服务器才允许写入 资料 Redis 复制要点 Redis 使用异步复制。但从 Redis 2.8 开始，从服务器会周期性的应答从复制流中处理的数据量。 一个主服务器（master）可以有多个从服务器（slave）。 从服务器也可以接受其他从服务器的连接。除了多个从服务器连接到一个主服务器之外，多个从服务器也可以连接到一个从服务器上，形成一个图状结构 Redis 主从复制不阻塞主服务器端。也就是说当若干个从服务器在进行初始同步时，主服务器仍然可以处理请求。 主从复制也不阻塞从服务器端。当从服务器进行初始同步时，它使用旧版本的数据来应对查询请求，假设你在 redis.conf 配置文件是这么配置的。否则的话，你可以配置当复制流关闭时让从服务器给客户端返回一个错误。但是，当初始同步完成后，需要删除旧的数据集和加载新的数据集，在这个短暂的时间内，从服务器会阻塞连接进来的请求。 主从复制可以用来增强扩展性，使用多个从服务器来处理只读的请求（比如，繁重的排序操作可以放到从服务器去做），也可以简单的用来做数据冗余。 使用主从复制可以为主服务器免除把数据写入磁盘的消耗：在主服务器的 redis.conf 文件中配置“避免保存”（注释掉所有“保存“命令），然后连接一个配置为“进行保存”的从服务器即可。但是这个配置要确保主服务器不会自动重启（要获得更多信息请阅读下一段） 当主服务器不进行持久化时复制的安全性 在使用 Redis 复制功能时的设置中，强烈建议在 master 和在 slave 中启用持久化。当不可能启用时，例如由于非常慢的磁盘性能而导致的延迟问题，应该配置实例来避免重置后自动重启。 关闭了持久化并配置了自动重启的 master 是危险的，原因如下： 我们设置节点 A 为 master 并关闭它的持久化设置，节点 B 和 C 从 节点 A 复制数据。 节点 A 崩溃，但是他有一些自动重启的系统可以重启进程。但是由于持久化被关闭了，节点重启后其数据集合为空。 节点 B 和 节点 C 会从节点 A 复制数据，但是节点 A 的数据集是空的，因此复制的结果是它们会销毁自身之前的数据副本。 当在高可用系统中使用 Redis Sentinel，关闭了主服务器的持久化，并且允许自动重启，这种情况是很危险的。比如主服务器可能在很短的时间就完成了重启，以至于 Sentinel 都无法检测到这次失败，那么上面说的这种失败的情况就发生了。 如果数据比较重要，并且在使用主从复制时关闭了主服务器持久化功能的场景中，都应该禁止实例自动重启。 Redis 主从复制是如何工作的 如果你设置了一个从服务器，在连接时它发送了一个 SYNC 命令，不管它是第一次连接还是再次连接都没有关系。 然后主服务器开始后台存储，并且开始缓存新连接进来的修改数据的命令。当后台存储完成后，主服务器把数据文件发送到从服务器，从服务器将其保存在磁盘上，然后加载到内存中。然后主服务器把刚才缓存的命令发送到从服务器。这是作为命令流来完成的，并且和 Redis 协议本身格式相同。 你可以通过 telnet 自己尝试一下。在 Redis 服务器工作时连接到 Redis 端口，发送 SYNC 命令，会看到一个批量的传输，并且主服务器接收的每一个命令都会通过 telnet 会话重新发送一遍。 当主从服务器之间的连接由于某些原因断开时，从服务器可以自动进行重连接。当有多个从服务器同时请求同步时，主服务器只进行一个后台存储。 当连接断开又重新连上之后，一般都会进行一个完整的重新同步，但是从 Redis2.8 开始，只重新同步一部分也可以。 部分重新同步 从 Redis 2.8 开始，如果遭遇连接断开，重新连接之后可以从中断处继续进行复制，而不必重新同步。 它的工作原理是这样，主服务器端为复制流维护一个内存缓冲区（in-memory backlog）。主从服务器都维护一个复制偏移量（replication offset）和 master run id ，当连接断开时，从服务器会重新连接上主服务器，然后请求继续复制，假如主从服务器的两个 master run id 相同，并且指定的偏移量在内存缓冲区中还有效，复制就会从上次中断的点开始继续。如果其中一个条件不满足，就会进行完全重新同步（在 2.8 版本之前就是直接进行完全重新同步）。因为主运行 id 不保存在磁盘中，如果从服务器重启了的话就只能进行完全同步了。 部分重新同步这个新特性内部使用 PSYNC 命令，旧的实现中使用 SYNC 命令。Redis2.8 版本可以检测出它所连接的服务器是否支持 PSYNC 命令，不支持的话使用 SYNC 命令。 无磁盘复制 通常来讲，一个完全重新同步需要在磁盘上创建一个 RDB 文件，然后加载这个文件以便为从服务器发送数据。 如果使用比较低速的磁盘，这种操作会给主服务器带来较大的压力。Redis 从 2.8.18 版本开始尝试支持无磁盘的复制。使用这种设置时，子进程直接将 RDB 通过网络发送给从服务器，不使用磁盘作为中间存储。 这一特性目前只是实验性的。 配置 主从复制的配置十分简单：把下面这行加入到从服务器的配置文件中即可。 slaveof 192.168.1.1 6379 当然你需要把其中的 192.168.1.1 6379 替换为你自己的主服务器 IP（或者主机名 hostname）和端口。另外你可以调用 SLAVEOF 命令，主服务器就会开始与从服务器同步。 关于部分重新同步，还有一些针对复制内存缓冲区的优化参数。查看 Redis 介质中的 Redis.conf 示例获得更多信息。 使用 repl-diskless-sync 配置参数来启动无磁盘复制。使用 repl-diskless-sync-delay 参数来配置传输开始的延迟时间，以便等待更多的从服务器连接上来。查看 Redis 介质中的 Redis.conf 示例获得更多信息。 只读从服务器 从 Redis 2.6 开始，从服务器支持只读模式，并且是默认模式。这个行为是由 Redis.conf 文件中的 slave-read-only 参数控制的，可以在运行中通过 CONFIG SET 来启用或者禁用。 只读的从服务器会拒绝所有写命令，所以对从服务器不会有误写操作。但这不表示可以把从服务器实例暴露在危险的网络环境下，因为像 DEBUG 或者 CONFIG 这样的管理命令还是可以运行的。不过你可以通过使用 rename-command 命令来为这些命令改名来增加安全性。 你可能想知道为什么只读限制还可以被还原，使得从服务器还可以进行写操作。虽然当主从服务器进行重新同步或者从服务器重启后，这些写操作都会失效，还是有一些使用场景会想从服务器中写入临时数据的，但将来这个特性可能会被去掉。 设置从服务器到主服务器验证 如果主服务器设置了密码，配置从服务器在所有同步中使用这个密码十分简单。 对于运行中的实例，使用 redis-cli 并输入： &lt;/pre&gt;&lt;pre&gt;&lt;code&gt;config set masterauth &lt;password&gt;&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt; 要使配置永久生效，把如下命令加入到配置文件中： &lt;/pre&gt;&lt;pre&gt;&lt;code&gt;masterauth &lt;password&gt;&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt; 限制有 N 个以上从服务器才允许写入 从 Redis 2.8 版本开始，可以配置主服务器连接 N 个以上从服务器才允许对主服务器进行写操作。但是，因为 Redis 使用的是异步主从复制，没办法确保从服务器确实收到了要写入的数据，所以还是有一定的数据丢失的可能性。 这一特性的工作原理如下： 从服务器每秒钟 ping 一次主服务器，确认处理的复制流数量。 主服务器记住每个从服务器最近一次 ping 的时间。 用户可以配置最少要有 N 个服务器有小于 M 秒的确认延迟。 如果有 N 个以上从服务器，并且确认延迟小于 M 秒，主服务器接受写操作。 你可以把这看做是 CAP 原则（一致性，可用性，分区容错性）不严格的一致性实现，虽然不能百分百确保一致性，但至少保证了丢失的数据不会超过 M 秒内的数据量。 如果条件不满足，主服务器会拒绝写操作并返回一个错误。 min-slaves-to-write（最小从服务器数） min-slaves-max-lag（从服务器最大确认延迟） 查看 Redis 介质中的 Redis.conf 示例获得更多信息。 资料 http://ifeve.com/redis-replication/]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>nosql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 哨兵]]></title>
    <url>%2Fblog%2F2018%2F06%2F19%2Fdatabase%2Fnosql%2Fredis%2FRedis%E5%93%A8%E5%85%B5%2F</url>
    <content type="text"><![CDATA[Redis 哨兵 QuickStart 获取 Sentinel 运行 Sentinel 部署之前了解关于 Sentinel 的基本东西 Sentinel 配置 其他的 Sentinels 选项 Sentinel 部署示例 快速教程 询问 Sentinel 关于主节点的状态 获取当前主节点的地址 故障转移测试 Sentinel API Sentinel 命令 运行时重新配置 Sentinel 添加和移除 sentinels 移除旧的主节点或不可达的从节点 发布/订阅消息 BUSY 状态的处理 更高级的概念 SDOWN 和 ODOWN 失败状态 Sentinels 和从节点自动发现 故障转移之外重新配置 从节点选举和优先级 算法和内部结构 Quorum 配置 epochs 配置传播 Sentinel 持久化状态 TILT 模式 Redis Sentinel 为 Redis 提供了高可用解决方案。实际上这意味着使用 Sentinel 可以部署一套 Redis，在没有人为干预的情况下去应付各种各样的失败事件。 Redis Sentinel 同时提供了一些其他的功能，例如：监控、通知、并为 client 提供配置。 下面是 Sentinel 的功能列表： 监控（Monitoring）：Sentinel 不断的去检查你的主从实例是否按照预期在工作。 通知（Notification）：Sentinel 可以通过一个 api 来通知系统管理员或者另外的应用程序，被监控的 Redis 实例有一些问题。 自动故障转移（Automatic failover）：如果一个主节点没有按照预期工作，Sentinel 会开始故障转移过程，把一个从节点提升为主节点，并重新配置其他的从节点使用新的主节点，使用 Redis 服务的应用程序在连接的时候也被通知新的地址。 配置提供者（Configuration provider）：Sentinel 给客户端的服务发现提供来源：对于一个给定的服务，客户端连接到 Sentinels 来寻找当前主节点的地址。当故障转移发生的时候，Sentinels 将报告新的地址。 Sentinel 的分布式特性 Redis Sentinel 是一个分布式系统，Sentinel 运行在有许多 Sentinel 进程互相合作的环境下，它本身就是这样被设计的。有许多 Sentinel 进程互相合作的优点如下： 当多个 Sentinel 同意一个 master 不再可用的时候，就执行故障检测。这明显降低了错误概率。 即使并非全部的 Sentinel 都在工作，Sentinel 也可以正常工作，这种特性，让系统非常的健康。 所有的 Sentinels，Redis 实例，连接到 Sentinel 和 Redis 的客户端，本身就是一个有着特殊性质的大型分布式系统。在这篇文章中，我将逐步地介绍这些概念，最开始是一些基本的信息来理解 Sentinel 的基本属性，后面是更复杂的信息来理解 Sentinel 是怎么工作的。 QuickStart 获取 Sentinel 当前版本的 Sentinel 的被称为 Sentinel 2 。它使用更强更简单的预测算法重写了 Sentinel 的初始化实现（文章的后面将会解释）。 Redis Sentinel 的一个稳定版本是随着 Redis2.8 和 3.0 一起的。这两个是 Redis 最新的稳定版。 新的进展在 unstable 分支下进行，一旦新的特性是稳定的，就会被合并到 2.8 和 3.0 分支。 和 Redis 2.6 一起的 Redis Sentinel 版本 1，是过时的。我们不该使用它。 运行 Sentinel 如果你使用 redis-sentinel 可执行文件，你可以使用下面的命令来运行 Sentinel： redis-sentinel /path/to/sentinel.conf 另外，你可以直接使用 redis-server 并以 Sentinel 模式来启动： redis-server /path/to/sentinel.conf --sentinel 两种方式是一样的。 不管咋样，使用一个配置文件来运行 Sentinel 是必须的，这个文件被系统使用来存储当前状态，如果重启，这些状态会被重新载入。如果没有配置文件或者配置文件的路径不对，Sentinel 将会拒绝启动。 默认情况下，Sentinels 监听 TCP 端口 26379，所以为了让 Sentinels 运行，你的机器的 26379 端口必须是打开的，用来接收其他 Sentinel 实例的连接，否则，Sentinels 不能互相交流，也不知道该干什么，也不会执行故障转移。 部署之前了解关于 Sentinel 的基本东西 一个健康的集群部署，至少需要三个 Sentinel 实例 三个 Sentinel 实例应该被放在失败独立的电脑上或虚拟机中，比如说不同的物理机或者在不同的可用区域上执行的虚拟机。 Sentinel + Redis 分布式系统在失败期间并不确保写入请求被保存，因为 Redis 使用异步拷贝。可是有很多部署 Sentinel 的 方式来让窗口把丢失写入限制在特定的时刻，当然也有另外的不安全的方式来部署。 如果你在开发环境中没有经常测试，或者在生产环境中也没有，那就没有高可用的设置是安全的。你或许有一个错误的配置而仅仅只是在很晚的时候才出现（凌晨 3 点你的主节点宕掉了）。 Sentinel，Docker ，其他的网络地址转换表，端口映射 使用应该很小心的使用：Docker 执行端口重新映射，破坏 Sentinel 自动发现另外的 Sentinel 进程和一个主节点的从节点列表。在文章的稍后部分查看更过关于 Sentinel 和 Docker 的信息。 Sentinel 配置 Redis 源码中包含一个名为 sentinel.conf 的文件，是一个你可以用来配置 Sentinel 的示例配置文件。一个典型的最小配置文件像下面这样： sentinel monitor mymaster 127.0.0.1 6379 2sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1sentinel monitor resque 192.168.1.3 6380 4sentinel down-after-milliseconds resque 10000sentinel failover-timeout resque 180000sentinel parallel-syncs resque 5 你仅仅只需要指定要监控的主节点，并给每个单独的主节点一个不同的名称。不需要指定从节点，从节点会被自动发现。Sentinel 将会根据从节点额外的信息自动更新配置（为了在重启时保留信息）。在故障转移中每当一个从节点被提升为主节点或者当一个新的 Sentinel 被发现的时候，配置信息也被重新写入。 示例配置在上面，监控两个 Redis 实例集合，每个集合由一个主节点和不明确数量的从节点组成。一个集合叫做 mymaster，另外一个叫做 resque。 sentinel monitor 参数的意思在下面 sentinel monitor &lt;master-group-name&gt; &lt;ip&gt; &lt;port&gt; &lt;quorum&gt; 为了更加清晰明了，让我们一行一行来检查配置选项的意思： 第一行用来告诉 Redis 监控一个叫做 mymaster 的主节点，地址是 127.0.0.1 端口号是 6379，并且有 2 个仲裁机器。所有的意思都很明显，但是除了这个 quorum 参数： quorum 是 需要同意主节点不可用的 Sentinels 的数量 然而 quorum 仅仅只是用来检测失败。为了实际的执行故障转移，Sentinels 中的一个需要被选定为 leader 并且被授权进行操作，这仅仅发生在大多数 Sentinels 进行投票的时候。 比如如果你有五个 Sentinel 进程，对于一个主节点 quorum 被设置为 2，下面是发生的事情： 同时有两个 Sentinels 同意主节点不可用，其中的一个将会尝试开始故障转移。 如果至少有三个 Sentinels 是可用的，故障转移将会被授权并且开始。 实际中，这意味着在失败时，如果大多数的 Sentinel 进程没有同意，Sentinel 永远不会开始故障转移。 其他的 Sentinels 选项 其他的选项几乎都是如下形式： sentinel &lt;option_name&gt; &lt;master_name&gt; &lt;option_value&gt; 用途如下： down-after-milliseconds：当一个实例失去联系（要么不回复我们的请求，要么回复一个错误）超过了这个时间（毫秒为单位），Sentinel 就开始认为这个实例挂掉了。 parallel-syncs：设置的从节点的数量，这些从节点在一次故障转移过后可以使用新的主节点进行重新配置。数量越少，完成故障转移过程将花费更多的时间，如果从节点为旧的数据提供服务，你或许不想所有的从节点使用主节点进行重新同步。复制进程对于从节点来说大部分是非阻塞的，还是有一个时刻它会停下来去从主节点加载数据。你或许想确保一次只有一个从节点是不可达的，可以通过设置这个选项的值为 1 来完成。 别的选项在文章的其他部分进行描述。 所有的配置参数都可以在运行时使用 SENTINEL SET 命令进行更改，查看 Reconfiguring Sentinel at runtime 章节获取更多内容。 Sentinel 部署示例 现在你已经知道了 Sentinel 的基本信息，你或许想知道哪里放置你的 Sentinel 进程，需要多少个 Sentinel 进程等等。这个章节给出了几个部署的例子。 为了以图形（graphical ）格式展示配置示例，我们使用 ASCII 艺术。下面是不同的符号的意思： +--------------------+| 这是一个独立电脑 || 或者VM。我们称它为 || “box” |+--------------------+ 我们把我们想要运行的东西写到 boxes 里： +-------------------+| Redis master M1 || Redis Sentinel S1 |+-------------------+ 不同的 box 之间通过一条线连接，表示他们之间可以互相交流： +-------------+ +-------------+| Sentinel S1 |---------------| Sentinel S2 |+-------------+ +-------------+ 中断的线条表示不同的网络分区： +-------------+ +-------------+| Sentinel S1 |------ // ------| Sentinel S2 |+-------------+ +-------------+ 同时还要注意： 主节点称为 M1，M2，M3，…，Mn。 从节点称为 R1，R2，R3，…，Rn。 Sentinels 称为 S1，S2，S3，…，Sn。 客户端称为 C1，C2，C3，…，Cn。 当一个实例因为 Sentinels 的行为转换角色，我们把它放在方括号里，所以[M1]表示一个实例现在是主节点。 注意永远不要设置只有两个 Sentinels，因为开始一个故障转移，Sentinels 总是需要和大多数 Sentinels 交流。 示例 1：仅仅只有两个 Sentinels，永远不要这么做 +----+ +----+| M1 |---------| R1 || S1 | | S2 |+----+ +----+Configuration: quorum = 1 在这个设置中，如果 M1 宕掉了，R1 将会被提升至主节点，因为两个 Sentinels 将会达成一致（显然把 quorum 设置为 1），并且授权开始一个故障转移因为大多数是两个。显然，表面上可以工作，但是请检查下一个点来看看为什么这种设置是不可以的。 如果 M1 的 box 停止工作，M1 也会停止。运行在另外一个 box 中的 S2 将不会被授权进行故障转移，所以系统将不可用。 注意，需要大多数是为了应付不同的故障，最新的配置稍后会传播给所有的 Sentinels。同时注意在上述设置中单独一边的故障转移能力，没有任何协议，将是非常危险的： +----+ +------+| M1 |----//-----| [M1] || S1 | | S2 |+----+ +------+ 在上面的配置中，我们完美对称地创建了两个主节点（假设 S2 在没有授权的情况下可以进行故障转移），客户端或许会不确定写往哪一边，并且没有办法理解当分区治愈时候哪边的配置是正确的。 所以请至少部署三个 Sentinels 在三个不同的 box 当中。 示例 2：三个 box 的基本设置 这是一个非常简单的设置，拥有更加安全的优点。它是基于三个 boxes 的，每个 box 运行一个 Redis 进程和 Sentinel 进程。 +----+ | M1 | | S1 | +----+ |+----+ | +----+| R2 |----+----| R3 || S2 | | S3 |+----+ +----+Configuration: quorum = 2 如果 M1 挂掉，S2 和 S3 将认同这次失败，并且能授权开始一次故障转移，这样使客户端可以继续使用。 在每一个 Sentinel 设置中，Redis 是异步复制的，总是有丢失一些写入数据的危险，因为当一个从节点被提升为主节点的时候一个写入确认还没有到达。然而在上面的设置中，还有一种更加危险的情况，由于客户端和一个老的主节点在一个网络分区中，就像下面这样： +----+ | M1 | | S1 | &lt;- C1 (writes will be lost) +----+ | / /+------+ | +----+| [M2] |----+----| R3 || S2 | | S3 |+------+ +----+ 在这种情况下，网络分区把旧的主节点[M1]给孤立了，所以从节点 R2 被提升为主节点。然而，像客户端 C1，和旧的主节点在同一个网络分区中，或许继续像旧的主节点写入数据。当分区治愈，这些数据将永久丢失，这个旧得主节点将会被重新配置，作为新的主节点下的一个从节点，并丢弃它自己的数据。 可以使用下面的 Redis 复制特性减轻这个问题，如果一个主节点发现它不再能够把它的写入请求发送给指定数量的从节点，它就停止接受写入请求。 min-slaves-to-write 1min-slaves-max-lag 10 当上面的配置应用于一个 Redis 实例。Redis 发现它不能写入至少一个 1 从节点，作为主节点的 Reids 将会停止接受写入请求。由于复制是异步，不能写入也意味着从节点也是断开的，或者超过了指定的 max-lag 秒数没有发送异步回应。 在上面的示例中，使用这个配置的旧的主节点 M1，在 10 秒过后就不可用了。当分区治愈，Sentinel 配置将会统一为新的，客户端 C1 将获取到一个有效的配置并且继续。 然而天下没有免费的午餐，在这种改进下，如果两个从节点挂掉了，主节点将会停止接收写入请求，这就是一个权衡。 示例 3：Sentinel 在客户端所在的 box 中 有时候，我们只有两个 Redis box 是可用的，一个给主节点，一个给从节点。在那种情况下，示例 2 中的配置是不可行的，我们可以采取下面的方法，Sentinels 被放置在客户端所在的地方： +----+ +----+ | M1 |----+----| R1 | | S1 | | | S2 | +----+ | +----+ | +------------+------------+ | | | | | | +----+ +----+ +----+ | C1 | | C2 | | C3 | | S1 | | S2 | | S3 | +----+ +----+ +----+ Configuration: quorum = 2在这种设置下，Sentinels的视角和客户端是 一样的：如 在这种设置下，Sentinels 的视角和客户端是 一样的：如果大部分的客户端认为一个主节点是可用的，它就是可用的。这里的 C1，C2，C3 是一般的客户端， 并不意味着 C1 是连接到 Redis 的单个客户端，它更像一个应用服务器，一个 Redis app，或者类似的东西。 如果 M1 和 S1 所在的 box 挂掉了，故障转移将会进行，但是很明显的看到不同的网络分区将导致不同的行为。比如说，如果客户端和 Redis 服务断开连接，Sentinel 将不会被设置，因为 Redis 的主节点和从节点都是不可用的。 注意如果 C3 和 M1 在一个分区，我们有了一个和示例 2 中描述的类似的问题，不同的是，这里我们没有办法打破对称，因为只有一个主节点和从节点，所以主节点不会停止接收请求。 所以这是一个有效的设置，但是实例 2 中的设置更有优势，比如 Redis 高可用系统，Redis 运行在同一个 box 中，更容易被管理，并且可以限制在小部分的分区中主节点接收写入请求的时间。 示例 4：Sentinel 客户端 这一边少于三个客户端 示例 3 描述的设置中，如果客户端这一边的 box 少于不够三个，这个 设置就不能使用。在这种情况下，我们需要借助混合设置，像下面这样： +----+ +----+ | M1 |----+----| R1 | | S1 | | | S2 | +----+ | +----+ | +------+-----+ | | | | +----+ +----+ | C1 | | C2 | | S3 | | S4 | +----+ +----+Configuration: quorum = 3 这和示例 3 中的设置非常相似，但是这里我们在可用的四个 box 中运行了四个 Sentinel。如果主节点 M1 变成不可用节点，其他三个 Sentinel 将执行故障转移。 理论上，当移除 S2 和 S4 正在运行的 box，这个设置可以工作，把 quorum 设置为 2。然而，在应用层没有高可用的系统，想在 Redis 这一边得到高可用是不太可能的。 Sentinel，Docker,NAT 和可能的问题 Docker 使用被称为端口映射的技术：与一个程序认为他使用的端口相比，运行在 Docker 容器里面的程序可能被暴露在不同的端口上。为了运行多个容器在相同的服务器上同时使用同一个端口，这是非常有用的。 Docker 不是唯一会发生这件事情的软件系统，也有其他的网络地址转换设置导致端口是被重映射，并且有时候没有端口，只有 IP 地址。 端口和地址重映射在两个方面制造了与 Sentinel 有关的问题： Sentinel 的自动发现服务将停止工作，因为它使基于每个 Sentinel 往它监听的端口和 IP 地址广播 hello 消息来实现的。但是 Sentinels 没有办法来理解端口和 IP 地址被重映射了，所以他会宣布它和其他的 Sentinels 的连接是不正常的。 在一个主节点的 INFO 输出中，从节点 被列出来也是类似的方式：主节点检查远端对等的 TCP 连接来发现地址，在握手过程中，从节点自己广告他的端口，然而由于相同的原因，端口或许是错误的。 因为 Sentinels 自动发现从节点使用主节点的 INFO 输出信息，发现的从节点是不可达的，并且 Sentinel 将永远不会开始故障转移，因为从系统的观点来看，没有好的从节点，所以目前没有方式监控使用 Docker 部署的主节点和从节点实例，除非你通知 Docker 以 1:1 映射端口。 对于第一个问题，万一你想使用 Docker 运行一堆 Sentinel 实例，你可以使用下面的两个 Sentinel 配置，为了强迫 Sentinel 宣布一个指定的端口和 IP： sentinel announce-ip &lt;ip&gt;sentinel announce-port &lt;port&gt; 注意，Docker 可以运行 host networking 模式。这就不会有问题因为端口不会被重新映射。 快速教程 在文章接下来的部分中，所有的说明都是关于 Sentinel API，配置和语义。对于想尽快上手的人，这部分的教程展示了三个 Sentinel 怎么配置和交互。 现在我假设三个实例分别在端口 5000、5001、5002 上。我也假设你在 6379 上有一个主节点 Redis 实例，6380 上有一个从节点实例。在本教程中我们将使用 IPV4 回调地址 127.0.0.1，假设你在你的电脑上运行了 模拟环境。 三个 Sentinel 配置文件应该看起来像下面这样： port 5000sentinel monitor mymaster 127.0.0.1 6379 2sentinel down-after-milliseconds mymaster 5000sentinel failover-timeout mymaster 60000sentinel parallel-syncs mymaster 1 另外的两个配置文件也是相同的，但是使用 5001,5002 作为端口号。 上面的配置中需要注意的一些事情： 主节点集群称为 mymaster，它定义了主节点和它的从节点。因为每个 master set 有一个不同的名称，Sentinel 能同时监控不同的主节点和从节点的集合。 quorum 被设置为 2。 down-after-milliseconds 的值是 5000 毫秒，就是 5 秒钟，所以在这个时间内一旦我们不能收到回复，主节点将发现失败。 一旦你启动了三个 Sentinels，可以看到他们打印的一些信息： +monitor master mymaster 127.0.0.1 637这是一个Sentinel事件，如果你 SUBSCRIBE 了指定名称的事件，你可以收到这种事件通过发布/订阅。 Sentinel 在故障检测和故障转移中生成和打印不同的事件。 询问 Sentinel 关于主节点的状态 Sentinel 开始启动的时候，要做的事情是检查主节点的监控是否正常： $ redis-cli -p 5000127.0.0.1:5000&gt; sentinel master mymaster 1) "name" 2) "mymaster" 3) "ip" 4) "127.0.0.1" 5) "port" 6) "6379" 7) "runid" 8) "953ae6a589449c13ddefaee3538d356d287f509b" 9) "flags"10) "master"11) "link-pending-commands"12) "0"13) "link-refcount"14) "1"15) "last-ping-sent"16) "0"17) "last-ok-ping-reply"18) "735"19) "last-ping-reply"20) "735"21) "down-after-milliseconds"22) "5000"23) "info-refresh"24) "126"25) "role-reported"26) "master"27) "role-reported-time"28) "532439"29) "config-epoch"30) "1"31) "num-slaves"32) "1"33) "num-other-sentinels"34) "2"35) "quorum"36) "2"37) "failover-timeout"38) "60000"39) "parallel-syncs"40) "1" 像你所见的，它打印了主节点的一些信息。有几个是我们特别有兴趣的： num-other-sentinels 是 2，所以我们知道对于这个主节点 Sentinel 已经发现了两个以上的 Sentinels。如果你检查日志，你可以看到+sentinel 事件发生。 flags 是 master。如果主节点挂掉了，我们可以看到 s_down 或者 o_down 标志。 num-slaves 现在是 1，所以 Sentinel 发现有一个从节点。 为了探测关于这个实例更多的信息，你可以尝试下面的两个命令： SENTINEL slaves mymasterSENTINEL sentinels mymaster 第一个将提供关于从节点类似的信息，第二个是关于另外的 Sentinels。 获取当前主节点的地址 Sentinel 也作为一个配置提供者，提供给客户端它们想连接的主节点和从节点的集群。因为可能的故障转移和重配置，客户端不知道一个集群实例内当前的活着的主节点，所以 Sentinel 提供了一个 API： 127.0.0.1:5000&gt; SENTINEL get-master-addr-by-name mymaster1) "127.0.0.1"2) "6379" 故障转移测试 现在我们部署 Sentinel 可以被测试了。我们可以杀死主节点然后查看配置变化。做我们可以做的： redis-cli -p 6379 DEBUG sleep 30 这个命令让我们的主节点变为不可达，睡眠 30 秒，它基本上模拟了主节点挂掉的一些原因。 如果你检查 Sentinel 的日志，你应该能看到许多动作： 每个 Sentinel 发现了主节点挂掉了并有一个+sdown 事件 这个事件稍候升级到+odown，意味着大多数 Sentinel 已经同意了主节点是不可达的。 Sentinels 开始投票一个 Sentinel 开始并尝试故障转移 故障转移开始 如果你重新询问 mymaster 的当前主节点的地址，这次我们会得到一个不同的回复： 127.0.0.1:5000&gt; SENTINEL get-master-addr-by-name mymaster1) "127.0.0.1"2) "6380" 目前为止一切都很顺利，现在你可以创建你自己的 Sentinel 部署或者阅读更多来理解 Sentinel 的命令和内部原理。 Sentinel API Sentinel 提供了一个 API，可以用来检查它的状态，检查主节点和从节点的健康，订阅具体的通知并在运行时改变 Sentinel 的配置。 默认情况下 Sentinel 使用 TCP 端口号 26379。Sentinels 接收使用 Redis 的协议命令，所以你可以使用 redis-cli 或者其他未修改的 Redis 客户端来和 Sentinel 交流。 直接查询一个 Sentinel 来检查所监控的 Redis 实例的状态，看看另外的 Sentinels 所知道是可能的。有两种方式，使用发布/订阅，每当一些事件发生，比如说一次故障转移，或一个实例发生错误等，都可能接收到一个从 Sentinels 推送过来的通知。 Sentinel 命令 下面是可以接收的命令列表，没有覆盖到那些用来改变 Sentinel 配置的命令： PING 这个命令仅仅返回 PONG。 SENTINEL masters 展示监控的主节点和它们的状态列表 SENTINEL master 展示指定的主节点的信息 SENTINEL salves 展示这个主节点的从节点，以及它们的状态 SENTINEL sentinels 展示这个主节点的 sentinel 实例，以及它们的状态 SENTINEL get-master-addr-by-name 返回主节点的 IP 和端口号。如果这个主节点的一次故障转移正在进行，就返回提升的从节点的 IP 和端口号 SENTINEL reset 这个命令将会根据匹配的名称重置主节点，pattern 参数是通配符（glob-style）类型，重置进程清除主节点中之前的所有状态，并且移除主节点发现和关联的从节点和 sentinel。 SENTINEL failover 如果主节点不可达，强制开始故障转移，不需要另外的 Sentinels 同意。 SENTINEL ckquorum 检查当前的 Sentinel 配置对于主节点的故障转移是否能达到仲裁人数，并且大多数是需要的来授权故障转移。这个命令应该在监控系统中使用来检查一个 Sentinel 部署是否正常。 SENTINEL flushconfig 强制 Sentinel 重新写入它的配置到磁盘上，包括当前 Sentinel 状态。通常，每次当它状态里的一些东西改变，Sentinel 就会重写配置信息。然而有时候配置文件会丢失，由于错误的操作、磁盘故障、包升级脚本、或配置管理。在那种情况下，强制 Sentinel 重写它的配置文件是容易的。甚至之前的配置文件完全丢失，这个命令也能很好的工作。 运行时重新配置 Sentinel 从 Redis 2.8.4 开始，Sentinel 提供了一个 API 为了增加、移除或者改变一个给定的主节点的配置。注意如果你有多个 sentinels，为了工作正常，你应该改变所有的 Redis Sentinel 实例。这意味着改变单个 Sentinel 的配置不会把变化发送给在网络中另外的 Sentinels. 下面是 SENTINEL 自命令列表，用来更新一个 Sentinel 实例的配置： SENTINEL MONITOR 这个命令告诉 Sentinel 开始监控一个指定名称、IP、端口号、quorum 的主节点，它和 sentinel.conf 配置文件中的 sentinel monitor 配置指令是完全相同的，不同的是这里不能使用主机名作为 IP，需要提供一个 IPV4 或 IPV6 地址。 SENTINEL REMOVE 用来移除指定的主节点：主节点不再被监控，并且将被从 Sentinel 的内部状态中被完全移除，所以不会被 SENTINEL masters 列出。 SENTINEL SET SET 命令和 Reids 的 CONFIG SET 指令非常相似，被用来改变一个指定主节点的配置参数。多个选项-值可以被指定。所有通过 sentinel.conf 配置的参数可以使用 SET 命令重新配置。 下面是 SENTINEL SET 命令的一个例子，为了修改一个名为 objects-cache 的主节点的 down-after-milliseconds 配置： SENTINEL SET objects-cache-master down-after-milliseconds 1000 正如我们提到的，SENTINEL SET 可以被用来设置所有的在启动配置文件中被设置的参数。而且，还可以仅仅改变主节点的 quorum 配置，而不需要使用 SENTINEL REMOVE 和 SENTINEL MONITOR 来删除或者增加主节点，只需要使用： SENTINEL SET objects-cache-master quorum 5 注意，没有等价的 GET 命令，因为 SENTINEL MASTER 以一种易于解析的格式提供了所有的配置参数。 添加和移除 sentinels 添加一个新的 sentinel 到你的部署中是很容易的一个过程，因为 Sentinel 有自动发现机制。所有的你需要做的事情是开启一个新的 Sentinel 来监控当前的主节点。10 秒过后，Sentinel 将获取到其他的 Sentinels 列表和当前主节点的从节点。 如果你想一次性增加多个 Sentinels，建议你一个接一个的增加，等所有的 Sentinels 已经知道第一个再添加另一个。在添加的新的 Sentinels 过程中错误有可能发生，在这时候保证在一次网络分区内中大部分是可用是很有用的。 在没有网络分区时，通过在 30 秒后增加每个新的节点，这是很容易实现的。 最后，可以使用 SENTINEL MASTER mastername 命令来检查是否全部 Sentinels 都同意了监控主节点的 Sentinels 的总数。 移除一个 Sentinel 稍微复杂一点：Sentinels 永远不会忘记已经看到的 Sentinels，甚至他们在相当长的一段时间内不可达，因为我们不想动态的改变授权一次故障转移和创建新的配置所需要的大多数。在没有网络分区的说话，需要执行下面的步骤来移除一个 Sentinel： 停止你想要移除的 Sentinel 的进程 发送一个 SENTINEL RESET * 命令到其他的 Sentinel 实例，相继的，两次发送到实例之间至少等待 30 秒 检查所有的 Sentinels 赞同的当前存活的 Sentinels 的数量，通过检查每个 SENTINEL MASTER mastername 的输出。 移除旧的主节点或不可达的从节点 Sentinels 永远不会忘记一个主节点的从节点，甚至当他们很长时间都不可达。这是很有用的，因为在一次网络分区或失败事件发生后，Sentinels 应该能正确地重新配置一个返回的从节点。 而且，在故障转移发生之后，被故障转移的主节点实际上被添加为新的主节点的从节点，一旦它可用的时候，这种方式将重新配置来复制新的主节点。 然而有时候你想从 Sentinels 监控的从节点列表中永久的移除一个从节点。 为了做这件事，你需要发送一个 SENTINEL RESET mastername 命令给所有的 Sentinels：它们将在十秒后刷新从节点列表，只添加当前主节点的 INFO 输出中正确的复制列表。 发布/订阅消息 一个客户端能使用一个 Sentinel 作为一个 Redis 兼容的发布/订阅服务器，为了 SUBSCRIBE 或者 PSUBSCRIBE 到指定频道，获取指定事件通知。 频道的名称和事件的名称是一样的。比如说名称为+sdown 的频道将收到所有的关于实例进入 SDOWN 条件的通知。 使用 PSUBSCRIBE * 订阅来获取所有的消息。 下面是一个频道列表，以及使用 API，你可以接收到的消息格式。第一个词是频道/事件名称，剩余部分是数据格式。 注意，指定 instance details 的地方意味着提供了下面的参数用于表示目标实例： &lt;instance-type&gt; &lt;name&gt; &lt;ip&gt; &lt;port&gt; @ &lt;master-name&gt; &lt;master-ip&gt; &lt;master-port&gt; 标识主节点的部分（从@开始到结束）是可选的，只有实例本身不是主节点的时指定。 +reset-master — 主节点被重置。 +slave — 一个新的从节点被发现和关联。 +failover-state-reconf-slaves — 故障转移状态被转换为 reconf-slaves 状态。 +failover-detected — 另一个 Sentinel 开始了故障转移或者其他的外部实体被发现（一个关联的从节点变为主节点）。 +slave-reconf-sent — 为了给新的从节点重新配置，sentinel 中的 leader 发送 SLAVEOF 命令到这个实例。 +slave-reconf-inprog –从节点被重新配置展示一个主节点的从节点，但是同步过程尚未完成。 +slave-reconf-done — 从节点现在和主节点是同步的。 -dup-sentinel –指定的主节点，一个或者多个 sentinels 被 移除，因为是重复的。 +sentinel — 这个主节点的一个新的 sentinel 被发现和关联。 +sdown — 指定的实例现在处于主观下线状态。 -sdown — 指定的实例不再处于主观下线状态。 +odown — 指定的实例现在处于客观下线状态。 -odown — 指定的实例现在不处于客观下线状态。 +new-epoch — 当前时间被更新。 +try-failover — 准备新的故障转移，等待大多数的选举。 +elected-leader — 赢得了选举，开始故障转移。 +failover-state-select-slave — 新的故障转移状态是 select-slave：我们 正在寻找合适提升为主节点的从节点。 no-good-slave — 没有合适进行提升的从节点。一般会在稍后重试，但是这或许会改变并且终止故障转移。 selected-slave — 我们找到了指定的从节点来进行提升。 failover-state-send-slaveof-noone — 我们尝试重新配置这个提升后的主节点，等待它切换。 failover-end-for-timeout — 故障转移由于超时而停止，无论如何从节点最后被配置为复制新的主节点。 failover-end — 故障转移由于成功而停止，所有的从节点被配置为复制新的主节点。 switch-master — 配置改变后，主节点新的 IP 和地址都是指定的。这是大多数外部用户感兴趣的消息。 +tilt — 进入 Tilt 模式。 -tilt — 退出 Tilt 模式。 BUSY 状态的处理 当一个 Lua 脚本的运行时间超过了配置中指定的 Lua 脚本时间限制，Redis 实例将返回 -BUSY 错误。当这个发生的时候，在触发故障转移之前 Redis Sentinel 将尝试发送 SCRIPT KILL 命令，如果脚本是只读的，就会成功。 如果在这个尝试后，实例仍然处于失败情况，它最后会开始故障转移。 从节点优先 Redis 实例有个配置参数叫 slave-priority。这个信息在 Redis 从节点实例的 INFO 输出中展示出来，并且 Sentinel 使用它来选择一个从节点在一次故障转移中： 如果从节点的优先级被设置为 0，这个从节点永远不会被提升为主节点。 Sentinel 首选一个由更低（ lower）优先级的从节点。 比如在当前主节点的同一个数据中心有一个从节点 S1，并且有另外的从节点 S2 在另外的数据中心，可以将 S1 优先级设置为 10，S2 优先级设置为 100，如果主节点挂掉了并且 S1 和 S2 都是可用的，S1 将是首选的。 查看关于从节点选举的更多信息，请查看本文章的 slave selection and priority 章节。 Sentinel 和 Redis 权限 当主节点被配置为从客户端需要密码，作为一个安全措施，从节点也需要知道这个密码为了主节点认证并且创建主-从连接用于异步复制协议。 使用下列的配置选项来实现： requirepass 在主节点中，为了设置认证密码，并且确保实例不会处理来自没有认证的客户端的请求。 masterauth 在从节点中，为了取得主节点的认证，来从主节点正确的复制 数据。 当 Sentinel 使用的时候，没有一个单独的主节点，因为一次故障转移过后，从节点将扮演主节点的角色，并且老的主节点被重新配置作为一个从节点，所以你要做的是在全部的实例中设置上面的选项，包括主节点和从节点。 这通常是一个理智的设置，因为你不想要仅仅在主节点中保护你的数据，在从节点中有同样的数据。 然而，在罕见的情况下，你需要一个从节点是可进入的而不需要认证，你可以设置一个优先级为 0 的从节点来实现，阻止这个从节点被提升为主节点，配置这个从节点的 masterauth 选项，不要使用 requirepass 选项，以便数据可以被读在没有认证的情况下。 Sentinel 客户端实现 Sentinel 需要显式的客户端支持，除非系统配置为执行脚本来执行一个透明的重定向对于所有的主节点实例的请求（虚拟 IP 或类似的系统）。可以参考文档 Sentinel clients guidelines。 更高级的概念 下面的章节是关于 Sentinel 怎么工作的一些细节，没有付诸于实现的想法和算法在文章的最后章节。 SDOWN 和 ODOWN 失败状态 Redis Sentine 有两个不同概念的下线，一个被称为主观下线（Subjectively Down ）条件（SDOWN），是一个本地 Sentinel 实例下线条件。另一个被称为客观下线（Objectively Down ）条件（ODOWN），是当足够的 Sentinels 具有 SDOWN 条件就满足 ODOWN，并且从其他的 Sentinels 使用 SENTINEL is-master-down-by-addr 命令得到反馈。 从一个 Sentinel 的角度来看，满足一个 SDOWN 条件就是在指定的时间内对于 PING 请求不能收到有效的回复，这个时间在配置文件中是 is-master-down-after-milliseconds 参数。 一个 PING 请求可接受的回复是下列之一： 回复+PONG。 回复 -LOADING 错误。 回复-MASTERDOWN 错误。 其他的回复（或根本没有回复）被认为是无效的。注意一个合理的主节点在 INFO 输出中通知他自己是一个从节点被认为是下线的。 注意 SDOWN 需要在配置中整个的时间间隔都没有收到有效的回复，因此对于实例如果时间间隔是 30000 毫秒，并且我们每隔 29 秒收到有效的回复，这个实例就被认为在工作。 SDOWN 还不够触发故障转移：它仅仅意味着一个单独的 Sentinel 相信一个 Redis 实例不可达。要触发故障转移，必须达到 ODOWN 状态。 从 SDOWN 转换到 ODOWN，没有使用强一致性算法，而仅仅是 gossip 的形式：如果一个 Sentinel 在一个给定的时间范围内从足够的 Sentinels 得到一个报告说一个主节点没有在工作，SDOWN 被提升为 ODOWN。如果这个确认稍候消失，这个标识也会清除。 一个更加严格的授权是使用大多数需要为了真正的开始故障转移，但是在达到 ODOWN 状态之前不会触发故障转移。 ODOWN 条件只适用于主节点。对于其他类型的实例，Sentinel 不需要采取行动，所以对于从节点和其他的 sentinels 来说 ODOWN 状态永远不可能达到，而仅仅只有 SDOWN 状态。 然而 SDOWN 也有语义的影响，比如一个从节点在 SDOWN 状态不会被选举来提升来执行一个故障转移。 Sentinels 和从节点自动发现 Sentinels 和其他的 Sentinels 保持连接为了互相之间检查是否可达和交换消息。然而你不需要在每个运行的 Sentinel 实例中配置其他的 Sentinel 地址列表，Sentinel 使用 Redis 实例的发布/订阅能力来发现其他的监控相同的主节点和从节点的 Sentinels。 通过往名称为sentinel:hello 的通道发送 hello 消息（hello messages）来实现这个特性。 同样的，你不需要配置一个主节点关联的从节点的列表，Sentinel 也会自动发现这个列表通过问询 Redis： 每隔两秒，每个 Sentinel 向每个监控的主节点和从节点的发布/订阅通道sentinel:hello 来公布一个消息，宣布它自己的 IP，端口，id。 每个 Sentinel 都订阅每个主节点和从节点的发布/订阅通道sentinel:hello，寻找未知的 sentinels。当新的 sentinels 被检测到，他们增加这个主节点的 sentinels。 Hello 消息也包含主节点的全部配置信息，如果接收的 Sentinel 有一个更旧的配置，它会立即更新它的配置。 在增加一个主节点的新的 sentinel 之前，Sentinel 总是要检查是否已经有一个有相同的 id、地址的 sentinel。在这种情况下，所有匹配的 sentinels 被移除，新的被增加。 故障转移之外重新配置 即使没有故障转移，Sentinels 将尝试设置当前的配置到监控的实例上面。 特别的： 从节点声称为主节点，将被作为从节点配置来复制当前的主节点。 从节点连接了一个错误的主节点，也会被重新配置来复制正确的主节点。 Sentinels 重新配置从节点，错误的配置在一段时间内应该被观察到，比在广播新的配置的时候要好得多。 这个阻止了有一个过时配置（比如说从一个分区中重新加入）的 Sentinels 在收到更新之前去交换从节点的配置。 同样注意： 主节点的故障转移被重新配置作为从节点当他们返回可用的时候 在一个网络分区中，从节点一旦可达，被重新配置。 本章最重要的教训就是：Sentinels 是每个进程总是尝试去把最后的配置施加到监控的实例上的一个系统。 从节点选举和优先级 当一个 Sentinel 实例准备执行故障转移，因为主节点在 ODOWN 状态下并且 Sentinel 从大多数已知的 Sentinel 实例中收到了授权开始故障转移，一个合适的从节点要被选举出来。 从节点选举过程评估从节点的下列信息： 与主节点断开的时间 从节点优先级 复制偏移处理 运行 ID 一个从节点被发现从主节点断开超过主节点配置超时（down-after-milliseconds 选项）时间十倍以上，加上从正在执行故障转移的 Sentinel 的角度看主节点不可用的时间，将被认为是不合适的并且会被跳过。 在更严格的条件下，一个从节点的 INFO 输出建议了从主节点断开超过： (down-after-milliseconds \* 10) + milliseconds_since_master_is_in_SDOWN_state 被认为是不可靠的并且会被无视。 从节点选举只会考虑通过了上述测试的从节点，并根据上面的条件进行排序，以下列顺序： 根据 Redis 实例中的 redis.conf 文件中配置的 slave-priority 进行排序，更低的优先级会被优先。 如果优先级相同，检查复制偏移处理，从主节点收到更加新的数据的从节点会被选择。 如果多个从节点有相同的优先级和数据偏移，执行进一步检查，选择有着更小运行 ID 的从节点。有一个更小的 ID 并不是具有正真的优点，但是对于从节点选举来说更确定，而不是随机选择一个从节点。 如果有机器是首选，Redis 主节点、从节点必须被配置一个 slave-priority。否则，所有的实例都有一个默认的 ID。 一个 Redis 实例可以被配置指定 slave-priority 为 0 为了永远不被 Sentinels 选择为新的主节点。然而一个这样配置的从节点会被 Sentinels 重新配置，为了在一次故障转移后复制新的主节点，唯一不同的是，它永远不会成为主节点。 算法和内部结构 下面的章节，我们将会探索 Sentinel 特性的细节。对于使用者来说，并不需要知道全部的细节，但是一个更深入的理解可能会帮助部署和操作 Sentinel 以一个更加有效的方式。 Quorum 前面的章节展示了每个被 Sentinel 监控的主节点和一个配置的 quorum 相关联。它指定了需要同意主节点是不可达或者错误的 Sentinel 进程的数量为了触发一次故障转移。 可是，在故障转移触发后，为了真正地执行故障转移，至少大多数的 Sentinels 必须授权一个 Sentinel 开始故障转移。当只有小部分的 Sentinels 存在的一个网络分区中，故障转移永远不会执行。 我们尝试让这件事更加清晰： Quorum：为了把一个主节点标记成 ODOWN，需要的 Sentinel 进程数量来发现错误条件。 ODOWN 状态触发故障转移。 一旦故障转移被触发，Sentinel 尝试向大多数的 Sentinels 请求授权。 不同之处看起来很微妙，但是实际上很简单地理解和使用。如果你又 5 个 Sentinel 实例，quorum 被设置为 2，一旦 2 个 Sentinel 认为主节点不可达，故障转移就会被触发。然而 2 个 Sentinels 中的一个得到 3 个 Sentinels 的授权才开始故障转移。 把 quorum 设置为 5，必须所有的 Sentinels 同意主节点失败，并为了开始故障转移，需要得到所有 Sentinels 的授权。 这意味着 quorum 在两方面可以被用来调整 Sentinel： 如果 quorum 被设置为小于我们部署的 Sentinels 大多数，我们使 Sentinel 对主节点失败更加敏感，并一旦少数的 Sentinels 不再和主节点交流就会触发故障转移。 如果 quorum 被设置为大于我们部署的 Sentinels 大多数，仅仅当大多数连接良好的 Sentinels 同意主节点挂掉的时候，Sentinel 才能开始故障转移。 配置 epochs 为了开始故障转移，Sentinels 需要从大多数得到授权，有下面几个重要的原因： 当一个 Sentinel 被授权，它为故障转移的主节点获得一个独一无二的配置 epoch。这将用来标识新的配置的版本在故障转移完成之后。因为大多数同意一个版本被分配给指定的 Sentinel，没有其他的 Sentinel 可以使用它。这意味着，每次故障转移的配置都有一个独一无二的版本号。我们将看到这为什么是很重要的。 此外 Sentinels 有一个规则：如果一个 Sentinel 投票给其他的 Sentinel 在一次故障转移中，它将等待一段时间再次尝试故障转移这个主节点，你可以在 sentinel.conf 中配置这个延迟时间 failover-timeout。这意味着 Sentinels 在相同的时间内不会尝试故障转移相同的主节点，第一次请求授权的将会尝试，如果失败了，另一个将会在一段时间后尝试，等等。 Redis Sentinel 保证了活性（liveness）性质，如果大多数 Sentinels 能够交流。如果主节点挂了，最后将有一个会被授权开始故障转移。 Redis Sentinel 同样也保证了安全（safety ）性质，每个 Sentinel 将使用不同的配置 epoch（configuration epoch）来故障转移同一个主节点。 配置传播 一旦一个 Sentinel 能成功的故障转移一个主节点，它将开始广播新的配置以便其他的 Sentinels 更新他们关于主节点的信息。 为了认定一次故障转移是成功的，它需要 Sentinel 能发送 SLAVEOF NO ONE 指令给选举的从节点，并且切换为主节点，稍后能在主节点的 INFO 输出中观察到。 这时候，即使从节点的重新配置正在进行，故障转移也被认为是成功的，并且所有的 Sentinels 需要开始报告新的配置。 一个新的配置广播的方式，就是为什么我们需要每次 Sentinel 被授权故障转移时有一个不同的版本号的原因。 每个 Sentinel 使用 Redis 发布/订阅消息来连续不断的广播它的一个主节点的配置的版本号，所有的从节点和主节点。同时，所有的 Sentinels 等待消息来查看其他的 Sentinels 广播的配置。 配置在sentinel:hello 发布/订阅频道中被广播。 因为每个配置都有一个不同的版本号，大的版本号总是赢得小的版本号。 例如 一开始所有的 Sentinels 认为主节点 mymaster 的配置为 192.168.1.50:6379。这个配置的版本号为 1。一段时间后，一个被授权开始故障转移有版本号 2，如果故障转移成功，它将广播新的配置，它说是 192.168.1.50:9000，版本号为 2,。所有其他的实例将看到这个配置并更新他们的配置，因为新的配置有更高的版本号。 这意味着 Sentinel 保证第二个活性属性：一个 Sentinels 集合能互相交流并且把配置信息收敛到一个更高的版本号。 基本上，如果网络是分区的，每个分区将收敛到一个更高的本地配置。没有网络分区的特殊性情况下，只有一个分区并且每个 Sentinel 将同意配置。 分区下的一致性 Redis Sentinel 配置是最终一致的，所以每个分区将收敛到更高的可用的配置。然而在使用 Sentinel 的真实世界的系统中，有三个不同的角色： Redis 实例 Sentinel 实例 客户端 为了定义系统的行为，我们考虑所有的三种。 下面是一个简单的有三个节点的网络，每个都运行一个 Redis 实例和一个 Sentinel 实例： +-------------+ | Sentinel 1 |----- Client A | Redis 1 (M) | +-------------+ | |+-------------+ | +------------+| Sentinel 2 |-----+-- // ----| Sentinel 3 |----- Client B| Redis 2 (S) | | Redis 3 (M)|+-------------+ +------------+ 在这个系统中，原始状态是 Redis3 是主节点，Redis1 和 Redis2 是从节点。一个网络分区发生隔离了旧的主节点。Sentinels1 和 2 开始一个故障转移过程提升 Sentinel 1 为新的主节点。 Sentinel 的属性保证 Sentinel 1 和 2 现在有了一个主节点的新的配置。可是 Sentinel 3 依然有旧的配置因为它在一个不同的分区中存活。 我们知道 Sentinel 3 将得到他的配置更新当网络分区治愈的时候，但是如果有客户端和旧的主节点在一起，分区时会发生什么呢？ 客户端仍然可以向 Redis 3 写入数据。当网络分区治愈，Redis 3 变成 Reids 1 的一个从节点，在分区期间写入的数据都会丢失。 根据你的配置，你可以想或不想让这种情况发生： 如果你使用 Redis 作为缓存，客户端 B 仍然可以向旧的主节点写入数据是很方便的，即使数据将会丢失。 如果你使用 Redis 作为存储，这是不好的，你需要配置系统为了部分的阻止这个问题。 因为 Redis 是异步复制的，这种情况下，没有办法完全阻止数据丢失，但是你可以使用下面的 Redis 配置选项来限制 Redis 3 和 Redis 1 之间的分歧： min-slaves-to-write 1min-slaves-max-lag 10 当一个 Redis 有上面的配置，当作为主节点的时候，如果他不能向至少一个从节点写入数据，将会停止接受写入请求。因为复制是异步的，不能写（not being able to write ）意味着从节点都是分离的，或者没有发送异步确认超过了指定的 max-lag 的时间。 使用这个配置，上面的例子中的 Redis 3 将会在 10 秒之后变得不可用。当分区治愈，Sentinel 3 的配置将会是新的，Client B 能获取到一个有效的配置并继续工作。 总之， Redis + Sentinel 是一个最终一致性系统（ eventually consistent system），功能是最后一个故障转移获胜（ last failover wins）。旧节点中的数据会被丢弃，从当前主节点复制数据，所以总有一个丢失确认写的窗口。这是由于 Redis 的异步复制和系统的“虚拟”合并功能的丢弃性质。注意，Sentinel 本身没有限制，如果你把故障转移编排起来，相同的属性仍然适用，仅仅有两种方式来避免丢失写入确认： 使用同步复制 使用一个最终一致的系统，相同物体的不同版本能被合并 Redis 现在不能使用上面的任何系统，是目前的发展目标。可是有一个代理实现解决方案 2 在 Redis 存储之上，比如说 SoundCloud Roshi，或者 Netflix Dynomite。 Sentinel 持久化状态 Sentinel 状态保存在 sentinel 配置文件中。例如，每次一个收到一个新的配置，主节点，配置和配置 epoch 一起被保存在磁盘上。这意味着停止和重启 Sentinel 进程是很安全的。 TILT 模式 Redis Sentinel 严重依赖电脑时间：例如为了推断一个实例是否可达，它会记住最后一次成功回复 PING 命令的时间，并和当前时间比较来推断哪个是旧的。 可是，如果电脑时间意外改变了，或者电脑非常繁忙，或进程由于某些原因阻塞。Sentinel 或许开始表现意外的行为。 TILT 模式是一个特殊的“保护”模式，当发现奇怪的事情可能降低系统的可靠性，一个 Sentinel 可以进入这个模式。Sentinel 定时中断调用每秒 10 次，所以我们期待定时中断调用之间间隔 100 毫米左右。 Sentinel 所做的就是登记之前的中断调用时间，并和当前的调用时间比较：如果结果是负数或意外的数，将会进入 TILT 模式。 当处于 Sentinel 模式 Sentinel 将会继续监控每件事，但是： 停止一切动作 开始回复负数给 SENTINEL is-master-down-by-addr 请求让检测失败不再有信 如果 30 秒内每件事都表现正常，将退出 TILT 模式。 注意某些情况下，使用许多内核提供的单调时钟 API 代替 TILT 模式。可是它仍然是不清晰的如果这是一个很好的解决方案，因为在进程只是仅仅挂起或调度很长时间没有执行的情况下，当前的系统会避免这个问题。]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>nosql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shiro]]></title>
    <url>%2Fblog%2F2018%2F06%2F13%2Fjava%2Fjavaweb%2Fsecurity%2Fshiro%2F</url>
    <content type="text"><![CDATA[Shiro Shiro 是一个安全框架，具有认证、授权、加密、会话管理功能。 概述 Shiro 功能 Shiro 架构 认证 授权 资料 概述 Shiro 功能 Authentication - 身份认证/登录，验证用户是不是拥有相应的身份； Authorization - 授权，即权限验证，验证某个已认证的用户是否拥有某个权限；即判断用户是否能做事情，常见的如：验证某个用户是否拥有某个角色。或者细粒度的验证某个用户对某个资源是否具有某个权限； Session Manager - 会话管理，即用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中；会话可以是普通 JavaSE 环境的，也可以是如 Web 环境的； Cryptography - 加密，保护数据的安全性，如密码加密存储到数据库，而不是明文存储； Web Support - Web 支持，可以非常容易的集成到 Web 环境； Caching - 缓存，比如用户登录后，其用户信息、拥有的角色 / 权限不必每次去查，这样可以提高效率； Concurrency - shiro 支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去； Testing - 提供测试支持； Run As - 允许一个用户假装为另一个用户（如果他们允许）的身份进行访问； Remember Me - 记住我，这个是非常常见的功能，即一次登录后，下次再来的话不用登录了。 记住一点，Shiro 不会去维护用户、维护权限；这些需要我们自己去提供；然后通过相应的接口注入给 Shiro 即可。 Shiro 架构 Subject - 主体，代表了当前“用户”，这个用户不一定是一个具体的人，与当前应用交互的任何东西都是 Subject，如网络爬虫，机器人等；即一个抽象概念；所有 Subject 都绑定到 SecurityManager，与 Subject 的所有交互都会委托给 SecurityManager；可以把 Subject 认为是一个门面；SecurityManager 才是实际的执行者； SecurityManager - 安全管理器；即所有与安全有关的操作都会与 SecurityManager 交互；且它管理着所有 Subject；可以看出它是 Shiro 的核心，它负责与后边介绍的其他组件进行交互，如果学习过 SpringMVC，你可以把它看成 DispatcherServlet 前端控制器； Realm - 域，Shiro 从从 Realm 获取安全数据（如用户、角色、权限），就是说 SecurityManager 要验证用户身份，那么它需要从 Realm 获取相应的用户进行比较以确定用户身份是否合法；也需要从 Realm 得到用户相应的角色/权限进行验证用户是否能进行操作；可以把 Realm 看成 DataSource，即安全数据源。 Subject - 主体，可以看到主体可以是任何可以与应用交互的“用户”； SecurityManager - 相当于 SpringMVC 中的 DispatcherServlet 或者 Struts2 中的 FilterDispatcher；是 Shiro 的心脏；所有具体的交互都通过 SecurityManager 进行控制；它管理着所有 Subject、且负责进行认证和授权、及会话、缓存的管理。 Authenticator - 认证器，负责主体认证的，这是一个扩展点，如果用户觉得 Shiro 默认的不好，可以自定义实现；其需要认证策略（Authentication Strategy），即什么情况下算用户认证通过了； Authrizer - 授权器，或者访问控制器，用来决定主体是否有权限进行相应的操作；即控制着用户能访问应用中的哪些功能； Realm - 可以有 1 个或多个 Realm，可以认为是安全实体数据源，即用于获取安全实体的；可以是 JDBC 实现，也可以是 LDAP 实现，或者内存实现等等；由用户提供；注意 - Shiro 不知道你的用户/权限存储在哪及以何种格式存储；所以我们一般在应用中都需要实现自己的 Realm； SessionManager - 如果写过 Servlet 就应该知道 Session 的概念，Session 呢需要有人去管理它的生命周期，这个组件就是 SessionManager；而 Shiro 并不仅仅可以用在 Web 环境，也可以用在如普通的 JavaSE 环境、EJB 等环境；所有呢，Shiro 就抽象了一个自己的 Session 来管理主体与应用之间交互的数据；这样的话，比如我们在 Web 环境用，刚开始是一台 Web 服务器；接着又上了台 EJB 服务器；这时想把两台服务器的会话数据放到一个地方，这个时候就可以实现自己的分布式会话（如把数据放到 Memcached 服务器）； SessionDAO - DAO 大家都用过，数据访问对象，用于会话的 CRUD，比如我们想把 Session 保存到数据库，那么可以实现自己的 SessionDAO，通过如 JDBC 写到数据库；比如想把 Session 放到 Memcached 中，可以实现自己的 Memcached SessionDAO；另外 SessionDAO 中可以使用 Cache 进行缓存，以提高性能； CacheManager - 缓存控制器，来管理如用户、角色、权限等的缓存的；因为这些数据基本上很少去改变，放到缓存中后可以提高访问的性能 Cryptography - 密码模块，Shiro 提高了一些常见的加密组件用于如密码加密/解密的。 认证 授权 资料 shiro 官方文档 跟我学 Shiro]]></content>
      <categories>
        <category>security</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>security</tag>
        <tag>shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2Fblog%2F2018%2F06%2F13%2Fjava%2Fjavaweb%2Fdistributed%2Fmq%2Fkafka-advanced%2F</url>
    <content type="text"><![CDATA[Kafka Kafka 是一个分布式的、可水平扩展的、基于发布/订阅模式的、支持容错的消息系统。 1. 概述 1.1. 分布式 1.2. 容错 1.3. 提交日志 1.4. 消息队列 1.5. 为什么要使用消息系统 1.6. Kafka 的关键功能 1.7. Kafka 基本概念 1.8. Kafka 核心 API 1.9. Topic 和日志 2. Kafka 工作原理 3. 持久化 4. 复制 5. 流处理 5.1. 无状态处理 5.2. 有状态处理 6. Kafka 应用场景 7. 幂等性 7.1. 幂等性实现 7.2. 幂等性的应用实例 8. 事务 8.1. 事务属性理解 8.2. 引入事务目的 8.3. 事务操作的 API 8.4. 事务属性的应用实例 8.5. 生产者事务的实现 8.6. 其他思考 9. 资料 9.1. 官方资料 9.2. 第三方资料 1. 概述 1.1. 分布式 分布式系统是一个由多个运行机器组成的系统，所有这些机器在一个集群中一起工作，对最终端用户表现为一个节点。 Kafka 的分布式意义在于：它在不同的节点上存储、接收和发送消息。 1.2. 容错 分布式系统一般都会设计容错机制，保证集群中几个节点出现故障时，仍能对外提供服务。 1.3. 提交日志 提交日志（也称为预写日志，事务日志）是仅支持附加的持久有序数据结构。您不能修改或删除记录。它从左到右读取并保证项目排序。 Kafka 实际上将所有的消息存储到磁盘，并在结构中对它们进行排序，以便利用顺序磁盘读取。 1.4. 消息队列 消息队列技术是分布式应用间交换信息的一种技术。消息队列可驻留在内存或磁盘上, 队列存储消息直到它们被应用程序读走。通过消息队列，应用程序可独立地执行–它们不需要知道彼此的位置、或在继续执行前不需要等待接收程序接收此消息。在分布式计算环境中，为了集成分布式应用，开发者需要对异构网络环境下的分布式应用提供有效的通信手段。为了管理需要共享的信息，对应用提供公共的信息交换机制是重要的。常用的消息队列技术是 Message Queue。 Message Queue 的通信模式： 点对点：点对点方式是最为传统和常见的通讯方式，它支持一对一、一对多、多对多、多对一等多种配置方式，支持树状、网状等多种拓扑结构。 多点广播：MQ 适用于不同类型的应用。其中重要的，也是正在发展中的是&quot;多点广播&quot;应用，即能够将消息发送到多个目标站点 (Destination List)。可以使用一条 MQ 指令将单一消息发送到多个目标站点，并确保为每一站点可靠地提供信息。MQ 不仅提供了多点广播的功能，而且还拥有智能消息分发功能，在将一条消息发送到同一系统上的多个用户时，MQ 将消息的一个复制版本和该系统上接收者的名单发送到目标 MQ 系统。目标 MQ 系统在本地复制这些消息，并将它们发送到名单上的队列，从而尽可能减少网络的传输量。 发布/订阅 (Publish/Subscribe)：发布/订阅功能使消息的分发可以突破目的队列地理指向的限制，使消息按照特定的主题甚至内容进行分发，用户或应用程序可以根据主题或内容接收到所需要的消息。发布/订阅功能使得发送者和接收者之间的耦合关系变得更为松散，发送者不必关心接收者的目的地址，而接收者也不必关心消息的发送地址，而只是根据消息的主题进行消息的收发。 集群 (Cluster)：为了简化点对点通讯模式中的系统配置，MQ 提供 Cluster(集群) 的解决方案。集群类似于一个域 (Domain)，集群内部的队列管理器之间通讯时，不需要两两之间建立消息通道，而是采用集群 (Cluster) 通道与其它成员通讯，从而大大简化了系统配置。此外，集群中的队列管理器之间能够自动进行负载均衡，当某一队列管理器出现故障时，其它队列管理器可以接管它的工作，从而大大提高系统的高可靠性。 1.5. 为什么要使用消息系统 解耦 在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 冗余 有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的&quot;插入-获取-删除&quot;范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。 灵活性 &amp; 峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证 在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka 保证一个 Partition 内的消息的有序性。 缓冲 在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。 异步通信 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 1.6. Kafka 的关键功能 发布和订阅流记录，类似于消息队列或企业级消息系统。 以容错、持久化的方式存储流记录。 处理流记录。 1.7. Kafka 基本概念 Kafka 作为一个集群运行在一台或多台可以跨越多个数据中心的服务器上。 Kafka 集群在称为 Topic 的类别中存储记录流。 Kafka 的每个记录由一个键，一个值和一个时间戳组成。 1.8. Kafka 核心 API Producer - 允许应用程序将记录流发布到一个或多个 Kafka Topic。 Consumer - 允许应用程序订阅一个或多个 Topic 并处理为他们生成的记录流。 Streams - 允许应用程序充当流处理器，从一个或多个 Topic 中消费输入流，并将输出流生成为一个或多个输出 Topic，从而将输入流有效地转换为输出流。 Connector - 允许构建和运行可重复使用的生产者或消费者，将 Kafka Topic 连接到现有的应用程序或数据系统。例如，连接到关系数据库的连接器可能会捕获对表的每个更改。 在 Kafka 中，客户端和服务器之间的通信是采用 TCP 协议方式。 1.9. Topic 和日志 Topic 是一个目录名，它保存着发布记录。kafka 的 Topic 始终是多订阅者的，也就是说，一个主题可以有零个，一个或多个订阅写入数据的 Consumer。 在 Kafka 中，任意一个 Topic 维护一个 Partition 的日志，类似下图： 每个 Partition 都是一个有序的，不可变的记录序列，不断追加到结构化的提交日志中。Partition 中的记录每个分配一个连续的 id 号，称为偏移量，用于唯一标识 Partition 内的每条记录。 Kafka 集群持久化保存（使用可配置的保留期限）所有发布记录——无论它们是否被消费。例如，如果保留期限被设置为两天，则在记录发布后的两天之内，它都可以被消费，超过时间后将被丢弃以释放空间。 实际上，保留在每个 Consumer 基础上的唯一元数据是该 Consumer 在日志中的抵消或位置。这个偏移量是由 Consumer 控制的：Consumer 通常会在读取记录时线性地推进其偏移量，但实际上，由于位置由 Consumer 控制，因此它可以按照喜欢的任何顺序消费记录。 这种功能组合意味着 Kafka Consumer 的开销很小——它们的出现对集群和其他 Consumer 没有多少影响。 日志中的 Partition 有多种目的。首先，它们允许日志的大小超出服务器限制的大小。每个单独的 Partition 必须适合承载它的服务器，但是一个主题可能有很多 Partition，因此它可以处理任意数量的数据。其次，它们作为并行的单位。 2. Kafka 工作原理 Broker - Kafka 集群包含一个或多个服务器，这种服务器被称为 broker。 Topic - 每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。（物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处）。 Partition - Parition 是物理上的概念，每个 Topic 包含一个或多个 Partition。 Producer - 负责发布消息到 Kafka broker。 Consumer - 消息消费者，向 Kafka broker 读取消息的客户端。 Consumer Group - 每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 group name，若不指定 group name 则属于默认的 group）。 Producer 将消息（记录）发送到 Kafka 节点（Broker），消息由称为 Consumer 的其他应用程序处理。消息被存储在 Topic 中，并且 Consumer 订阅该主题以接收新消息。 随着 Topic 变得日益庞大，它们会被分割成更小的 Partition 以提高性能和可伸缩性。Kafka 保证 Partition 内的所有消息按照它们出现的顺序排序。区分特定消息的方式是通过它的偏移量，您可以将它看作普通数组索引，每个新消息都会增加一个序列号在一个 Partition 中。 Kafka 遵循发布/订阅模式。这意味着 Kafka 不会跟踪 Kafka 读取哪些记录并删除它们，而是将它们存储一段时间（例如一天）或直到满足某个大小阈值。Consumer 自己对 Kafka 进行新的消息调查并说出他们想要阅读的记录。这使得他们可以按照自己的意愿递增/递减偏移量，从而能够重播和重新处理事件。 Kafka 集群持久化保存（使用可配置的保留期限）所有发布记录——无论它们是否被消费。例如，如果保留期限被设置为两天，则在记录发布后的两天之内，它都可以被消费，超过时间后将被丢弃以释放空间。 值得注意的是，Consumer 实际上是内部拥有一个或多个 Consumer 流程的 Consumer 群体。为了避免两个进程读两次相同的消息，每个 Partition 仅与每个组的一个 Consumer 进程相关联。 3. 持久化 Kafka 实际上将其所有记录存储在磁盘中，并且不会将任何内容保留在 RAM 中。 Kafka 有一个将消息分组在一起的协议。它允许网络请求将消息分组在一起以减少网络开销。服务器一气呵成的将消息的数据块持久化并立即获取较大的线性块。 线性读取/写入磁盘速度很快。现代磁盘速度较慢的概念是由于大量的磁盘搜索，这在大型线性操作中不是问题。 所说的线性操作由操作系统通过预读（预取大块数倍）和后写（将小的逻辑写入大物理写入）技术进行了大量优化。 现代操作系统将磁盘缓存在可用 RAM 中。这被称为 pagecache。 由于 Kafka 在整个流程（生产者 -&gt; 经纪 -&gt; 消费者）中以标准化的二进制格式存储未修改的消息，所以它可以利用零拷贝优化。这就是操作系统将数据从页面缓存直接复制到套接字时，完全绕过了 Kafka 经纪人应用程序。 所有这些优化都允许 Kafka 以接近网络速度传递消息。 4. 复制 分区数据在多个代理中复制，以便在一个代理死亡的情况下保存数据。 在任何时候，一个代理“拥有”一个分区，并且是应用程序通过该分区读写数据的节点。这被称为分区领导。它将它收到的数据复制到 N 个其他代理（称为追随者）。他们也存储数据，并准备在领导者节点死亡的情况下取代领导者。这就是典型的一主多从模式。 生产者/消费者如何知道分区的领导者是谁？ 对于生产者/消费者来说，从一个分区写入/读取，他们需要知道它的领导者，对吧？这些信息需要从某处获得。Kafka 将这种元数据存储在一个名为 Zookeeper 的服务中。 生产者和消费者都和 Zookeeper 连接并通信。Kafka 一直在摆脱这种耦合，自 0.8 和 0.9 版分别开始，客户端直接从 Kafka 经纪人那里获取元数据信息，他们自己与 Zookeeper 交谈。 5. 流处理 在 Kafka 中，流处理器是任何需要从输入主题中持续输入数据流，对该输入执行一些处理并生成输出主题的数据流（或外部服务，数据库，垃圾桶，无论哪里真的…） 可以直接使用生产者/消费者 API 进行简单处理，但对于更复杂的转换（如将流连接在一起），Kafka 提供了一个集成的 Streams API 库。 此 API 旨在用于您自己的代码库中，它不在代理上运行。它与消费者 API 类似，可帮助您扩展多个应用程序的流处理工作（类似于消费者群体）。 5.1. 无状态处理 流的无状态处理是确定性处理，不依赖于任何外部。你知道，对于任何给定的数据，你将总是产生独立于其他任何东西的相同输出。 一个流可以被解释为一个表，一个表可以被解释为一个流。 流可以被解释为数据的一系列更新，其中聚合是表的最终结果。 如果您看看如何实现同步数据库复制，您会发现它是通过所谓的流式复制，其中表中的每个更改都发送到副本服务器。 Kafka 流可以用同样的方式解释 - 当从最终状态积累时的事件。这样的流聚合被保存在本地的 RocksDB 中（默认情况下），被称为 KTable。 可以将表格视为流中每个键的最新值的快照。以同样的方式，流记录可以产生一个表，表更新可以产生一个更新日志流。 5.2. 有状态处理 一些简单的操作，如 map() 或 filter() 是无状态的，并且不要求您保留有关处理的任何数据。但是，在现实生活中，你要做的大多数操作都是有状态的（例如 count()），因此需要存储当前的累积状态。 维护流处理器上的状态的问题是流处理器可能会失败！你需要在哪里保持这个状态才能容错？ 一种天真的做法是简单地将所有状态存储在远程数据库中，并通过网络连接到该存储。问题在于没有数据的地方和大量的网络往返，这两者都会显著减慢你的应用程序。一个更微妙但重要的问题是，您的流处理作业的正常运行时间将与远程数据库紧密耦合，并且作业不会自成体系（数据库中来自另一个团队的更改可能会破坏您的处理过程）。 那么更好的方法是什么？ 回想一下表和流的双重性。这使我们能够将数据流转换为与我们的处理共处一地的表格。它还为我们提供了处理容错的机制 - 通过将流存储在 Kafka 代理中。 流处理器可以将其状态保存在本地表（例如 RocksDB）中，该表将从输入流更新（可能是某种任意转换之后）。当进程失败时，它可以通过重放流来恢复其数据。 您甚至可以让远程数据库成为流的生产者，从而有效地广播更新日志，以便在本地重建表。 6. Kafka 应用场景 构建实时的流数据管道，在系统或应用间获取可靠数据。 构建实时的流应用程序，用于转换或响应数据流。 正如我们已经介绍的那样，Kafka 允许您将大量消息通过集中介质存储并存储，而不用担心性能或数据丢失等问题。 这意味着它非常适合用作系统架构的核心，充当连接不同应用程序的集中介质。 Kafka 可以成为事件驱动架构的核心部分，并允许您真正将应用程序彼此分离。 Kafka 允许您轻松分离不同（微）服务之间的通信。利用 Streams API，现在比以往更容易编写业务逻辑，丰富了 Kafka 主题数据以便服务消费。 7. 幂等性 幂等性引入目的：生产者重复生产消息。生产者进行 retry 会产生重试时，会重复产生消息。有了幂等性之后，在进行 retry 重试时，只会生成一个消息。 7.1. 幂等性实现 7.1.1. PID 和 Sequence Number 为了实现 Producer 的幂等性，Kafka 引入了 Producer ID（即 PID）和 Sequence Number。 PID。每个新的 Producer 在初始化的时候会被分配一个唯一的 PID，这个 PID 对用户是不可见的。 Sequence Numbler。（对于每个 PID，该 Producer 发送数据的每个&lt;Topic, Partition&gt;都对应一个从 0 开始单调递增的 Sequence Number。 Broker 端在缓存中保存了这 seq number，对于接收的每条消息，如果其序号比 Broker 缓存中序号大于 1 则接受它，否则将其丢弃。这样就可以实现了消息重复提交了。但是，只能保证单个 Producer 对于同一个&lt;Topic, Partition&gt;的 Exactly Once 语义。不能保证同一个 Producer 一个 topic 不同的 partion 幂等。 实现幂等之后 7.1.2. 生成 PID 的流程 在执行创建事务时，如下： Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); 会创建一个 Sender，并启动线程，执行如下 run 方法，在 maybeWaitForProducerId()中生成一个 producerId，如下： ====================================类名：Sender====================================void run(long now) &#123; if (transactionManager != null) &#123; try &#123; ........ if (!transactionManager.isTransactional()) &#123; // 为idempotent producer生成一个producer id maybeWaitForProducerId(); &#125; else if (transactionManager.hasUnresolvedSequences() &amp;&amp; !transactionManager.hasFatalError()) &#123; ........ 7.2. 幂等性的应用实例 （1）配置属性 需要设置： enable.idempotence，需要设置为 ture,此时就会默认把 acks 设置为 all，所以不需要再设置 acks 属性了。 private Producer buildIdempotProducer()&#123; // create instance for properties to access producer configs Properties props = new Properties(); // bootstrap.servers是Kafka集群的IP地址。多个时,使用逗号隔开 props.put("bootstrap.servers", "localhost:9092"); props.put("enable.idempotence",true); //If the request fails, the producer can automatically retry, props.put("retries", 3); //Reduce the no of requests less than 0 props.put("linger.ms", 1); //The buffer.memory controls the total amount of memory available to the producer for buffering. props.put("buffer.memory", 33554432); // Kafka消息是以键值对的形式发送,需要设置key和value类型序列化器 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); return producer;&#125; （2）发送消息 跟一般生产者一样，如下 public void produceIdempotMessage(String topic, String message) &#123; // 创建Producer Producer producer = buildIdempotProducer(); // 发送消息 producer.send(new ProducerRecord&lt;String, String&gt;(topic, message)); producer.flush();&#125; 此时，因为我们并没有配置 transaction.id 属性，所以不能使用事务相关 API，如下 producer.initTransactions(); 否则会出现如下错误： Exception in thread “main” java.lang.IllegalStateException: Transactional method invoked on a non-transactional producer. at org.apache.kafka.clients.producer.internals.TransactionManager.ensureTransactional(TransactionManager.java:777) at org.apache.kafka.clients.producer.internals.TransactionManager.initializeTransactions(TransactionManager.java:202) at org.apache.kafka.clients.producer.KafkaProducer.initTransactions(KafkaProducer.java:544) 8. 事务 8.1. 事务属性理解 事务属性是 2017 年 Kafka 0.11.0.0 引入的新特性。类似于数据库事务，只是这里的数据源是 Kafka，kafka 事务属性是指一系列的生产者生产消息和消费者提交偏移量的操作在一个事务，或者说是是一个原子操作），同时成功或者失败。 注意：在理解消息的事务时，一直处于一个错误理解就是如下代码中，把操作 db 的业务逻辑跟操作消息当成是一个事务。其实这个是有问题的，操作 DB 数据库的数据源是 DB，消息数据源是 kfaka，这是完全不同两个数据，一种数据源（如 mysql，kafka）对应一个事务，所以它们是两个独立的事务：kafka 事务指 kafka 一系列 生产、消费消息等操作组成一个原子操作；db 事务是指操作数据库的一系列增删改操作组成一个原子操作。 void kakfa_in_tranction() &#123; // 1.kafa的操作：读取消息或者生产消息 kafkaOperation(); // 2.db操作 dbOperation();&#125; 8.2. 引入事务目的 在事务属性之前先引入了生产者幂等性，它的作用为： 生产者多次发送消息可以封装成一个原子操作，要么都成功，要么失败 consumer-transform-producer 模式下，因为消费者提交偏移量出现问题，导致在重复消费消息时，生产者重复生产消息。需要将这个模式下消费者提交偏移量操作和生产者一系列生成消息的操作封装成一个原子操作。 消费者提交偏移量导致重复消费消息的场景：消费者在消费消息完成提交便宜量 o2 之前挂掉了（假设它最近提交的偏移量是 o1），此时执行再均衡时，其它消费者会重复消费消息(o1 到 o2 之间的消息）。 8.3. 事务操作的 API producer 提供了 initTransactions, beginTransaction, sendOffsets, commitTransaction, abortTransaction 五个事务方法。 /** * 初始化事务。需要注意的有： * 1、前提 * 需要保证transation.id属性被配置。 * 2、这个方法执行逻辑是： * （1）Ensures any transactions initiated by previous instances of the producer with the same * transactional.id are completed. If the previous instance had failed with a transaction in * progress, it will be aborted. If the last transaction had begun completion, * but not yet finished, this method awaits its completion. * （2）Gets the internal producer id and epoch, used in all future transactional * messages issued by the producer. * */public void initTransactions();/** * 开启事务 */public void beginTransaction() throws ProducerFencedException ;/** * 为消费者提供的在事务内提交偏移量的操作 */public void sendOffsetsToTransaction(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, String consumerGroupId) throws ProducerFencedException ;/** * 提交事务 */public void commitTransaction() throws ProducerFencedException;/** * 放弃事务，类似回滚事务的操作 */public void abortTransaction() throws ProducerFencedException ; 8.4. 事务属性的应用实例 在一个原子操作中，根据包含的操作类型，可以分为三种情况，前两种情况是事务引入的场景，最后一种情况没有使用价值。 只有 Producer 生产消息； 消费消息和生产消息并存，这个是事务场景中最常用的情况，就是我们常说的“consume-transform-produce ”模式 只有 consumer 消费消息，这种操作其实没有什么意义，跟使用手动提交效果一样，而且也不是事务属性引入的目的，所以一般不会使用这种情况 8.4.1. 相关属性配置 使用 kafka 的事务 api 时的一些注意事项： 需要消费者的自动模式设置为 false,并且不能子再手动的进行执行 consumer#commitSync 或者 consumer#commitAsyc 生产者配置 transaction.id 属性 生产者不需要再配置 enable.idempotence，因为如果配置了 transaction.id，则此时 enable.idempotence 会被设置为 true 消费者需要配置 Isolation.level。在 consume-trnasform-produce 模式下使用事务时，必须设置为 READ_COMMITTED。 8.4.2. 只有写 创建一个事务，在这个事务操作中，只有生成消息操作。代码如下： /** * 在一个事务只有生产消息操作 */public void onlyProduceInTransaction() &#123; Producer producer = buildProducer(); // 1.初始化事务 producer.initTransactions(); // 2.开启事务 producer.beginTransaction(); try &#123; // 3.kafka写操作集合 // 3.1 do业务逻辑 // 3.2 发送消息 producer.send(new ProducerRecord&lt;String, String&gt;("test", "transaction-data-1")); producer.send(new ProducerRecord&lt;String, String&gt;("test", "transaction-data-2")); // 3.3 do其他业务逻辑,还可以发送其他topic的消息。 // 4.事务提交 producer.commitTransaction(); &#125; catch (Exception e) &#123; // 5.放弃事务 producer.abortTransaction(); &#125;&#125; 创建生产者，代码如下,需要: 配置 transactional.id 属性 配置 enable.idempotence 属性 /** * 需要: * 1、设置transactional.id * 2、设置enable.idempotence * @return */private Producer buildProducer() &#123; // create instance for properties to access producer configs Properties props = new Properties(); // bootstrap.servers是Kafka集群的IP地址。多个时,使用逗号隔开 props.put("bootstrap.servers", "localhost:9092"); // 设置事务id props.put("transactional.id", "first-transactional"); // 设置幂等性 props.put("enable.idempotence",true); //Set acknowledgements for producer requests. props.put("acks", "all"); //If the request fails, the producer can automatically retry, props.put("retries", 1); //Specify buffer size in config,这里不进行设置这个属性,如果设置了,还需要执行producer.flush()来把缓存中消息发送出去 //props.put("batch.size", 16384); //Reduce the no of requests less than 0 props.put("linger.ms", 1); //The buffer.memory controls the total amount of memory available to the producer for buffering. props.put("buffer.memory", 33554432); // Kafka消息是以键值对的形式发送,需要设置key和value类型序列化器 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); return producer;&#125; 8.4.3. 消费-生产并存（consume-transform-produce） 在一个事务中，既有生产消息操作又有消费消息操作，即常说的 Consume-tansform-produce 模式。如下实例代码 /** * 在一个事务内,即有生产消息又有消费消息 */public void consumeTransferProduce() &#123; // 1.构建上产者 Producer producer = buildProducer(); // 2.初始化事务(生成productId),对于一个生产者,只能执行一次初始化事务操作 producer.initTransactions(); // 3.构建消费者和订阅主题 Consumer consumer = buildConsumer(); consumer.subscribe(Arrays.asList("test")); while (true) &#123; // 4.开启事务 producer.beginTransaction(); // 5.1 接受消息 ConsumerRecords&lt;String, String&gt; records = consumer.poll(500); try &#123; // 5.2 do业务逻辑; System.out.println("customer Message---"); Map&lt;TopicPartition, OffsetAndMetadata&gt; commits = Maps.newHashMap(); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; // 5.2.1 读取消息,并处理消息。print the offset,key and value for the consumer records. System.out.printf("offset = %d, key = %s, value = %s\n", record.offset(), record.key(), record.value()); // 5.2.2 记录提交的偏移量 commits.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset())); // 6.生产新的消息。比如外卖订单状态的消息,如果订单成功,则需要发送跟商家结转消息或者派送员的提成消息 producer.send(new ProducerRecord&lt;String, String&gt;("test", "data2")); &#125; // 7.提交偏移量 producer.sendOffsetsToTransaction(commits, "group0323"); // 8.事务提交 producer.commitTransaction(); &#125; catch (Exception e) &#123; // 7.放弃事务 producer.abortTransaction(); &#125; &#125;&#125; 创建消费者代码，需要： 将配置中的自动提交属性（auto.commit）进行关闭 而且在代码里面也不能使用手动提交 commitSync( )或者 commitAsync( ) 设置 isolation.level /** * 需要: * 1、关闭自动提交 enable.auto.commit * 2、isolation.level为 * @return */public Consumer buildConsumer() &#123; Properties props = new Properties(); // bootstrap.servers是Kafka集群的IP地址。多个时,使用逗号隔开 props.put("bootstrap.servers", "localhost:9092"); // 消费者群组 props.put("group.id", "group0323"); // 设置隔离级别 props.put("isolation.level","read_committed"); // 关闭自动提交 props.put("enable.auto.commit", "false"); props.put("session.timeout.ms", "30000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer &lt;String, String&gt;(props); return consumer;&#125; 8.4.4. 只有读 创建一个事务，在这个事务操作中，只有生成消息操作，如下代码。这种操作其实没有什么意义，跟使用手动提交效果一样，无法保证消费消息操作和提交偏移量操作在一个事务。 /** * 在一个事务只有消息操作 */public void onlyConsumeInTransaction() &#123; Producer producer = buildProducer(); // 1.初始化事务 producer.initTransactions(); // 2.开启事务 producer.beginTransaction(); // 3.kafka读消息的操作集合 Consumer consumer = buildConsumer(); while (true) &#123; // 3.1 接受消息 ConsumerRecords&lt;String, String&gt; records = consumer.poll(500); try &#123; // 3.2 do业务逻辑; System.out.println("customer Message---"); Map&lt;TopicPartition, OffsetAndMetadata&gt; commits = Maps.newHashMap(); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; // 3.2.1 处理消息 print the offset,key and value for the consumer records. System.out.printf("offset = %d, key = %s, value = %s\n", record.offset(), record.key(), record.value()); // 3.2.2 记录提交偏移量 commits.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset())); &#125; // 4.提交偏移量 producer.sendOffsetsToTransaction(commits, "group0323"); // 5.事务提交 producer.commitTransaction(); &#125; catch (Exception e) &#123; // 6.放弃事务 producer.abortTransaction(); &#125; &#125;&#125; 8.5. 生产者事务的实现 8.5.1. 相关配置 8.5.2. 幂等性和事务性的关系 两者关系 事务属性实现前提是幂等性，即在配置事务属性 transaction id 时，必须还得配置幂等性；但是幂等性是可以独立使用的，不需要依赖事务属性。 幂等性引入了 Porducer ID 事务属性引入了 Transaction Id 属性。、 设置 enable.idempotence = true，transactional.id 不设置：只支持幂等性。 enable.idempotence = true，transactional.id 设置：支持事务属性和幂等性 enable.idempotence = false，transactional.id 不设置：没有事务属性和幂等性的 kafka enable.idempotence = false，transactional.id 设置：无法获取到 PID，此时会报错 tranaction id 、productid 和 epoch 一个 app 有一个 tid，同一个应用的不同实例 PID 是一样的，只是 epoch 的值不同。如： 同一份代码运行两个实例，分步执行如下：在实例 1 没有进行提交事务前，开始执行实例 2 的初始化事务 step1 实例 1-初始化事务。的打印出对应 productId 和 epoch，信息如下： [2018-04-21 20:56:23,106] INFO [TransactionCoordinator id=0] Initialized transactionalId first-transactional with producerId 8000 and producer epoch 123 on partition __transaction_state-12 (kafka.coordinator.transaction.TransactionCoordinator) step2 实例 1-发送消息。 step3 实例 2-初始化事务。初始化事务时的打印出对应 productId 和 epoch，信息如下： 18-04-21 20:56:48,373] INFO [TransactionCoordinator id=0] Initialized transactionalId first-transactional with producerId 8000 and producer epoch 124 on partition __transaction_state-12 (kafka.coordinator.transaction.TransactionCoordinator) step4 实例 1-提交事务，此时报错 org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer’s transaction has been expired by the broker. step5 实例 2-提交事务 为了避免这种错误，同一个事务 ID，只有保证如下顺序 epch 小 producer 执行 init-transaction 和 committransaction，然后 epoch 较大的 procuder 才能开始执行 init-transaction 和 commit-transaction，如下顺序： 有了 transactionId 后，Kafka 可保证： 跨 Session 的数据幂等发送。当具有相同 Transaction ID 的新的 Producer 实例被创建且工作时，旧的且拥有相同 Transaction ID 的 Producer 将不再工作【上面的实例可以验证】。kafka 保证了关联同一个事务的所有 producer（一个应用有多个实例）必须按照顺序初始化事务、和提交事务，否则就会有问题，这保证了同一事务 ID 中消息是有序的（不同实例得按顺序创建事务和提交事务）。 8.5.3. 事务最佳实践-单实例的事务性 通过上面实例中可以看到 kafka 是跨 Session 的数据幂等发送，即如果应用部署多个实例时常会遇到上面的问题“org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer’s transaction has been expired by the broker.”，必须保证这些实例生产者的提交事务顺序和创建顺序保持一致才可以，否则就无法成功。其实，在实践中，我们更多的是如何实现对应用单实例的事务性。可以通过 spring-kafaka 实现思路来学习，即每次创建生产者都设置一个不同的 transactionId 的值，如下代码： 在 spring-kafka 中，对于一个线程创建一个 producer，事务提交之后，还会关闭这个 producer 并清除，后续同一个线程或者新的线程重新执行事务时，此时就会重新创建 producer。 ====================================类名：ProducerFactoryUtils====================================/** * Obtain a Producer that is synchronized with the current transaction, if any. * @param producerFactory the ConnectionFactory to obtain a Channel for * @param &lt;K&gt; the key type. * @param &lt;V&gt; the value type. * @return the resource holder. */public static &lt;K, V&gt; KafkaResourceHolder&lt;K, V&gt; getTransactionalResourceHolder( final ProducerFactory&lt;K, V&gt; producerFactory) &#123; Assert.notNull(producerFactory, "ProducerFactory must not be null"); // 1.对于每一个线程会生成一个唯一key，然后根据key去查找resourceHolder @SuppressWarnings("unchecked") KafkaResourceHolder&lt;K, V&gt; resourceHolder = (KafkaResourceHolder&lt;K, V&gt;) TransactionSynchronizationManager .getResource(producerFactory); if (resourceHolder == null) &#123; // 2.创建一个消费者 Producer&lt;K, V&gt; producer = producerFactory.createProducer(); // 3.开启事务 producer.beginTransaction(); resourceHolder = new KafkaResourceHolder&lt;K, V&gt;(producer); bindResourceToTransaction(resourceHolder, producerFactory); &#125; return resourceHolder;&#125; 创建消费者代码 ====================================类名：DefaultKafkaProducerFactory====================================protected Producer&lt;K, V&gt; createTransactionalProducer() &#123; Producer&lt;K, V&gt; producer = this.cache.poll(); if (producer == null) &#123; Map&lt;String, Object&gt; configs = new HashMap&lt;&gt;(this.configs); // 对于每一次生成producer时，都设置一个不同的transactionId configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, this.transactionIdPrefix + this.transactionIdSuffix.getAndIncrement()); producer = new KafkaProducer&lt;K, V&gt;(configs, this.keySerializer, this.valueSerializer); // 1.初始化话事务。 producer.initTransactions(); return new CloseSafeProducer&lt;K, V&gt;(producer, this.cache); &#125; else &#123; return producer; &#125;&#125; 8.5.4. Consume-transform-Produce 的流程 流程 1 **：**查找 Tranaction Corordinator。 Producer 向任意一个 brokers 发送 FindCoordinatorRequest 请求来获取 Transaction Coordinator 的地址。 **流程 2：**初始化事务 initTransaction Producer 发送 InitpidRequest 给事务协调器，获取一个 Pid**。InitpidRequest 的处理过程是同步阻塞的，一旦该调用正确返回，Producer 就可以开始新的事务**。TranactionalId 通过 InitpidRequest 发送给 Tranciton Corordinator，然后在 Tranaciton Log 中记录这&lt;TranacionalId,pid&gt;的映射关系。除了返回 PID 之外，还具有如下功能： 对 PID 对应的 epoch 进行递增，这样可以保证同一个 app 的不同实例对应的 PID 是一样的，但是 epoch 是不同的。 回滚之前的 Producer 未完成的事务（如果有）。 流程 3： 开始事务 beginTransaction 执行 Producer 的 beginTransacion()，它的作用是 Producer 在本地记录下这个 transaction 的状态为开始状态。 注意：这个操作并没有通知 Transaction Coordinator。 流程 4： Consume-transform-produce loop 流程 4.0： 通过 Consumtor 消费消息，处理业务逻辑 流程 4.1： producer 向 TransactionCordinantro 发送 AddPartitionsToTxnRequest 在 producer 执行 send 操作时，如果是第一次给&lt;topic,partion&gt;发送数据，此时会向 Trasaction Corrdinator 发送一个 AddPartitionsToTxnRequest 请求，Transaction Corrdinator 会在 transaction log 中记录下 tranasactionId 和&lt;topic,partion&gt;一个映射关系，并将状态改为 begin。AddPartionsToTxnRequest 的数据结构如下： AddPartitionsToTxnRequest =&gt; TransactionalId PID Epoch [Topic [Partition]] TransactionalId =&gt; string PID =&gt; int64 Epoch =&gt; int16 Topic =&gt; string Partition =&gt; int32 流程 4.2： producer#send 发送 ProduceRequst 生产者发送数据，虽然没有还没有执行 commit 或者 absrot，但是此时消息已经保存到 kafka 上，可以参考如下图断点位置处，此时已经可以查看到消息了，而且即使后面执行 abort，消息也不会删除，只是更改状态字段标识消息为 abort 状态。 流程 4.3： AddOffsetCommitsToTxnRequest Producer 通过 KafkaProducer.sendOffsetsToTransaction 向事务协调器器发送一个 AddOffesetCommitsToTxnRequests： AddOffsetsToTxnRequest =&gt; TransactionalId PID Epoch ConsumerGroupID TransactionalId =&gt; string PID =&gt; int64 Epoch =&gt; int16 ConsumerGroupID =&gt; string 在执行事务提交时，可以根据 ConsumerGroupID 来推断_customer_offsets 主题中相应的 TopicPartions 信息。这样在 流程 4.4: TxnOffsetCommitRequest Producer 通过 KafkaProducer.sendOffsetsToTransaction 还会向消费者协调器 Cosumer Corrdinator 发送一个 TxnOffsetCommitRequest，在主题_consumer_offsets 中保存消费者的偏移量信息。 TxnOffsetCommitRequest =&gt; ConsumerGroupID PID Epoch RetentionTime OffsetAndMetadata ConsumerGroupID =&gt; string PID =&gt; int64 Epoch =&gt; int32 RetentionTime =&gt; int64 OffsetAndMetadata =&gt; [TopicName [Partition Offset Metadata]] TopicName =&gt; string Partition =&gt; int32 Offset =&gt; int64 Metadata =&gt; string 流程 5： 事务提交和事务终结(放弃事务) 通过生产者的 commitTransaction 或 abortTransaction 方法来提交事务和终结事务，这两个操作都会发送一个 EndTxnRequest 给 Transaction Coordinator。 流程 5.1：EndTxnRequest。Producer 发送一个 EndTxnRequest 给 Transaction Coordinator，然后执行如下操作： Transaction Coordinator 会把 PREPARE_COMMIT or PREPARE_ABORT 消息写入到 transaction log 中记录 执行流程 5.2 执行流程 5.3 流程 5.2：WriteTxnMarkerRequest WriteTxnMarkersRequest =&gt; [CoorinadorEpoch PID Epoch Marker [Topic [Partition]]] CoordinatorEpoch =&gt; int32 PID =&gt; int64 Epoch =&gt; int16 Marker =&gt; boolean (false(0) means ABORT, true(1) means COMMIT) Topic =&gt; string Partition =&gt; int32 对于 Producer 生产的消息。Tranaction Coordinator 会发送 WriteTxnMarkerRequest 给当前事务涉及到每个&lt;topic,partion&gt;的 leader，leader 收到请求后，会写入一个 COMMIT(PID) 或者 ABORT(PID)的控制信息到 data log 中 对于消费者偏移量信息，如果在这个事务里面包含_consumer-offsets 主题。Tranaction Coordinator 会发送 WriteTxnMarkerRequest 给 Transaction Coordinartor，Transaction Coordinartor 收到请求后，会写入一个 COMMIT(PID) 或者 ABORT(PID)的控制信息到 data log 中。 **流程 5.3：**Transaction Coordinator 会将最终的 COMPLETE_COMMIT 或 COMPLETE_ABORT 消息写入 Transaction Log 中以标明该事务结束。 只会保留这个事务对应的 PID 和 timstamp。然后把当前事务其他相关消息删除掉，包括 PID 和 tranactionId 的映射关系。 文件类型和查看命令 kafka 文件主要包括 broker 的 data（主题：test）、事务协调器对应的 transaction_log（主题：__tranaction_state）、偏移量信息（主题:_consumer_offsets）三种类型。如下图 这三种文件类型其实都是 topic 的分区，所以对于每一个目录都包含*.log、*.index、.timeindex、.txnindex 文件（仅这个文件是为了实现事务属性引入的）。segment 和 segmengt 对应 index、timeindex、txnindex 文件命名中序号表示的是第几个消息。如下图中，00000000000000368769.index 和 00000000000000568769.log 中“368969”就是表示文件中存储的第一个消息是 468969 个消息。 对于索引文案包含两部分： baseOffset：索引对应 segment 文件中的第几条 message。 position：在 segment 中的绝对位置。 查看文件内容： bin/kafka-run-class.sh kafka.tools.DumpLogSegments –files /Users/wuzhonghu/data/kafka-logs/firtstopic-0/00000000000000000002.log –print-data-log ControlMessage 和 Transaction markers Trasaction markers 就是 kafka 为了实现事务定义的 Controll Message。这个消息和数据消息都存放在 log 中，在 Consumer 读取事务消息时有用，可以参考下面章节-4.5.1 老版本-读取事务消息顺序。 Transaction Coordinator 和 Transaction Log Transaction Log 如下放置在“_tranaction_state”主题下面，默认是 50 个分区，每一个分区中文件格式和 broker 存储消息是一样的,都有 log/index/timeindex 文件，如下： 8.5.5. 消费读取事务消息(READ_COMMITED) Consumer 为了实现事务，新增了一个 isolation.level 配置，有两个值如下， READ_UNCOMMITTED，类似于没有事务属性的消费者。 READ_COMMITED，只获取执行了事务提交的消息。 在本小节中我们主要讲 READ_COMMITED 模式下读取消息的流程的两种版本的演化 老版本-读取事务消息顺序 如下图中，按顺序保存到 broker 中消息有：事务 1 消息 T1-M1、对于事务 2 的消息有 T2-M1、事务 1 消息 T1-M2、非事务消息 M1，最终到达 client 端的循序是 M1-&gt; T2-M1 -&gt; T1-M1 -&gt; T1-M2。 具体步骤如下： step1 Consumer 接受到事务消息 T1-M1、T2-M2、T1-M2 和非事务消息 M1，因为没有收到事务 T1 和 T2 的控制消息，所以此时把事务相关消息 T1-M1、T2-M2、T1-M2 保存到内存，然后只把非事务消息 M1 返回给 client。 step2 Consumer 接受到事务 2 的控制消息 T2-C，此时就把事务消息 T2-M1 发送给 Clinet。 step3 C onsumer 接受到事务 1 的控制消息 T1-C,此时就把事务消息 T1-M1 和 T1-M2 发送给 Client 新版本-读取事务消息顺序 第一种方式，需要在 consumer 客户端缓存消息，当存在耗时比较长的事务时，占用客户端大量的内存资源。为了解决这个问题，通过 LSO 和 Abort Index 文件来解决这个问题，参考： https://docs.google.com/document/d/1Rlqizmk7QCDe8qAnVW5e5X8rGvn6m2DCR3JR2yqwVjc/edit （1） LSO，Last stable offset。Broker 在缓存中维护了所有处于运行状态的事务对应的 initial offsets,LSO 的值就是这些 offsets 中最小值-1。这样在 LSO 之前数据都是已经 commit 或者 abort 的数据，只有这些数据才对 Consumer 可见，即 consumer 读取数据只能读取到 LSO 的位置。 LSO 并没有持久化某一个位置，而是实时计算出来的，并保存在缓存中。 （2）Absort Index 文件 Conusmer 发送 FetchRequest 中，新增了 Isolation 字段，表示是那种模式 ReplicaId MaxWaitTime MinBytes [TopicName [Partition FetchOffset MaxBytes]] ReplicaId =&gt; int32 MaxWaitTime =&gt; int32 MinBytes =&gt; int32 TopicName =&gt; string Partition =&gt; int32 FetchOffset =&gt; int64 MaxBytes =&gt; int32 Isolation =&gt; READ_COMMITTED | READ_UNCOMMITTED 返回数据类型为 FetchResponse 的格式为： ThrottleTime [TopicName [Partition ErrorCode HighwaterMarkOffset AbortedTransactions MessageSetSize MessageSet]] 对应各个给字段类型为 ThrottleTime =&gt; int32 TopicName =&gt; string Partition =&gt; int32 ErrorCode =&gt; int16 HighwaterMarkOffset =&gt; int64 AbortedTransactions =&gt; [PID FirstOffset] PID =&gt; int64 FirstOffset =&gt; int64 MessageSetSize =&gt; int32 设置成 READ_UNCOMMITTED 模式时, the AbortedTransactions array is null. 设置为 READ_COMMITTED 时，the Last Stable Offset(LSO)，当事务提交之后，LSO 向前移动 offset 数据如下： 存放数据的 log 存放 Absort Index 的内容如下： 执行读取数据流程如下： step1: 假设 consumer 读取数据的 fetched offsets 的区间是 0 到 4。 首先，broker 读取 data log 中数据 然后，broker 依次读取 abort index 的内容，发现 LSO 大于等于 4 就停止。如上可以获取到 P2 对应的 offset 从 2 到 5 的消息都是被丢弃的： 最后，broker 将上面 data log 和 abort index 中满足条件的数据返回给 consumer。 **step2 ：**在 consumer 端根据 absrot index 中返回的内容，过滤丢弃的消息，最终给用户消息为 Absorted Transaction Index 在 broker 中数据中新增一个索引文件，保存 aborted tranasation 对应的 offsets，只有事务执行 abort 时，才会往这个文件新增一个记录，初始这个文件是不存在的，只有第一条 abort 时，才会创建这个文件。 这个索引文件结构的每一行结构是 TransactionEntry： Version =&gt; int16 PID =&gt; int64 FirstOffset =&gt; int64 LastOffset =&gt; int64 LastStableOffset =&gt; int64 当 broker 接受到控制消息（producer 执行 commitTransaction()或者 abortTransaction()）时, 执行如下操作: (1)计算 LSO。 Broker 在缓存中维护了所有处于运行状态的事务对应的 initial offsets,LSO 的值就是这些 offsets 中最小值-1。 举例说明下 LSO 的计算，对于一个 data log 中内如如下 对应的 abort index 文件中内如如下：LSO 是递增的 (2)第二步 如果事务是提交状态，则在索引文件中新增 TransactionEntry。 (3)第三步 从 active 的 tranaction set 中移除这个 transaton，然后更新 LSO。 问题 1、问题 1：producer 通过事务提交消息时抛异常了， 对于使用非事务的消费者，是否可以获取此消息？ 对于事务消息，必须是执行 commit 或者 abstort 之后，消息才对消费者可见，即使是非事务的消费者。只是非事务消费者相比事务消费者区别，在于可以读取执行了 absort 的消息。 8.6. 其他思考 1、如何保证消息不丢。 （1）在消费端可以建立一个日志表，和业务处理在一个事务 定时扫描没有表发送没有被处理的消息 （2）消费端，消费消息之后，修改消息表的中消息状态为已处理成功。 2、如何保证消息提交和业务处理在同一个事务内完成 在消费端可以建立一个日志表，和业务处理在一个事务 3、消费者角度，如何保证消息不被重复消费。 （1）通过 seek 操作 （2）通过 kafka 事务操作。 4、生产者角度，如何保证消息不重复生产 （1）kakfka 幂等性 9. 资料 9.1. 官方资料 Github | 官方文档 9.2. 第三方资料 Kafka Manager - Kafka 管理工具 Kafka 剖析（一）：Kafka 背景及架构介绍 Thorough Introduction to Apache Kafka Kafak(04) Kafka 生产者事务和幂等]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaweb</tag>
        <tag>分布式</tag>
        <tag>mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo]]></title>
    <url>%2Fblog%2F2018%2F06%2F12%2Fjava%2Fjavaweb%2Fdistributed%2Frpc%2Fdubbo%2F</url>
    <content type="text"><![CDATA[Dubbo Dubbo 是一个基于 Java 开发的高性能 RPC 框架。 概述 QuickStart Dubbo 配置 配置方式 配置项 Dubbo 支持的协议 服务治理 集群容错 负载均衡 路由规则 服务降级 访问控制 动态配置 Dubbo 架构 整体设计 资料 概述 Dubbo 是一个基于 Java 开发的高性能 RPC 框架。 Dubbo 的三个关键功能： 基于接口的远程调用； 容错机制以及负载均衡； 自动服务注册以及自动服务发现。 QuickStart （1）添加 maven 依赖 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;$&#123;dubbo.version&#125;&lt;/version&gt;&lt;/dependency&gt; （2）定义 Provider package com.alibaba.dubbo.demo;public interface DemoService &#123; String sayHello(String name);&#125; （3）实现 Provider package com.alibaba.dubbo.demo.provider;import com.alibaba.dubbo.demo.DemoService;public class DemoServiceImpl implements DemoService &#123; public String sayHello(String name) &#123; return "Hello " + name; &#125;&#125; （4）配置 Provider &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application name="demo-provider"/&gt; &lt;dubbo:registry address="multicast://224.5.6.7:1234"/&gt; &lt;dubbo:protocol name="dubbo" port="20880"/&gt; &lt;dubbo:service interface="com.alibaba.dubbo.demo.DemoService" ref="demoService"/&gt; &lt;bean id="demoService" class="com.alibaba.dubbo.demo.provider.DemoServiceImpl"/&gt;&lt;/beans&gt; （5）启动 Provider import org.springframework.context.support.ClassPathXmlApplicationContext;public class Provider &#123; public static void main(String[] args) throws Exception &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext( new String[] &#123;"META-INF/spring/dubbo-demo-provider.xml"&#125;); context.start(); // press any key to exit System.in.read(); &#125;&#125; （6）配置 Consumer &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application name="demo-consumer"/&gt; &lt;dubbo:registry address="multicast://224.5.6.7:1234"/&gt; &lt;dubbo:reference id="demoService" interface="com.alibaba.dubbo.demo.DemoService"/&gt;&lt;/beans&gt; （7）启动 Consumer import com.alibaba.dubbo.demo.DemoService;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Consumer &#123; public static void main(String[] args) throws Exception &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext( new String[]&#123;"META-INF/spring/dubbo-demo-consumer.xml"&#125;); context.start(); // obtain proxy object for remote invocation DemoService demoService = (DemoService) context.getBean("demoService"); // execute remote invocation String hello = demoService.sayHello("world"); // show the result System.out.println(hello); &#125;&#125; Dubbo 配置 dubbo 所有配置最终都将转换为 URL 表示，并由服务提供方生成，经注册中心传递给消费方，各属性对应 URL 的参数，参见配置项一览表中的 “对应 URL 参数” 列。 注意 只有 group，interface，version 是服务的匹配条件，三者决定是不是同一个服务，其它配置项均为调优和治理参数。 URL 格式 protocol://username:password@host:port/path?key=value&amp;key=value 配置方式 Dubbo 支持多种配置方式： xml 配置 properties 配置 API 配置 注解配置 如果同时存在多种配置方式，遵循以下覆盖策略： JVM 启动 -D 参数优先，这样可以使用户在部署和启动时进行参数重写，比如在启动时需改变协议的端口。 XML 次之，如果在 XML 中有配置，则 dubbo.properties 中的相应配置项无效。 Properties 最后，相当于缺省值，只有 XML 没有配置时，dubbo.properties 的相应配置项才会生效，通常用于共享公共配置，比如应用名。 xml 配置 示例： &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://dubbo.apache.org/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application name="hello-world-app" /&gt; &lt;dubbo:registry address="multicast://224.5.6.7:1234" /&gt; &lt;dubbo:protocol name="dubbo" port="20880" /&gt; &lt;dubbo:service interface="com.alibaba.dubbo.demo.DemoService" ref="demoServiceLocal" /&gt; &lt;dubbo:reference id="demoServiceRemote" interface="com.alibaba.dubbo.demo.DemoService" /&gt; &lt;/beans&gt; properties 配置 示例： dubbo.application.name=foodubbo.application.owner=bardubbo.registry.address=10.20.153.10:9090 配置项 所有配置项分为三大类 服务发现：表示该配置项用于服务的注册与发现，目的是让消费方找到提供方。 服务治理：表示该配置项用于治理服务间的关系，或为开发测试提供便利条件。 性能调优：表示该配置项用于调优性能，不同的选项对性能会产生影响。 标签 用途 解释 dubbo:service 服务配置 用于暴露一个服务，定义服务的元信息，一个服务可以用多个协议暴露，一个服务也可以注册到多个注册中心 dubbo:reference 引用配置 用于创建一个远程服务代理，一个引用可以指向多个注册中心 dubbo:protocol 协议配置 用于配置提供服务的协议信息，协议由提供方指定，消费方被动接受 dubbo:application 应用配置 用于配置当前应用信息，不管该应用是提供者还是消费者 dubbo:module 模块配置 用于配置当前模块信息，可选 dubbo:registry 注册中心配置 用于配置连接注册中心相关信息 dubbo:monitor 监控中心配置 用于配置连接监控中心相关信息，可选 dubbo:provider 提供方配置 当 ProtocolConfig 和 ServiceConfig 某属性没有配置时，采用此缺省值，可选 dubbo:consumer 消费方配置 当 ReferenceConfig 某属性没有配置时，采用此缺省值，可选 dubbo:method 方法配置 用于 ServiceConfig 和 ReferenceConfig 指定方法级的配置信息 dubbo:argument 参数配置 用于指定方法参数配置 详细配置说明请参考：官方配置 配置之间的关系 配置覆盖关系 以 timeout 为例，显示了配置的查找顺序，其它 retries, loadbalance, actives 等类似： 方法级优先，接口级次之，全局配置再次之。 如果级别一样，则消费方优先，提供方次之。 其中，服务提供方配置，通过 URL 经由注册中心传递给消费方。 Dubbo 支持的协议 Dubbo 支持以下通信协议： dubbo rmi hessian http webservice thrift memcached redis 不同协议适合不同的服务场景，可以根据实际应用场景来选择合适的协议。 dubbo 协议是 dubbo 默认的协议。dubbo 协议采用单一长连接和 NIO 异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。反之，dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。 选用哪个协议，可以通过 &lt;dubbo:protocol&gt; 标签配置。 更多详情请参考：Dubbo 官方协议参考手册 服务治理 当服务越来越多时，服务 URL 配置管理变得非常困难，F5 硬件负载均衡器的单点压力也越来越大。 当进一步发展，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。 接着，服务的调用量越来越大，服务的容量问题就暴露出来，这个服务需要多少机器支撑？什么时候该加机器？ 以上问题可以归纳为服务治理问题，这也是 Dubbo 的核心功能。 集群容错 Failover - 失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries=“2” 来设置重试次数(不含第一次)。 Failfast - 快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 Failsafe - 失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback - 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking - 并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=“2” 来设置最大并行数。 Broadcast - 播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息。 负载均衡 Random 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin 轮循，按公约后的权重设置轮循比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash 一致性 Hash，相同参数的请求总是发到同一提供者。 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 算法参见：http://en.wikipedia.org/wiki/Consistent_hashing 缺省只对第一个参数 Hash，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.arguments&quot; value=&quot;0,1&quot; /&gt; 缺省用 160 份虚拟节点，如果要修改，请配置 &lt;dubbo:parameter key=&quot;hash.nodes&quot; value=&quot;320&quot; /&gt; 路由规则 路由规则决定一次 dubbo 服务调用的目标服务器，分为条件路由规则和脚本路由规则，并且支持可扩展。 向注册中心写入路由规则的操作通常由监控中心或治理中心的页面完成。 RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();Registry registry = registryFactory.getRegistry(URL.valueOf("zookeeper://10.20.153.10:2181"));registry.register(URL.valueOf("condition://0.0.0.0/com.foo.BarService?category=routers&amp;dynamic=false&amp;rule=" + URL.encode("host = 10.20.153.10 =&gt; host = 10.20.153.11") + ")); condition:// - 表示路由规则的类型，支持条件路由规则和脚本路由规则，可扩展，必填。 0.0.0.0 - 表示对所有 IP 地址生效，如果只想对某个 IP 的生效，请填入具体 IP，必填。 com.foo.BarService - 表示只对指定服务生效，必填。 category=routers - 表示该数据为动态配置类型，必填。 dynamic=false - 表示该数据为持久数据，当注册方退出时，数据依然保存在注册中心，必填。 enabled=true - 覆盖规则是否生效，可不填，缺省生效。 force=false - 当路由结果为空时，是否强制执行，如果不强制执行，路由结果为空的路由规则将自动失效，可不填，缺省为 flase。 runtime=false - 是否在每次调用时执行路由规则，否则只在提供者地址列表变更时预先执行并缓存结果，调用时直接从缓存中获取路由结果。如果用了参数路由，必须设为 true，需要注意设置会影响调用的性能，可不填，缺省为 flase。 priority=1 - 路由规则的优先级，用于排序，优先级越大越靠前执行，可不填，缺省为 0。 rule=URL.encode(“host = 10.20.153.10 =&gt; host = 10.20.153.11”) - 表示路由规则的内容，必填。 服务降级 可以通过服务降级功能临时屏蔽某个出错的非关键服务，并定义降级后的返回策略。 向注册中心写入动态配置覆盖规则： RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();Registry registry = registryFactory.getRegistry(URL.valueOf("zookeeper://10.20.153.10:2181"));registry.register(URL.valueOf("override://0.0.0.0/com.foo.BarService?category=configurators&amp;dynamic=false&amp;application=foo&amp;mock=force:return+null")); 其中： mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 还可以改为 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响。 访问控制 直连 在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连，点对点直联方式，将以服务接口为单位，忽略注册中心的提供者列表，A 接口配置点对点，不影响 B 接口从注册中心获取列表。 配置方式： （1）通过 XML 配置 如果是线上需求需要点对点，可在 dubbo:reference 中配置 url 指向提供者，将绕过注册中心，多个地址用分号隔开，配置如下： &lt;dubbo:reference id="xxxService" interface="com.alibaba.xxx.XxxService" url="dubbo://localhost:20890" /&gt; （2）通过 -D 参数指定 在 JVM 启动参数中加入-D 参数映射服务地址： java -Dcom.alibaba.xxx.XxxService=dubbo://localhost:20890 （3）通过文件映射 如果服务比较多，也可以用文件映射，用 -Ddubbo.resolve.file 指定映射文件路径，此配置优先级高于 dubbo:reference 中的配置： java -Ddubbo.resolve.file=xxx.properties 然后在映射文件 xxx.properties 中加入配置，其中 key 为服务名，value 为服务提供者 URL： com.alibaba.xxx.XxxService=dubbo://localhost:20890 只订阅 为方便开发测试，经常会在线下共用一个所有服务可用的注册中心，这时，如果一个正在开发中的服务提供者注册，可能会影响消费者不能正常运行。 可以让服务提供者开发方，只订阅服务(开发的服务可能依赖其它服务)，而不注册正在开发的服务，通过直连测试正在开发的服务。 禁用注册配置： &lt;dubbo:registry address="10.20.153.10:9090" register="false" /&gt; 或者 &lt;dubbo:registry address="10.20.153.10:9090?register=false" /&gt; 只注册 如果有两个镜像环境，两个注册中心，有一个服务只在其中一个注册中心有部署，另一个注册中心还没来得及部署，而两个注册中心的其它应用都需要依赖此服务。这个时候，可以让服务提供者方只注册服务到另一注册中心，而不从另一注册中心订阅服务。 禁用订阅配置 &lt;dubbo:registry id="hzRegistry" address="10.20.153.10:9090" /&gt;&lt;dubbo:registry id="qdRegistry" address="10.20.141.150:9090" subscribe="false" /&gt; 或者 &lt;dubbo:registry id="hzRegistry" address="10.20.153.10:9090" /&gt;&lt;dubbo:registry id="qdRegistry" address="10.20.141.150:9090?subscribe=false" /&gt; 静态服务 有时候希望人工管理服务提供者的上线和下线，此时需将注册中心标识为非动态管理模式。 &lt;dubbo:registry address="10.20.141.150:9090" dynamic="false" /&gt; 或者 &lt;dubbo:registry address="10.20.141.150:9090?dynamic=false" /&gt; 服务提供者初次注册时为禁用状态，需人工启用。断线时，将不会被自动删除，需人工禁用。 动态配置 向注册中心写入动态配置覆盖规则。该功能通常由监控中心或治理中心的页面完成。 RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();Registry registry = registryFactory.getRegistry(URL.valueOf("zookeeper://10.20.153.10:2181"));registry.register(URL.valueOf("override://0.0.0.0/com.foo.BarService?category=configurators&amp;dynamic=false&amp;application=foo&amp;timeout=1000")); 其中： override:// - 表示数据采用覆盖方式，支持 override 和 absent，可扩展，必填。 0.0.0.0 - 表示对所有 IP 地址生效，如果只想覆盖某个 IP 的数据，请填入具体 IP，必填。 com.foo.BarService - 表示只对指定服务生效，必填。 category=configurators - 表示该数据为动态配置类型，必填。 dynamic=false - 表示该数据为持久数据，当注册方退出时，数据依然保存在注册中心，必填。 enabled=true - 覆盖规则是否生效，可不填，缺省生效。 application=foo - 表示只对指定应用生效，可不填，表示对所有应用生效。 timeout=1000 - 表示将满足以上条件的 timeout 参数的值覆盖为 1000。如果想覆盖其它参数，直接加在 override 的 URL 参数上。 示例： 禁用提供者：(通常用于临时踢除某台提供者机器，相似的，禁止消费者访问请使用路由规则) override://10.20.153.10/com.foo.BarService?category=configurators&amp;dynamic=false&amp;disbaled=true 调整权重：(通常用于容量评估，缺省权重为 100) override://10.20.153.10/com.foo.BarService?category=configurators&amp;dynamic=false&amp;weight=200 调整负载均衡策略：(缺省负载均衡策略为 random) override://10.20.153.10/com.foo.BarService?category=configurators&amp;dynamic=false&amp;loadbalance=leastactive 服务降级：(通常用于临时屏蔽某个出错的非关键服务) override://0.0.0.0/com.foo.BarService?category=configurators&amp;dynamic=false&amp;application=foo&amp;mock=force:return+null Dubbo 架构 节点角色： 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 调用关系： 务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 整体设计 图例说明： 图中左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口，位于中轴线上的为双方都用到的接口。 图中从下至上分为十层，各层均为单向依赖，右边的黑色箭头代表层之间的依赖关系，每一层都可以剥离上层被复用，其中，Service 和 Config 层为 API，其它各层均为 SPI。 图中绿色小块的为扩展接口，蓝色小块为实现类，图中只显示用于关联各层的实现类。 图中蓝色虚线为初始化过程，即启动时组装链，红色实线为方法调用过程，即运行时调时链，紫色三角箭头为继承，可以把子类看作父类的同一个节点，线上的文字为调用的方法。 各层说明 config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 各层关系说明 在 RPC 中，Protocol 是核心层，也就是只要有 Protocol + Invoker + Exporter 就可以完成非透明的 RPC 调用，然后在 Invoker 的主过程上 Filter 拦截点。 图中的 Consumer 和 Provider 是抽象概念，只是想让看图者更直观的了解哪些类分属于客户端与服务器端，不用 Client 和 Server 的原因是 Dubbo 在很多场景下都使用 Provider, Consumer, Registry, Monitor 划分逻辑拓普节点，保持统一概念。 而 Cluster 是外围概念，所以 Cluster 的目的是将多个 Invoker 伪装成一个 Invoker，这样其它人只要关注 Protocol 层 Invoker 即可，加上 Cluster 或者去掉 Cluster 对其它层都不会造成影响，因为只有一个提供者时，是不需要 Cluster 的。 Proxy 层封装了所有接口的透明化代理，而在其它层都以 Invoker 为中心，只有到了暴露给用户使用时，才用 Proxy 将 Invoker 转成接口，或将接口实现转成 Invoker，也就是去掉 Proxy 层 RPC 是可以 Run 的，只是不那么透明，不那么看起来像调本地服务一样调远程服务。 而 Remoting 实现是 Dubbo 协议的实现，如果你选择 RMI 协议，整个 Remoting 都不会用上，Remoting 内部再划为 Transport 传输层和 Exchange 信息交换层，Transport 层只负责单向消息传输，是对 Mina, Netty, Grizzly 的抽象，它也可以扩展 UDP 传输，而 Exchange 层是在传输层之上封装了 Request-Response 语义。 Registry 和 Monitor 实际上不算一层，而是一个独立的节点，只是为了全局概览，用层的方式画在一起。 资料 Github | 用户手册 | 开发手册 | 管理员手册]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>rpc</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 发布订阅]]></title>
    <url>%2Fblog%2F2018%2F06%2F11%2Fdatabase%2Fnosql%2Fredis%2FRedis%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85%2F</url>
    <content type="text"><![CDATA[Redis 发布订阅 Redis 通过 PUBLISH 、SUBSCRIBE 等命令实现了订阅与发布模式，这个功能提供两种信息机制，分别是订阅/发布到频道和订阅/发布到模式。 命令 描述 SUBSCRIBE 订阅给定的一个或多个频道。 UNSUBSCRIBE 退订给定的一个或多个频道，如果执行时灭有给定任何频道，那么退订所有频道。 PUBLISH 向给定频道发送消息。 PSUBSCRIBE 订阅与给定模式相匹配的所有频道。 PUNSUBSCRIBE 退订给定的模式，如果执行时没有给定任何模式，那么退订所有模式。 频道的订阅与信息发送 Redis 的 SUBSCRIBE 命令可以让客户端订阅任意数量的频道，每当有新信息发送到被订阅的频道时，信息就会被发送给所有订阅指定频道的客户端。 订阅频道 发送信息到频道 模式的订阅与信息发送 资料 Redis 官网 Redis 实战 Redis 设计与实现]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>nosql</tag>
        <tag>key-value</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 持久化]]></title>
    <url>%2Fblog%2F2018%2F06%2F11%2Fdatabase%2Fnosql%2Fredis%2FRedis%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Redis 持久化 Redis 支持持久化，即把数据存储到硬盘中。 Redis 提供了两种持久化方式： RDB 快照（snapshot） - 将存在于某一时刻的所有数据都写入到硬盘中。 只追加文件（append-only file，AOF） - 它会在执行写命令时，将被执行的写命令复制到硬盘中。 这两种持久化方式既可以同时使用，也可以单独使用。 将内存中的数据存储到硬盘的一个主要原因是为了在之后重用数据，或者是为了防止系统故障而将数据备份到一个远程位置。另外，存储在 Redis 里面的数据有可能是经过长时间计算得出的，或者有程序正在使用 Redis 存储的数据进行计算，所以用户会希望自己可以将这些数据存储起来以便之后使用，这样就不必重新计算了。 快照 快照的原理 快照的配置 快照的优点 快照的缺点 AOF AOF 的原理 AOF 的配置 重写/压缩 AOF AOF 的优点 AOF 的缺点 选择持久化方式 怎样从快照方式切换为 AOF 方式 AOF 和快照之间的相互作用 备份 容灾备份 Redis 复制的启动过程 资料 Redis 提供了两种持久方式：RDB 和 AOF。你可以同时开启两种持久化方式。在这种情况下, 当 redis 重启的时候会优先载入 AOF 文件来恢复原始的数据，因为在通常情况下 AOF 文件保存的数据集要比 RDB 文件保存的数据集要完整。 快照 Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。在创建快照之后，用户可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本，还可以将快好留在原地以便重启服务器时使用。 根据配置，快照将被写入 dbfilename 选项指定的文件里面，并存储在 dir 选项指定的路径上面。 创建快照的方法： 客户端可以通过向 Redis 发送 BGSAVE 命令来创建一个快照。对于支持 BGSAVE 命令的平台来说，Redis 会创建一个子进程，然后子进程负责将快照写入到硬盘，而父进程则继续处理命令请求。 客户端还可以通过向 Redis 发送 SAVE 命令来创建一个快照。接到 SAVE 命令的 Redis 服务器在快照创建完毕之前将不再响应任何其他命令。 如果用户设置了 save 配置选项，比如 save 60 10000，当这个条件被满足时，Redis 就会自动触发 BGSAVE 命令。如果用户设置了多个 save 配置选项所设置的条件被满足时，Redis 就会触发一次 BGSAVE 命令。 当 Redis 通过 SHUTDOWN 命令接受到关闭服务器的请求时，或者接收到标准 TERM 信号时，会执行一个 SAVE 命令，阻塞所有客户端，不再执行客户端发送的任何命令，并在 SAVE 命令执行完毕之后关闭服务器。 当一个 Redis 服务器连接另一个 Redis 服务器，并向对方发送 SYNC 命令来开始一次复制操作的时候，如果主服务器目前没有在执行 BGSAVE 操作，或者主服务器并非刚刚执行完 BGSAVE 操作，那么主服务器就会执行 BGSAVE 命令。 快照持久化方式能够在指定的时间间隔能对整个数据进行快照存储。 使用快照持久化来保存数据是，需要记住：如果系统真的发生崩溃，用户将丢失最近一次生成快照之后更改的所有数据。 快照配置： save 60 1000stop-writes-on-bgsave-error nordbcompression yesdbfilename dump.rdb 快照的原理 在默认情况下，Redis 将数据库快照保存在名字为 dump.rdb 的二进制文件中。你可以对 Redis 进行设置， 让它在“N 秒内数据集至少有 M 个改动”这一条件被满足时， 自动保存一次数据集。你也可以通过调用 SAVE 或者 BGSAVE，手动让 Redis 进行数据集保存操作。这种持久化方式被称为快照。 当 Redis 需要保存 dump.rdb 文件时， 服务器执行以下操作: Redis 创建一个子进程。 子进程将数据集写入到一个临时快照文件中。 当子进程完成对新 快照文件的写入时，Redis 用新快照文件替换原来的快照文件，并删除旧的快照文件。 这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益。 快照的配置 比如说， 在 redis.conf 中添加如下配置，表示让 Redis 在满足“ 60 秒内有至少有 1000 个键被改动”这一条件时， 自动保存一次数据集: save 60 10000 快照的优点 RDB 是一个非常紧凑的文件，它保存了某个时间点的数据集，非常适用于数据集的备份。比如你可以在每个小时报保存一下过去 24 小时内的数据，同时每天保存过去 30 天的数据，这样即使出了问题你也可以根据需求恢复到不同版本的数据集。 RDB 是一个紧凑的单一文件，很方便传送到另一个远端数据中心或者亚马逊的 S3（可能加密），非常适用于灾难恢复。 快照在保存 RDB 文件时父进程唯一需要做的就是 fork 出一个子进程，接下来的工作全部由子进程来做，父进程不需要再做其他 IO 操作，所以快照持久化方式可以最大化 redis 的性能。 与 AOF 相比，在恢复大的数据集的时候，DB 方式会更快一些。 快照的缺点 如果你希望在 redis 意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么 快照不适合你。虽然你可以配置不同的 save 时间点(例如每隔 5 分钟并且对数据集有 100 个写的操作)，是 Redis 要完整的保存整个数据集是一个比较繁重的工作，你通常会每隔 5 分钟或者更久做一次完整的保存，万一在 Redis 意外宕机，你可能会丢失几分钟的数据。 快照需要经常 fork 子进程来保存数据集到硬盘上。当数据集比较大的时候，fork 的过程是非常耗时的，可能会导致 Redis 在一些毫秒级内不能响应客户端的请求。如果数据集巨大并且 CPU 性能不是很好的情况下，这种情况会持续 1 秒。AOF 也需要 fork，但是你可以调节重写日志文件的频率来提高数据集的耐久度。 AOF AOF 持久化方式记录每次对服务器执行的写操作。当服务器重启的时候会重新执行这些命令来恢复原始的数据。 AOF 命令以 redis 协议追加保存每次写的操作到文件末尾。Redis 还能对 AOF 文件进行后台重写。使得 AOF 文件的体积不至于过大。 appendonly noappendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb AOF 的原理 Redis 创建一个子进程。 子进程开始将新 AOF 文件的内容写入到临时文件。 对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾，这样样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。 当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。 搞定！现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。 AOF 的配置 AOF 持久化通过在 redis.conf 中的 appendonly yes 配置选项来开启。 可以通过 appendfsync 配置选项来设置同步频率： always - 每个 Redis 写命令都要同步写入硬盘。这样做会严重降低 Redis 的速度。 everysec - 每秒执行一次同步，显示地将多个写命令同步到硬盘。 no - 让操作系统来决定应该何时进行同步。 为了兼顾数据安全和写入性能，推荐使用 appendfsync everysec 选项。Redis 每秒同步一次 AOF 文件时的性能和不使用任何持久化特性时的性能相差无几。 重写/压缩 AOF 随着 Redis 不断运行，AOF 的体积也会不断增长，这将导致两个问题： AOF 耗尽磁盘可用空间。 Redis 重启后需要执行 AOF 文件记录的所有写命令来还原数据集，如果 AOF 过大，则还原操作执行的时间就会非常长。 这个问题的解决方法： 执行 BGREWRITEAOF 命令，这个命令会通过移除 AOF 中的冗余命令来重写 AOF 文件，使 AOF 文件的体积尽可能地小。 BGREWRITEAOF 命令与 BGSAVE 原理类似：通过创建一个子进程，然后由子进程负责对 AOF 文件进行重写。 可以通过设置 auto-aof-rewrite-percentage 和 auto-aof-rewrite-min-size，使得 Redis 在满足条件时，自动执行 BGREWRITEAOF。 假设配置如下： auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 表明，当 AOF 大于 64MB，且 AOF 体积比上一次重写后的体积大了至少 100% 时，执行 BGREWRITEAOF。 AOF 的优点 使用 AOF 会让你的 Redis 更加耐久: 你可以使用不同的 fsync 策略：无 fsync；每秒 fsync；每次写的时候 fsync。使用默认的每秒 fsync 策略，Redis 的性能依然很好(fsync 是由后台线程进行处理的,主线程会尽力处理客户端请求)，一旦出现故障，你最多丢失 1 秒的数据。 AOF 文件是一个只进行追加的日志文件，所以不需要写入 seek，即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令，你也也可使用 redis-check-aof 工具修复这些问题。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写：重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。 AOF 文件有序地保存了对数据库执行的所有写入操作，这些写入操作以 Redis 协议的格式保存。因此 AOF 文件的内容非常容易被人读懂，对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单。举个例子，如果你不小心执行了 FLUSHALL 命令，但只要 AOF 文件未被重写，那么只要停止服务器，移除 AOF 文件末尾的 FLUSHALL 命令，并重启 Redis ，就可以将数据集恢复到 FLUSHALL 执行之前的状态。 AOF 的缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 根据所使用的 fsync 策略，AOF 的速度可能会慢于快照。在一般情况下，每秒 fsync 的性能依然非常高，而关闭 fsync 可以让 AOF 的速度和快照一样快，即使在高负荷之下也是如此。不过在处理巨大的写入载入时，快照可以提供更有保证的最大延迟时间（latency）。 选择持久化方式 如果你只希望你的数据在服务器运行的时候存在，你可以不使用任何持久化方式。 如果你非常关心你的数据，但仍然可以承受数分钟以内的数据丢失，那么你可以只使用 快照持久化。 如果你不能承受数分钟以内的数据丢失，那么你可以同时使用快照持久化和 AOF 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份，并且快照恢复数据集的速度也要比 AOF 恢复的速度要快，除此之外，使用快照还可以避免之前提到的 AOF 程序的 bug 。 怎样从快照方式切换为 AOF 方式 在 Redis 2.2 或以上版本，可以在不重启的情况下，从快照切换到 AOF ： 为最新的 dump.rdb 文件创建一个备份。 将备份放到一个安全的地方。 执行以下两条命令: redis-cli config set appendonly yes redis-cli config set save “” 确保写命令会被正确地追加到 AOF 文件的末尾。 执行的第一条命令开启了 AOF 功能： Redis 会阻塞直到初始 AOF 文件创建完成为止， 之后 Redis 会继续处理命令请求， 并开始将写入命令追加到 AOF 文件末尾。 执行的第二条命令用于关闭快照功能。 这一步是可选的， 如果你愿意的话， 也可以同时使用快照和 AOF 这两种持久化功能。 重要:别忘了在 redis.conf 中打开 AOF 功能！ 否则的话， 服务器重启之后， 之前通过 CONFIG SET 设置的配置就会被遗忘， 程序会按原来的配置来启动服务器。 AOF 和快照之间的相互作用 在版本号大于等于 2.4 的 Redis 中， BGSAVE 执行的过程中， 不可以执行 BGREWRITEAOF 。 反过来说， 在 BGREWRITEAOF 执行的过程中， 也不可以执行 BGSAVE。这可以防止两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。 如果 BGSAVE 正在执行， 并且用户显示地调用 BGREWRITEAOF 命令， 那么服务器将向用户回复一个 OK 状态， 并告知用户， BGREWRITEAOF 已经被预定执行： 一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始。 当 Redis 启动时， 如果快照持久化和 AOF 持久化都被打开了， 那么程序会优先使用 AOF 文件来恢复数据集， 因为 AOF 文件所保存的数据通常是最完整的。 备份 务必确保你的数据有完整的备份。 磁盘故障、节点失效，诸如此类的问题都可能让你的数据消失不见，不进行备份是非常危险的。 备份 Redis 数据建议采用如下策略： 备份 Redis 数据建议采用快照方式。RDB 文件一旦创建，就不会进行任何修改，所以十分安全。 Redis 快照备份过程： 创建一个定期任务（cron job），每小时将一个 RDB 文件备份到一个文件夹，并且每天将一个 RDB 文件备份到另一个文件夹。 确保快照的备份都带有相应的日期和时间信息，每次执行定期任务脚本时，使用 find 命令来删除过期的快照：比如说，你可以保留最近 48 小时内的每小时快照，还可以保留最近一两个月的每日快照。 至少每天一次，将 RDB 备份到你的数据中心之外，或者至少是备份到你运行 Redis 服务器的物理机器之外。 容灾备份 Redis 的容灾备份基本上就是对数据进行备份，并将这些备份传送到多个不同的外部数据中心。 容灾备份可以在 Redis 运行并产生快照的主数据中心发生严重的问题时，仍然让数据处于安全状态。 以下是一些实用的容灾备份方法： Amazon S3 ，以及其他类似 S3 的服务，是一个构建灾难备份系统的好地方。 最简单的方法就是将你的每小时或者每日 RDB 备份加密并传送到 S3 。对数据的加密可以通过 gpg -c 命令来完成（对称加密模式）。记得把你的密码放到几个不同的、安全的地方去（比如你可以把密码复制给你组织里最重要的人物）。同时使用多个储存服务来保存数据文件，可以提升数据的安全性。 传送快照可以使用 SCP 来完成（SSH 的组件）。 以下是简单并且安全的传送方法： 买一个离你的数据中心非常远的 VPS ，装上 SSH ，创建一个无口令的 SSH 客户端 key ，并将这个 key 添加到 VPS 的 authorized_keys 文件中，这样就可以向这个 VPS 传送快照备份文件了。为了达到最好的数据安全性，至少要从两个不同的提供商那里各购买一个 VPS 来进行数据容灾备份。 需要注意的是，这类容灾系统如果没有小心地进行处理的话，是很容易失效的。最低限度下，你应该在文件传送完毕之后，检查所传送备份文件的体积和原始快照文件的体积是否相同。如果你使用的是 VPS ，那么还可以通过比对文件的 SHA1 校验和来确认文件是否传送完整。 另外， 你还需要一个独立的警报系统， 让它在负责传送备份文件的传送器（transfer）失灵时通知你。 Redis 复制的启动过程 当多个从服务器尝试连接同一个主服务器时： 上图步骤 3 尚未执行：所有从服务器都会接收到相同的快照文件和相同的缓冲区写命令。 上图步骤 3 正在执行或已经执行完毕：当主服务器与较早进行连接的从服务器执行完复制所需的 5 个步骤之后，主服务器会与新连接的从服务器执行一次新的步骤 1 至步骤 5。 资料 Redis 官网 Redis 实战 Redis Persistence]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>nosql</tag>
        <tag>key-value</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 事务]]></title>
    <url>%2Fblog%2F2018%2F06%2F11%2Fdatabase%2Fnosql%2Fredis%2FRedis%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Redis 事务 事务简介 EXEC MULTI DISCARD WATCH 取消 WATCH 的场景 使用 WATCH 创建原子操作 Redis 不支持回滚 Redis 脚本和事务 资料 事务简介 事务可以一次执行多个命令，并且有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 EXEC EXEC 命令负责触发并执行事务中的所有命令。 如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。 MULTI MULTI 命令用于开启一个事务，它总是返回 OK。 MULTI 执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当 EXEC 命令被调用时，所有队列中的命令才会被执行。 以下是一个事务例子， 它原子地增加了 foo 和 bar 两个键的值： &gt; MULTIOK&gt; INCR fooQUEUED&gt; INCR barQUEUED&gt; EXEC1) (integer) 12) (integer) 1 DISCARD 当执行 DISCARD 命令时，事务会被放弃，事务队列会被清空，并且客户端会从事务状态中退出。 示例： &gt; SET foo 1OK&gt; MULTIOK&gt; INCR fooQUEUED&gt; DISCARDOK&gt; GET foo"1" WATCH WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回 null 来表示事务已经失败。 WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey $valEXEC 使用上面的代码，如果在 WATCH 执行之后， EXEC 执行之前，有其他客户端修改了 mykey 的值，那么当前客户端的事务就会失败。程序需要做的，就是不断重试这个操作，直到没有发生碰撞为止。 这种形式的锁被称作乐观锁，它是一种非常强大的锁机制。并且因为大多数情况下，不同的客户端会访问不同的键，碰撞的情况一般都很少，所以通常并不需要进行重试。 WATCH 使得 EXEC 命令需要有条件地执行：事务只能在所有被监视键都没有被修改的前提下执行，如果这个前提不能满足的话，事务就不会被执行。 WATCH 命令可以被调用多次。对键的监视从 WATCH 执行之后开始生效，直到调用 EXEC 为止。 用户还可以在单个 WATCH 命令中监视任意多个键，例如： redis&gt; WATCH key1 key2 key3OK 取消 WATCH 的场景 当 EXEC 被调用时，不管事务是否成功执行，对所有键的监视都会被取消。 另外，当客户端断开连接时，该客户端对键的监视也会被取消。 使用无参数的 UNWATCH 命令可以手动取消对所有键的监视。对于一些需要改动多个键的事务，有时候程序需要同时对多个键进行加锁，然后检查这些键的当前值是否符合程序的要求。当值达不到要求时，就可以使用 UNWATCH 命令来取消目前对键的监视，中途放弃这个事务，并等待事务的下次尝试。 使用 WATCH 创建原子操作 WATCH 可以用于创建 Redis 没有内置的原子操作。 举个例子，以下代码实现了原创的 ZPOP 命令，它可以原子地弹出有序集合中分值（score）最小的元素： WATCH zsetelement = ZRANGE zset 0 0MULTIZREM zset elementEXEC Redis 不支持回滚 Redis 不支持回滚的理由： Redis 命令只会因为错误的语法而失败，或是命令用在了错误类型的键上面。 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 Redis 脚本和事务 从定义上来说，Redis 中的脚本本身就是一种事务，所以任何在事务里可以完成的事，在脚本里面也能完成。并且一般来说，使用脚本要来得更简单，并且速度更快。 资料 Redis 官网 事务 Redis 实战]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>nosql</tag>
        <tag>key-value</tag>
        <tag>transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 事件]]></title>
    <url>%2Fblog%2F2018%2F06%2F11%2Fdatabase%2Fnosql%2Fredis%2FRedis%E4%BA%8B%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Redis 事件 Redis 服务器是一个事件驱动程序。 文件事件 服务器通过套接字与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。 Redis 基于 Reactor 模式开发了自己的网络事件处理器，使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用响应的事件处理器。 时间事件 服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。 时间事件又分为： 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 Redis 将所有时间事件都放在一个无序链表中，通过遍历整个链表查找出已到达的时间事件，并调用响应的事件处理器。 事件的调度与执行 服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。 事件调度与执行由 aeProcessEvents 函数负责，伪代码如下： def aeProcessEvents(): # 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() # 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms &lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) # 处理所有已产生的文件事件 procesFileEvents() # 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下： def main(): # 初始化服务器 init_server() # 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() # 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下：]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>nosql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 数据类型]]></title>
    <url>%2Fblog%2F2018%2F06%2F09%2Fdatabase%2Fnosql%2Fredis%2FRedis%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Redis 数据类型 STRING LIST SET HASH ZSET 数据类型 可以存储的值 操作 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 LIST 列表 从两端压入或者弹出元素 读取单个或者多个元素 进行修剪，只保留一个范围内的元素 SET 无序集合 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 ZSET 有序集合 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 STRING 命令： 命令 行为 GET 获取存储在给定键中的值 SET 设置存储在给定键中的值 DEL 删除存储在给定键中的值（这个命令可以用于所有类型） 示例： 127.0.0.1:6379&gt; set name jackOK127.0.0.1:6379&gt; get name"jack"127.0.0.1:6379&gt; del name(integer) 1127.0.0.1:6379&gt; get name(nil) LIST 命令： 命令 行为 RPUSH 将给定值推入列表的右端 LRANGE 获取列表在给定范围上的所有值 LINDEX 获取列表在给定位置上的单个元素 LPOP 从列表的左端弹出一个值，并返回被弹出的值 示例： 127.0.0.1:6379&gt; rpush list item1(integer) 1127.0.0.1:6379&gt; rpush list item2(integer) 2127.0.0.1:6379&gt; rpush list item3(integer) 3127.0.0.1:6379&gt; lrange list 0 -11) "item1"2) "item2"3) "item3"127.0.0.1:6379&gt; lindex list 1"item2"127.0.0.1:6379&gt; lpop list"item1"127.0.0.1:6379&gt; lrange list 0 -11) "item2"2) "item3" SET 命令： 命令 行为 SADD 将给定元素添加到集合 SMEMBERS 返回集合包含的所有元素 SISMEMBER 检查给定元素是否存在于集合中 SREM 如果给定的元素存在于集合中，那么移除这个元素 示例： 127.0.0.1:6379&gt; sadd set item1(integer) 1127.0.0.1:6379&gt; sadd set item2(integer) 1127.0.0.1:6379&gt; sadd set item3(integer) 1127.0.0.1:6379&gt; sadd set item3(integer) 0127.0.0.1:6379&gt; smembers set1) "item3"2) "item2"3) "item1"127.0.0.1:6379&gt; sismember set item2(integer) 1127.0.0.1:6379&gt; sismember set item6(integer) 0127.0.0.1:6379&gt; srem set item2(integer) 1127.0.0.1:6379&gt; srem set item2(integer) 0127.0.0.1:6379&gt; smembers set1) "item3"2) "item1" HASH 命令： 命令 行为 HSET 在散列里面关联起给定的键值对 HGET 获取指定散列键的值 HGETALL 获取散列包含的所有键值对 HDEL 如果给定键存在于散列里面，那么移除这个键 示例： 127.0.0.1:6379&gt; hset myhash key1 value1(integer) 1127.0.0.1:6379&gt; hset myhash key2 value2(integer) 1127.0.0.1:6379&gt; hset myhash key3 value3(integer) 1127.0.0.1:6379&gt; hset myhash key3 value2(integer) 0127.0.0.1:6379&gt; hgetall myhash1) "key1"2) "value1"3) "key2"4) "value2"5) "key3"6) "value2"127.0.0.1:6379&gt; hdel myhash key2(integer) 1127.0.0.1:6379&gt; hdel myhash key2(integer) 0127.0.0.1:6379&gt; hget myhash key2(nil)127.0.0.1:6379&gt; hgetall myhash1) "key1"2) "value1"3) "key3"4) "value2"127.0.0.1:6379&gt; ZSET 命令： 命令 行为 ZADD 将一个带有给定分值得成员添加到有序集合里面 ZRANGE 根据元素在有序排列中所处的位置，从有序集合里面获取多个元素 ZRANGEBYSCORE 获取有序集合在给定分值范围内的所有元素 ZREM 如果给定成员存在于有序集合，那么移除这个成员 示例： 127.0.0.1:6379&gt; zadd zset 1 redis(integer) 1127.0.0.1:6379&gt; zadd zset 2 mongodb(integer) 1127.0.0.1:6379&gt; zadd zset 3 mysql(integer) 1127.0.0.1:6379&gt; zadd zset 3 mysql(integer) 0127.0.0.1:6379&gt; zadd zset 4 mysql(integer) 0127.0.0.1:6379&gt; zrange zset 0 -1 withscores1) "redis"2) "1"3) "mongodb"4) "2"5) "mysql"6) "4"127.0.0.1:6379&gt; zrangebyscore zset 0 2 withscores1) "redis"2) "1"3) "mongodb"4) "2"127.0.0.1:6379&gt; zrem zset mysql(integer) 1127.0.0.1:6379&gt; zrange zset 0 -1 withscores1) "redis"2) "1"3) "mongodb"4) "2"]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>nosql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2Fblog%2F2018%2F06%2F09%2Fjava%2Fjavaweb%2Fdistributed%2Fcache%2Fredis%2F</url>
    <content type="text"><![CDATA[Redis 1. 概述 1.1. Redis 简介 1.2. Redis 的优势 1.3. Redis 与 Memcached 2. 数据类型 2.1. STRING 2.2. LIST 2.3. SET 2.4. HASH 2.5. ZSET 3. 使用场景 4. Redis 管道 5. 键的过期时间 6. 数据淘汰策略 7. 持久化 7.1. 快照持久化 7.2. AOF 持久化 8. 发布与订阅 9. 事务 9.1. EXEC 9.2. MULTI 9.3. DISCARD 9.4. WATCH 10. 事件 10.1. 文件事件 10.2. 时间事件 10.3. 事件的调度与执行 11. 集群 11.1. 复制 11.2. 哨兵 11.3. 分片 12. Redis Client 13. 资料 1. 概述 1.1. Redis 简介 Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射。 键的类型只能为字符串，值支持的五种类型数据类型为：字符串、列表、集合、有序集合、散列表。 Redis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。 1.2. Redis 的优势 性能极高 – Redis 能读的速度是 110000 次/s,写的速度是 81000 次/s。 丰富的数据类型 - 支持字符串、列表、集合、有序集合、散列表。 原子 - Redis 的所有操作都是原子性的。单个操作是原子性的。多个操作也支持事务，即原子性，通过 MULTI 和 EXEC 指令包起来。 持久化 - Redis 支持数据的持久化。可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 备份 - Redis 支持数据的备份，即 master-slave 模式的数据备份。 丰富的特性 - Redis 还支持发布订阅, 通知, key 过期等等特性。 1.3. Redis 与 Memcached Redis 与 Memcached 因为都可以用于缓存，所以常常被拿来做比较，二者主要有以下区别： 数据类型 Memcached 仅支持字符串类型； 而 Redis 支持五种不同种类的数据类型，使得它可以更灵活地解决问题。 数据持久化 Memcached 不支持持久化； Redis 支持两种持久化策略：RDB 快照和 AOF 日志。 分布式 Memcached 不支持分布式，只能通过在客户端使用像一致性哈希这样的分布式算法来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。 Redis Cluster 实现了分布式的支持。 内存管理机制 Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题，但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。 在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘。而 Memcached 的数据则会一直在内存中。 2. 数据类型 数据类型 可以存储的值 操作 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 对整数和浮点数执行自增或者自减操作 LIST 列表 从两端压入或者弹出元素 读取单个或者多个元素 进行修剪，只保留一个范围内的元素 SET 无序集合 添加、获取、移除单个元素 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对 获取所有键值对 检查某个键是否存在 ZSET 有序集合 添加、获取、删除元素 根据分值范围或者成员来获取元素 计算一个键的排名 What Redis data structures look like 2.1. STRING 命令： 命令 行为 GET 获取存储在给定键中的值 SET 设置存储在给定键中的值 DEL 删除存储在给定键中的值（这个命令可以用于所有类型） 示例： 127.0.0.1:6379&gt; set name jackOK127.0.0.1:6379&gt; get name"jack"127.0.0.1:6379&gt; del name(integer) 1127.0.0.1:6379&gt; get name(nil) 2.2. LIST 命令： 命令 行为 RPUSH 获取存储在给定键中的值 LRANGE 设置存储在给定键中的值 LINDEX 删除存储在给定键中的值（这个命令可以用于所有类型） LPOP 删除存储在给定键中的值（这个命令可以用于所有类型） 示例： 127.0.0.1:6379&gt; rpush list item1(integer) 1127.0.0.1:6379&gt; rpush list item2(integer) 2127.0.0.1:6379&gt; rpush list item3(integer) 3127.0.0.1:6379&gt; lrange list 0 -11) "item1"2) "item2"3) "item3"127.0.0.1:6379&gt; lindex list 1"item2"127.0.0.1:6379&gt; lpop list"item1"127.0.0.1:6379&gt; lrange list 0 -11) "item2"2) "item3" 2.3. SET 命令： 命令 行为 SADD 添加一个或多个元素到集合里 SMEMBERS 获取集合里面的所有元素 SISMEMBER 确定一个给定的值是一个集合的成员 SREM 从集合里删除一个或多个元素 示例： 127.0.0.1:6379&gt; sadd set item1(integer) 1127.0.0.1:6379&gt; sadd set item2(integer) 1127.0.0.1:6379&gt; sadd set item3(integer) 1127.0.0.1:6379&gt; sadd set item3(integer) 0127.0.0.1:6379&gt; smembers set1) "item3"2) "item2"3) "item1"127.0.0.1:6379&gt; sismember set item2(integer) 1127.0.0.1:6379&gt; sismember set item6(integer) 0127.0.0.1:6379&gt; srem set item2(integer) 1127.0.0.1:6379&gt; srem set item2(integer) 0127.0.0.1:6379&gt; smembers set1) "item3"2) "item1" 2.4. HASH 命令： 命令 行为 HSET 设置 hash 里面一个字段的值 HGET 获取 hash 中域的值 HGETALL 从 hash 中读取全部的域和值 HDEL 删除一个或多个域 示例： 127.0.0.1:6379&gt; hset myhash key1 value1(integer) 1127.0.0.1:6379&gt; hset myhash key2 value2(integer) 1127.0.0.1:6379&gt; hset myhash key3 value3(integer) 1127.0.0.1:6379&gt; hset myhash key3 value2(integer) 0127.0.0.1:6379&gt; hgetall myhash1) "key1"2) "value1"3) "key2"4) "value2"5) "key3"6) "value2"127.0.0.1:6379&gt; hdel myhash key2(integer) 1127.0.0.1:6379&gt; hdel myhash key2(integer) 0127.0.0.1:6379&gt; hget myhash key2(nil)127.0.0.1:6379&gt; hgetall myhash1) "key1"2) "value1"3) "key3"4) "value2"127.0.0.1:6379&gt; 2.5. ZSET 命令： 命令 行为 ZADD 添加到有序 set 的一个或多个成员，或更新的分数，如果它已经存在 ZRANGE 根据指定的 index 返回，返回 sorted set 的成员列表 ZRANGEBYSCORE 返回有序集合中指定分数区间内的成员，分数由低到高排序。 ZREM 从排序的集合中删除一个或多个成员 示例： 127.0.0.1:6379&gt; zadd zset 1 redis(integer) 1127.0.0.1:6379&gt; zadd zset 2 mongodb(integer) 1127.0.0.1:6379&gt; zadd zset 3 mysql(integer) 1127.0.0.1:6379&gt; zadd zset 3 mysql(integer) 0127.0.0.1:6379&gt; zadd zset 4 mysql(integer) 0127.0.0.1:6379&gt; zrange zset 0 -1 withscores1) "redis"2) "1"3) "mongodb"4) "2"5) "mysql"6) "4"127.0.0.1:6379&gt; zrangebyscore zset 0 2 withscores1) "redis"2) "1"3) "mongodb"4) "2"127.0.0.1:6379&gt; zrem zset mysql(integer) 1127.0.0.1:6379&gt; zrange zset 0 -1 withscores1) "redis"2) "1"3) "mongodb"4) "2" 3. 使用场景 缓存 - 将热点数据放到内存中，设置内存的最大使用量以及过期淘汰策略来保证缓存的命中率。 计数器 - Redis 这种内存数据库能支持计数器频繁的读写操作。 应用限流 - 限制一个网站访问流量。 消息队列 - 使用 List 数据类型，它是双向链表。 查找表 - 使用 HASH 数据类型。 交集运算 - 使用 SET 类型，例如求两个用户的共同好友。 排行榜 - 使用 ZSET 数据类型。 分布式 Session - 多个应用服务器的 Session 都存储到 Redis 中来保证 Session 的一致性。 分布式锁 - 除了可以使用 SETNX 实现分布式锁之外，还可以使用官方提供的 RedLock 分布式锁实现。 4. Redis 管道 Redis 是一种基于 C/S 模型以及请求/响应协议的 TCP 服务。 Redis 支持管道技术。管道技术允许请求以异步方式发送，即旧请求的应答还未返回的情况下，允许发送新请求。这种方式可以大大提高传输效率。 使用管道发送命令时，服务器将被迫回复一个队列答复，占用很多内存。所以，如果你需要发送大量的命令，最好是把他们按照合理数量分批次的处理。 5. 键的过期时间 Redis 可以为每个键设置过期时间，当键过期时，会自动删除该键。 对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。 可以使用 EXPIRE 或 EXPIREAT 来为 key 设置过期时间。 注意：当 EXPIRE 的时间如果设置的是负数，EXPIREAT 设置的时间戳是过期时间，将直接删除 key。 示例： redis&gt; SET mykey "Hello""OK"redis&gt; EXPIRE mykey 10(integer) 1redis&gt; TTL mykey(integer) 10redis&gt; SET mykey "Hello World""OK"redis&gt; TTL mykey(integer) -1redis&gt; 6. 数据淘汰策略 可以设置内存最大使用量，当内存使用量超过时施行淘汰策略，具体有 6 种淘汰策略。 策略 描述 volatile-lru 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰 volatile-ttl 从已设置过期时间的数据集中挑选将要过期的数据淘汰 volatile-random 从已设置过期时间的数据集中任意选择数据淘汰 allkeys-lru 从所有数据集中挑选最近最少使用的数据淘汰 allkeys-random 从所有数据集中任意选择数据进行淘汰 noeviction 禁止驱逐数据 如果使用 Redis 来缓存数据时，要保证所有数据都是热点数据，可以将内存最大使用量设置为热点数据占用的内存量，然后启用 allkeys-lru 淘汰策略，将最近最少使用的数据淘汰。 作为内存数据库，出于对性能和内存消耗的考虑，Redis 的淘汰算法（LRU、TTL）实际实现上并非针对所有 key，而是抽样一小部分 key 从中选出被淘汰 key，抽样数量可通过 maxmemory-samples 配置。 7. 持久化 Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。 7.1. 快照持久化 将某个时间点的所有数据都存放到硬盘上。 可以将快照复制到其它服务器从而创建具有相同数据的服务器副本。 如果系统发生故障，将会丢失最后一次创建快照之后的数据。 如果数据量很大，保存快照的时间会很长。 7.2. AOF 持久化 将写命令添加到 AOF 文件（Append Only File）的末尾。 对硬盘的文件进行写入时，写入的内容首先会被存储到缓冲区，然后由操作系统决定什么时候将该内容同步到硬盘，用户可以调用 file.flush() 方法请求操作系统尽快将缓冲区存储的数据同步到硬盘。可以看出写入文件的数据不会立即同步到硬盘上，在将写命令添加到 AOF 文件时，要根据需求来保证何时同步到硬盘上。 有以下同步选项： 选项 同步频率 always 每个写命令都同步 everysec 每秒同步一次 no 让操作系统来决定何时同步 always 选项会严重减低服务器的性能； everysec 选项比较合适，可以保证系统奔溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响； no 选项并不能给服务器性能带来多大的提升，而且也会增加系统奔溃时数据丢失的数量。 随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令。 8. 发布与订阅 订阅者订阅了频道之后，发布者向频道发送字符串消息会被所有订阅者接收到。 某个客户端使用 SUBSCRIBE 订阅一个频道，其它客户端可以使用 PUBLISH 向这个频道发送消息。 发布与订阅模式和观察者模式有以下不同： 观察者模式中，观察者和主题都知道对方的存在；而在发布与订阅模式中，发布者与订阅者不知道对方的存在，它们之间通过频道进行通信。 观察者模式是同步的，当事件触发时，主题会去调用观察者的方法；而发布与订阅模式是异步的； 9. 事务 MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。 事务可以一次执行多个命令， 并且有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 9.1. EXEC EXEC 命令负责触发并执行事务中的所有命令： 如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。 9.2. MULTI MULTI 命令用于开启一个事务，它总是返回 OK 。 MULTI 执行之后， 客户端可以继续向服务器发送任意多条命令， 这些命令不会立即被执行， 而是被放到一个队列中， 当 EXEC 命令被调用时， 所有队列中的命令才会被执行。 9.3. DISCARD 当执行 DISCARD 命令时， 事务会被放弃， 事务队列会被清空， 并且客户端会从事务状态中退出。 示例： &gt; SET foo 1OK&gt; MULTIOK&gt; INCR fooQUEUED&gt; DISCARDOK&gt; GET foo"1" 9.4. WATCH WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回 nil-reply 来表示事务已经失败。 WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey $valEXEC 使用上面的代码， 如果在 WATCH 执行之后， EXEC 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。 这种形式的锁被称作乐观锁， 它是一种非常强大的锁机制。 并且因为大多数情况下， 不同的客户端会访问不同的键， 碰撞的情况一般都很少， 所以通常并不需要进行重试。 WATCH 使得 EXEC 命令需要有条件地执行：事务只能在所有被监视键都没有被修改的前提下执行，如果这个前提不能满足的话，事务就不会被执行。 WATCH 命令可以被调用多次。对键的监视从 WATCH 执行之后开始生效，直到调用 EXEC 为止。 用户还可以在单个 WATCH 命令中监视任意多个键，例如： redis&gt; WATCH key1 key2 key3OK 当 EXEC 被调用时， 不管事务是否成功执行， 对所有键的监视都会被取消。 另外， 当客户端断开连接时， 该客户端对键的监视也会被取消。 使用无参数的 UNWATCH 命令可以手动取消对所有键的监视。 对于一些需要改动多个键的事务， 有时候程序需要同时对多个键进行加锁， 然后检查这些键的当前值是否符合程序的要求。 当值达不到要求时， 就可以使用 UNWATCH 命令来取消目前对键的监视， 中途放弃这个事务， 并等待事务的下次尝试。 10. 事件 Redis 服务器是一个事件驱动程序。 10.1. 文件事件 服务器通过套接字与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。 Redis 基于 Reactor 模式开发了自己的网络时间处理器，使用 I/O 多路复用程序来同时监听多个套接字，并将到达的时间传送给文件事件分派器，分派器会根据套接字产生的事件类型调用响应的时间处理器。 10.2. 时间事件 服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。 时间事件又分为： 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 Redis 将所有时间事件都放在一个无序链表中，通过遍历整个链表查找出已到达的时间事件，并调用响应的事件处理器。 10.3. 事件的调度与执行 服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能监听太久，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。 事件调度与执行由 aeProcessEvents 函数负责，伪代码如下： def aeProcessEvents(): ## 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() ## 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() ## 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms &lt; 0: remaind_ms = 0 ## 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) ## 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) ## 处理所有已产生的文件事件 procesFileEvents() ## 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下： def main(): ## 初始化服务器 init_server() ## 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() ## 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下： 11. 集群 11.1. 复制 通过使用 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。 一个从服务器只能有一个主服务器，并且不支持主主复制。 12.1. 连接过程 主服务器创建快照文件，发送给从服务器，并在发送期间使用缓冲区记录执行的写命令。快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令； 从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令； 主服务器每执行一次写命令，就向从服务器发送相同的写命令。 12.2. 主从链 随着负载不断上升，主服务器可能无法很快地更新所有从服务器，或者重新连接和重新同步从服务器将导致系统超载。为了解决这个问题，可以创建一个中间层来分担主服务器的复制工作。中间层的服务器是最上层服务器的从服务器，又是最下层服务器的主服务器。 11.2. 哨兵 Sentinel（哨兵）可以监听主服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。 11.3. 分片 分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，也可以从多台机器里面获取数据，这种方法在解决某些问题时可以获得线性级别的性能提升。 假设有 4 个 Reids 实例 R0，R1，R2，R3，还有很多表示用户的键 user:1，user:2，… 等等，有不同的方式来选择一个指定的键存储在哪个实例中。最简单的方式是范围分片，例如用户 id 从 0~1000 的存储到实例 R0 中，用户 id 从 1001~2000 的存储到实例 R1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。还有一种方式是哈希分片，使用 CRC32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。 主要有三种分片方式： 客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。 代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。 服务器分片：Redis Cluster。 12. Redis Client Redis 社区中有多种编程语言的客户端，可以在这里查找合适的客户端：https://redis.io/clients redis 官方推荐的 Java Redis Client： jedis redisson lettuce 13. 资料 Redis 官网 awesome-redis Redis 实战]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>key-value</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 序列化]]></title>
    <url>%2Fblog%2F2018%2F06%2F04%2Fjava%2Fjavacore%2Fbasics%2FJava%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[深入理解 Java 序列化 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「javacore」 简介 序列化和反序列化 Serializable 接口 serialVersionUID 默认序列化机制 非默认序列化机制 transient 关键字 Externalizable 接口 Externalizable 接口的替代方法 readResolve() 方法 序列化工具 小结 参考资料 简介 序列化（serialize） - 序列化是将对象转换为字节流。 反序列化（deserialize） - 反序列化是将字节流转换为对象。 序列化用途 序列化可以将对象的字节序列持久化——保存在内存、文件、数据库中。 在网络上传送对象的字节序列。 RMI(远程方法调用) 🔔 注意：使用 Java 对象序列化，在保存对象时，会把其状态保存为一组字节，在未来，再将这些字节组装成对象。必须注意地是，对象序列化保存的是对象的”状态”，即它的成员变量。由此可知，对象序列化不会关注类中的静态变量。 序列化和反序列化 Java 通过对象输入输出流来实现序列化和反序列化： java.io.ObjectOutputStream 类的 writeObject() 方法可以实现序列化； java.io.ObjectInputStream 类的 readObject() 方法用于实现反序列化。 序列化和反序列化示例： public class SerializeDemo01 &#123; enum Sex &#123; MALE, FEMALE &#125; static class Person implements Serializable &#123; private static final long serialVersionUID = 1L; private String name = null; private Integer age = null; private Sex sex; public Person() &#123; &#125; public Person(String name, Integer age, Sex sex) &#123; this.name = name; this.age = age; this.sex = sex; &#125; @Override public String toString() &#123; return "Person&#123;" + "name='" + name + '\'' + ", age=" + age + ", sex=" + sex + '&#125;'; &#125; &#125; /** * 序列化 */ private static void serialize(String filename) throws IOException &#123; File f = new File(filename); // 定义保存路径 OutputStream out = new FileOutputStream(f); // 文件输出流 ObjectOutputStream oos = new ObjectOutputStream(out); // 对象输出流 oos.writeObject(new Person("Jack", 30, Sex.MALE)); // 保存对象 oos.close(); out.close(); &#125; /** * 反序列化 */ private static void deserialize(String filename) throws IOException, ClassNotFoundException &#123; File f = new File(filename); // 定义保存路径 InputStream in = new FileInputStream(f); // 文件输入流 ObjectInputStream ois = new ObjectInputStream(in); // 对象输入流 Object obj = ois.readObject(); // 读取对象 ois.close(); in.close(); System.out.println(obj); &#125; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; final String filename = "d:/text.dat"; serialize(filename); deserialize(filename); &#125;&#125;// Output:// Person&#123;name='Jack', age=30, sex=MALE&#125; Serializable 接口 被序列化的类必须属于 Enum、Array 和 Serializable 类型其中的任何一种。 如果不是 Enum、Array 的类，如果需要序列化，必须实现 java.io.Serializable 接口，否则将抛出 NotSerializableException 异常。这是因为：在序列化操作过程中会对类型进行检查，如果不满足序列化类型要求，就会抛出异常。 我们不妨做一个小尝试：将 SerializeDemo01 示例中 Person 类改为如下实现，然后看看运行结果。 public class UnSerializeDemo &#123; static class Person &#123; // 其他内容略 &#125; // 其他内容略&#125; 输出：结果就是出现如下异常信息。 Exception in thread "main" java.io.NotSerializableException:... serialVersionUID 请注意 serialVersionUID 字段，你可以在 Java 世界的无数类中看到这个字段。 serialVersionUID 有什么作用，如何使用 serialVersionUID？ serialVersionUID 是 Java 为每个序列化类产生的版本标识。它可以用来保证在反序列时，发送方发送的和接受方接收的是可兼容的对象。如果接收方接收的类的 serialVersionUID 与发送方发送的 serialVersionUID 不一致，会抛出 InvalidClassException。 如果可序列化类没有显式声明 serialVersionUID，则序列化运行时将基于该类的各个方面计算该类的默认 serialVersionUID 值。尽管这样，还是建议在每一个序列化的类中显式指定 serialVersionUID 的值。因为不同的 jdk 编译很可能会生成不同的 serialVersionUID 默认值，从而导致在反序列化时抛出 InvalidClassExceptions 异常。 serialVersionUID 字段必须是 static final long 类型。 我们来举个例子： （1）有一个可序列化类 Person public class Person implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; private Integer age; private String address; // 构造方法、get、set 方法略&#125; （2）开发过程中，对 Person 做了修改，增加了一个字段 email，如下： public class Person implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; private Integer age; private String address; private String email; // 构造方法、get、set 方法略&#125; 由于这个类和老版本不兼容，我们需要修改版本号： private static final long serialVersionUID = 2L; 再次进行反序列化，则会抛出 InvalidClassException 异常。 综上所述，我们大概可以清楚：serialVersionUID 用于控制序列化版本是否兼容。若我们认为修改的可序列化类是向后兼容的，则不修改 serialVersionUID。 默认序列化机制 如果仅仅只是让某个类实现 Serializable 接口，而没有其它任何处理的话，那么就会使用默认序列化机制。 使用默认机制，在序列化对象时，不仅会序列化当前对象本身，还会对其父类的字段以及该对象引用的其它对象也进行序列化。同样地，这些其它对象引用的另外对象也将被序列化，以此类推。所以，如果一个对象包含的成员变量是容器类对象，而这些容器所含有的元素也是容器类对象，那么这个序列化的过程就会较复杂，开销也较大。 注意：这里的父类和引用对象既然要进行序列化，那么它们当然也要满足序列化要求：被序列化的类必须属于 Enum、Array 和 Serializable 类型其中的任何一种。 非默认序列化机制 在现实应用中，有些时候不能使用默认序列化机制。比如，希望在序列化过程中忽略掉敏感数据，或者简化序列化过程。下面将介绍若干影响序列化的方法。 transient 关键字 当某个字段被声明为 transient 后，默认序列化机制就会忽略该字段。 我们将 SerializeDemo01 示例中的内部类 Person 的 age 字段声明为 transient，如下所示： public class SerializeDemo02 &#123; static class Person implements Serializable &#123; transient private Integer age = null; // 其他内容略 &#125; // 其他内容略&#125;// Output:// name: Jack, age: null, sex: MALE 从输出结果可以看出，age 字段没有被序列化。 Externalizable 接口 无论是使用 transient 关键字，还是使用 writeObject() 和 readObject() 方法，其实都是基于 Serializable 接口的序列化。 JDK 中提供了另一个序列化接口–Externalizable。 可序列化类实现 Externalizable 接口之后，基于 Serializable 接口的默认序列化机制就会失效。 我们来基于 SerializeDemo02 再次做一些改动，代码如下： public class ExternalizeDemo01 &#123; static class Person implements Externalizable &#123; transient private Integer age = null; // 其他内容略 private void writeObject(ObjectOutputStream out) throws IOException &#123; out.defaultWriteObject(); out.writeInt(age); &#125; private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125; @Override public void writeExternal(ObjectOutput out) throws IOException &#123; &#125; @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; &#125; &#125; // 其他内容略&#125;// Output:// call Person()// name: null, age: null, sex: null 从该结果，一方面可以看出 Person 对象中任何一个字段都没有被序列化。另一方面，如果细心的话，还可以发现这此次序列化过程调用了 Person 类的无参构造方法。 Externalizable 继承于 Serializable，它增添了两个方法：writeExternal() 与 readExternal()。这两个方法在序列化和反序列化过程中会被自动调用，以便执行一些特殊操作。当使用该接口时，序列化的细节需要由程序员去完成。如上所示的代码，由于 writeExternal() 与 readExternal() 方法未作任何处理，那么该序列化行为将不会保存/读取任何一个字段。这也就是为什么输出结果中所有字段的值均为空。 另外，若使用 Externalizable 进行序列化，当读取对象时，会调用被序列化类的无参构造方法去创建一个新的对象；然后再将被保存对象的字段的值分别填充到新对象中。这就是为什么在此次序列化过程中 Person 类的无参构造方法会被调用。由于这个原因，实现 Externalizable 接口的类必须要提供一个无参的构造方法，且它的访问权限为 public。 对上述 Person 类作进一步的修改，使其能够对 name 与 age 字段进行序列化，但要忽略掉 gender 字段，如下代码所示： public class ExternalizeDemo02 &#123; static class Person implements Externalizable &#123; transient private Integer age = null; // 其他内容略 private void writeObject(ObjectOutputStream out) throws IOException &#123; out.defaultWriteObject(); out.writeInt(age); &#125; private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125; @Override public void writeExternal(ObjectOutput out) throws IOException &#123; out.writeObject(name); out.writeInt(age); &#125; @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; name = (String) in.readObject(); age = in.readInt(); &#125; &#125; // 其他内容略&#125;// Output:// call Person()// name: Jack, age: 30, sex: null Externalizable 接口的替代方法 实现 Externalizable 接口可以控制序列化和反序列化的细节。它有一个替代方法：实现 Serializable 接口，并添加 writeObject(ObjectOutputStream out) 与 readObject(ObjectInputStream in) 方法。序列化和反序列化过程中会自动回调这两个方法。 示例如下所示： public class SerializeDemo03 &#123; static class Person implements Serializable &#123; transient private Integer age = null; // 其他内容略 private void writeObject(ObjectOutputStream out) throws IOException &#123; out.defaultWriteObject(); out.writeInt(age); &#125; private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125; // 其他内容略 &#125; // 其他内容略&#125;// Output:// name: Jack, age: 30, sex: MALE 在 writeObject() 方法中会先调用 ObjectOutputStream 中的 defaultWriteObject() 方法，该方法会执行默认的序列化机制，如上节所述，此时会忽略掉 age 字段。然后再调用 writeInt() 方法显示地将 age 字段写入到 ObjectOutputStream 中。readObject() 的作用则是针对对象的读取，其原理与 writeObject() 方法相同。 注意：writeObject() 与 readObject() 都是 private 方法，那么它们是如何被调用的呢？毫无疑问，是使用反射。详情可见 ObjectOutputStream 中的 writeSerialData 方法，以及 ObjectInputStream 中的 readSerialData 方法。 readResolve() 方法 当我们使用 Singleton 模式时，应该是期望某个类的实例应该是唯一的，但如果该类是可序列化的，那么情况可能会略有不同。此时对第 2 节使用的 Person 类进行修改，使其实现 Singleton 模式，如下所示： public class SerializeDemo04 &#123; enum Sex &#123; MALE, FEMALE &#125; static class Person implements Serializable &#123; private static final long serialVersionUID = 1L; private String name = null; transient private Integer age = null; private Sex sex; static final Person instatnce = new Person("Tom", 31, Sex.MALE); private Person() &#123; System.out.println("call Person()"); &#125; private Person(String name, Integer age, Sex sex) &#123; this.name = name; this.age = age; this.sex = sex; &#125; public static Person getInstance() &#123; return instatnce; &#125; private void writeObject(ObjectOutputStream out) throws IOException &#123; out.defaultWriteObject(); out.writeInt(age); &#125; private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125; public String toString() &#123; return "name: " + this.name + ", age: " + this.age + ", sex: " + this.sex; &#125; &#125; /** * 序列化 */ private static void serialize(String filename) throws IOException &#123; File f = new File(filename); // 定义保存路径 OutputStream out = new FileOutputStream(f); // 文件输出流 ObjectOutputStream oos = new ObjectOutputStream(out); // 对象输出流 oos.writeObject(new Person("Jack", 30, Sex.MALE)); // 保存对象 oos.close(); out.close(); &#125; /** * 反序列化 */ private static void deserialize(String filename) throws IOException, ClassNotFoundException &#123; File f = new File(filename); // 定义保存路径 InputStream in = new FileInputStream(f); // 文件输入流 ObjectInputStream ois = new ObjectInputStream(in); // 对象输入流 Object obj = ois.readObject(); // 读取对象 ois.close(); in.close(); System.out.println(obj); System.out.println(obj == Person.getInstance()); &#125; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; final String filename = "d:/text.dat"; serialize(filename); deserialize(filename); &#125;&#125;// Output:// name: Jack, age: null, sex: MALE// false 值得注意的是，从文件中获取的 Person 对象与 Person 类中的单例对象并不相等。为了能在单例类中仍然保持序列的特性，可以使用 readResolve() 方法。在该方法中直接返回 Person 的单例对象。我们在 SerializeDemo04 示例的基础上添加一个 readObject 方法， 如下所示： public class SerializeDemo05 &#123; // 其他内容略 static class Person implements Serializable &#123; // 添加此方法 private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125; // 其他内容略 &#125; // 其他内容略&#125;// Output:// name: Tom, age: 31, sex: MALE// true 序列化工具 Java 官方的序列化存在许多问题，因此，很多人更愿意使用优秀的第三方序列化工具来替代 Java 自身的序列化机制。 Java 官方的序列化主要体现在以下方面： Java 官方的序列化性能不高，序列化后的数据相对于一些优秀的序列化的工具，还是要大不少，这大大影响存储和传输的效率。 Java 官方的序列化一定需要实现 Serializable 接口。 Java 官方的序列化需要关注 serialVersionUID。 Java 官方的序列无法跨语言使用。 当然我们还有更加优秀的一些序列化和反序列化的工具，根据不同的使用场景可以自行选择！ thrift、protobuf - 适用于对性能敏感，对开发体验要求不高的内部系统。 hessian - 适用于对开发体验敏感，性能有要求的内外部系统。 jackson、gson、fastjson - 适用于对序列化后的数据要求有良好的可读性（转为 json 、xml 形式）。 小结 参考资料 Java 编程思想 JAVA 核心技术（卷 1） http://www.hollischuang.com/archives/1140 http://www.codenuclear.com/serialization-deserialization-java/ http://www.blogjava.net/jiangshachina/archive/2012/02/13/369898.html Java 序列化的高级认识 https://agapple.iteye.com/blog/859052]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
        <tag>serialize</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 序列化]]></title>
    <url>%2Fblog%2F2018%2F06%2F04%2Fjava%2Fjavacore%2Fio%2FJava%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Java 序列化 📓 本文已归档到：「blog」 关键词：Serializable、Externalizable、ObjectInputStream、ObjectOutputStream 简介 定义 用途 序列化和反序列化 Serializable 接口 serialVersionUID 默认序列化机制 非默认序列化机制 transient 关键字 Externalizable 接口 Externalizable 接口的替代方法 readResolve() 方法 总结 推荐阅读 参考资料 简介 定义 序列化：序列化是将对象转换为字节流。 反序列化：反序列化是将字节流转换为对象。 用途 序列化的用途有： 序列化可以将对象的字节序列持久化——保存在内存、文件、数据库中。 在网络上传送对象的字节序列。 RMI(远程方法调用) 序列化和反序列化 Java 通过对象输入输出流来实现序列化和反序列化： 序列化：java.io.ObjectOutputStream 类的 writeObject() 方法可以实现序列化； 反序列化：java.io.ObjectInputStream 类的 readObject() 方法用于实现反序列化。 序列化和反序列化示例： public class SerializeDemo01 &#123; enum Sex &#123; MALE, FEMALE &#125; static class Person implements Serializable &#123; private static final long serialVersionUID = 1L; private String name = null; private Integer age = null; private Sex sex; public Person() &#123; System.out.println("call Person()"); &#125; public Person(String name, Integer age, Sex sex) &#123; this.name = name; this.age = age; this.sex = sex; &#125; public String toString() &#123; return "name: " + this.name + ", age: " + this.age + ", sex: " + this.sex; &#125; &#125; /** * 序列化 */ private static void serialize(String filename) throws IOException &#123; File f = new File(filename); // 定义保存路径 OutputStream out = new FileOutputStream(f); // 文件输出流 ObjectOutputStream oos = new ObjectOutputStream(out); // 对象输出流 oos.writeObject(new Person("Jack", 30, Sex.MALE)); // 保存对象 oos.close(); out.close(); &#125; /** * 反序列化 */ private static void deserialize(String filename) throws IOException, ClassNotFoundException &#123; File f = new File(filename); // 定义保存路径 InputStream in = new FileInputStream(f); // 文件输入流 ObjectInputStream ois = new ObjectInputStream(in); // 对象输入流 Object obj = ois.readObject(); // 读取对象 ois.close(); in.close(); System.out.println(obj); &#125; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; final String filename = "d:/text.dat"; serialize(filename); deserialize(filename); &#125;&#125; 输出： name: Jack, age: 30, sex: MALE Serializable 接口 被序列化的类必须属于 Enum、Array 和 Serializable 类型其中的任何一种。 如果不是 Enum、Array 的类，如果需要序列化，必须实现 java.io.Serializable 接口，否则将抛出 NotSerializableException 异常。这是因为：在序列化操作过程中会对类型进行检查，如果不满足序列化类型要求，就会抛出异常。 我们不妨做一个小尝试：将 SerializeDemo01 示例中 Person 类改为如下实现，然后看看运行结果。 public class UnSerializeDemo &#123; static class Person &#123; // 其他内容略 &#125; // 其他内容略&#125; 输出：结果就是出现如下异常信息。 Exception in thread "main" java.io.NotSerializableException:... serialVersionUID 请注意 serialVersionUID 字段，你可以在 Java 世界的无数类中看到这个字段。 serialVersionUID 有什么作用，如何使用 serialVersionUID？ serialVersionUID 是 Java 为每个序列化类产生的版本标识。它可以用来保证在反序列时，发送方发送的和接受方接收的是可兼容的对象。如果接收方接收的类的 serialVersionUID 与发送方发送的 serialVersionUID 不一致，会抛出 InvalidClassException。 如果可序列化类没有显式声明 serialVersionUID，则序列化运行时将基于该类的各个方面计算该类的默认 serialVersionUID 值。尽管这样，还是建议在每一个序列化的类中显式指定 serialVersionUID 的值。因为不同的 jdk 编译很可能会生成不同的 serialVersionUID 默认值，从而导致在反序列化时抛出 InvalidClassExceptions 异常。 serialVersionUID 字段必须是 static final long 类型。 我们来举个例子： （1）有一个可序列化类 Person public class Person implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; private Integer age; private String address; // 构造方法、get、set 方法略&#125; （2）开发过程中，对 Person 做了修改，增加了一个字段 email，如下： public class Person implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; private Integer age; private String address; private String email; // 构造方法、get、set 方法略&#125; 由于这个类和老版本不兼容，我们需要修改版本号： private static final long serialVersionUID = 2L; 再次进行反序列化，则会抛出 InvalidClassException 异常。 综上所述，我们大概可以清楚：serialVersionUID 用于控制序列化版本是否兼容。若我们认为修改的可序列化类是向后兼容的，则不修改 serialVersionUID。 默认序列化机制 如果仅仅只是让某个类实现 Serializable 接口，而没有其它任何处理的话，那么就是使用默认序列化机制。 使用默认机制，在序列化对象时，不仅会序列化当前对象本身，还会对其父类的字段以及该对象引用的其它对象也进行序列化。同样地，这些其它对象引用的另外对象也将被序列化，以此类推。所以，如果一个对象包含的成员变量是容器类对象，而这些容器所含有的元素也是容器类对象，那么这个序列化的过程就会较复杂，开销也较大。 注意：这里的父类和引用对象既然要进行序列化，那么它们当然也要满足序列化要求：被序列化的类必须属于 Enum、Array 和 Serializable 类型其中的任何一种。 非默认序列化机制 在现实应用中，有些时候不能使用默认序列化机制。比如，希望在序列化过程中忽略掉敏感数据，或者简化序列化过程。下面将介绍若干影响序列化的方法。 transient 关键字 当某个字段被声明为 transient 后，默认序列化机制就会忽略该字段。 我们将 SerializeDemo01 示例中的内部类 Person 的 age 字段声明为 transient，如下所示： public class SerializeDemo02 &#123; static class Person implements Serializable &#123; transient private Integer age = null; // 其他内容略 &#125; // 其他内容略&#125; 输出： name: Jack, age: null, sex: MALE 从输出结果可以看出，age 字段没有被序列化。 Externalizable 接口 无论是使用 transient 关键字，还是使用 writeObject()和 readObject()方法，其实都是基于 Serializable 接口的序列化。 JDK 中提供了另一个序列化接口–Externalizable。 可序列化类实现 Externalizable 接口之后，基于 Serializable 接口的默认序列化机制就会失效。 我们来基于 SerializeDemo02 再次做一些改动，代码如下： public class ExternalizeDemo01 &#123; static class Person implements Externalizable &#123; transient private Integer age = null; // 其他内容略 private void writeObject(ObjectOutputStream out) throws IOException &#123; out.defaultWriteObject(); out.writeInt(age); &#125; private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125; @Override public void writeExternal(ObjectOutput out) throws IOException &#123; &#125; @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; &#125; &#125; // 其他内容略&#125; 输出： call Person()name: null, age: null, sex: null 从该结果，一方面可以看出 Person 对象中任何一个字段都没有被序列化。另一方面，如果细心的话，还可以发现这此次序列化过程调用了 Person 类的无参构造方法。 Externalizable 继承于 Serializable，它增添了两个方法：writeExternal() 与 readExternal()。这两个方法在序列化和反序列化过程中会被自动调用，以便执行一些特殊操作。当使用该接口时，序列化的细节需要由程序员去完成。如上所示的代码，由于 writeExternal() 与 readExternal() 方法未作任何处理，那么该序列化行为将不会保存/读取任何一个字段。这也就是为什么输出结果中所有字段的值均为空。 另外，若使用 Externalizable 进行序列化，当读取对象时，会调用被序列化类的无参构造方法去创建一个新的对象；然后再将被保存对象的字段的值分别填充到新对象中。这就是为什么在此次序列化过程中 Person 类的无参构造方法会被调用。由于这个原因，实现 Externalizable 接口的类必须要提供一个无参的构造方法，且它的访问权限为 public。 对上述 Person 类作进一步的修改，使其能够对 name 与 age 字段进行序列化，但要忽略掉 gender 字段，如下代码所示： public class ExternalizeDemo02 &#123; static class Person implements Externalizable &#123; transient private Integer age = null; // 其他内容略 private void writeObject(ObjectOutputStream out) throws IOException &#123; out.defaultWriteObject(); out.writeInt(age); &#125; private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125; @Override public void writeExternal(ObjectOutput out) throws IOException &#123; out.writeObject(name); out.writeInt(age); &#125; @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; name = (String) in.readObject(); age = in.readInt(); &#125; &#125; // 其他内容略&#125; 输出： call Person()name: Jack, age: 30, sex: null Externalizable 接口的替代方法 实现 Externalizable 接口可以控制序列化和反序列化的细节。它有一个替代方法：实现 Serializable 接口，并添加 writeObject(ObjectOutputStream out) 与 readObject(ObjectInputStream in) 方法。序列化和反序列化过程中会自动回调这两个方法。 示例如下所示： public class SerializeDemo03 &#123; static class Person implements Serializable &#123; transient private Integer age = null; // 其他内容略 private void writeObject(ObjectOutputStream out) throws IOException &#123; out.defaultWriteObject(); out.writeInt(age); &#125; private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125; // 其他内容略 &#125; // 其他内容略&#125; 输出： name: Jack, age: 30, sex: MALE 在 writeObject()方法中会先调用 ObjectOutputStream 中的 defaultWriteObject()方法，该方法会执行默认的序列化机制，如 5.1 节所述，此时会忽略掉 age 字段。然后再调用 writeInt() 方法显示地将 age 字段写入到 ObjectOutputStream 中。readObject() 的作用则是针对对象的读取，其原理与 writeObject()方法相同。 注意：writeObject()与 readObject()都是 private 方法，那么它们是如何被调用的呢？毫无疑问，是使用反射。详情可见 ObjectOutputStream 中的 writeSerialData 方法，以及 ObjectInputStream 中的 readSerialData 方法。 readResolve() 方法 当我们使用 Singleton 模式时，应该是期望某个类的实例应该是唯一的，但如果该类是可序列化的，那么情况可能会略有不同。此时对第 2 节使用的 Person 类进行修改，使其实现 Singleton 模式，如下所示： public class SerializeDemo04 &#123; enum Sex &#123; MALE, FEMALE &#125; static class Person implements Serializable &#123; private static final long serialVersionUID = 1L; private String name = null; transient private Integer age = null; private Sex sex; static final Person instatnce = new Person("Tom", 31, Sex.MALE); private Person() &#123; System.out.println("call Person()"); &#125; private Person(String name, Integer age, Sex sex) &#123; this.name = name; this.age = age; this.sex = sex; &#125; public static Person getInstance() &#123; return instatnce; &#125; private void writeObject(ObjectOutputStream out) throws IOException &#123; out.defaultWriteObject(); out.writeInt(age); &#125; private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125; public String toString() &#123; return "name: " + this.name + ", age: " + this.age + ", sex: " + this.sex; &#125; &#125; /** * 序列化 */ private static void serialize(String filename) throws IOException &#123; File f = new File(filename); // 定义保存路径 OutputStream out = new FileOutputStream(f); // 文件输出流 ObjectOutputStream oos = new ObjectOutputStream(out); // 对象输出流 oos.writeObject(new Person("Jack", 30, Sex.MALE)); // 保存对象 oos.close(); out.close(); &#125; /** * 反序列化 */ private static void deserialize(String filename) throws IOException, ClassNotFoundException &#123; File f = new File(filename); // 定义保存路径 InputStream in = new FileInputStream(f); // 文件输入流 ObjectInputStream ois = new ObjectInputStream(in); // 对象输入流 Object obj = ois.readObject(); // 读取对象 ois.close(); in.close(); System.out.println(obj); System.out.println(obj == Person.getInstance()); &#125; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; final String filename = "d:/text.dat"; serialize(filename); deserialize(filename); &#125;&#125; 输出： name: Jack, age: 30, sex: MALEfalse 值得注意的是，从文件中获取的 Person 对象与 Person 类中的单例对象并不相等。为了能在单例类中仍然保持序列的特性，可以使用 readResolve() 方法。在该方法中直接返回 Person 的单例对象。我们在 SerializeDemo04 示例的基础上添加一个 readObject 方法， 如下所示： public class SerializeDemo05 &#123; // 其他内容略 static class Person implements Serializable &#123; // 添加此方法 private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; in.defaultReadObject(); age = in.readInt(); &#125; // 其他内容略 &#125; // 其他内容略&#125; 输出： name: Jack, age: 30, sex: MALEtrue 总结 通过上面的内容，相各位已经了解了 Java 序列化的使用。这里用一张脑图来总结知识点。 推荐阅读 本文示例代码见：源码 本文同步维护在：Java 系列教程 参考资料 Java 编程思想（Thinking in java） http://www.hollischuang.com/archives/1140 http://www.codenuclear.com/serialization-deserialization-java/ http://www.blogjava.net/jiangshachina/archive/2012/02/13/369898.html https://github.com/giantray/stackoverflow-java-top-qa/blob/master/contents/what-is-a-serialversionuid-and-why-should-i-use-it.md]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>io</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>io</tag>
        <tag>serialize</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 泛型]]></title>
    <url>%2Fblog%2F2018%2F06%2F02%2Fjava%2Fjavacore%2Fbasics%2FJava%E6%B3%9B%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[深入理解 Java 泛型 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「javacore」 为什么需要泛型 泛型类型 泛型类 泛型接口 泛型方法 类型擦除 泛型和继承 类型边界 类型通配符 上界通配符 下界通配符 无界通配符 通配符和向上转型 泛型的约束 泛型最佳实践 泛型命名 使用泛型的建议 小结 参考资料 为什么需要泛型 JDK5 引入了泛型机制。 为什么需要泛型呢？回答这个问题前，先让我们来看一个示例。 public class NoGenericsDemo &#123; public static void main(String[] args) &#123; List list = new ArrayList&lt;&gt;(); list.add("abc"); list.add(18); list.add(new double[] &#123;1.0, 2.0&#125;); Object obj1 = list.get(0); Object obj2 = list.get(1); Object obj3 = list.get(2); System.out.println("obj1 = [" + obj1 + "]"); System.out.println("obj2 = [" + obj2 + "]"); System.out.println("obj3 = [" + obj3 + "]"); int num1 = (int)list.get(0); int num2 = (int)list.get(1); int num3 = (int)list.get(2); System.out.println("num1 = [" + num1 + "]"); System.out.println("num2 = [" + num2 + "]"); System.out.println("num3 = [" + num3 + "]"); &#125;&#125;// Output:// obj1 = [abc]// obj2 = [18]// obj3 = [[D@47089e5f]// Exception in thread "main" java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer// at io.github.dunwu.javacore.generics.NoGenericsDemo.main(NoGenericsDemo.java:23) 示例说明： 在上面的示例中，List 容器没有指定存储数据类型，这种情况下，可以向 List 添加任意类型数据，编译器不会做类型检查，而是默默的将所有数据都转为 Object。 假设，最初我们希望向 List 存储的是整形数据，假设，某个家伙不小心存入了其他数据类型。当你试图从容器中取整形数据时，由于 List 当成 Object 类型来存储，你不得不使用类型强制转换。在运行时，才会发现 List 中数据不存储一致的问题，这就为程序运行带来了很大的风险（无形伤害最为致命）。 而泛型的出现，解决了类型安全问题。 泛型具有以下优点： 编译时的强类型检查 泛型要求在声明时指定实际数据类型，Java 编译器在编译时会对泛型代码做强类型检查，并在代码违反类型安全时发出告警。早发现，早治理，把隐患扼杀于摇篮，在编译时发现并修复错误所付出的代价远比在运行时小。 避免了类型转换 未使用泛型： List list = new ArrayList();list.add("hello");String s = (String) list.get(0); 使用泛型： List&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add("hello");String s = list.get(0); // no cast 泛型编程可以实现通用算法 通过使用泛型，程序员可以实现通用算法，这些算法可以处理不同类型的集合，可以自定义，并且类型安全且易于阅读。 泛型类型 泛型类型是被参数化的类或接口。 泛型类 泛型类的语法形式： class name&lt;T1, T2, ..., Tn&gt; &#123; /* ... */ &#125; 泛型类的声明和非泛型类的声明类似，除了在类名后面添加了类型参数声明部分。由尖括号（&lt;&gt;）分隔的类型参数部分跟在类名后面。它指定类型参数（也称为类型变量）T1，T2，…和 Tn。 一般将泛型中的类名称为原型，而将 &lt;&gt; 指定的参数称为类型参数。 未应用泛型的类 在泛型出现之前，如果一个类想持有一个可以为任意类型的数据，只能使用 Object 做类型转换。示例如下： public class Info &#123; private Object value; public Object getValue() &#123; return value; &#125; public void setValue(Object value) &#123; this.value = value; &#125;&#125; 单类型参数的泛型类 public class Info&lt;T&gt; &#123; private T value; public Info() &#123; &#125; public Info(T value) &#123; this.value = value; &#125; public T getValue() &#123; return value; &#125; public void setValue(T value) &#123; this.value = value; &#125; @Override public String toString() &#123; return "Info&#123;" + "value=" + value + '&#125;'; &#125;&#125;public class GenericsClassDemo01 &#123; public static void main(String[] args) &#123; Info&lt;Integer&gt; info = new Info&lt;&gt;(); info.setValue(10); System.out.println(info.getValue()); Info&lt;String&gt; info2 = new Info&lt;&gt;(); info2.setValue("xyz"); System.out.println(info2.getValue()); &#125;&#125;// Output:// 10// xyz 在上面的例子中，在初始化一个泛型类时，使用 &lt;&gt; 指定了内部具体类型，在编译时就会根据这个类型做强类型检查。 实际上，不使用 &lt;&gt; 指定内部具体类型，语法上也是支持的（不推荐这么做），如下所示： public static void main(String[] args) &#123; Info info = new Info(); info.setValue(10); System.out.println(info.getValue()); info.setValue("abc"); System.out.println(info.getValue());&#125; 示例说明： 上面的例子，不会产生编译错误，也能正常运行。但这样的调用就失去泛型类型的优势。 多个类型参数的泛型类 public class MyMap&lt;K,V&gt; &#123; private K key; private V value; public MyMap(K key, V value) &#123; this.key = key; this.value = value; &#125; @Override public String toString() &#123; return "MyMap&#123;" + "key=" + key + ", value=" + value + '&#125;'; &#125;&#125;public class GenericsClassDemo02 &#123; public static void main(String[] args) &#123; MyMap&lt;Integer, String&gt; map = new MyMap&lt;&gt;(1, "one"); System.out.println(map); &#125;&#125;// Output:// MyMap&#123;key=1, value=one&#125; 泛型类的类型嵌套 public class GenericsClassDemo03 &#123; public static void main(String[] args) &#123; Info&lt;String&gt; info = new Info("Hello"); MyMap&lt;Integer, Info&lt;String&gt;&gt; map = new MyMap&lt;&gt;(1, info); System.out.println(map); &#125;&#125;// Output:// MyMap&#123;key=1, value=Info&#123;value=Hello&#125;&#125; 泛型接口 接口也可以声明泛型。 泛型接口语法形式： public interface Content&lt;T&gt; &#123; T text();&#125; 泛型接口有两种实现方式： 实现接口的子类明确声明泛型类型 public class GenericsInterfaceDemo01 implements Content&lt;Integer&gt; &#123; private int text; public GenericsInterfaceDemo01(int text) &#123; this.text = text; &#125; @Override public Integer text() &#123; return text; &#125; public static void main(String[] args) &#123; GenericsInterfaceDemo01 demo = new GenericsInterfaceDemo01(10); System.out.print(demo.text()); &#125;&#125;// Output:// 10 实现接口的子类不明确声明泛型类型 public class GenericsInterfaceDemo02&lt;T&gt; implements Content&lt;T&gt; &#123; private T text; public GenericsInterfaceDemo02(T text) &#123; this.text = text; &#125; @Override public T text() &#123; return text; &#125; public static void main(String[] args) &#123; GenericsInterfaceDemo02&lt;String&gt; gen = new GenericsInterfaceDemo02&lt;&gt;("ABC"); System.out.print(gen.text()); &#125;&#125;// Output:// ABC 泛型方法 泛型方法是引入其自己的类型参数的方法。泛型方法可以是普通方法、静态方法以及构造方法。 泛型方法语法形式如下： public &lt;T&gt; T func(T obj) &#123;&#125; 是否拥有泛型方法，与其所在的类是否是泛型没有关系。 泛型方法的语法包括一个类型参数列表，在尖括号内，它出现在方法的返回类型之前。对于静态泛型方法，类型参数部分必须出现在方法的返回类型之前。类型参数能被用来声明返回值类型，并且能作为泛型方法得到的实际类型参数的占位符。 使用泛型方法的时候，通常不必指明类型参数，因为编译器会为我们找出具体的类型。这称为类型参数推断（type argument inference）。类型推断只对赋值操作有效，其他时候并不起作用。如果将一个泛型方法调用的结果作为参数，传递给另一个方法，这时编译器并不会执行推断。编译器会认为：调用泛型方法后，其返回值被赋给一个 Object 类型的变量。 public class GenericsMethodDemo01 &#123; public static &lt;T&gt; void printClass(T obj) &#123; System.out.println(obj.getClass().toString()); &#125; public static void main(String[] args) &#123; printClass("abc"); printClass(10); &#125;&#125;// Output:// class java.lang.String// class java.lang.Integer 泛型方法中也可以使用可变参数列表 public class GenericVarargsMethodDemo &#123; public static &lt;T&gt; List&lt;T&gt; makeList(T... args) &#123; List&lt;T&gt; result = new ArrayList&lt;T&gt;(); Collections.addAll(result, args); return result; &#125; public static void main(String[] args) &#123; List&lt;String&gt; ls = makeList("A"); System.out.println(ls); ls = makeList("A", "B", "C"); System.out.println(ls); &#125;&#125;// Output:// [A]// [A, B, C] 类型擦除 Java 语言引入泛型是为了在编译时提供更严格的类型检查，并支持泛型编程。不同于 C++ 的模板机制，Java 泛型是使用类型擦除来实现的，使用泛型时，任何具体的类型信息都被擦除了。 那么，类型擦除做了什么呢？它做了以下工作： 把泛型中的所有类型参数替换为 Object，如果指定类型边界，则使用类型边界来替换。因此，生成的字节码仅包含普通的类，接口和方法。 擦除出现的类型声明，即去掉 &lt;&gt; 的内容。比如 T get() 方法声明就变成了 Object get() ；List&lt;String&gt; 就变成了 List。如有必要，插入类型转换以保持类型安全。 生成桥接方法以保留扩展泛型类型中的多态性。类型擦除确保不为参数化类型创建新类；因此，泛型不会产生运行时开销。 让我们来看一个示例： public class GenericsErasureTypeDemo &#123; public static void main(String[] args) &#123; List&lt;Object&gt; list1 = new ArrayList&lt;Object&gt;(); List&lt;String&gt; list2 = new ArrayList&lt;String&gt;(); System.out.println(list1.getClass()); System.out.println(list2.getClass()); &#125;&#125;// Output:// class java.util.ArrayList// class java.util.ArrayList 示例说明： 上面的例子中，虽然指定了不同的类型参数，但是 list1 和 list2 的类信息却是一样的。 这是因为：使用泛型时，任何具体的类型信息都被擦除了。这意味着：ArrayList&lt;Object&gt; 和 ArrayList&lt;String&gt; 在运行时，JVM 将它们视为同一类型。 Java 泛型的实现方式不太优雅，但这是因为泛型是在 JDK5 时引入的，为了兼容老代码，必须在设计上做一定的折中。 泛型和继承 泛型不能用于显式地引用运行时类型的操作之中，例如：转型、instanceof 操作和 new 表达式。因为所有关于参数的类型信息都丢失了。当你在编写泛型代码时，必须时刻提醒自己，你只是看起来好像拥有有关参数的类型信息而已。 正是由于泛型时基于类型擦除实现的，所以，泛型类型无法向上转型。 向上转型是指用子类实例去初始化父类，这是面向对象中多态的重要表现。 Integer 继承了 Object；ArrayList 继承了 List；但是 List&lt;Interger&gt; 却并非继承了 List&lt;Object&gt;。 这是因为，泛型类并没有自己独有的 Class 类对象。比如：并不存在 List&lt;Object&gt;.class 或是 List&lt;Interger&gt;.class，Java 编译器会将二者都视为 List.class。 List&lt;Integer&gt; list = new ArrayList&lt;&gt;();List&lt;Object&gt; list2 = list; // Erorr 类型边界 有时您可能希望限制可在参数化类型中用作类型参数的类型。类型边界可以对泛型的类型参数设置限制条件。例如，对数字进行操作的方法可能只想接受 Number 或其子类的实例。 要声明有界类型参数，请列出类型参数的名称，然后是 extends 关键字，后跟其限制类或接口。 类型边界的语法形式如下： &lt;T extends XXX&gt; 示例： public class GenericsExtendsDemo01 &#123; static &lt;T extends Comparable&lt;T&gt;&gt; T max(T x, T y, T z) &#123; T max = x; // 假设x是初始最大值 if (y.compareTo(max) &gt; 0) &#123; max = y; //y 更大 &#125; if (z.compareTo(max) &gt; 0) &#123; max = z; // 现在 z 更大 &#125; return max; // 返回最大对象 &#125; public static void main(String[] args) &#123; System.out.println(max(3, 4, 5)); System.out.println(max(6.6, 8.8, 7.7)); System.out.println(max("pear", "apple", "orange")); &#125;&#125;// Output:// 5// 8.8// pear 示例说明： 上面的示例声明了一个泛型方法，类型参数 T extends Comparable&lt;T&gt; 表明传入方法中的类型必须实现了 Comparable 接口。 类型边界可以设置多个，语法形式如下： &lt;T extends B1 &amp; B2 &amp; B3&gt; 注意：extends 关键字后面的第一个类型参数可以是类或接口，其他类型参数只能是接口。 示例： public class GenericsExtendsDemo02 &#123; static class A &#123; /* ... */ &#125; interface B &#123; /* ... */ &#125; interface C &#123; /* ... */ &#125; static class D1 &lt;T extends A &amp; B &amp; C&gt; &#123; /* ... */ &#125; static class D2 &lt;T extends B &amp; A &amp; C&gt; &#123; /* ... */ &#125; // 编译报错 static class E extends A implements B, C &#123; /* ... */ &#125; public static void main(String[] args) &#123; D1&lt;E&gt; demo1 = new D1&lt;&gt;(); System.out.println(demo1.getClass().toString()); D1&lt;String&gt; demo2 = new D1&lt;&gt;(); // 编译报错 &#125;&#125; 类型通配符 类型通配符一般是使用 ? 代替具体的类型参数。例如 List&lt;?&gt; 在逻辑上是 List&lt;String&gt; ，List&lt;Integer&gt; 等所有 List&lt;具体类型实参&gt; 的父类。 上界通配符 可以使用**上界通配符**来缩小类型参数的类型范围。 它的语法形式为：&lt;? extends Number&gt; public class GenericsUpperBoundedWildcardDemo &#123; public static double sumOfList(List&lt;? extends Number&gt; list) &#123; double s = 0.0; for (Number n : list) &#123; s += n.doubleValue(); &#125; return s; &#125; public static void main(String[] args) &#123; List&lt;Integer&gt; li = Arrays.asList(1, 2, 3); System.out.println("sum = " + sumOfList(li)); &#125;&#125;// Output:// sum = 6.0 下界通配符 **下界通配符**将未知类型限制为该类型的特定类型或超类类型。 注意：上界通配符和下界通配符不能同时使用。 它的语法形式为：&lt;? super Number&gt; public class GenericsLowerBoundedWildcardDemo &#123; public static void addNumbers(List&lt;? super Integer&gt; list) &#123; for (int i = 1; i &lt;= 5; i++) &#123; list.add(i); &#125; &#125; public static void main(String[] args) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); addNumbers(list); System.out.println(Arrays.deepToString(list.toArray())); &#125;&#125;// Output:// [1, 2, 3, 4, 5] 无界通配符 无界通配符有两种应用场景： 可以使用 Object 类中提供的功能来实现的方法。 使用不依赖于类型参数的泛型类中的方法。 语法形式：&lt;?&gt; public class GenericsUnboundedWildcardDemo &#123; public static void printList(List&lt;?&gt; list) &#123; for (Object elem : list) &#123; System.out.print(elem + " "); &#125; System.out.println(); &#125; public static void main(String[] args) &#123; List&lt;Integer&gt; li = Arrays.asList(1, 2, 3); List&lt;String&gt; ls = Arrays.asList("one", "two", "three"); printList(li); printList(ls); &#125;&#125;// Output:// 1 2 3// one two three 通配符和向上转型 前面，我们提到：泛型不能向上转型。但是，我们可以通过使用通配符来向上转型。 public class GenericsWildcardDemo &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; intList = new ArrayList&lt;&gt;(); List&lt;Number&gt; numList = intList; // Error List&lt;? extends Integer&gt; intList2 = new ArrayList&lt;&gt;(); List&lt;? extends Number&gt; numList2 = intList2; // OK &#125;&#125; 扩展阅读：Oracle 泛型文档 泛型的约束 泛型类型的类型参数不能是值类型 Pair&lt;int, char&gt; p = new Pair&lt;&gt;(8, 'a'); // 编译错误 不能创建类型参数的实例 public static &lt;E&gt; void append(List&lt;E&gt; list) &#123; E elem = new E(); // 编译错误 list.add(elem);&#125; 不能声明类型为类型参数的静态成员 public class MobileDevice&lt;T&gt; &#123; private static T os; // error // ...&#125; 类型参数不能使用类型转换或 instanceof public static &lt;E&gt; void rtti(List&lt;E&gt; list) &#123; if (list instanceof ArrayList&lt;Integer&gt;) &#123; // 编译错误 // ... &#125;&#125; List&lt;Integer&gt; li = new ArrayList&lt;&gt;();List&lt;Number&gt; ln = (List&lt;Number&gt;) li; // 编译错误 不能创建类型参数的数组 List&lt;Integer&gt;[] arrayOfLists = new List&lt;Integer&gt;[2]; // 编译错误 不能创建、catch 或 throw 参数化类型对象 // Extends Throwable indirectlyclass MathException&lt;T&gt; extends Exception &#123; /* ... */ &#125; // 编译错误// Extends Throwable directlyclass QueueFullException&lt;T&gt; extends Throwable &#123; /* ... */ // 编译错误 public static &lt;T extends Exception, J&gt; void execute(List&lt;J&gt; jobs) &#123; try &#123; for (J job : jobs) // ... &#125; catch (T e) &#123; // compile-time error // ... &#125;&#125; 仅仅是泛型类相同，而类型参数不同的方法不能重载 public class Example &#123; public void print(Set&lt;String&gt; strSet) &#123; &#125; public void print(Set&lt;Integer&gt; intSet) &#123; &#125; // 编译错误&#125; 泛型最佳实践 泛型命名 泛型一些约定俗成的命名： E - Element K - Key N - Number T - Type V - Value S,U,V etc. - 2nd, 3rd, 4th types 使用泛型的建议 消除类型检查告警 List 优先于数组 优先考虑使用泛型来提高代码通用性 优先考虑泛型方法来限定泛型的范围 利用有限制通配符来提升 API 的灵活性 优先考虑类型安全的异构容器 小结 参考资料 Java 编程思想 JAVA 核心技术（卷 1） Effective java Oracle 泛型文档 Java 泛型详解]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
        <tag>generic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红黑树]]></title>
    <url>%2Fblog%2F2018%2F06%2F01%2Falgorithm%2Fdata-structure%2Ftree%2Fred-black-tree%2F</url>
    <content type="text"><![CDATA[红黑树 红黑树（英语：Red–black tree）是一种自平衡二叉查找树，是在计算机科学中用到的一种数据结构，典型的用途是实现关联数组。它是复杂的，但它的操作有着良好的最坏情况运行时间，并且在实践中是高效的：它可以在 O(log⁡2N)O(\log_2 N)O(log2​N) 时间内做查找，插入和删除，这里的 n 是树中元素的数目。 红黑树的性质 红黑树，顾名思义，通过红黑两种颜色域保证树的高度近似平衡。它的每个节点是一个五元组：color（颜色），key（数据），left（左孩子），right（右孩子）和 p（父节点）。 红黑树的定义也是它的性质，有以下五条： 节点是红色或黑色。 根是黑色。 所有叶子都是黑色（叶子是 NIL 节点）。 每个红色节点必须有两个黑色的子节点。（从每个叶子到根的所有路径上不能有两个连续的红色节点。 从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。 这五个性质强制了红黑树的关键性质: 从根到叶子的最长的可能路径不多于最短的可能路径的两倍长。为什么呢？性质 4 暗示着任何一个简单路径上不能有两个毗连的红色节点，这样，最短的可能路径全是黑色节点，最长的可能路径有交替的红色和黑色节点。同时根据性质 5 知道：所有最长的路径都有相同数目的黑色节点，这就表明了没有路径能多于任何其他路径的两倍长。 红黑树的操作 因为红黑树也是二叉查找树，因此红黑树上的查找操作与普通二叉查找树上的查找操作相同。然而，红黑树上的插入操作和删除操作会导致不再符合红黑树的性质。恢复红黑树的性质需要少量(O(log⁡2N)O(\log_2 N)O(log2​N))的颜色变更(实际是非常快速的)和不超过三次树旋转(对于插入操作是两次)。虽然插入和删除很复杂，但操作时间仍可以保持为 O(log⁡2N)O(\log_2 N)O(log2​N) 次。 插入操作 插入操作可以概括为以下几个步骤： 查找要插入的位置，时间复杂度为：O(N)O(N)O(N) 将新节点的 color 赋为红色 自下而上重新调整该树为红黑树 其中，第 1 步的查找方法跟普通二叉查找树一样，第 2 步之所以将新插入的节点的颜色赋为红色，是因为：如果设为黑色，就会导致根到叶子的路径上有一条路上，多一个额外的黑节点，这个是很难调整的。但是设为红色节点后，可能会导致出现两个连续红色节点的冲突，那么可以通过颜色调换（color flips）和树旋转来调整，这样简单多了。下面讨论步骤 3 的一些细节： 设要插入的节点为 N，其父节点为 P，其父节点 P 的兄弟节点为 U（即 P 和 U 是同一个节点的两个子节点）。 如果 P 是黑色的，则整棵树不必调整便是红黑树。 如果 P 是红色的（可知，其父节点 G 一定是黑色的），则插入 N 后，违背了性质 4，需要进行调整。调整时分以下 3 种情况： 3.1. 如果父节点 P 和叔父节点 U 二者都是红色 如上图所示，我们将 P 和 U 重绘为黑色，并重绘节点 G 为红色(用来保持性质 5)。 现在新节点 N 有了一个黑色的父节点 P，因为通过父节点 P 或叔父节点 U 的任何路径都必定通过祖父节点 G，在这些路径上的黑节点数目没有改变。 但是，红色的祖父节点 G 的父节点也有可能是红色的，这就违反了性质 4。为了解决这个问题，我们在祖父节点 G 上递归调整颜色。 3.2. 父节点 P 是红色而叔父节点 U 是黑色或缺少，新节点 N 是右孩子节点，而父节点 P 又是其父节点 G 的左孩子节点。 在这种情形下，我们进行一次左旋转调换新节点和其父节点的角色；接着，我们按情形 3.3 处理以前的父节点 P 以解决仍然失效的性质 4。注意这个改变会导致某些路径通过它们以前不通过的新节点 N（比如图中 1 号叶子节点）或不通过节点 P（比如图中 3 号叶子节点），但由于这两个节点都是红色的，所以性质 5 仍有效。 3.3. 父节点 P 是红色而叔父节点 U 是黑色或缺少，新节点 N 是左孩子节点，而父节点 P 又是其父节点 G 的左孩子节点。 在这种情形下，我们进行针对祖父节点 G 的一次右旋转；在旋转产生的树中，以前的父节点 P 现在是新节点 N 和以前的祖父节点 G 的父节点。我们知道以前的祖父节点 G 是黑色，否则父节点 P 就不可能是红色（如果 P 和 G 都是红色就违反了性质 4，所以 G 必须是黑色）。我们切换以前的父节点 P 和祖父节点 G 的颜色，结果的树满足性质 4。性质 5 也仍然保持满足，因为通过这三个节点中任何一个的所有路径以前都通过祖父节点 G，现在它们都通过以前的父节点 P。在各自的情形下，这都是三个节点中唯一的黑色节点。 删除操作 删除操作可以概括为以下几个步骤： 查找要删除位置，时间复杂度为：O(N) 用删除节点后继或者节点替换该节点（只进行数据替换即可，不必调整指针，后继节点是中序遍历中紧挨着该节点的节点，即：右孩子的最左孩子节点） 如果删除节点的替换节点为黑色，则需重新调整该树为红黑树 其中，第 1 步的查找方法跟普通二叉查找树一样，第 2 步之所以用后继节点替换删除节点，是因为这样可以保证该后继节点之上仍是一个红黑树，而后继节点可能是一个叶节点或者只有右子树的节点，这样只需用有节点替换后继节点即可达到删除的目的。如果需要删除的节点有两个儿子，那么问题可以被转化成删除另一个只有一个儿子的节点的问题。 在第 3 步中 如果，如果删除节点为红色节点，则他的父亲和孩子全为黑节点，这样直接删除该节点即可，不必进行任何调整。 如果删除节点是黑节点，分四种情况： 设要删除的节点为 N，其父节点为 P，其兄弟节点为 S。 由于 N 是黑色的，则 P 可能是黑色的，也可能是红色的，S 也可能是黑色的或者红色的 3.1 S 是红色的 此时 P 肯定是红色的。我们对 N 的父节点进行左旋转，然后把红色兄弟转换成 N 的祖父。我们接着对调 N 的父亲和祖父的颜色。尽管所有的路径仍然有相同数目的黑色节点，现在 N 有了一个黑色的兄弟和一个红色的父亲，所以我们可以接下去按 (2)、(3)或(4)情况来处理。 3.2 S和S的孩子全是黑色的 在这种情况下，P 可能是黑色的或者红色的，我们简单的重绘 S 为红色。结果是通过 S 的所有路径，它们就是以前不通过 N 的那些路径，都少了一个黑色节点。因为删除 N 的初始的父亲使通过 N 的所有路径少了一个黑色节点，这使事情都平衡了起来。但是，通过 P 的所有路径现在比不通过 P 的路径少了一个黑色节点。接下来，要调整以 P 作为 N 递归调整树。 3.3 S是黑色的，S的左孩子是红色，右孩子是黑色 这种情况下我们在 S 上做右旋转，这样 S 的左儿子成为 S 的父亲和 N 的新兄弟。我们接着交换 S 和它的新父亲的颜色。所有路径仍有同样数目的黑色节点，但是现在 N 有了一个右儿子是红色的黑色兄弟，所以我们进入了情况（4）。N 和它的父亲都不受这个变换的影响。 3.4 S是黑色的，S的右孩子是红色 在这种情况下我们在 N 的父亲上做左旋转，这样 S 成为 N 的父亲和 S 的右儿子的父亲。我们接着交换 N 的父亲和 S 的颜色，并使 S 的右儿子为黑色。子树在它的根上的仍是同样的颜色，所以属性 3 没有被违反。但是，N 现在增加了一个黑色祖先: 要么 N 的父亲变成黑色，要么它是黑色而 S 被增加为一个黑色祖父。所以，通过 N 的路径都增加了一个黑色节点。 示例代码 红黑树插入操作调整 fixAfterInsertion 方法摘自 JDK8 的 TreeMap.java。 阅读本示例前，请参看本文的“插入操作”一节。 private void fixAfterInsertion(Entry&lt;K,V&gt; x) &#123; // 2. 将新节点的 color 赋为红色 x.color = RED; // 3. 自下而上重新调整该树为红黑树 while (x != null &amp;&amp; x != root &amp;&amp; x.parent.color == RED) &#123; // 如果父节点是黑色的，则整棵树不必调整便是红黑树。 if (parentOf(x) == leftOf(parentOf(parentOf(x)))) &#123; // 父节点是祖父节点的左节点 Entry&lt;K,V&gt; y = rightOf(parentOf(parentOf(x))); // 叔叔节点 if (colorOf(y) == RED) &#123; // 3.1 叔叔节点是红色的 setColor(parentOf(x), BLACK); setColor(y, BLACK); setColor(parentOf(parentOf(x)), RED); x = parentOf(parentOf(x)); &#125; else &#123; // 3.2 新节点是右孩子节点：左旋新节点和父节点；调换新节点和父节点的颜色；右旋祖父节点 if (x == rightOf(parentOf(x))) &#123; x = parentOf(x); rotateLeft(x); // 父节点左旋 &#125; setColor(parentOf(x), BLACK); setColor(parentOf(parentOf(x)), RED); rotateRight(parentOf(parentOf(x))); &#125; &#125; else &#123; // 父节点是祖父节点的右节点 Entry&lt;K,V&gt; y = leftOf(parentOf(parentOf(x))); // 叔叔节点 if (colorOf(y) == RED) &#123; // 3.1 叔叔节点是红色的 setColor(parentOf(x), BLACK); setColor(y, BLACK); setColor(parentOf(parentOf(x)), RED); x = parentOf(parentOf(x)); &#125; else &#123; // 新节点是左孩子节点 if (x == leftOf(parentOf(x))) &#123; x = parentOf(x); rotateRight(x); // 父节点右旋 &#125; setColor(parentOf(x), BLACK); // 原父亲节点设为黑色 setColor(parentOf(parentOf(x)), RED); // 原祖父节点设为红色 rotateLeft(parentOf(parentOf(x))); &#125; &#125; &#125; root.color = BLACK;&#125; 红黑树删除操作调整 fixAfterDeletion 方法摘自 JDK8 的 TreeMap.java。 阅读本示例前，请参看本文的“删除操作”一节。 private void fixAfterDeletion(Entry&lt;K,V&gt; x) &#123; while (x != root &amp;&amp; colorOf(x) == BLACK) &#123; if (x == leftOf(parentOf(x))) &#123; Entry&lt;K,V&gt; sib = rightOf(parentOf(x)); if (colorOf(sib) == RED) &#123; setColor(sib, BLACK); setColor(parentOf(x), RED); rotateLeft(parentOf(x)); sib = rightOf(parentOf(x)); &#125; if (colorOf(leftOf(sib)) == BLACK &amp;&amp; colorOf(rightOf(sib)) == BLACK) &#123; setColor(sib, RED); x = parentOf(x); &#125; else &#123; if (colorOf(rightOf(sib)) == BLACK) &#123; setColor(leftOf(sib), BLACK); setColor(sib, RED); rotateRight(sib); sib = rightOf(parentOf(x)); &#125; setColor(sib, colorOf(parentOf(x))); setColor(parentOf(x), BLACK); setColor(rightOf(sib), BLACK); rotateLeft(parentOf(x)); x = root; &#125; &#125; else &#123; // symmetric Entry&lt;K,V&gt; sib = leftOf(parentOf(x)); if (colorOf(sib) == RED) &#123; setColor(sib, BLACK); setColor(parentOf(x), RED); rotateRight(parentOf(x)); sib = leftOf(parentOf(x)); &#125; if (colorOf(rightOf(sib)) == BLACK &amp;&amp; colorOf(leftOf(sib)) == BLACK) &#123; setColor(sib, RED); x = parentOf(x); &#125; else &#123; if (colorOf(leftOf(sib)) == BLACK) &#123; setColor(rightOf(sib), BLACK); setColor(sib, RED); rotateLeft(sib); sib = leftOf(parentOf(x)); &#125; setColor(sib, colorOf(parentOf(x))); setColor(parentOf(x), BLACK); setColor(leftOf(sib), BLACK); rotateRight(parentOf(x)); x = root; &#125; &#125; &#125; setColor(x, BLACK);&#125; 资料 https://zh.wikipedia.org/wiki/红黑树]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 容器之 Map]]></title>
    <url>%2Fblog%2F2018%2F05%2F31%2Fjava%2Fjavacore%2Fcontainer%2Fjava-container-map%2F</url>
    <content type="text"><![CDATA[Java 容器之 Map 📓 本文已归档到：「blog」 Map 架构 Map 接口 Map.Entry 接口 AbstractMap 抽象类 SortedMap 接口 NavigableMap 接口 Dictionary 抽象类 HashMap 类 LinkedHashMap 类 TreeMap 类 WeakHashMap 资料 Map 架构 Map 是映射接口，Map 中存储的内容是键值对(key-value)。 AbstractMap 是继承于 Map 的抽象类，它实现了 Map 中的大部分 API。其它 Map 的实现类可以通过继承 AbstractMap 来减少重复编码。 SortedMap 是继承于 Map 的接口。SortedMap 中的内容是排序的键值对，排序的方法是通过比较器(Comparator)。 NavigableMap 是继承于 SortedMap 的接口。相比于 SortedMap，NavigableMap 有一系列的导航方法；如&quot;获取大于/等于某对象的键值对&quot;、“获取小于/等于某对象的键值对”等等。 TreeMap 继承于 AbstractMap，且实现了 NavigableMap 接口；因此，TreeMap 中的内容是“有序的键值对”！ HashMap 继承于 AbstractMap，但没实现 NavigableMap 接口；因此，HashMap 的内容是“键值对，但不保证次序”！ Hashtable 虽然不是继承于 AbstractMap，但它继承于 Dictionary(Dictionary 也是键值对的接口)，而且也实现 Map 接口；因此，Hashtable 的内容也是“键值对，也不保证次序”。但和 HashMap 相比，Hashtable 是线程安全的，而且它支持通过 Enumeration 去遍历。 WeakHashMap 继承于 AbstractMap。它和 HashMap 的键类型不同，WeakHashMap 的键是弱键。 Map 接口 Map 的定义如下： public interface Map&lt;K,V&gt; &#123; &#125; Map 是一个键值对(key-value)映射接口。Map 映射中不能包含重复的键；每个键最多只能映射到一个值。 Map 接口提供三种 collection 视图，允许以键集、值集或键-值映射关系集的形式查看某个映射的内容。 Map 映射顺序。有些实现类，可以明确保证其顺序，如 TreeMap；另一些映射实现则不保证顺序，如 HashMap 类。 Map 的实现类应该提供 2 个“标准的”构造方法： void（无参数）构造方法，用于创建空映射； 带有单个 Map 类型参数的构造方法，用于创建一个与其参数具有相同键-值映射关系的新映射。 实际上，后一个构造方法允许用户复制任意映射，生成所需类的一个等价映射。尽管无法强制执行此建议（因为接口不能包含构造方法），但是 JDK 中所有通用的映射实现都遵从它。 Map.Entry 接口 Map.Entry 一般用于迭代访问 Map。 Map.Entry 是 Map 中内部的一个接口，Map.Entry 是键值对，Map 通过 entrySet() 获取 Map.Entry 的键值对集合，从而通过该集合实现对键值对的操作。 AbstractMap 抽象类 AbstractMap 的定义如下： public abstract class AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt; &#123;&#125; AbstractMap 类提供 Map 接口的骨干实现，以最大限度地减少实现 Map 接口所需的工作。 要实现不可修改的映射，编程人员只需扩展此类并提供 entrySet() 方法的实现即可，该方法将返回映射的映射关系 set 视图。通常，返回的 set 将依次在 AbstractSet 上实现。此 set 不支持 add() 或 remove() 方法，其迭代器也不支持 remove() 方法。 要实现可修改的映射，编程人员必须另外重写此类的 put 方法（否则将抛出 UnsupportedOperationException），entrySet().iterator() 返回的迭代器也必须另外实现其 remove() 方法。 SortedMap 接口 SortedMap 的定义如下： public interface SortedMap&lt;K,V&gt; extends Map&lt;K,V&gt; &#123; &#125; SortedMap 是一个继承了 Map 接口的接口。它是一个有序的键值映射。 SortedMap 的排序方式有两种：自然排序或者用户指定比较器。插入有序 SortedMap 的所有元素都必须实现 Comparable 接口（或者被指定的比较器所接受）。 另外，所有 SortedMap 实现类都应该提供 4 个“标准”构造方法： void（无参数）构造方法，它创建一个空的有序映射，按照键的自然顺序进行排序。 带有一个 Comparator 类型参数的构造方法，它创建一个空的有序映射，根据指定的比较器进行排序。 带有一个 Map 类型参数的构造方法，它创建一个新的有序映射，其键-值映射关系与参数相同，按照键的自然顺序进行排序。 带有一个 SortedMap 类型参数的构造方法，它创建一个新的有序映射，其键-值映射关系和排序方法与输入的有序映射相同。无法保证强制实施此建议，因为接口不能包含构造方法。 NavigableMap 接口 NavigableMap 的定义如下： public interface NavigableMap&lt;K,V&gt; extends SortedMap&lt;K,V&gt; &#123; &#125; NavigableMap 是继承于 SortedMap 的接口。它是一个可导航的键-值对集合，具有了为给定搜索目标报告最接近匹配项的导航方法。 NavigableMap 分别提供了获取“键”、“键-值对”、“键集”、“键-值对集”的相关方法。 NavigableMap 除了继承 SortedMap 的特性外，它的提供的功能可以分为 4 类： 提供操作键-值对的方法。 lowerEntry、floorEntry、ceilingEntry 和 higherEntry 方法，它们分别返回与小于、小于等于、大于等于、大于给定键的键关联的 Map.Entry 对象。 firstEntry、pollFirstEntry、lastEntry 和 pollLastEntry 方法，它们返回和/或移除最小和最大的映射关系（如果存在），否则返回 null。 提供操作键的方法。这个和第 1 类比较类似。 lowerKey、floorKey、ceilingKey 和 higherKey 方法，它们分别返回与小于、小于等于、大于等于、大于给定键的键。 获取键集。 navigableKeySet、descendingKeySet 分别获取正序/反序的键集。 获取键-值对的子集。 Dictionary 抽象类 Dictionary 的定义如下： public abstract class Dictionary&lt;K,V&gt; &#123;&#125; NavigableMap 是 JDK 1.0 定义的键值对的接口，它也包括了操作键值对的基本方法。 HashMap 类 HashMap 要点 HashMap 是一个散列表，它存储的内容是键值对(key-value)映射。 基于哈希表的 Map 接口实现。该实现提供了所有可选的 Map 操作，并允许使用空值和空键。 （HashMap 类大致等同于 Hashtable，除了它是不同步的并且允许为空值。）这个类不保序；特别是，它的元素顺序可能会随着时间的推移变化。 HashMap 的一个实例有两个影响其性能的参数：初始容量和负载因子。 容量是哈希表中桶的数量，初始容量就是哈希表创建时的容量。 加载因子是散列表在其容量自动扩容之前被允许的最大饱和量。当哈希表中的 entry 数量超过负载因子和当前容量的乘积时，散列表就会被重新映射（即重建内部数据结构），一般散列表大约是存储桶数量的两倍。 通常，默认加载因子（0.75）在时间和空间成本之间提供了良好的平衡。较高的值会减少空间开销，但会增加查找成本（反映在大部分 HashMap 类的操作中，包括 get 和 put）。在设置初始容量时，应考虑映射中的条目数量及其负载因子，以尽量减少重新运行操作的次数。如果初始容量大于最大入口数除以负载因子，则不会发生重新刷新操作。 如果许多映射要存储在 HashMap 实例中，使用足够大的容量创建映射将允许映射存储的效率高于根据需要执行自动重新散列以增长表。请注意，使用多个具有相同 hashCode() 的密钥是降低任何散列表性能的一个可靠方法。为了改善影响，当键是 Comparable 时，该类可以使用键之间的比较顺序来帮助断开关系。 HashMap 不是并发安全的。 HashMap 源码 HashMap 定义 public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; // 该表在初次使用时初始化，并根据需要调整大小。分配时，长度总是2的幂。 transient Node&lt;K,V&gt;[] table; // 保存缓存的 entrySet()。请注意，AbstractMap 字段用于 keySet() 和 values()。 transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; // map 中的键值对数 transient int size; // 这个HashMap被结构修改的次数结构修改是那些改变HashMap中的映射数量或者修改其内部结构（例如，重新散列）的修改。 transient int modCount; // 下一个调整大小的值（容量*加载因子）。 int threshold; // 散列表的加载因子 final float loadFactor;&#125; 构造方法 public HashMap(); // 默认加载因子0.75public HashMap(int initialCapacity); // 默认加载因子0.75；以 initialCapacity 初始化容量public HashMap(int initialCapacity, float loadFactor); // 以 initialCapacity 初始化容量；以 loadFactor 初始化加载因子public HashMap(Map&lt;? extends K, ? extends V&gt; m) // 默认加载因子0.75 put 方法的实现 put 方法大致的思路为： 对 key 的 hashCode()做 hash，然后再计算 index; 如果没碰撞直接放到 bucket 里； 如果碰撞了，以链表的形式存在 buckets 后； 如果碰撞导致链表过长(大于等于 TREEIFY_THRESHOLD)，就把链表转换成红黑树； 如果节点已经存在就替换 old value(保证 key 的唯一性) 如果 bucket 满了(超过 load factor * current capacity)，就要 resize。 具体代码的实现如下： public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // tab 为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 计算 index，并对 null 做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 节点存在 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 该链为树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 写入 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; get 方法的实现 在理解了 put 之后，get 就很简单了。大致思路如下： bucket 里的第一个节点，直接命中； 如果有冲突，则通过 key.equals(k)去查找对应的 entry 若为树，则在树中通过 key.equals(k)查找，O(logn)； 若为链表，则在链表中通过 key.equals(k)查找，O(n)。 具体代码的实现如下： public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 直接命中 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 未命中 if ((e = first.next) != null) &#123; // 在树中 get if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 在链表中 get do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; hash 方法的实现 在 get 和 put 的过程中，计算下标时，先对 hashCode 进行 hash 操作，然后再通过 hash 值进一步计算下标，如下图所示： 在对 hashCode() 计算 hash 时具体实现是这样的： static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 可以看到这个方法大概的作用就是：高 16bit 不变，低 16bit 和高 16bit 做了一个异或。 在设计 hash 方法时，因为目前的 table 长度 n 为 2 的幂，而计算下标的时候，是这样实现的(使用&amp;位操作，而非%求余)： (n - 1) &amp; hash 设计者认为这方法很容易发生碰撞。为什么这么说呢？不妨思考一下，在 n - 1 为 15(0x1111) 时，其实散列真正生效的只是低 4bit 的有效位，当然容易碰撞了。 因此，设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)，就是把高 16bit 和低 16bit 异或了一下。设计者还解释到因为现在大多数的 hashCode 的分布已经很不错了，就算是发生了碰撞也用 O(logn)的 tree 去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table 长度比较小时)，从而引起的碰撞。 如果还是产生了频繁的碰撞，会发生什么问题呢？作者注释说，他们使用树来处理频繁的碰撞(we use trees to handle large sets of collisions in bins)，在 JEP-180 中，描述了这个问题： Improve the performance of java.util.HashMap under high hash-collision conditions by using balanced trees rather than linked lists to store map entries. Implement the same improvement in the LinkedHashMap class. 之前已经提过，在获取 HashMap 的元素时，基本分两步： 首先根据 hashCode()做 hash，然后确定 bucket 的 index； 如果 bucket 的节点的 key 不是我们需要的，则通过 keys.equals()在链中找。 在 JDK8 之前的实现中是用链表解决冲突的，在产生碰撞的情况下，进行 get 时，两步的时间复杂度是 O(1)+O(n)。因此，当碰撞很厉害的时候 n 很大，O(n)的速度显然是影响速度的。 因此在 JDK8 中，利用红黑树替换链表，这样复杂度就变成了 O(1)+O(logn)了，这样在 n 很大的时候，能够比较理想的解决这个问题，在 JDK8：HashMap 的性能提升一文中有性能测试的结果。 resize 的实现 当 put 时，如果发现目前的 bucket 占用程度已经超过了 Load Factor 所希望的比例，那么就会发生 resize。在 resize 的过程，简单的说就是把 bucket 扩充为 2 倍，之后重新计算 index，把节点再放到新的 bucket 中。 当超过限制的时候会 resize，然而又因为我们使用的是 2 次幂的扩展(指长度扩为原来 2 倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动 2 次幂的位置。 怎么理解呢？例如我们从 16 扩展为 32 时，具体的变化如下所示： 因此元素在重新计算 hash 之后，因为 n 变为 2 倍，那么 n-1 的 mask 范围在高位多 1bit(红色)，因此新的 index 就会发生这样的变化： 因此，我们在扩充 HashMap 的时候，不需要重新计算 hash，只需要看看原来的 hash 值新增的那个 bit 是 1 还是 0 就好了，是 0 的话索引没变，是 1 的话索引变成“原索引+oldCap”。可以看看下图为 16 扩充为 32 的 resize 示意图： 这个设计确实非常的巧妙，既省去了重新计算 hash 值的时间，而且同时，由于新增的 1bit 是 0 还是 1 可以认为是随机的，因此 resize 的过程，均匀的把之前的冲突的节点分散到新的 bucket 了。 final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的 2 倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的 resize 上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个 bucket 都移动到新的 buckets 中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 小结 我们现在可以回答开始的几个问题，加深对 HashMap 的理解： 什么时候会使用 HashMap？他有什么特点？ 是基于 Map 接口的实现，存储键值对时，它可以接收 null 的键值，是非同步的，HashMap 存储着 Entry(hash, key, value, next)对象。 你知道 HashMap 的工作原理吗？ 通过 hash 的方法，通过 put 和 get 存储和获取对象。存储对象时，我们将 K/V 传给 put 方法时，它调用 hashCode 计算 hash 从而得到 bucket 位置，进一步存储，HashMap 会根据当前 bucket 的占用情况自动调整容量(超过 Load Facotr 则 resize 为原来的 2 倍)。获取对象时，我们将 K 传给 get，它调用 hashCode 计算 hash 从而得到 bucket 位置，并进一步调用 equals()方法确定键值对。如果发生碰撞的时候，Hashmap 通过链表将产生碰撞冲突的元素组织起来，在 Java 8 中，如果一个 bucket 中碰撞冲突的元素超过某个限制(默认是 8)，则使用红黑树来替换链表，从而提高速度。 你知道 get 和 put 的原理吗？equals()和 hashCode()的都有什么作用？ 通过对 key 的 hashCode()进行 hashing，并计算下标( n-1 &amp; hash)，从而获得 buckets 的位置。如果产生碰撞，则利用 key.equals()方法去链表或树中去查找对应的节点 你知道 hash 的实现吗？为什么要这样实现？ 在 Java 1.8 的实现中，是通过 hashCode()的高 16 位异或低 16 位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在 bucket 的 n 比较小的时候，也能保证考虑到高低 bit 都参与到 hash 的计算中，同时不会有太大的开销。 如果 HashMap 的大小超过了负载因子(load factor)定义的容量，怎么办？ 如果超过了负载因子(默认 0.75)，则会重新 resize 一个原来长度两倍的 HashMap，并且重新调用 hash 方法。 LinkedHashMap 类 LinkedHashMap 要点 LinkedHashMap 通过维护一个运行于所有条目的双向链表，保证了元素迭代的顺序。 关注点 结论 LinkedHashMap 是否允许键值对为 null Key 和 Value 都允许 null LinkedHashMap 是否允许重复数据 Key 重复会覆盖、Value 允许重复 LinkedHashMap 是否有序 有序 LinkedHashMap 是否线程安全 非线程安全 LinkedHashMap 源码 LinkedHashMap 定义 public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt; &#123; // 双链表的头指针 transient LinkedHashMap.Entry&lt;K,V&gt; head; // 双链表的尾指针 transient LinkedHashMap.Entry&lt;K,V&gt; tail; // 迭代排序方法：true 表示访问顺序；false 表示插入顺序 final boolean accessOrder;&#125; LinkedHashMap 继承了 HashMap 的 put 方法，本身没有实现 put 方法。 TreeMap 类 TreeMap 要点 TreeMap 基于红黑树实现。 TreeMap 是有序的。它的排序规则是：根据 map 中的 key 的自然顺序或提供的比较器的比较顺序。 TreeMap 不是并发安全的。 TreeMap 源码 put 方法 public V put(K key, V value) &#123; Entry&lt;K,V&gt; t = root; // 如果根节点为 null，插入第一个节点 if (t == null) &#123; compare(key, key); // type (and possibly null) check root = new Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; &#125; int cmp; Entry&lt;K,V&gt; parent; // split comparator and comparable paths Comparator&lt;? super K&gt; cpr = comparator; // 每个节点的左孩子节点的值小于它；右孩子节点的值大于它 // 如果有比较器，使用比较器进行比较 if (cpr != null) &#123; do &#123; parent = t; cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; // 没有比较器，使用 key 的自然顺序进行比较 else &#123; if (key == null) throw new NullPointerException(); @SuppressWarnings("unchecked") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do &#123; parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; // 通过上面的遍历未找到 key 值，则新插入节点 Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); if (cmp &lt; 0) parent.left = e; else parent.right = e; // 插入后，为了维持红黑树的平衡需要调整 fixAfterInsertion(e); size++; modCount++; return null;&#125; get 方法 public V get(Object key) &#123; Entry&lt;K,V&gt; p = getEntry(key); return (p==null ? null : p.value);&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; // Offload comparator-based version for sake of performance if (comparator != null) return getEntryUsingComparator(key); if (key == null) throw new NullPointerException(); @SuppressWarnings("unchecked") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; Entry&lt;K,V&gt; p = root; // 按照二叉树搜索的方式进行搜索，搜到返回 while (p != null) &#123; int cmp = k.compareTo(p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; &#125; return null;&#125; remove 方法 public V remove(Object key) &#123; Entry&lt;K,V&gt; p = getEntry(key); if (p == null) return null; V oldValue = p.value; deleteEntry(p); return oldValue;&#125;private void deleteEntry(Entry&lt;K,V&gt; p) &#123; modCount++; size--; // 如果当前节点有左右孩子节点，使用后继节点替换要删除的节点 // If strictly internal, copy successor's element to p and then make p // point to successor. if (p.left != null &amp;&amp; p.right != null) &#123; Entry&lt;K,V&gt; s = successor(p); p.key = s.key; p.value = s.value; p = s; &#125; // p has 2 children // Start fixup at replacement node, if it exists. Entry&lt;K,V&gt; replacement = (p.left != null ? p.left : p.right); if (replacement != null) &#123; // 要删除的节点有一个孩子节点 // Link replacement to parent replacement.parent = p.parent; if (p.parent == null) root = replacement; else if (p == p.parent.left) p.parent.left = replacement; else D:\codes\zp\java\database\docs\redis\分布式锁.md p.parent.right = replacement; // Null out links so they are OK to use by fixAfterDeletion. p.left = p.right = p.parent = null; // Fix replacement if (p.color == BLACK) fixAfterDeletion(replacement); &#125; else if (p.parent == null) &#123; // return if we are the only node. root = null; &#125; else &#123; // No children. Use self as phantom replacement and unlink. if (p.color == BLACK) fixAfterDeletion(p); if (p.parent != null) &#123; if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; &#125; &#125;&#125; TreeMap 示例 public class TreeMapDemo &#123; private static final String[] chars = "A B C D E F G H I J K L M N O P Q R S T U V W X Y Z".split(" "); public static void main(String[] args) &#123; TreeMap&lt;Integer, String&gt; treeMap = new TreeMap&lt;&gt;(); for (int i = 0; i &lt; chars.length; i++) &#123; treeMap.put(i, chars[i]); &#125; System.out.println(treeMap); Integer low = treeMap.firstKey(); Integer high = treeMap.lastKey(); System.out.println(low); System.out.println(high); Iterator&lt;Integer&gt; it = treeMap.keySet().iterator(); for (int i = 0; i &lt;= 6; i++) &#123; if (i == 3) &#123; low = it.next(); &#125; if (i == 6) &#123; high = it.next(); &#125; else &#123; it.next(); &#125; &#125; System.out.println(low); System.out.println(high); System.out.println(treeMap.subMap(low, high)); System.out.println(treeMap.headMap(high)); System.out.println(treeMap.tailMap(low)); &#125;&#125; WeakHashMap WeakHashMap 的定义如下： public class WeakHashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt; &#123;&#125; WeakHashMap 继承了 AbstractMap，实现了 Map 接口。 和 HashMap 一样，WeakHashMap 也是一个散列表，它存储的内容也是键值对(key-value)映射，而且键和值都可以是 null。 不过 WeakHashMap 的键是弱键。在 WeakHashMap 中，当某个键不再正常使用时，会被从 WeakHashMap 中被自动移除。更精确地说，对于一个给定的键，其映射的存在并不阻止垃圾回收器对该键的丢弃，这就使该键成为可终止的，被终止，然后被回收。某个键被终止时，它对应的键值对也就从映射中有效地移除了。 这个弱键的原理呢？大致上就是，通过 WeakReference 和 ReferenceQueue 实现的。 WeakHashMap 的 key 是弱键，即是 WeakReference 类型的；ReferenceQueue 是一个队列，它会保存被 GC 回收的弱键。实现步骤是： 新建 WeakHashMap，将键值对添加到 WeakHashMap 中。 实际上，WeakHashMap 是通过数组 table 保存 Entry(键值对)；每一个 Entry 实际上是一个单向链表，即 Entry 是键值对链表。 当某弱键不再被其它对象引用，并被 GC 回收时。在 GC 回收该弱键时，这个弱键也同时会被添加到 ReferenceQueue(queue)队列中。 当下一次我们需要操作 WeakHashMap 时，会先同步 table 和 queue。table 中保存了全部的键值对，而 queue 中保存被 GC 回收的键值对；同步它们，就是删除 table 中被 GC 回收的键值对。 这就是弱键如何被自动从 WeakHashMap 中删除的步骤了。 和 HashMap 一样，WeakHashMap 是不同步的。可以使用 Collections.synchronizedMap 方法来构造同步的 WeakHashMap。 资料 Java-HashMap 工作原理及实现 Map 综述（二）：彻头彻尾理解 LinkedHashMap Java 集合系列 09 之 Map 架构 Java 集合系列13之 WeakHashMap详细介绍(源码解析)和使用示例]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>container</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 容器之 Set]]></title>
    <url>%2Fblog%2F2018%2F05%2F31%2Fjava%2Fjavacore%2Fcontainer%2Fjava-container-set%2F</url>
    <content type="text"><![CDATA[Java 容器之 Set 📓 本文已归档到：「blog」 Set 架构 Set 接口 SortedSet 接口 NavigableSet 接口 AbstractSet 抽象类 HashSet 类 TreeSet 类 LinkedHashSet 类 EnumSet 类 资料 Set 架构 Set 继承了 Collection 的接口。实际上 Set 就是 Collection，只是行为略有不同：Set 集合不允许有重复元素。 SortedSet 继承了 Set 的接口。SortedSet 中的内容是排序的唯一值，排序的方法是通过比较器(Comparator)。 NavigableSet 继承了 SortedSet 的接口。相比于 NavigableSet 有一系列的导航方法；如&quot;获取大于/等于某值的元素&quot;、“获取小于/等于某值的元素”等等。 AbstractSet 是一个抽象类，它继承于 AbstractCollection，AbstractCollection 实现了 Set 中的绝大部分函数，为 Set 的实现类提供了便利。 HashSet 类依赖于 HashMap，它实际上是通过 HashMap 实现的。HashSet 中的元素是无序的。 TreeSet 类依赖于 TreeMap，它实际上是通过 TreeMap 实现的。TreeSet 中的元素是有序的。 LinkedHashSet 类具有 HashSet 的查找效率，且内部使用链表维护元素的插入顺序。 EnumSet 中所有元素都必须是指定枚举类型的枚举值。 Set 接口 Set 接口定义如下： public interface Set&lt;E&gt; extends Collection&lt;E&gt; &#123;&#125; Set 继承了 Collection 的接口。实际上，Set 就是 Collection，二者提供的方法完全相同。 SortedSet 接口 SortedSet 接口定义如下： public interface SortedSet&lt;E&gt; extends Set&lt;E&gt; &#123;&#125; SortedSet 接口新扩展的方法： comparator - 返回 Comparator subSet - 返回指定区间的子集 headSet - 返回小于指定元素的子集 tailSet - 返回大于指定元素的子集 first - 返回第一个元素 last - 返回最后一个元素 spliterator NavigableSet 接口 NavigableSet 接口定义如下： public interface NavigableSet&lt;E&gt; extends SortedSet&lt;E&gt; &#123;&#125; NavigableSet 接口新扩展的方法： lower - 返回小于指定值的元素中最接近的元素 higher - 返回大于指定值的元素中最接近的元素 floor - 返回小于或等于指定值的元素中最接近的元素 ceiling - 返回大于或等于指定值的元素中最接近的元素 pollFirst - 检索并移除第一个（最小的）元素 pollLast - 检索并移除最后一个（最大的）元素 descendingSet - 返回反序排列的 Set descendingIterator - 返回反序排列的 Set 的迭代器 subSet - 返回指定区间的子集 headSet - 返回小于指定元素的子集 tailSet - 返回大于指定元素的子集 AbstractSet 抽象类 AbstractSet 抽象类定义如下： public abstract class AbstractSet&lt;E&gt; extends AbstractCollection&lt;E&gt; implements Set&lt;E&gt; &#123;&#125; AbstractSet 类提供 Set 接口的骨干实现，以最大限度地减少实现 Set 接口所需的工作。 事实上，主要的实现已经在 AbstractCollection 中完成。 HashSet 类 HashSet 类定义如下： public class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123;&#125; HashSet 要点 HashSet 类通过继承 AbstractSet 实现了 Set 接口中的骨干方法。 HashSet 实现了 Cloneable，所以支持克隆。 HashSet 实现了 Serializable，所以支持序列化。 HashSet 中存储的元素是无序的。 HashSet 允许 null 值的元素。 HashSet 不是线程安全的。 HashSet 原理 // HashSet 的核心，通过维护一个 HashMap 实体来实现 HashSet 方法private transient HashMap&lt;E,Object&gt; map;// PRESENT 是用于关联 map 中当前操作元素的一个虚拟值private static final Object PRESENT = new Object();&#125; HashSet 是基于 HashMap 实现的。 HashSet 中维护了一个 HashMap 对象 map，HashSet 的重要方法，如 add、remove、iterator、clear、size 等都是围绕 map 实现的。 PRESENT 是用于关联 map 中当前操作元素的一个虚拟值。 HashSet 类中通过定义 writeObject() 和 readObject() 方法确定了其序列化和反序列化的机制。 TreeSet 类 TreeSet 类定义如下： public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable &#123;&#125; TreeSet 要点 TreeSet 类通过继承 AbstractSet 实现了 NavigableSet 接口中的骨干方法。 TreeSet 实现了 Cloneable，所以支持克隆。 TreeSet 实现了 Serializable，所以支持序列化。 TreeSet 中存储的元素是有序的。排序规则是自然顺序或比较器（Comparator）中提供的顺序规则。 TreeSet 不是线程安全的。 TreeSet 源码 // TreeSet 的核心，通过维护一个 NavigableMap 实体来实现 TreeSet 方法private transient NavigableMap&lt;E,Object&gt; m;// PRESENT 是用于关联 map 中当前操作元素的一个虚拟值private static final Object PRESENT = new Object(); TreeSet 是基于 TreeMap 实现的。 TreeSet 中维护了一个 NavigableMap 对象 map（实际上是一个 TreeMap 实例），TreeSet 的重要方法，如 add、remove、iterator、clear、size 等都是围绕 map 实现的。 PRESENT 是用于关联 map 中当前操作元素的一个虚拟值。TreeSet 中的元素都被当成 TreeMap 的 key 存储，而 value 都填的是 PRESENT。 LinkedHashSet 类 LinkedHashSet 类定义如下： public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123;&#125; LinkedHashSet 要点 LinkedHashSet 类通过继承 HashSet 实现了 Set 接口中的骨干方法。 LinkedHashSet 实现了 Cloneable，所以支持克隆。 LinkedHashSet 实现了 Serializable，所以支持序列化。 LinkedHashSet 中存储的元素是按照插入顺序保存的。 LinkedHashSet 不是线程安全的。 LinkedHashSet 原理 LinkedHashSet 有三个构造方法，无一例外，都是调用父类 HashSet 的构造方法。 public LinkedHashSet(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor, true);&#125;public LinkedHashSet(int initialCapacity) &#123; super(initialCapacity, .75f, true);&#125;public LinkedHashSet() &#123; super(16, .75f, true);&#125; 需要强调的是：LinkedHashSet 构造方法实际上调用的是父类 HashSet 的非 public 构造方法。 HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor);&#125; 不同于 HashSet public 构造方法中初始化的 HashMap 实例，这个构造方法中，初始化了 LinkedHashMap 实例。 也就是说，实际上，LinkedHashSet 维护了一个双链表。由双链表的特性可以知道，它是按照元素的插入顺序保存的。所以，这就是 LinkedHashSet 中存储的元素是按照插入顺序保存的原理。 EnumSet 类 EnumSet 类定义如下： public abstract class EnumSet&lt;E extends Enum&lt;E&gt;&gt; extends AbstractSet&lt;E&gt; implements Cloneable, java.io.Serializable &#123;&#125; EnumSet 要点 EnumSet 类继承了 AbstractSet，所以有 Set 接口中的骨干方法。 EnumSet 实现了 Cloneable，所以支持克隆。 EnumSet 实现了 Serializable，所以支持序列化。 EnumSet 通过 &lt;E extends Enum&lt;E&gt;&gt; 限定了存储元素必须是枚举值。 EnumSet 没有构造方法，只能通过类中的 static 方法来创建 EnumSet 对象。 EnumSet 是有序的。以枚举值在 EnumSet 类中的定义顺序来决定集合元素的顺序。 EnumSet 不是线程安全的。 资料 Java 编程思想（Thinking in java）]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>container</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>container</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾收集]]></title>
    <url>%2Fblog%2F2018%2F05%2F29%2Fjava%2Fjavacore%2Fjvm%2F%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[垃圾收集 📓 本文已归档到：「blog」 判断一个对象是否可回收 1. 引用计数算法 2. 可达性分析算法 3. 引用类型 4. 方法区的回收 5. finalize() 垃圾收集算法 1. 标记 - 清除 2. 标记 - 整理 3. 复制 4. 分代收集 垃圾收集器 1. Serial 收集器 2. ParNew 收集器 3. Parallel Scavenge 收集器 4. Serial Old 收集器 5. Parallel Old 收集器 6. CMS 收集器 7. G1 收集器 8. 比较 内存分配与回收策略 1. Minor GC 和 Full GC 2. 内存分配策略 3. Full GC 的触发条件 参考资料 程序计数器、虚拟机栈和本地方法栈这三个区域属于线程私有的，只存在于线程的生命周期内，线程结束之后也会消失，因此不需要对这三个区域进行垃圾回收。垃圾回收主要是针对 Java 堆和方法区进行。 判断一个对象是否可回收 1. 引用计数算法 给对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的对象可被回收。 两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收。 public class ReferenceCountingGC &#123; public Object instance = null; public static void main(String[] args) &#123; ReferenceCountingGC objectA = new ReferenceCountingGC(); ReferenceCountingGC objectB = new ReferenceCountingGC(); objectA.instance = objectB; objectB.instance = objectA; &#125;&#125; 正因为循环引用的存在，因此 Java 虚拟机不适用引用计数算法。 2. 可达性分析算法 通过 GC Roots 作为起始点进行搜索，能够到达到的对象都是存活的，不可达的对象可被回收。 Java 虚拟机使用该算法来判断对象是否可被回收，在 Java 中 GC Roots 一般包含以下内容： 虚拟机栈中引用的对象 本地方法栈中引用的对象 方法区中类静态属性引用的对象 方法区中的常量引用的对象 3. 引用类型 无论是通过引用计算算法判断对象的引用数量，还是通过可达性分析算法判断对象的引用链是否可达，判定对象是否可被回收都与引用有关。 Java 具有四种强度不同的引用类型。 （一）强引用 被强引用关联的对象不会被垃圾收集器回收。 使用 new 一个新对象的方式来创建强引用。 Object obj = new Object(); （二）软引用 被软引用关联的对象，只有在内存不够的情况下才会被回收。 使用 SoftReference 类来创建软引用。 Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 （三）弱引用 被弱引用关联的对象一定会被垃圾收集器回收，也就是说它只能存活到下一次垃圾收集发生之前。 使用 WeakReference 类来实现弱引用。 Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null; WeakHashMap 的 Entry 继承自 WeakReference，主要用来实现缓存。 private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; Tomcat 中的 ConcurrentCache 就使用了 WeakHashMap 来实现缓存功能。ConcurrentCache 采取的是分代缓存，经常使用的对象放入 eden 中，而不常用的对象放入 longterm。eden 使用 ConcurrentHashMap 实现，longterm 使用 WeakHashMap，保证了不常使用的对象容易被回收。 public final class ConcurrentCache&lt;K, V&gt; &#123; private final int size; private final Map&lt;K, V&gt; eden; private final Map&lt;K, V&gt; longterm; public ConcurrentCache(int size) &#123; this.size = size; this.eden = new ConcurrentHashMap&lt;&gt;(size); this.longterm = new WeakHashMap&lt;&gt;(size); &#125; public V get(K k) &#123; V v = this.eden.get(k); if (v == null) &#123; v = this.longterm.get(k); if (v != null) this.eden.put(k, v); &#125; return v; &#125; public void put(K k, V v) &#123; if (this.eden.size() &gt;= size) &#123; this.longterm.putAll(this.eden); this.eden.clear(); &#125; this.eden.put(k, v); &#125;&#125; （四）虚引用 又称为幽灵引用或者幻影引用。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象实例。 为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。 使用 PhantomReference 来实现虚引用。 Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj);obj = null; 4. 方法区的回收 因为方法区主要存放永久代对象，而永久代对象的回收率比新生代差很多，因此在方法区上进行回收性价比不高。 主要是对常量池的回收和对类的卸载。 类的卸载条件很多，需要满足以下三个条件，并且满足了也不一定会被卸载： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法。 可以通过 -Xnoclassgc 参数来控制是否对类进行卸载。 在大量使用反射、动态代理、CGLib 等 ByteCode 框架、动态生成 JSP 以及 OSGi 这类频繁自定义 ClassLoader 的场景都需要虚拟机具备类卸载功能，以保证不会出现内存溢出。 5. finalize() finalize() 类似 C++ 的析构函数，用来做关闭外部资源等工作。但是 try-finally 等方式可以做的更好，并且该方法运行代价高昂，不确定性大，无法保证各个对象的调用顺序，因此最好不要使用。 当一个对象可被回收时，如果需要执行该对象的 finalize() 方法，那么就有可能通过在该方法中让对象重新被引用，从而实现自救。 垃圾收集算法 1. 标记 - 清除 将需要回收的对象进行标记，然后清理掉被标记的对象。 不足： 标记和清除过程效率都不高； 会产生大量不连续的内存碎片，导致无法给大对象分配内存。 2. 标记 - 整理 让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 3. 复制 将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。 主要不足是只使用了内存的一半。 现在的商业虚拟机都采用这种收集算法来回收新生代，但是并不是将内存划分为大小相等的两块，而是分为一块较大的 Eden 空间和两块较小的 Survior 空间，每次使用 Eden 空间和其中一块 Survivor。在回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另一块 Survivor 空间上，最后清理 Eden 和使用过的那一块 Survivor。HotSpot 虚拟机的 Eden 和 Survivor 的大小比例默认为 8:1，保证了内存的利用率达到 90 %。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 空间就不够用了，此时需要依赖于老年代进行分配担保，也就是借用老年代的空间存储放不下的对象。 4. 分代收集 现在的商业虚拟机采用分代收集算法，它根据对象存活周期将内存划分为几块，不同块采用适当的收集算法。 一般将 Java 堆分为新生代和老年代。 新生代使用：复制算法 老年代使用：标记 - 清理 或者 标记 - 整理 算法 垃圾收集器 以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使用。 1. Serial 收集器 Serial 翻译为串行，可以理解为垃圾收集和用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序。除了 CMS 和 G1 之外，其它收集器都是以串行的方式执行。CMS 和 G1 可以使得垃圾收集和用户程序同时执行，被称为并发执行。 它是单线程的收集器，只会使用一个线程进行垃圾收集工作。 它的优点是简单高效，对于单个 CPU 环境来说，由于没有线程交互的开销，因此拥有最高的单线程收集效率。 它是 Client 模式下的默认新生代收集器，因为在用户的桌面应用场景下，分配给虚拟机管理的内存一般来说不会很大。Serial 收集器收集几十兆甚至一两百兆的新生代停顿时间可以控制在一百多毫秒以内，只要不是太频繁，这点停顿是可以接受的。 2. ParNew 收集器 它是 Serial 收集器的多线程版本。 是 Server 模式下的虚拟机首选新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合工作。 默认开始的线程数量与 CPU 数量相同，可以使用 -XX:ParallelGCThreads 参数来设置线程数。 3. Parallel Scavenge 收集器 与 ParNew 一样是并行的多线程收集器。 其它收集器关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户代码的时间占总时间的比值。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间 -XX:MaxGCPauseMillis 参数以及直接设置吞吐量大小的 -XX:GCTimeRatio 参数（值为大于 0 且小于 100 的整数）。缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 还提供了一个参数 -XX:+UseAdaptiveSizePolicy，这是一个开关参数，打开参数后，就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种方式称为 GC 自适应的调节策略（GC Ergonomics）。 4. Serial Old 收集器 是 Serial 收集器的老年代版本，也是给 Client 模式下的虚拟机使用。如果用在 Server 模式下，它有两大用途： 在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用。 作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 5. Parallel Old 收集器 是 Parallel Scavenge 收集器的老年代版本。 在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6. CMS 收集器 CMS（Concurrent Mark Sweep），Mark Sweep 指的是标记 - 清除算法。 特点：并发收集、低停顿。并发指的是用户线程和 GC 线程同时运行。 分为以下四个流程： 初始标记：仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：不需要停顿。 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。 具有以下缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。可以使用 -XX:CMSInitiatingOccupancyFraction 来改变触发 CMS 收集器工作的内存占用百分，如果这个值设置的太大，导致预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 7. G1 收集器 G1（Garbage-First），它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。 Java 堆被分为新生代、老年代和永久代，其它收集器进行收集的范围都是整个新生代或者老生代，而 G1 可以直接对新生代和永久代一起回收。 G1 把新生代和老年代划分成多个大小相等的独立区域（Region），新生代和永久代不再物理隔离。 通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 记录垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤： 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿是时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 具备如下特点： 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 更详细内容请参考：Getting Started with the G1 Garbage Collector 8. 比较 收集器 串行/并行/并发 新生代/老年代 收集算法 目标 适用场景 Serial 串行 新生代 复制 响应速度优先 单 CPU 环境下的 Client 模式 Serial Old 串行 老年代 标记-整理 响应速度优先 单 CPU 环境下的 Client 模式、CMS 的后备预案 ParNew 串行 + 并行 新生代 复制算法 响应速度优先 多 CPU 环境时在 Server 模式下与 CMS 配合 Parallel Scavenge 串行 + 并行 新生代 复制算法 吞吐量优先 在后台运算而不需要太多交互的任务 Parallel Old 串行 + 并行 老年代 标记-整理 吞吐量优先 在后台运算而不需要太多交互的任务 CMS 并行 + 并发 老年代 标记-清除 响应速度优先 集中在互联网站或 B/S 系统服务端上的 Java 应用 G1 并行 + 并发 新生代 + 老年代 标记-整理 + 复制算法 响应速度优先 面向服务端应用，将来替换 CMS 内存分配与回收策略 对象的内存分配，也就是在堆上分配。主要分配在新生代的 Eden 区上，少数情况下也可能直接分配在老年代中。 1. Minor GC 和 Full GC Minor GC：发生在新生代上，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比较快。 Full GC：发生在老年代上，老年代对象和新生代的相反，其存活时间长，因此 Full GC 很少执行，而且执行速度会比 Minor GC 慢很多。 2. 内存分配策略 （一）对象优先在 Eden 分配 大多数情况下，对象在新生代 Eden 区分配，当 Eden 区空间不够时，发起 Minor GC。 （二）大对象直接进入老年代 大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。 经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 区和 Survivor 区之间的大量内存复制。 （三）长期存活的对象进入老年代 为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁，增加到一定年龄则移动到老年代中。 -XX:MaxTenuringThreshold 用来定义年龄的阈值。 （四）动态对象年龄判定 虚拟机并不是永远地要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 区中相同年龄所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。 （五）空间分配担保 在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的；如果不成立的话虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC，尽管这次 Minor GC 是有风险的；如果小于，或者 HandlePromotionFailure 设置不允许冒险，那这时也要改为进行一次 Full GC。 3. Full GC 的触发条件 对于 Minor GC，其触发条件非常简单，当 Eden 区空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件： （一）调用 System.gc() 此方法的调用是建议虚拟机进行 Full GC，虽然只是建议而非一定，但很多情况下它会触发 Full GC，从而增加 Full GC 的频率，也即增加了间歇性停顿的次数。因此强烈建议能不使用此方法就不要使用，让虚拟机自己去管理它的内存。可通过 -XX:DisableExplicitGC 来禁止 RMI 调用 System.gc()。 （二）老年代空间不足 老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等，当执行 Full GC 后空间仍然不足，则抛出 Java.lang.OutOfMemoryError。为避免以上原因引起的 Full GC，调优时应尽量做到让对象在 Minor GC 阶段被回收、让对象在新生代多存活一段时间以及不要创建过大的对象及数组。 （三）空间分配担保失败 使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果出现了 HandlePromotionFailure 担保失败，则会触发 Full GC。 （四）JDK 1.7 及以前的永久代空间不足 在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据，当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError，为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 （五）Concurrent Mode Failure 执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足（有时候“空间不足”是 CMS GC 时当前的浮动垃圾过多导致暂时性的空间不足触发 Full GC），便会报 Concurrent Mode Failure 错误，并触发 Full GC。 参考资料 深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第 2 版） 从表到里学习 JVM 实现]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>jvm</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 调优]]></title>
    <url>%2Fblog%2F2018%2F05%2F29%2Fjava%2Fjavacore%2Fjvm%2FJVM%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[JVM 调优 📓 本文已归档到：「blog」 JVM 调优概述 性能定义 调优原则 GC 优化的过程 命令 jmap jstack jps jstat jhat jinfo HotSpot VM 参数 JVM 内存配置 GC 类型配置 辅助配置 典型配置 堆大小设置 回收器选择 JVM 实战 分析 GC 日志 获取 GC 日志 如何分析 GC 日志 OutOfMemory(OOM)分析 参考资料 JVM 调优概述 性能定义 吞吐量 - 指不考虑 GC 引起的停顿时间或内存消耗，垃圾收集器能支撑应用达到的最高性能指标。 延迟 - 其度量标准是缩短由于垃圾啊收集引起的停顿时间或者完全消除因垃圾收集所引起的停顿，避免应用运行时发生抖动。 内存占用 - 垃圾收集器流畅运行所需要的内存数量。 调优原则 GC 优化的两个目标： 将进入老年代的对象数量降到最低 减少 Full GC 的执行时间 GC 优化的基本原则是：将不同的 GC 参数应用到两个及以上的服务器上然后比较它们的性能，然后将那些被证明可以提高性能或减少 GC 执行时间的参数应用于最终的工作服务器上。 将进入老年代的对象数量降到最低 除了可以在 JDK7 及更高版本中使用的 G1 收集器以外，其他分代 GC 都是由 Oracle JVM 提供的。关于分代 GC，就是对象在 Eden 区被创建，随后被转移到 Survivor 区，在此之后剩余的对象会被转入老年代。也有一些对象由于占用内存过大，在 Eden 区被创建后会直接被传入老年代。老年代 GC 相对来说会比新生代 GC 更耗时，因此，减少进入老年代的对象数量可以显著降低 Full GC 的频率。你可能会以为减少进入老年代的对象数量意味着把它们留在新生代，事实正好相反，新生代内存的大小是可以调节的。 降低 Full GC 的时间 Full GC 的执行时间比 Minor GC 要长很多，因此，如果在 Full GC 上花费过多的时间（超过 1s），将可能出现超时错误。 如果通过减小老年代内存来减少 Full GC 时间，可能会引起 OutOfMemoryError 或者导致 Full GC 的频率升高。 另外，如果通过增加老年代内存来降低 Full GC 的频率，Full GC 的时间可能因此增加。 因此，你需要把老年代的大小设置成一个“合适”的值。 GC 优化需要考虑的 JVM 参数 类型 参数 描述 堆内存大小 -Xms 启动 JVM 时堆内存的大小 -Xmx 堆内存最大限制 新生代空间大小 -XX:NewRatio 新生代和老年代的内存比 -XX:NewSize 新生代内存大小 -XX:SurvivorRatio Eden 区和 Survivor 区的内存比 GC 优化时最常用的参数是-Xms,-Xmx和-XX:NewRatio。-Xms和-Xmx参数通常是必须的，所以NewRatio的值将对 GC 性能产生重要的影响。 有些人可能会问如何设置永久代内存大小，你可以用-XX:PermSize和-XX:MaxPermSize参数来进行设置，但是要记住，只有当出现OutOfMemoryError错误时你才需要去设置永久代内存。 GC 优化的过程 GC 优化的过程和大多数常见的提升性能的过程相似，下面是笔者使用的流程： 1.监控 GC 状态 你需要监控 GC 从而检查系统中运行的 GC 的各种状态。 2.分析监控结果后决定是否需要优化 GC 在检查 GC 状态后，你需要分析监控结构并决定是否需要进行 GC 优化。如果分析结果显示运行 GC 的时间只有 0.1-0.3 秒，那么就不需要把时间浪费在 GC 优化上，但如果运行 GC 的时间达到 1-3 秒，甚至大于 10 秒，那么 GC 优化将是很有必要的。 但是，如果你已经分配了大约 10GB 内存给 Java，并且这些内存无法省下，那么就无法进行 GC 优化了。在进行 GC 优化之前，你需要考虑为什么你需要分配这么大的内存空间，如果你分配了 1GB 或 2GB 大小的内存并且出现了OutOfMemoryError，那你就应该执行**堆快照（heap dump）**来消除导致异常的原因。 注意： **堆快照（heap dump）**是一个用来检查 Java 内存中的对象和数据的内存文件。该文件可以通过执行 JDK 中的jmap命令来创建。在创建文件的过程中，所有 Java 程序都将暂停，因此，不要在系统执行过程中创建该文件。 你可以在互联网上搜索 heap dump 的详细说明。 3.设置 GC 类型/内存大小 如果你决定要进行 GC 优化，那么你需要选择一个 GC 类型并且为它设置内存大小。此时如果你有多个服务器，请如上文提到的那样，在每台机器上设置不同的 GC 参数并分析它们的区别。 4.分析结果 在设置完 GC 参数后就可以开始收集数据，请在收集至少 24 小时后再进行结果分析。如果你足够幸运，你可能会找到系统的最佳 GC 参数。如若不然，你还需要分析输出日志并检查分配的内存，然后需要通过不断调整 GC 类型/内存大小来找到系统的最佳参数。 5.如果结果令人满意，将参数应用到所有服务器上并结束 GC 优化 如果 GC 优化的结果令人满意，就可以将参数应用到所有服务器上，并停止 GC 优化。 在下面的章节中，你将会看到上述每一步所做的具体工作。 命令 jmap jmap 即 JVM Memory Map。 jmap 用于生成 heap dump 文件。 如果不使用这个命令，还可以使用 -XX:+HeapDumpOnOutOfMemoryError 参数来让虚拟机出现 OOM 的时候，自动生成 dump 文件。 jmap 不仅能生成 dump 文件，还可以查询 finalize 执行队列、Java 堆和永久代的详细信息，如当前使用率、当前使用的是哪种收集器等。 命令格式： jmap [option] LVMID option 参数： dump - 生成堆转储快照 finalizerinfo - 显示在 F-Queue 队列等待 Finalizer 线程执行 finalizer 方法的对象 heap - 显示 Java 堆详细信息 histo - 显示堆中对象的统计信息 permstat - to print permanent generation statistics F - 当-dump 没有响应时，强制生成 dump 快照 示例：jmap -dump PID 生成堆快照 dump 堆到文件，format 指定输出格式，live 指明是活着的对象，file 指定文件名 $ jmap -dump:live,format=b,file=dump.hprof 28920Dumping heap to /home/xxx/dump.hprof ...Heap dump file created dump.hprof 这个后缀是为了后续可以直接用 MAT(Memory Anlysis Tool)打开。 示例：jmap -heap 查看指定进程的堆信息 注意：使用 CMS GC 情况下，jmap -heap 的执行有可能会导致 java 进程挂起。 jmap -heap PID[root@chances bin]# ./jmap -heap 12379Attaching to process ID 12379, please wait...Debugger attached successfully.Server compiler detected.JVM version is 17.0-b16using thread-local object allocation.Parallel GC with 6 thread(s)Heap Configuration: MinHeapFreeRatio = 40 MaxHeapFreeRatio = 70 MaxHeapSize = 83886080 (80.0MB) NewSize = 1310720 (1.25MB) MaxNewSize = 17592186044415 MB OldSize = 5439488 (5.1875MB) NewRatio = 2 SurvivorRatio = 8 PermSize = 20971520 (20.0MB) MaxPermSize = 88080384 (84.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 9306112 (8.875MB) used = 5375360 (5.1263427734375MB) free = 3930752 (3.7486572265625MB) 57.761608714788736% usedFrom Space: capacity = 9306112 (8.875MB) used = 3425240 (3.2665634155273438MB) free = 5880872 (5.608436584472656MB) 36.80634834397007% usedTo Space: capacity = 9306112 (8.875MB) used = 0 (0.0MB) free = 9306112 (8.875MB) 0.0% usedPS Old Generation capacity = 55967744 (53.375MB) used = 48354640 (46.11457824707031MB) free = 7613104 (7.2604217529296875MB) 86.39733629427693% usedPS Perm Generation capacity = 62062592 (59.1875MB) used = 60243112 (57.452308654785156MB) free = 1819480 (1.7351913452148438MB) 97.06831451706046% used jstack jstack 用于生成 java 虚拟机当前时刻的线程快照。 线程快照是当前 java 虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过 jstack 来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 如果 java 程序崩溃生成 core 文件，jstack 工具可以用来获得 core 文件的 java stack 和 native stack 的信息，从而可以轻松地知道 java 程序是如何崩溃和在程序何处发生问题。另外，jstack 工具还可以附属到正在运行的 java 程序中，看到当时运行的 java 程序的 java stack 和 native stack 的信息, 如果现在运行的 java 程序呈现 hung 的状态，jstack 是非常有用的。 命令格式： jstack [option] LVMID option 参数： -F - 当正常输出请求不被响应时，强制输出线程堆栈 -l - 除堆栈外，显示关于锁的附加信息 -m - 如果调用到本地方法的话，可以显示 C/C++的堆栈 jps jps(JVM Process Status Tool)，显示指定系统内所有的 HotSpot 虚拟机进程。 命令格式： jps [options] [hostid] option 参数： -l - 输出主类全名或 jar 路径 -q - 只输出 LVMID -m - 输出 JVM 启动时传递给 main()的参数 -v - 输出 JVM 启动时显示指定的 JVM 参数 其中[option]、[hostid]参数也可以不写。 $ jps -l -m28920 org.apache.catalina.startup.Bootstrap start11589 org.apache.catalina.startup.Bootstrap start25816 sun.tools.jps.Jps -l -m jstat jstat(JVM statistics Monitoring)，是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT 编译等运行数据。 命令格式： jstat [option] LVMID [interval] [count] 参数： [option] - 操作参数 LVMID - 本地虚拟机进程 ID [interval] - 连续输出的时间间隔 [count] - 连续输出的次数 jhat jhat(JVM Heap Analysis Tool)，是与 jmap 搭配使用，用来分析 jmap 生成的 dump，jhat 内置了一个微型的 HTTP/HTML 服务器，生成 dump 的分析结果后，可以在浏览器中查看。 注意：一般不会直接在服务器上进行分析，因为 jhat 是一个耗时并且耗费硬件资源的过程，一般把服务器生成的 dump 文件复制到本地或其他机器上进行分析。 命令格式： jhat [dumpfile] jinfo jinfo(JVM Configuration info)，用于实时查看和调整虚拟机运行参数。 之前的 jps -v 口令只能查看到显示指定的参数，如果想要查看未被显示指定的参数的值就要使用 jinfo 口令 命令格式： jinfo [option] [args] LVMID option 参数： -flag : 输出指定 args 参数的值 -flags : 不需要 args 参数，输出所有 JVM 参数的值 -sysprops : 输出系统属性，等同于 System.getProperties() HotSpot VM 参数 详细参数说明请参考官方文档：Java HotSpot VM Options，这里仅列举常用参数。 JVM 内存配置 配置 描述 -Xms 堆空间初始值。 -Xmx 堆空间最大值。 -XX:NewSize 新生代空间初始值。 -XX:MaxNewSize 新生代空间最大值。 -Xmn 新生代空间大小。 -XX:PermSize 永久代空间的初始值。 -XX:MaxPermSize 永久代空间的最大值。 GC 类型配置 配置 描述 -XX:+UseSerialGC 串行垃圾回收器 -XX:+UseParallelGC 并行垃圾回收器 -XX:+UseParNewGC 使用 ParNew + Serial Old 垃圾回收器组合 -XX:+UseConcMarkSweepGC 并发标记扫描垃圾回收器 -XX:ParallelCMSThreads= 并发标记扫描垃圾回收器 = 为使用的线程数量 -XX:+UseG1GC G1 垃圾回收器 辅助配置 配置 描述 -XX:+PrintGCDetails 打印 GC 日志 -Xloggc:&lt;filename&gt; 指定 GC 日志文件名 -XX:+HeapDumpOnOutOfMemoryError 内存溢出时输出堆快照文件 典型配置 堆大小设置 年轻代的设置很关键。 JVM 中最大堆大小有三方面限制： 相关操作系统的数据模型（32-bt 还是 64-bit）限制； 系统的可用虚拟内存限制； 系统的可用物理内存限制。 整个堆大小 = 年轻代大小 + 年老代大小 + 持久代大小 持久代一般固定大小为 64m。使用 -XX:PermSize 设置。 官方推荐年轻代占整个堆的 3/8。使用 -Xmn 设置。 回收器选择 JVM 给了三种选择：串行收集器、并行收集器、并发收集器。 JVM 实战 分析 GC 日志 获取 GC 日志 获取 GC 日志有两种方式： 使用命令动态查看 在容器中设置相关参数打印 GC 日志 jstat -gc 统计垃圾回收堆的行为： jstat -gc 1262 S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT26112.0 24064.0 6562.5 0.0 564224.0 76274.5 434176.0 388518.3 524288.0 42724.7 320 6.417 1 0.398 6.815 也可以设置间隔固定时间来打印： $ jstat -gc 1262 2000 20 这个命令意思就是每隔 2000ms 输出 1262 的 gc 情况，一共输出 20 次 Tomcat 设置示例： JAVA_OPTS="-server -Xms2000m -Xmx2000m -Xmn800m -XX:PermSize=64m -XX:MaxPermSize=256m -XX:SurvivorRatio=4-verbose:gc -Xloggc:$CATALINA_HOME/logs/gc.log-Djava.awt.headless=true-XX:+PrintGCTimeStamps -XX:+PrintGCDetails-Dsun.rmi.dgc.server.gcInterval=600000 -Dsun.rmi.dgc.client.gcInterval=600000-XX:+UseConcMarkSweepGC -XX:MaxTenuringThreshold=15" -Xms2000m -Xmx2000m -Xmn800m -XX:PermSize=64m -XX:MaxPermSize=256m Xms，即为 jvm 启动时得 JVM 初始堆大小,Xmx 为 jvm 的最大堆大小，xmn 为新生代的大小，permsize 为永久代的初始大小，MaxPermSize 为永久代的最大空间。 -XX:SurvivorRatio=4 SurvivorRatio 为新生代空间中的 Eden 区和救助空间 Survivor 区的大小比值，默认是 8，则两个 Survivor 区与一个 Eden 区的比值为 2:8,一个 Survivor 区占整个年轻代的 1/10。调小这个参数将增大 survivor 区，让对象尽量在 survitor 区呆长一点，减少进入年老代的对象。去掉救助空间的想法是让大部分不能马上回收的数据尽快进入年老代，加快年老代的回收频率，减少年老代暴涨的可能性，这个是通过将-XX:SurvivorRatio 设置成比较大的值（比如 65536)来做到。 -verbose:gc -Xloggc:$CATALINA_HOME/logs/gc.log 将虚拟机每次垃圾回收的信息写到日志文件中，文件名由 file 指定，文件格式是平文件，内容和-verbose:gc 输出内容相同。 -Djava.awt.headless=true Headless 模式是系统的一种配置模式。在该模式下，系统缺少了显示设备、键盘或鼠标。 -XX:+PrintGCTimeStamps -XX:+PrintGCDetails 设置 gc 日志的格式 -Dsun.rmi.dgc.server.gcInterval=600000 -Dsun.rmi.dgc.client.gcInterval=600000 指定 rmi 调用时 gc 的时间间隔 -XX:+UseConcMarkSweepGC -XX:MaxTenuringThreshold=15 采用并发 gc 方式，经过 15 次 minor gc 后进入年老代 如何分析 GC 日志 Young GC 回收日志: 2016-07-05T10:43:18.093+0800: 25.395: [GC [PSYoungGen: 274931K-&gt;10738K(274944K)] 371093K-&gt;147186K(450048K), 0.0668480 secs] [Times: user=0.17 sys=0.08, real=0.07 secs] Full GC 回收日志: 2016-07-05T10:43:18.160+0800: 25.462: [Full GC [PSYoungGen: 10738K-&gt;0K(274944K)] [ParOldGen: 136447K-&gt;140379K(302592K)] 147186K-&gt;140379K(577536K) [PSPermGen: 85411K-&gt;85376K(171008K)], 0.6763541 secs] [Times: user=1.75 sys=0.02, real=0.68 secs] 通过上面日志分析得出，PSYoungGen、ParOldGen、PSPermGen 属于 Parallel 收集器。其中 PSYoungGen 表示 gc 回收前后年轻代的内存变化；ParOldGen 表示 gc 回收前后老年代的内存变化；PSPermGen 表示 gc 回收前后永久区的内存变化。young gc 主要是针对年轻代进行内存回收比较频繁，耗时短；full gc 会对整个堆内存进行回城，耗时长，因此一般尽量减少 full gc 的次数 通过两张图非常明显看出 gc 日志构成： OutOfMemory(OOM)分析 OutOfMemory ，即内存溢出，是一个常见的 JVM 问题。那么分析 OOM 的思路是什么呢？ 首先，要知道有三种 OutOfMemoryError： OutOfMemoryError:Java heap space - 堆空间溢出 OutOfMemoryError:PermGen space - 方法区和运行时常量池溢出 OutOfMemoryError:unable to create new native thread - 线程过多 OutOfMemoryError:PermGen space OutOfMemoryError:PermGen space 表示方法区和运行时常量池溢出。 原因： Perm 区主要用于存放 Class 和 Meta 信息的，Class 在被 Loader 时就会被放到 PermGen space，这个区域称为年老代。GC 在主程序运行期间不会对年老区进行清理，默认是 64M 大小。 当程序程序中使用了大量的 jar 或 class，使 java 虚拟机装载类的空间不够，超过 64M 就会报这部分内存溢出了，需要加大内存分配，一般 128m 足够。 解决方案： （1）扩大永久代空间 JDK7 以前使用 -XX:PermSize 和 -XX:MaxPermSize 来控制永久代大小。 JDK8 以后把原本放在永久代的字符串常量池移出, 放在 Java 堆中(元空间 Metaspace)中，元数据并不在虚拟机中，使用的是本地的内存。使用 -XX:MetaspaceSize 和 -XX:MaxMetaspaceSize 控制元空间大小。 注意：-XX:PermSize 一般设为 64M （2）清理应用程序中 WEB-INF/lib 下的 jar，用不上的 jar 删除掉，多个应用公共的 jar 移动到 Tomcat 的 lib 目录，减少重复加载。 OutOfMemoryError:Java heap space OutOfMemoryError:Java heap space 表示堆空间溢出。 原因：JVM 分配给堆内存的空间已经用满了。 问题定位 （1）使用 jmap 或 -XX:+HeapDumpOnOutOfMemoryError 获取堆快照。 （2）使用内存分析工具（visualvm、mat、jProfile 等）对堆快照文件进行分析。 （3）根据分析图，重点是确认内存中的对象是否是必要的，分清究竟是是内存泄漏（Memory Leak）还是内存溢出（Memory Overflow）。 内存泄露 内存泄漏是指由于疏忽或错误造成程序未能释放已经不再使用的内存的情况。 内存泄漏并非指内存在物理上的消失，而是应用程序分配某段内存后，由于设计错误，失去了对该段内存的控制，因而造成了内存的浪费。 内存泄漏随着被执行的次数越多-最终会导致内存溢出。 而因程序死循环导致的不断创建对象-只要被执行到就会产生内存溢出。 内存泄漏常见几个情况： 静态集合类 声明为静态（static）的 HashMap、Vector 等集合 通俗来讲 A 中有 B，当前只把 B 设置为空，A 没有设置为空，回收时 B 无法回收-因被 A 引用。 监听器 监听器被注册后释放对象时没有删除监听器 物理连接 DataSource.getConnection()建立链接，必须通过 close()关闭链接 内部类和外部模块等的引用 发现它的方式同内存溢出，可再加个实时观察 jstat -gcutil 7362 2500 70 重点关注： FGC — 从应用程序启动到采样时发生 Full GC 的次数。 FGCT — 从应用程序启动到采样时 Full GC 所用的时间（单位秒）。 FGC 次数越多，FGCT 所需时间越多-可非常有可能存在内存泄漏。 解决方案 （1）检查程序，看是否有死循环或不必要地重复创建大量对象。有则改之。 下面是一个重复创建内存的示例： public class OOM &#123; public static void main(String[] args) &#123; Integer sum1=300000; Integer sum2=400000; OOM oom = new OOM(); System.out.println("往ArrayList中加入30w内容"); oom.javaHeapSpace(sum1); oom.memoryTotal(); System.out.println("往ArrayList中加入40w内容"); oom.javaHeapSpace(sum2); oom.memoryTotal(); &#125; public void javaHeapSpace(Integer sum)&#123; Random random = new Random(); ArrayList openList = new ArrayList(); for(int i=0;i&lt;sum;i++)&#123; String charOrNum = String.valueOf(random.nextInt(10)); openList.add(charOrNum); &#125; &#125; public void memoryTotal()&#123; Runtime run = Runtime.getRuntime(); long max = run.maxMemory(); long total = run.totalMemory(); long free = run.freeMemory(); long usable = max - total + free; System.out.println("最大内存 = " + max); System.out.println("已分配内存 = " + total); System.out.println("已分配内存中的剩余空间 = " + free); System.out.println("最大可用内存 = " + usable); &#125;&#125; 执行结果： 往ArrayList中加入30w内容最大内存 = 20447232已分配内存 = 20447232已分配内存中的剩余空间 = 4032576最大可用内存 = 4032576往ArrayList中加入40w内容Exception in thread "main" java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:2245) at java.util.Arrays.copyOf(Arrays.java:2219) at java.util.ArrayList.grow(ArrayList.java:242) at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:216) at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:208) at java.util.ArrayList.add(ArrayList.java:440) at pers.qingqian.study.seven.OOM.javaHeapSpace(OOM.java:36) at pers.qingqian.study.seven.OOM.main(OOM.java:26) （2）扩大堆内存空间 使用 -Xms 和 -Xmx 来控制堆内存空间大小。 OutOfMemoryError: GC overhead limit exceeded 原因：JDK6 新增错误类型，当 GC 为释放很小空间占用大量时间时抛出；一般是因为堆太小，导致异常的原因，没有足够的内存。 解决方案： 查看系统是否有使用大内存的代码或死循环； 通过添加 JVM 配置，来限制使用内存： &lt;jvm-arg&gt;-XX:-UseGCOverheadLimit&lt;/jvm-arg&gt; OutOfMemoryError：unable to create new native thread 原因：线程过多 那么能创建多少线程呢？这里有一个公式： (MaxProcessMemory - JVMMemory - ReservedOsMemory) / (ThreadStackSize) = Number of threads MaxProcessMemory 指的是一个进程的最大内存 JVMMemory JVM内存 ReservedOsMemory 保留的操作系统内存 ThreadStackSize 线程栈的大小 当发起一个线程的创建时，虚拟机会在 JVM 内存创建一个 Thread 对象同时创建一个操作系统线程，而这个系统线程的内存用的不是 JVMMemory，而是系统中剩下的内存： (MaxProcessMemory - JVMMemory - ReservedOsMemory) 结论：你给 JVM 内存越多，那么你能用来创建的系统线程的内存就会越少，越容易发生 java.lang.OutOfMemoryError: unable to create new native thread。 CPU 过高 定位步骤： （1）执行 top -c 命令，找到 cpu 最高的进程的 id （2）jstack PID 导出 Java 应用程序的线程堆栈信息。 示例： jstack 6795"Low Memory Detector" daemon prio=10 tid=0x081465f8 nid=0x7 runnable [0x00000000..0x00000000] "CompilerThread0" daemon prio=10 tid=0x08143c58 nid=0x6 waiting on condition [0x00000000..0xfb5fd798] "Signal Dispatcher" daemon prio=10 tid=0x08142f08 nid=0x5 waiting on condition [0x00000000..0x00000000] "Finalizer" daemon prio=10 tid=0x08137ca0 nid=0x4 in Object.wait() [0xfbeed000..0xfbeeddb8] at java.lang.Object.wait(Native Method) - waiting on &lt;0xef600848&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:116) - locked &lt;0xef600848&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:132) at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159) "Reference Handler" daemon prio=10 tid=0x081370f0 nid=0x3 in Object.wait() [0xfbf4a000..0xfbf4aa38] at java.lang.Object.wait(Native Method) - waiting on &lt;0xef600758&gt; (a java.lang.ref.Reference$Lock) at java.lang.Object.wait(Object.java:474) at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116) - locked &lt;0xef600758&gt; (a java.lang.ref.Reference$Lock) "VM Thread" prio=10 tid=0x08134878 nid=0x2 runnable "VM Periodic Task Thread" prio=10 tid=0x08147768 nid=0x8 waiting on condition 在打印的堆栈日志文件中，tid 和 nid 的含义： nid : 对应的 Linux 操作系统下的 tid 线程号，也就是前面转化的 16 进制数字tid: 这个应该是 jvm 的 jmm 内存规范中的唯一地址定位 在 CPU 过高的情况下，查找响应的线程，一般定位都是用 nid 来定位的。而如果发生死锁之类的问题，一般用 tid 来定位。 （3）定位 CPU 高的线程打印其 nid 查看线程下具体进程信息的命令如下： top -H -p 6735 top - 14:20:09 up 611 days, 2:56, 1 user, load average: 13.19, 7.76, 7.82Threads: 6991 total, 17 running, 6974 sleeping, 0 stopped, 0 zombie%Cpu(s): 90.4 us, 2.1 sy, 0.0 ni, 7.0 id, 0.0 wa, 0.0 hi, 0.4 si, 0.0 stKiB Mem: 32783044 total, 32505008 used, 278036 free, 120304 buffersKiB Swap: 0 total, 0 used, 0 free. 4497428 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 6800 root 20 0 27.299g 0.021t 7172 S 54.7 70.1 187:55.61 java 6803 root 20 0 27.299g 0.021t 7172 S 54.4 70.1 187:52.59 java 6798 root 20 0 27.299g 0.021t 7172 S 53.7 70.1 187:55.08 java 6801 root 20 0 27.299g 0.021t 7172 S 53.7 70.1 187:55.25 java 6797 root 20 0 27.299g 0.021t 7172 S 53.1 70.1 187:52.78 java 6804 root 20 0 27.299g 0.021t 7172 S 53.1 70.1 187:55.76 java 6802 root 20 0 27.299g 0.021t 7172 S 52.1 70.1 187:54.79 java 6799 root 20 0 27.299g 0.021t 7172 S 51.8 70.1 187:53.36 java 6807 root 20 0 27.299g 0.021t 7172 S 13.6 70.1 48:58.60 java11014 root 20 0 27.299g 0.021t 7172 R 8.4 70.1 8:00.32 java10642 root 20 0 27.299g 0.021t 7172 R 6.5 70.1 6:32.06 java 6808 root 20 0 27.299g 0.021t 7172 S 6.1 70.1 159:08.40 java11315 root 20 0 27.299g 0.021t 7172 S 3.9 70.1 5:54.10 java12545 root 20 0 27.299g 0.021t 7172 S 3.9 70.1 6:55.48 java23353 root 20 0 27.299g 0.021t 7172 S 3.9 70.1 2:20.55 java24868 root 20 0 27.299g 0.021t 7172 S 3.9 70.1 2:12.46 java 9146 root 20 0 27.299g 0.021t 7172 S 3.6 70.1 7:42.72 java 由此可以看出占用 CPU 较高的线程，但是这些还不高，无法直接定位到具体的类。nid 是 16 进制的，所以我们要获取线程的 16 进制 ID： printf "%x\n" 6800 输出结果:45cd 然后根据输出结果到 jstack 打印的堆栈日志中查定位： "catalina-exec-5692" daemon prio=10 tid=0x00007f3b05013800 nid=0x45cd waiting on condition [0x00007f3ae08e3000] java.lang.Thread.State: TIMED_WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000006a7800598&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082) at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467) at org.apache.tomcat.util.threads.TaskQueue.poll(TaskQueue.java:86) at org.apache.tomcat.util.threads.TaskQueue.poll(TaskQueue.java:32) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:745) 参考资料 深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第 2 版） 从表到里学习 JVM 实现 JVM（4）：Jvm 调优-命令篇 Java 系列笔记(4) - JVM 监控与调优 Java 服务 GC 参数调优案例 JVM 调优总结（5）：典型配置 如何合理的规划一次 jvm 性能调优 jvm 系列(九):如何优化 Java GC「译」 作为测试你应该知道的 JAVA OOM 及定位分析 异常、堆内存溢出、OOM 的几种情况]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类加载机制]]></title>
    <url>%2Fblog%2F2018%2F05%2F29%2Fjava%2Fjavacore%2Fjvm%2F%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[类加载机制 📓 本文已归档到：「blog」 类的生命周期 类初始化时机 类加载过程 1. 加载 2. 验证 3. 准备 4. 解析 5. 初始化 类加载器 类与类加载器 类加载器分类 双亲委派模型 自定义类加载器实现 参考资料 类是在运行期间动态加载的。 类的生命周期 包括以下 7 个阶段： 加载（Loading） 验证（Verification） 准备（Preparation） 解析（Resolution） 初始化（Initialization） 使用（Using） 卸载（Unloading） 其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 的动态绑定。 类初始化时机 虚拟机规范中并没有强制约束何时进行加载，但是规范严格规定了有且只有下列五种情况必须对类进行初始化（加载、验证、准备都会随着发生）： 遇到 new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则必须先触发其初始化。最常见的生成这 4 条指令的场景是：使用 new 关键字实例化对象的时候；读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）的时候；以及调用一个类的静态方法的时候。 使用 java.lang.reflect 包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含 main() 方法的那个类），虚拟机会先初始化这个主类； 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic, REF_putStatic, REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化； 以上 5 种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用。被动引用的常见例子包括： 通过子类引用父类的静态字段，不会导致子类初始化。 System.out.println(SubClass.value); // value 字段在 SuperClass 中定义 通过数组定义来引用类，不会触发此类的初始化。该过程会对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法。 SuperClass[] sca = new SuperClass[10]; 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 System.out.println(ConstClass.HELLOWORLD); 类加载过程 包含了加载、验证、准备、解析和初始化这 5 个阶段。 1. 加载 加载是类加载的一个阶段，注意不要混淆。 加载过程完成以下三件事： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时存储结构。 在内存中生成一个代表这个类的 Class 对象，作为方法区这个类的各种数据的访问入口。 其中二进制字节流可以从以下方式中获取： 从 ZIP 包读取，这很常见，最终成为日后 JAR、EAR、WAR 格式的基础。 从网络中获取，这种场景最典型的应用是 Applet。 运行时计算生成，这种场景使用得最多得就是动态代理技术，在 java.lang.reflect.Proxy 中，就是用了 ProxyGenerator.generateProxyClass 的代理类的二进制字节流。 由其他文件生成，典型场景是 JSP 应用，即由 JSP 文件生成对应的 Class 类。 从数据库读取，这种场景相对少见，例如有些中间件服务器（如 SAP Netweaver）可以选择把程序安装到数据库中来完成程序代码在集群间的分发。 … 2. 验证 确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 文件格式验证：验证字节流是否符合 Class 文件格式的规范，并且能被当前版本的虚拟机处理。 元数据验证：对字节码描述的信息进行语义分析，以保证其描述的信息符合 Java 语言规范的要求。 字节码验证：通过数据流和控制流分析，确保程序语义是合法、符合逻辑的。 符号引用验证：发生在虚拟机将符号引用转换为直接引用的时候，对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验。 3. 准备 类变量是被 static 修饰的变量，准备阶段为类变量分配内存并设置初始值，使用的是方法区的内存。 实例变量不会在这阶段分配内存，它将会在对象实例化时随着对象一起分配在 Java 堆中。（实例化不是类加载的一个过程，类加载发生在所有实例化操作之前，并且类加载只进行一次，实例化可以进行多次） 初始值一般为 0 值，例如下面的类变量 value 被初始化为 0 而不是 123。 public static int value = 123; 如果类变量是常量，那么会按照表达式来进行初始化，而不是赋值为 0。 public static final int value = 123; 4. 解析 将常量池的符号引用替换为直接引用的过程。 5. 初始化 初始化阶段才真正开始执行类中的定义的 Java 程序代码。初始化阶段即虚拟机执行类构造器 &lt;clinit&gt;() 方法的过程。 在准备阶段，类变量已经赋过一次系统要求的初始值，而在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。 &lt;clinit&gt;() 方法具有以下特点： 是由编译器自动收集类中所有类变量的赋值动作和静态语句块（static{} 块）中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码： public class Test &#123; static &#123; i = 0; // 给变量赋值可以正常编译通过 System.out.print(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; 与类的构造函数（或者说实例构造器 &lt;init&gt;()）不同，不需要显式的调用父类的构造器。虚拟机会自动保证在子类的 &lt;clinit&gt;() 方法运行之前，父类的 &lt;clinit&gt;() 方法已经执行结束。因此虚拟机中第一个执行 &lt;clinit&gt;() 方法的类肯定为 java.lang.Object。 由于父类的 &lt;clinit&gt;() 方法先执行，也就意味着父类中定义的静态语句块要优于子类的变量赋值操作。例如以下代码： static class Parent &#123; public static int A = 1; static &#123; A = 2; &#125;&#125;static class Sub extends Parent &#123; public static int B = A;&#125;public static void main(String[] args) &#123; System.out.println(Sub.B); // 输出结果是父类中的静态变量 A 的值，也就是 2。&#125; &lt;clinit&gt;() 方法对于类或接口不是必须的，如果一个类中不包含静态语句块，也没有对类变量的赋值操作，编译器可以不为该类生成 &lt;clinit&gt;() 方法。 接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成 &lt;clinit&gt;() 方法。但接口与类不同的是，执行接口的 &lt;clinit&gt;() 方法不需要先执行父接口的 &lt;clinit&gt;() 方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的 &lt;clinit&gt;() 方法。 虚拟机会保证一个类的 &lt;clinit&gt;() 方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的 &lt;clinit&gt;() 方法，其它线程都会阻塞等待，直到活动线程执行 &lt;clinit&gt;() 方法完毕。如果在一个类的 &lt;clinit&gt;() 方法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽。 类加载器 实现类的加载动作。在 Java 虚拟机外部实现，以便让应用程序自己决定如何去获取所需要的类。 类与类加载器 两个类相等：类本身相等，并且使用同一个类加载器进行加载。这是因为每一个类加载器都拥有一个独立的类名称空间。 这里的相等，包括类的 Class 对象的 equals() 方法、isAssignableFrom() 方法、isInstance() 方法的返回结果为 true，也包括使用 instanceof 关键字做对象所属关系判定结果为 true。 类加载器分类 从 Java 虚拟机的角度来讲，只存在以下两种不同的类加载器： 启动类加载器（Bootstrap ClassLoader），这个类加载器用 C++ 实现，是虚拟机自身的一部分； 所有其他类的加载器，这些类由 Java 实现，独立于虚拟机外部，并且全都继承自抽象类 java.lang.ClassLoader。 从 Java 开发人员的角度看，类加载器可以划分得更细致一些： 启动类加载器（Bootstrap ClassLoader）此类加载器负责将存放在 &lt;JAVA_HOME&gt;\lib 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被 Java 程序直接引用，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器，直接使用 null 代替即可。 扩展类加载器（Extension ClassLoader）这个类加载器是由 ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将 &lt;JAVA_HOME&gt;/lib/ext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）这个类加载器是由 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，因此一般称为系统类加载器。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 双亲委派模型 应用程序都是由三种类加载器相互配合进行加载的，如果有必要，还可以加入自己定义的类加载器。 下图展示的类加载器之间的层次关系，称为类加载器的双亲委派模型（Parents Delegation Model）。该模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。这里类加载器之间的父子关系一般通过组合（Composition）关系来实现，而不是通过继承（Inheritance）的关系实现。 （一）工作过程 一个类加载器首先将类加载请求传送到父类加载器，只有当父类加载器无法完成类加载请求时才尝试加载。 （二）好处 使得 Java 类随着它的类加载器一起具有一种带有优先级的层次关系，从而是的基础类得到统一。 例如 java.lang.Object 存放在 rt.jar 中，如果编写另外一个 java.lang.Object 的类并放到 ClassPath 中，程序可以编译通过。因为双亲委派模型的存在，所以在 rt.jar 中的 Object 比在 ClassPath 中的 Object 优先级更高，因为 rt.jar 中的 Object 使用的是启动类加载器，而 ClassPath 中的 Object 使用的是应用程序类加载器。正因为 rt.jar 中的 Object 优先级更高，因为程序中所有的 Object 都是这个 Object。 （三）实现 以下是抽象类 java.lang.ClassLoader 的代码片段，其中的 loadClass() 方法运行过程如下：先检查类是否已经加载过，如果没有则让父类加载器去加载。当父类加载器加载失败时抛出 ClassNotFoundException，此时尝试自己去加载。 public abstract class ClassLoader &#123; // The parent class loader for delegation private final ClassLoader parent; public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false); &#125; protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. c = findClass(name); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; throw new ClassNotFoundException(name); &#125;&#125; 自定义类加载器实现 FileSystemClassLoader 是自定义类加载器，继承自 java.lang.ClassLoader，用于加载文件系统上的类。它首先根据类的全名在文件系统上查找类的字节代码文件（.class 文件），然后读取该文件内容，最后通过 defineClass() 方法来把这些字节代码转换成 java.lang.Class 类的实例。 java.lang.ClassLoader 类的方法 loadClass() 实现了双亲委派模型的逻辑，因此自定义类加载器一般不去重写它，而是通过重写 findClass() 方法。 public class FileSystemClassLoader extends ClassLoader &#123; private String rootDir; public FileSystemClassLoader(String rootDir) &#123; this.rootDir = rootDir; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = getClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] getClassData(String className) &#123; String path = classNameToPath(className); try &#123; InputStream ins = new FileInputStream(path); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead; while ((bytesNumRead = ins.read(buffer)) != -1) &#123; baos.write(buffer, 0, bytesNumRead); &#125; return baos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; private String classNameToPath(String className) &#123; return rootDir + File.separatorChar + className.replace('.', File.separatorChar) + ".class"; &#125;&#125; 参考资料 深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第 2 版） 从表到里学习 JVM 实现]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>jvm</tag>
        <tag>ClassLoader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运行时数据区域]]></title>
    <url>%2Fblog%2F2018%2F05%2F29%2Fjava%2Fjavacore%2Fjvm%2F%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[运行时数据区域 📓 本文已归档到：「blog」 程序计数器 虚拟机栈 本地方法栈 堆 方法区 运行时常量池 直接内存 参考资料 程序计数器 记录正在执行的虚拟机字节码指令的地址（如果正在执行的是本地方法则为空）。 虚拟机栈 每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 可以通过 -Xss 这个虚拟机参数来指定一个程序的 Java 虚拟机栈内存大小： java -Xss=512M HackTheJava 该区域可能抛出以下异常： 当线程请求的栈深度超过最大值，会抛出 StackOverflowError 异常； 栈进行动态扩展时如果无法申请到足够内存，会抛出 OutOfMemoryError 异常。 本地方法栈 本地方法不是用 Java 实现，对待这些方法需要特别处理。 与 Java 虚拟机栈类似，它们之间的区别只不过是本地方法栈为本地方法服务。 堆 所有对象实例都在这里分配内存。 是垃圾收集的主要区域（“GC 堆”），现代的垃圾收集器基本都是采用分代收集算法，该算法的思想是针对不同的对象采取不同的垃圾回收算法，因此虚拟机把 Java 堆分成以下三块： 新生代（Young Generation） 老年代（Old Generation） 永久代（Permanent Generation） 当一个对象被创建时，它首先进入新生代，之后有可能被转移到老年代中。新生代存放着大量的生命很短的对象，因此新生代在三个区域中垃圾回收的频率最高。为了更高效地进行垃圾回收，把新生代继续划分成以下三个空间： Eden From Survivor To Survivor Java 堆不需要连续内存，并且可以动态增加其内存，增加失败会抛出 OutOfMemoryError 异常。 可以通过 -Xms 和 -Xmx 两个虚拟机参数来指定一个程序的 Java 堆内存大小，第一个参数设置初始值，第二个参数设置最大值。 java -Xms=1M -Xmx=2M HackTheJava 方法区 用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 和 Java 堆一样不需要连续的内存，并且可以动态扩展，动态扩展失败一样会抛出 OutOfMemoryError 异常。 对这块区域进行垃圾回收的主要目标是对常量池的回收和对类的卸载，但是一般比较难实现。 JDK 1.7 之前，HotSpot 虚拟机把它当成永久代来进行垃圾回收，JDK 1.8 之后，取消了永久代，用 metaspace（元数据）区替代。 运行时常量池 运行时常量池是方法区的一部分。 Class 文件中的常量池（编译器生成的各种字面量和符号引用）会在类加载后被放入这个区域。 除了在编译期生成的常量，还允许动态生成，例如 String 类的 intern()。这部分常量也会被放入运行时常量池。 直接内存 在 JDK 1.4 中新加入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆中来回复制数据。 参考资料 深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第 2 版） 从表到里学习 JVM 实现]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>jvm</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子变量类]]></title>
    <url>%2Fblog%2F2018%2F05%2F24%2Fjava%2Fjavacore%2Fconcurrent%2F%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[原子变量类 📓 本文已归档到：「blog」 本文内容基于 JDK1.8。 原子更新基本类型 原子更新数组 原子更新引用类型 原子更新字段类 资料 原子变量比锁的粒度更细，量级更轻，并且对于在多处理器系统上实现高性能的并发代码来说是非常关键的。 原子变量类相当于一种泛化的 volatile 变量，能够支持原子的和有条件的读-改-写操作。 原子类在内部使用现代 CPU 支持的 CAS 指令来实现同步。这些指令通常比锁更快。 原子更新基本类型 AtomicBoolean - 原子更新布尔类型。 AtomicInteger - 原子更新整型。 AtomicLong - 原子更新长整型。 示例： public class AtomicIntegerDemo &#123; public static void main(String[] args) throws InterruptedException &#123; ExecutorService executorService = Executors.newFixedThreadPool(5); AtomicInteger count = new AtomicInteger(0); for (int i = 0; i &lt; 1000; i++) &#123; executorService.submit((Runnable) () -&gt; &#123; System.out.println(Thread.currentThread().getName() + " count=" + count.get()); count.incrementAndGet(); &#125;); &#125; executorService.shutdown(); executorService.awaitTermination(30, TimeUnit.SECONDS); System.out.println("Final Count is : " + count.get()); &#125;&#125; 原子更新数组 AtomicIntegerArray - 原子更新整型数组里的元素。 AtomicLongArray - 原子更新长整型数组里的元素。 AtomicReferenceArray - 原子更新引用类型数组的元素。 AtomicBooleanArray - 原子更新布尔类型数组的元素。 示例： public class AtomicIntegerArrayDemo &#123; private static AtomicIntegerArray atomicIntegerArray = new AtomicIntegerArray(10); public static void main(final String[] arguments) throws InterruptedException &#123; for (int i = 0; i &lt; atomicIntegerArray.length(); i++) &#123; atomicIntegerArray.set(i, i); &#125; Thread t1 = new Thread(new Increment()); Thread t2 = new Thread(new Compare()); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println("Final Values: "); for (int i = 0; i &lt; atomicIntegerArray.length(); i++) &#123; System.out.print(atomicIntegerArray.get(i) + " "); &#125; &#125; static class Increment implements Runnable &#123; public void run() &#123; for (int i = 0; i &lt; atomicIntegerArray.length(); i++) &#123; int add = atomicIntegerArray.incrementAndGet(i); System.out.println(Thread.currentThread().getName() + ", index " + i + ", value: " + add); &#125; &#125; &#125; static class Compare implements Runnable &#123; public void run() &#123; for (int i = 0; i &lt; atomicIntegerArray.length(); i++) &#123; boolean swapped = atomicIntegerArray.compareAndSet(i, 2, 3); if (swapped) &#123; System.out.println(Thread.currentThread().getName() + ", index " + i + ", value: 3"); &#125; &#125; &#125; &#125;&#125; 原子更新引用类型 AtomicReference - 原子更新引用类型。 AtomicReferenceFieldUpdater - 原子更新引用类型里的字段。 AtomicMarkableReference - 原子更新带有标记位的引用类型。可以原子更新一个布尔类型的标记位和应用类型。 public class AtomicReferenceDemo &#123; private static String message; private static Person person; private static AtomicReference&lt;String&gt; aRmessage; private static AtomicReference&lt;Person&gt; aRperson; public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(new MyRun1()); Thread t2 = new Thread(new MyRun2()); message = "hello"; person = new Person("Phillip", 23); aRmessage = new AtomicReference&lt;String&gt;(message); aRperson = new AtomicReference&lt;Person&gt;(person); System.out.println("Message is: " + message + "\nPerson is " + person.toString()); System.out.println("Atomic Reference of Message is: " + aRmessage.get() + "\nAtomic Reference of Person is " + aRperson.get().toString()); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println("\nNow Message is: " + message + "\nPerson is " + person.toString()); System.out.println("Atomic Reference of Message is: " + aRmessage.get() + "\nAtomic Reference of Person is " + aRperson.get().toString()); &#125; static class MyRun1 implements Runnable &#123; public void run() &#123; aRmessage.compareAndSet(message, "Thread 1"); message = message.concat("-Thread 1!"); person.setAge(person.getAge() + 1); person.setName("Thread 1"); aRperson.getAndSet(new Person("Thread 1", 1)); System.out.println("\n" + Thread.currentThread().getName() + " Values " + message + " - " + person.toString()); System.out.println("\n" + Thread.currentThread().getName() + " Atomic References " + message + " - " + person.toString()); &#125; &#125; static class MyRun2 implements Runnable &#123; public void run() &#123; message = message.concat("-Thread 2"); person.setAge(person.getAge() + 2); person.setName("Thread 2"); aRmessage.lazySet("Thread 2"); aRperson.set(new Person("Thread 2", 2)); System.out.println("\n" + Thread.currentThread().getName() + " Values: " + message + " - " + person.toString()); System.out.println("\n" + Thread.currentThread().getName() + " Atomic References: " + aRmessage.get() + " - " + aRperson.get().toString()); &#125; &#125; static class Person &#123; private String name; private int age; Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; int getAge() &#123; return age; &#125; void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "[name " + this.name + ", age " + this.age + "]"; &#125; &#125;&#125; 原子更新字段类 AtomicIntegerFieldUpdater - 原子更新整型的字段的更新器。 AtomicLongFieldUpdater - 原子更新长整型字段的更新器。 AtomicStampedReference - 原子更新带有版本号的引用类型。该类将整型数值与引用关联起来，可用于原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 public class AtomicStampedReferenceDemo &#123; private final static String INIT_REF = "abc"; public static void main(String[] args) throws InterruptedException &#123; AtomicStampedReference&lt;String&gt; asr = new AtomicStampedReference&lt;&gt;(INIT_REF, 0); System.out.println("初始对象为：" + asr.getReference()); final int stamp = asr.getStamp(); ExecutorService executorService = Executors.newFixedThreadPool(100); for (int i = 0; i &lt; 100; i++) &#123; executorService.submit(() -&gt; &#123; try &#123; Thread.sleep(Math.abs((int) (Math.random() * 100))); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (asr.compareAndSet(INIT_REF, Thread.currentThread().getName(), stamp, stamp + 1)) &#123; System.out.println(Thread.currentThread().getName() + " 修改了对象！"); System.out.println("新的对象为：" + asr.getReference()); &#125; &#125;); &#125; executorService.shutdown(); executorService.awaitTermination(60, TimeUnit.SECONDS); &#125;&#125; 资料 Java 并发编程实战 Java 并发编程的艺术 http://tutorials.jenkov.com/java-util-concurrent/atomicinteger.html http://tutorials.jenkov.com/java-util-concurrent/atomicintegerarray.html http://tutorials.jenkov.com/java-util-concurrent/atomicreference.html http://tutorials.jenkov.com/java-util-concurrent/atomicstampedreference.html]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>concurrent</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>concurrent</tag>
        <tag>atomic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程基础]]></title>
    <url>%2Fblog%2F2018%2F05%2F22%2Fjava%2Fjavacore%2Fconcurrent%2F%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[线程基础 📓 本文已归档到：「blog」 线程简介 什么是线程 线程生命周期 启动和终止线程 构造线程 中断线程 终止线程 Thread 中的重要方法 线程间通信 wait/notify/notifyAll 线程的礼让 ThreadLocal 管道输入/输出流 FAQ start() 和 run() 有什么区别？可以直接调用 Thread 类的 run() 方法么？ sleep()、yield()、join() 方法有什么区别？为什么 sleep()和 yield()方法是静态的？ 为什么 sleep() 和 yield() 方法是静态的 Java 的线程优先级如何控制？高优先级的 Java 线程一定先执行吗？ 什么是守护线程？为什么要用守护线程？如何创建守护线程？ 为什么线程通信的方法 wait(), notify()和 notifyAll()被定义在 Object 类里？ 为什么 wait(), notify()和 notifyAll()必须在同步方法或者同步块中被调用？ 资料 线程简介 什么是线程 现代操作系统调度的最小单元是线程，也叫轻量级进程（Light Weight Process），在一个进程里可以创建多个线程，这些线程都拥有各自的计数器、堆栈和局部变量等属性，并且能够访问共享的内存变量。 线程生命周期 java.lang.Thread.State 中定义了 6 种不同的线程状态，在给定的一个时刻，线程只能处于其中的一个状态。 以下是各状态的说明，以及状态间的联系： 开始（New） - 还没有调用 start() 方法的线程处于此状态。 可运行（Runnable） - 已经调用了 start() 方法的线程状态。此状态意味着，线程已经准备好了，一旦被线程调度器分配了 CPU 时间片，就可以运行线程。 阻塞（Blocked） - 阻塞状态。线程阻塞的线程状态等待监视器锁定。处于阻塞状态的线程正在等待监视器锁定，以便在调用 Object.wait() 之后输入同步块/方法或重新输入同步块/方法。 等待（Waiting） - 等待状态。一个线程处于等待状态，是由于执行了 3 个方法中的任意方法： Object.wait() Thread.join() LockSupport.park() 定时等待（Timed waiting） - 等待指定时间的状态。一个线程处于定时等待状态，是由于执行了以下方法中的任意方法： Thread.sleep(sleeptime) Object.wait(timeout) Thread.join(timeout) LockSupport.parkNanos(timeout) LockSupport.parkUntil(timeout) 终止(Terminated) - 线程 run() 方法执行结束，或者因异常退出了 run() 方法，则该线程结束生命周期。死亡的线程不可再次复生。 启动和终止线程 构造线程 构造线程主要有三种方式 继承 Thread 类 实现 Runnable 接口 实现 Callable 接口 继承 Thread 类 通过继承 Thread 类构造线程的步骤： 定义 Thread 类的子类，并重写该类的 run() 方法，该 run() 方法的方法体就代表了线程要完成的任务。因此把 run() 方法称为执行体。 创建 Thread 子类的实例，即创建了线程对象。 调用线程对象的 start() 方法来启动该线程。 示例： public class ThreadDemo02 &#123; public static void main(String[] args) &#123; Thread02 mt1 = new Thread02("线程A "); // 实例化对象 Thread02 mt2 = new Thread02("线程B "); // 实例化对象 mt1.start(); // 调用线程主体 mt2.start(); // 调用线程主体 &#125; static class Thread02 extends Thread &#123; private int ticket = 5; Thread02(String name) &#123; super(name); &#125; @Override public void run() &#123; for (int i = 0; i &lt; 100; i++) &#123; if (this.ticket &gt; 0) &#123; System.out.println(this.getName() + " 卖票：ticket = " + ticket--); &#125; &#125; &#125; &#125;&#125; 实现 Runnable 接口 通过实现 Runnable 接口构造线程的步骤： 定义 Runnable 接口的实现类，并重写该接口的 run() 方法，该 run() 方法的方法体同样是该线程的线程执行体。 创建 Runnable 实现类的实例，并依此实例作为 Thread 的 target 来创建 Thread 对象，该 Thread 对象才是真正的线程对象。 调用线程对象的 start() 方法来启动该线程。 示例： public class RunnableDemo &#123; public static void main(String[] args) &#123; MyThread t = new MyThread("Runnable 线程"); // 实例化对象 new Thread(t).run(); // 调用线程主体 new Thread(t).run(); // 调用线程主体 new Thread(t).run(); // 调用线程主体 &#125; static class MyThread implements Runnable &#123; private int ticket = 5; private String name; MyThread(String name) &#123; this.name = name; &#125; @Override public void run() &#123; for (int i = 0; i &lt; 100; i++) &#123; if (this.ticket &gt; 0) &#123; System.out.println(this.name + " 卖票：ticket = " + ticket--); &#125; &#125; &#125; &#125;&#125; 实现 Callable 接口 通过实现 Callable 接口构造线程的步骤： 创建 Callable 接口的实现类，并实现 call() 方法，该 call() 方法将作为线程执行体，并且有返回值。 创建 Callable 实现类的实例，使用 FutureTask 类来包装 Callable 对象，该 FutureTask 对象封装了该 Callable 对象的 call() 方法的返回值。 使用 FutureTask 对象作为 Thread 对象的 target 创建并启动新线程。 调用 FutureTask 对象的 get() 方法来获得子线程执行结束后的返回值。 示例： public class CallableAndFutureDemo &#123; public static void main(String[] args) &#123; Callable&lt;Integer&gt; callable = () -&gt; new Random().nextInt(100); FutureTask&lt;Integer&gt; future = new FutureTask&lt;&gt;(callable); new Thread(future).start(); try &#123; Thread.sleep(1000);// 可能做一些事情 System.out.println(future.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 三种创建线程方式对比 实现 Runnable 接口优于继承 Thread 类，因为实现接口方式更便于扩展类。 实现 Runnable 接口的线程没有返回值；而实现 Callable 接口的线程有返回值。 中断线程 当一个线程运行时，另一个线程可以直接通过 interrupt() 方法中断其运行状态。 public class ThreadInterruptDemo &#123; public static void main(String[] args) &#123; MyThread mt = new MyThread(); // 实例化Runnable子类对象 Thread t = new Thread(mt, "线程"); // 实例化Thread对象 t.start(); // 启动线程 try &#123; Thread.sleep(2000); // 线程休眠2秒 &#125; catch (InterruptedException e) &#123; System.out.println("3、休眠被终止"); &#125; t.interrupt(); // 中断线程执行 &#125; static class MyThread implements Runnable &#123; @Override public void run() &#123; System.out.println("1、进入run()方法"); try &#123; Thread.sleep(10000); // 线程休眠10秒 System.out.println("2、已经完成了休眠"); &#125; catch (InterruptedException e) &#123; System.out.println("3、休眠被终止"); return; // 返回调用处 &#125; System.out.println("4、run()方法正常结束"); &#125; &#125;&#125; 终止线程 Thread 中的 stop 方法有缺陷，已废弃。 安全地终止线程有两种方法： 中断状态是线程的一个标识位，而中断操作是一种简便的线程间交互方式，而这种交互方式最适合用来取消或停止任务。 还可以利用一个 boolean 变量来控制是否需要停止任务并终止该线程。 public class ThreadStopDemo03 &#123; public static void main(String[] args) throws Exception &#123; MyTask one = new MyTask(); Thread countThread = new Thread(one, "CountThread"); countThread.start(); // 睡眠1秒，main线程对CountThread进行中断，使CountThread能够感知中断而结束 TimeUnit.SECONDS.sleep(1); countThread.interrupt(); MyTask two = new MyTask(); countThread = new Thread(two, "CountThread"); countThread.start(); // 睡眠1秒，main线程对Runner two进行取消，使CountThread能够感知on为false而结束 TimeUnit.SECONDS.sleep(1); two.cancel(); &#125; private static class MyTask implements Runnable &#123; private long i; private volatile boolean on = true; @Override public void run() &#123; while (on &amp;&amp; !Thread.currentThread().isInterrupted()) &#123; i++; &#125; System.out.println("Count i = " + i); &#125; void cancel() &#123; on = false; &#125; &#125;&#125; Thread 中的重要方法 run - 线程的执行实体。 start - 线程的启动方法。 setName、getName - 可以通过 setName()、 getName() 来设置、获取线程名称。 setPriority、getPriority - 在 Java 中，所有线程在运行前都会保持在就绪状态，那么此时，哪个线程优先级高，哪个线程就有可能被先执行。可以通过 setPriority、getPriority 来设置、获取线程优先级。 setDaemon、isDaemon - 可以使用 setDaemon() 方法设置线程为守护线程；可以使用 isDaemon() 方法判断线程是否为守护线程。 isAlive - 可以通过 isAlive 来判断线程是否启动。 interrupt - 当一个线程运行时，另一个线程可以直接通过 interrupt() 方法中断其运行状态。 join - 使用 join() 方法让一个线程强制运行，线程强制运行期间，其他线程无法运行，必须等待此线程完成之后才可以继续执行。 Thread.sleep - 使用 Thread.sleep() 方法即可实现休眠。 Thread.yield - 可以使用 Thread.yield() 方法将一个线程的操作暂时让给其他线程执行。 设置/获取线程名称 在 Thread 类中可以通过 setName()、 getName() 来设置、获取线程名称。 public class ThreadNameDemo &#123; public static void main(String[] args) &#123; MyThread mt = new MyThread(); // 实例化Runnable子类对象 new Thread(mt).start(); // 系统自动设置线程名称 new Thread(mt, "线程-A").start(); // 手工设置线程名称 Thread t = new Thread(mt); // 手工设置线程名称 t.setName("线程-B"); t.start(); &#125; static class MyThread implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 3; i++) &#123; System.out.println(Thread.currentThread().getName() + "运行，i = " + i); // 取得当前线程的名字 &#125; &#125; &#125;&#125; 判断线程是否启动 在 Thread 类中可以通过 isAlive() 来判断线程是否启动。 public class ThreadAliveDemo &#123; public static void main(String[] args) &#123; MyThread mt = new MyThread(); // 实例化Runnable子类对象 Thread t = new Thread(mt, "线程"); // 实例化Thread对象 System.out.println("线程开始执行之前 --&gt; " + t.isAlive()); // 判断是否启动 t.start(); // 启动线程 System.out.println("线程开始执行之后 --&gt; " + t.isAlive()); // 判断是否启动 for (int i = 0; i &lt; 3; i++) &#123; System.out.println(" main运行 --&gt; " + i); &#125; // 以下的输出结果不确定 System.out.println("代码执行之后 --&gt; " + t.isAlive()); // 判断是否启动 &#125; static class MyThread implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 3; i++) &#123; System.out.println(Thread.currentThread().getName() + "运行，i = " + i); &#125; &#125; &#125;&#125; 守护线程 在 Java 程序中，只要前台有一个线程在运行，则整个 Java 进程就不会消失，所以此时可以设置一个守护线程，这样即使 Java 进程结束了，此守护线程依然会继续执行。可以使用 setDaemon() 方法设置线程为守护线程；可以使用 isDaemon() 方法判断线程是否为守护线程。 public class ThreadDaemonDemo &#123; public static void main(String[] args) &#123; Thread t = new Thread(new MyThread(), "线程"); t.setDaemon(true); // 此线程在后台运行 System.out.println("线程 t 是否是守护进程：" + t.isDaemon()); t.start(); // 启动线程 &#125; static class MyThread implements Runnable &#123; @Override public void run() &#123; while (true) &#123; System.out.println(Thread.currentThread().getName() + "在运行。"); &#125; &#125; &#125;&#125; 设置/获取线程优先级 在 Java 中，所有线程在运行前都会保持在就绪状态，那么此时，哪个线程优先级高，哪个线程就有可能被先执行。 public class ThreadPriorityDemo &#123; public static void main(String[] args) &#123; System.out.println("主方法的优先级：" + Thread.currentThread().getPriority()); System.out.println("MAX_PRIORITY = " + Thread.MAX_PRIORITY); System.out.println("NORM_PRIORITY = " + Thread.NORM_PRIORITY); System.out.println("MIN_PRIORITY = " + Thread.MIN_PRIORITY); Thread t1 = new Thread(new MyThread(), "线程A"); // 实例化线程对象 Thread t2 = new Thread(new MyThread(), "线程B"); // 实例化线程对象 Thread t3 = new Thread(new MyThread(), "线程C"); // 实例化线程对象 t1.setPriority(Thread.MIN_PRIORITY); // 优先级最低 t2.setPriority(Thread.MAX_PRIORITY); // 优先级最低 t3.setPriority(Thread.NORM_PRIORITY); // 优先级最低 t1.start(); // 启动线程 t2.start(); // 启动线程 t3.start(); // 启动线程 &#125; static class MyThread implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; try &#123; Thread.sleep(500); // 线程休眠 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 取得当前线程的名字 String out = Thread.currentThread().getName() + "，优先级：" + Thread.currentThread().getPriority() + "，运行：i = " + i; System.out.println(out); &#125; &#125; &#125;&#125; 线程间通信 wait/notify/notifyAll wait、notify、notifyAll 是 Object 类中的方法。 wait - 线程自动释放其占有的对象锁，并等待 notify。 notify - 唤醒一个正在 wait 当前对象锁的线程，并让它拿到对象锁。 notifyAll - 唤醒所有正在 wait 前对象锁的线程。 生产者、消费者示例： public class ThreadWaitNotifyDemo02 &#123; private static final int QUEUE_SIZE = 10; private static final PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;&gt;(QUEUE_SIZE); public static void main(String[] args) &#123; new Producer("生产者A").start(); new Producer("生产者B").start(); new Consumer("消费者A").start(); new Consumer("消费者B").start(); &#125; static class Consumer extends Thread &#123; Consumer(String name) &#123; super(name); &#125; @Override public void run() &#123; while (true) &#123; synchronized (queue) &#123; while (queue.size() == 0) &#123; try &#123; System.out.println("队列空，等待数据"); queue.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); queue.notifyAll(); &#125; &#125; queue.poll(); // 每次移走队首元素 queue.notifyAll(); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " 从队列取走一个元素，队列当前有：" + queue.size() + "个元素"); &#125; &#125; &#125; &#125; static class Producer extends Thread &#123; Producer(String name) &#123; super(name); &#125; @Override public void run() &#123; while (true) &#123; synchronized (queue) &#123; while (queue.size() == QUEUE_SIZE) &#123; try &#123; System.out.println("队列满，等待有空余空间"); queue.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); queue.notifyAll(); &#125; &#125; queue.offer(1); // 每次插入一个元素 queue.notifyAll(); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " 向队列取中插入一个元素，队列当前有：" + queue.size() + "个元素"); &#125; &#125; &#125; &#125;&#125; 线程的礼让 在线程操作中，可以使用 Thread.yield() 方法将一个线程的操作暂时让给其他线程执行。 public class ThreadYieldDemo &#123; public static void main(String[] args) &#123; MyThread t = new MyThread(); new Thread(t, "线程A").start(); new Thread(t, "线程B").start(); &#125; static class MyThread implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; try &#123; Thread.sleep(1000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + "运行，i = " + i); if (i == 2) &#123; System.out.print("线程礼让："); Thread.yield(); &#125; &#125; &#125; &#125;&#125; 线程的强制执行 在线程操作中，可以使用 join() 方法让一个线程强制运行，线程强制运行期间，其他线程无法运行，必须等待此线程完成之后才可以继续执行。 public class ThreadJoinDemo &#123; public static void main(String[] args) &#123; MyThread mt = new MyThread(); // 实例化Runnable子类对象 Thread t = new Thread(mt, "mythread"); // 实例化Thread对象 t.start(); // 启动线程 for (int i = 0; i &lt; 50; i++) &#123; if (i &gt; 10) &#123; try &#123; t.join(); // 线程强制运行 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("Main 线程运行 --&gt; " + i); &#125; &#125; static class MyThread implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 50; i++) &#123; System.out.println(Thread.currentThread().getName() + " 运行，i = " + i); // 取得当前线程的名字 &#125; &#125; &#125;&#125; 线程的休眠 直接使用 Thread.sleep() 方法即可实现休眠。 public class ThreadSleepDemo &#123; public static void main(String[] args) &#123; new Thread(new MyThread("线程A", 1000)).start(); new Thread(new MyThread("线程A", 2000)).start(); new Thread(new MyThread("线程A", 3000)).start(); &#125; static class MyThread implements Runnable &#123; private String name; private int time; private MyThread(String name, int time) &#123; this.name = name; // 设置线程名称 this.time = time; // 设置休眠时间 &#125; @Override public void run() &#123; try &#123; Thread.sleep(this.time); // 休眠指定的时间 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(this.name + "线程，休眠" + this.time + "毫秒。"); &#125; &#125;&#125; ThreadLocal ThreadLocal，很多地方叫做线程本地变量，也有些地方叫做线程本地存储，其实意思差不多。可能很多朋友都知道 ThreadLocal 为变量在每个线程中都创建了一个副本，那么每个线程可以访问自己内部的副本变量。 源码 ThreadLocal 的主要方法： public class ThreadLocal&lt;T&gt; &#123; public T get() &#123;&#125; public void remove() &#123;&#125; public void set(T value) &#123;&#125; public static &lt;S&gt; ThreadLocal&lt;S&gt; withInitial(Supplier&lt;? extends S&gt; supplier) &#123;&#125;&#125; get()方法是用来获取 ThreadLocal 在当前线程中保存的变量副本。 set()用来设置当前线程中变量的副本。 remove()用来移除当前线程中变量的副本。 initialValue()是一个 protected 方法，一般是用来在使用时进行重写的，它是一个延迟加载方法，下面会详细说明。 get() 源码实现 get 源码 public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 取得当前线程。 通过 getMap() 方法获取 ThreadLocalMap。 成功，返回 value；失败，返回 setInitialValue()。 ThreadLocalMap 源码实现 ThreadLocalMap 源码 ThreadLocalMap 是 ThreadLocal 的一个内部类。 ThreadLocalMap 的 Entry 继承了 WeakReference，并且使用 ThreadLocal 作为键值。 setInitialValue 源码实现 private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 如果 map 不为空，就设置键值对；为空，再创建 Map，看一下 createMap 的实现： void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; ThreadLocal 源码小结 至此，可能大部分朋友已经明白了 ThreadLocal 是如何为每个线程创建变量的副本的： 在每个线程 Thread 内部有一个 ThreadLocal.ThreadLocalMap 类型的成员变量 threadLocals，这个 threadLocals 就是用来存储实际的变量副本的，键值为当前 ThreadLocal 变量，value 为变量副本（即 T 类型的变量）。 在 Thread 里面，threadLocals 为空，当通过 ThreadLocal 变量调用 get()方法或者 set()方法，就会对 Thread 类中的 threadLocals 进行初始化，并且以当前 ThreadLocal 变量为键值，以 ThreadLocal 要保存的副本变量为 value，存到 threadLocals。 在当前线程里面，如果要使用副本变量，就可以通过 get 方法在 threadLocals 里面查找。 示例 ThreadLocal 最常见的应用场景为用于解决数据库连接、Session 管理等问题。 示例 - 数据库连接 private static ThreadLocal&lt;Connection&gt; connectionHolder= new ThreadLocal&lt;Connection&gt;() &#123;public Connection initialValue() &#123; return DriverManager.getConnection(DB_URL);&#125;&#125;;public static Connection getConnection() &#123;return connectionHolder.get();&#125; 示例 - Session 管理 private static final ThreadLocal threadSession = new ThreadLocal();public static Session getSession() throws InfrastructureException &#123; Session s = (Session) threadSession.get(); try &#123; if (s == null) &#123; s = getSessionFactory().openSession(); threadSession.set(s); &#125; &#125; catch (HibernateException ex) &#123; throw new InfrastructureException(ex); &#125; return s;&#125; 管道输入/输出流 管道输入/输出流和普通的文件输入/输出流或者网络输入/输出流不同之处在于，它主要用于线程之间的数据传输，而传输的媒介为内存。 管道输入/输出流主要包括了如下 4 种具体实现：PipedOutputStream、PipedInputStream、PipedReader 和 PipedWriter，前两种面向字节，而后两种面向字符。 public class Piped &#123; public static void main(String[] args) throws Exception &#123; PipedWriter out = new PipedWriter(); PipedReader in = new PipedReader(); // 将输出流和输入流进行连接，否则在使用时会抛出IOException out.connect(in); Thread printThread = new Thread(new Print(in), "PrintThread"); printThread.start(); int receive = 0; try &#123; while ((receive = System.in.read()) != -1) &#123; out.write(receive); &#125; &#125; finally &#123; out.close(); &#125; &#125; static class Print implements Runnable &#123; private PipedReader in; Print(PipedReader in) &#123; this.in = in; &#125; public void run() &#123; int receive = 0; try &#123; while ((receive = in.read()) != -1) &#123; System.out.print((char) receive); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; FAQ start() 和 run() 有什么区别？可以直接调用 Thread 类的 run() 方法么？ run() 方法是线程的执行体。 start() 方法会启动线程，然后 JVM 会让这个线程去执行 run() 方法。 可以直接调用 Thread 类的 run() 方法么？ 可以。但是如果直接调用 Thread 的 run()方法，它的行为就会和普通的方法一样。 为了在新的线程中执行我们的代码，必须使用 Thread.start()方法。 sleep()、yield()、join() 方法有什么区别？为什么 sleep()和 yield()方法是静态的？ yield() yield() 方法可以让当前正在执行的线程暂停，但它不会阻塞该线程，它只是将该线程从 Running 状态转入 Runnable 状态。 当某个线程调用了 yield() 方法暂停之后，只有优先级与当前线程相同，或者优先级比当前线程更高的处于就绪状态的线程才会获得执行的机会。 sleep() sleep() 方法需要指定等待的时间，它可以让当前正在执行的线程在指定的时间内暂停执行，进入 Blocked 状态。 该方法既可以让其他同优先级或者高优先级的线程得到执行的机会，也可以让低优先级的线程得到执行机会。 但是， sleep() 方法不会释放“锁标志”，也就是说如果有 synchronized 同步块，其他线程仍然不能访问共享数据。 join() join() 方法会使当前线程转入 Blocked 状态，等待调用 join() 方法的线程结束后才能继续执行。 参考阅读：Java 线程中 yield 与 join 方法的区别 参考阅读：sleep()，wait()，yield()和 join()方法的区别 为什么 sleep() 和 yield() 方法是静态的 Thread 类的 sleep() 和 yield() 方法将处理 Running 状态的线程。所以在其他处于非 Running 状态的线程上执行这两个方法是没有意义的。这就是为什么这些方法是静态的。它们可以在当前正在执行的线程中工作，并避免程序员错误的认为可以在其他非运行线程调用这些方法。 Java 的线程优先级如何控制？高优先级的 Java 线程一定先执行吗？ Java 中的线程优先级如何控制 Java 中的线程优先级的范围是 [1,10]，一般来说，高优先级的线程在运行时会具有优先权。可以通过 thread.setPriority(Thread.MAX_PRIORITY) 的方式设置，默认优先级为 5。 高优先级的 Java 线程一定先执行吗 即使设置了线程的优先级，也无法保证高优先级的线程一定先执行。 原因：这是因为线程优先级依赖于操作系统的支持，然而，不同的操作系统支持的线程优先级并不相同，不能很好的和 Java 中线程优先级一一对应。 结论：Java 线程优先级控制并不可靠。 什么是守护线程？为什么要用守护线程？如何创建守护线程？ 什么是守护线程 守护线程（Daemon Thread）是在后台执行并且不会阻止 JVM 终止的线程。 与守护线程（Daemon Thread）相反的，叫用户线程（User Thread），也就是非守护线程。 为什么要用守护线程 守护线程的优先级比较低，用于为系统中的其它对象和线程提供服务。典型的应用就是垃圾回收器。 如何创建守护线程 使用 thread.setDaemon(true) 可以设置 thread 线程为守护线程。 注意点： 正在运行的用户线程无法设置为守护线程，所以 thread.setDaemon(true) 必须在 thread.start() 之前设置，否则会抛出 llegalThreadStateException 异常； 一个守护线程创建的子线程依然是守护线程。 不要认为所有的应用都可以分配给 Daemon 来进行服务，比如读写操作或者计算逻辑。 参考阅读：Java 中守护线程的总结 为什么线程通信的方法 wait(), notify()和 notifyAll()被定义在 Object 类里？ Java 的每个对象中都有一个锁(monitor，也可以成为监视器) 并且 wait()，notify()等方法用于等待对象的锁或者通知其他线程对象的监视器可用。在 Java 的线程中并没有可供任何对象使用的锁和同步器。这就是为什么这些方法是 Object 类的一部分，这样 Java 的每一个类都有用于线程间通信的基本方法 为什么 wait(), notify()和 notifyAll()必须在同步方法或者同步块中被调用？ 当一个线程需要调用对象的 wait()方法的时候，这个线程必须拥有该对象的锁，接着它就会释放这个对象锁并进入等待状态直到其他线程调用这个对象上的 notify()方法。同样的，当一个线程需要调用对象的 notify()方法时，它会释放这个对象的锁，以便其他在等待的线程就可以得到这个对象锁。由于所有的这些方法都需要线程持有对象的锁，这样就只能通过同步来实现，所以他们只能在同步方法或者同步块中被调用。 参考阅读：Java 并发编程：volatile 关键字解析 资料 Java 并发编程实战 Java 并发编程的艺术 https://stackoverflow.com/questions/27406200/visualvm-thread-states https://docs.oracle.com/javase/8/docs/api/index.html https://www.journaldev.com/1037/java-thread-wait-notify-and-notifyall-example http://www.importnew.com/14958.html https://blog.csdn.net/xiangwanpeng/article/details/54972952 https://blog.csdn.net/shimiso/article/details/8964414]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>concurrent</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>concurrent</tag>
        <tag>thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发简介]]></title>
    <url>%2Fblog%2F2018%2F05%2F21%2Fjava%2Fjavacore%2Fconcurrent%2F%E5%B9%B6%E5%8F%91%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[并发简介 📓 本文已归档到：「blog」 并发术语 并发和并行 同步和异步 阻塞和非阻塞 进程和线程 多线程的优点 更好的资源利用 程序设计更简单 程序响应更快 多线程的风险 安全性问题 活跃性问题 性能问题 资料 并发术语 学习并发编程，免不了大量接触一些术语。其中，有些术语经常容易被混淆或分不清它们的差异，这里汇总一下。 并发和并行 例子 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。 你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。 你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。 定义 并发的关键是你有处理多个任务的能力，不一定要同时。 并行的关键是你有同时处理多个任务的能力。 最关键的点就是：是否是同时。 同步和异步 例子 同步就像是打电话，不挂电话，通话不会结束。 异步就像是发短信，发完短信后，就可以做其他事，短信来了，手机会提醒。 定义 所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。 异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 阻塞和非阻塞 例子 阻塞调用就像是打电话，通话不结束，不能放下。 非阻塞调用就像是发短信，发完短信后，就可以做其他事，短信来了，手机会提醒。 定义 阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态. 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 进程和线程 什么是进程 简言之，进程可视为一个正在运行的程序。 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动。进程是操作系统进行资源分配的基本单位。 什么是线程 线程是操作系统进行调度的基本单位。 进程 vs. 线程 一个程序至少有一个进程，一个进程至少有一个线程。 线程比进程划分更细，所以执行开销更小，并发性更高 进程是一个实体，拥有独立的资源；而同一个进程中的多个线程共享进程的资源。 JVM 在单个进程中运行，JVM 中的线程共享属于该进程的堆。这就是为什么几个线程可以访问同一个对象。线程共享堆并拥有自己的堆栈空间。这是一个线程如何调用一个方法以及它的局部变量是如何保持线程安全的。但是堆不是线程安全的并且为了线程安全必须进行同步。 多线程的优点 更好的资源利用 更简单的编程模型 程序响应更灵敏 更好的资源利用 想象一下，一个应用程序需要从本地文件系统中读取和处理文件的情景。比方说，从磁盘读取一个文件需要 5 秒，处理一个文件需要 2 秒。处理两个文件则需要： 5秒读取文件A2秒处理文件A5秒读取文件B2秒处理文件B---------------------总共需要14秒 从磁盘中读取文件的时候，大部分的 CPU 时间用于等待磁盘去读取数据。在这段时间里，CPU 非常的空闲。它可以做一些别的事情。通过改变操作的顺序，就能够更好的使用 CPU 资源。看下面的顺序： 5秒读取文件A5秒读取文件B + 2秒处理文件A2秒处理文件B---------------------总共需要12秒 CPU 等待第一个文件被读取完。然后开始读取第二个文件。当第二文件在被读取的时候，CPU 会去处理第一个文件。记住，在等待磁盘读取文件的时候，CPU 大 部分时间是空闲的。 总的说来，CPU 能够在等待 IO 的时候做一些其他的事情。这个不一定就是磁盘 IO。它也可以是网络的 IO，或者用户输入。通常情况下，网络和磁盘的 IO 比 CPU 和内存的 IO 慢的多。 程序设计更简单 在单线程应用程序中，如果你想编写程序手动处理上面所提到的读取和处理的顺序，你必须记录每个文件读取和处理的状态。相反，你可以启动两个线程，每个线程处理一个文件的读取和操作。线程会在等待磁盘读取文件的过程中被阻塞。在等待的时候，其他的线程能够使用 CPU 去处理已经读取完的文件。其结果就是，磁盘总是在繁忙地读取不同的文件到内存中。这会带来磁盘和 CPU 利用率的提升。而且每个线程只需要记录一个文件，因此这种方式也很容易编程实现。 程序响应更快 将一个单线程应用程序变成多线程应用程序的另一个常见的目的是实现一个响应更快的应用程序。设想一个服务器应用，它在某一个端口监听进来的请求。当一个请求到来时，它去处理这个请求，然后再返回去监听。 服务器的流程如下所述： while(server is active) &#123; listen for request process request&#125; 如果一个请求需要占用大量的时间来处理，在这段时间内新的客户端就无法发送请求给服务端。只有服务器在监听的时候，请求才能被接收。另一种设计是，监听线程把请求传递给工作者线程(worker thread)，然后立刻返回去监听。而工作者线程则能够处理这个请求并发送一个回复给客户端。这种设计如下所述： while(server is active) &#123; listen for request hand request to worker thread&#125; 这种方式，服务端线程迅速地返回去监听。因此，更多的客户端能够发送请求给服务端。这个服务也变得响应更快。 桌面应用也是同样如此。如果你点击一个按钮开始运行一个耗时的任务，这个线程既要执行任务又要更新窗口和按钮，那么在任务执行的过程中，这个应用程序看起来好像没有反应一样。相反，任务可以传递给工作者线程（word thread)。当工作者线程在繁忙地处理任务的时候，窗口线程可以自由地响应其他用户的请求。当工作者线程完成任务的时候，它发送信号给窗口线程。窗口线程便可以更新应用程序窗口，并显示任务的结果。对用户而言，这种具有工作者线程设计的程序显得响应速度更快。 多线程的风险 凡事有利有弊，引入多线程除了带来的好处以外，也产生了一些问题： 安全性问题 活跃性问题 性能问题 安全性问题 什么是线程安全？ 线程安全很难定义。我所看到的对于线程安全的定义都太过抽象。 所以，我觉得有必要反向理解，知道什么是线程不安全，那么就能定义线程安全的边界了。 在同一程序中运行多个线程本身不会导致问题，问题在于多个线程访问了相同的资源。如：同一内存区（变量，数组，或对象）、系统（数据库，web services 等）或文件。 线程不安全的示例 public class Counter &#123; protected long count = 0; public void add(long value)&#123; this.count = this.count + value; &#125;&#125; 想象下线程 A 和 B 同时执行同一个 Counter 对象的 add()方法，我们无法知道操作系统何时会在两个线程之间切换。JVM 并不是将这段代码视为单条指令来执行的，而是按照下面的顺序： 从内存获取 this.count 的值放到寄存器将寄存器中的值增加 value将寄存器中的值写回内存 观察线程 A 和 B 交错执行会发生什么： this.count = 0;A: 读取 this.count 到一个寄存器 (0)B: 读取 this.count 到一个寄存器 (0)B: 将寄存器的值加 2B: 回写寄存器值(2)到内存. this.count 现在等于 2A: 将寄存器的值加 3A: 回写寄存器值(3)到内存. this.count 现在等于 3 两个线程分别加了 2 和 3 到 count 变量上，两个线程执行结束后 count 变量的值应该等于 5。然而由于两个线程是交叉执行的，两个线程从内存中读出的初始值都是 0。然后各自加了 2 和 3，并分别写回内存。最终的值并不是期望的 5，而是最后写回内存的那个线程的值，上面例子中最后写回内存的是线程 A，但实际中也可能是线程 B。如果没有采用合适的同步机制，线程间的交叉执行情况就无法预料。 竞态条件和临界区 竞态条件（Race Condition）：当两个线程竞争同一资源时，如果对资源的访问顺序敏感，就称存在竞态条件。 临界区（Critical Sections）：导致竞态条件发生的代码区称作临界区。 上例中 add()方法就是一个临界区,它会产生竞态条件。在临界区中使用适当的同步就可以避免竞态条件。 活跃性问题 死锁（Deadlock） 什么是死锁 多个线程互相等待对方释放锁。 死锁是当线程进入无限期等待状态时发生的情况，因为所请求的锁被另一个线程持有，而另一个线程又等待第一个线程持有的另一个锁。 避免死锁的方法 （1）加锁顺序 当多个线程需要相同的一些锁，但是按照不同的顺序加锁，死锁就很容易发生。 如果能确保所有的线程都是按照相同的顺序获得锁，那么死锁就不会发生。 按照顺序加锁是一种有效的死锁预防机制。但是，这种方式需要你事先知道所有可能会用到的锁(译者注：并对这些锁做适当的排序)，但总有些时候是无法预知的。 （2）加锁时限 另外一个可以避免死锁的方法是在尝试获取锁的时候加一个超时时间，这也就意味着在尝试获取锁的过程中若超过了这个时限该线程则放弃对该锁请求。若一个线程没有在给定的时限内成功获得所有需要的锁，则会进行回退并释放所有已经获得的锁，然后等待一段随机的时间再重试。这段随机的等待时间让其它线程有机会尝试获取相同的这些锁，并且让该应用在没有获得锁的时候可以继续运行(译者注：加锁超时后可以先继续运行干点其它事情，再回头来重复之前加锁的逻辑)。 （3）死锁检测 死锁检测是一个更好的死锁预防机制，它主要是针对那些不可能实现按序加锁并且锁超时也不可行的场景。 每当一个线程获得了锁，会在线程和锁相关的数据结构中（map、graph 等等）将其记下。除此之外，每当有线程请求锁，也需要记录在这个数据结构中。 当一个线程请求锁失败时，这个线程可以遍历锁的关系图看看是否有死锁发生。 如果检测出死锁，有两种处理手段： 释放所有锁，回退，并且等待一段随机的时间后重试。这个和简单的加锁超时类似，不一样的是只有死锁已经发生了才回退，而不会是因为加锁的请求超时了。虽然有回退和等待，但是如果有大量的线程竞争同一批锁，它们还是会重复地死锁（编者注：原因同超时类似，不能从根本上减轻竞争）。 一个更好的方案是给这些线程设置优先级，让一个（或几个）线程回退，剩下的线程就像没发生死锁一样继续保持着它们需要的锁。如果赋予这些线程的优先级是固定不变的，同一批线程总是会拥有更高的优先级。为避免这个问题，可以在死锁发生的时候设置随机的优先级。 活锁（Livelock） 什么是活锁？ 活锁是一个递归的情况，两个或更多的线程会不断重复一个特定的代码逻辑。预期的逻辑通常为其他线程提供机会继续支持’this’线程。 想象这样一个例子：两个人在狭窄的走廊里相遇，二者都很礼貌，试图移到旁边让对方先通过。但是他们最终在没有取得任何进展的情况下左右摇摆，因为他们都在同一时间向相同的方向移动。 如图所示：两个线程想要通过一个 Worker 对象访问共享公共资源的情况，但是当他们看到另一个 Worker（在另一个线程上调用）也是“活动的”时，它们会尝试将该资源交给其他工作者并等待为它完成。如果最初我们让两名工作人员都活跃起来，他们将会面临活锁问题。 避免活锁 没有避免活锁的通用指南，但是在我们改变其他线程也使用的公共对象状态的场景中，例如在上述场景中，我们必须小心 Worker 对象。 饥饿（Starvation） 导致饥饿问题的原因 高优先级线程吞噬所有的低优先级线程的 CPU 时间。 线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续地对该同步块进行访问。 线程在等待一个本身(在其上调用 wait())也处于永久等待完成的对象，因为其他线程总是被持续地获得唤醒。 饥饿问题最经典的例子就是哲学家问题。如图所示：有五个哲学家用餐，每个人要活得两把叉子才可以就餐。当 2、4 就餐时，1、3、5 永远无法就餐，只能看着盘中的美食饥饿的等待着。 解决饥饿问题的方法 Java 不可能实现 100% 的公平性，我们依然可以通过同步结构在线程间实现公平性的提高。 使用锁，而不是同步块。 公平锁。 性能问题 并发执行一定比串行执行快吗？ 答案是：不一定。因为有创建线程和线程上下文切换的开销。 上下文切换 什么是上下文切换？ 当 CPU 从执行一个线程切换到执行另一个线程时，CPU 需要保存当前线程的本地数据，程序指针等状态，并加载下一个要执行的线程的本地数据，程序指针等。这个开关被称为“上下文切换”。 减少上下文切换的方法 无锁并发编程 - 多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的 ID 按照 Hash 算法取模分段，不同的线程处理不同段的数据。 CAS 算法 - Java 的 Atomic 包使用 CAS 算法来更新数据，而不需要加锁。 使用最少线程 - 避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态。 使用协程 - 在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。 资源限制 什么是资源限制 资源限制是指在进行并发编程时，程序的执行速度受限于计算机硬件资源或软件资源。 资源限制引发的问题 在并发编程中，将代码执行速度加快的原则是将代码中串行执行的部分变成并发执行，但是如果将某段串行的代码并发执行，因为受限于资源，仍然在串行执行，这时候程序不仅不会加快执行，反而会更慢，因为增加了上下文切换和资源调度的时间。 如何解决资源限制的问题 在资源限制情况下进行并发编程，根据不同的资源限制调整程序的并发度。 对于硬件资源限制，可以考虑使用集群并行执行程序。 对于软件资源限制，可以考虑使用资源池将资源复用。 资料 Java 并发编程实战：第 1 章 简介 Java 并发编程的艺术：第 1 章 Java 并发编程的挑战 http://tutorials.jenkov.com/java-concurrency/benefits.html https://www.logicbig.com/tutorials/core-java-tutorial/java-multi-threading/thread-deadlock.html https://www.logicbig.com/tutorials/core-java-tutorial/java-multi-threading/thread-livelock.html https://www.logicbig.com/tutorials/core-java-tutorial/java-multi-threading/thread-starvation.html https://www.zhihu.com/question/33515481 https://blog.csdn.net/yaosiming2011/article/details/44280797]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>concurrent</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存模型]]></title>
    <url>%2Fblog%2F2018%2F05%2F19%2Fjava%2Fjavacore%2Fconcurrent%2F%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[内存模型 📓 本文已归档到：「blog」 本文内容基于 JDK1.8。 Java 内存模型（Java Memory Model），以下简称 JMM。 内部原理 内存模型结构 线程栈 堆 硬件内存架构 JMM 和硬件内存架构之间的桥接 共享对象可见性 竞态条件 Happens-Before 资料 内部原理 JVM 中试图定义一种 JMM 来屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问效果。 JMM 的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节。此处的变量与 Java 编程中的变量有所区别，它包括了实例字段、静态字段和构成数组对象的元素，但不包括局部变量与方法参数，因为后者是线程私有的，不会被共享，自然就不会存在竞争问题。为了获得较好的执行效能，Java 内存模型并没有限制执行引擎使用处理器的特定寄存器或缓存来和主存进行交互，也没有限制即使编译器进行调整代码执行顺序这类优化措施。 JMM 是围绕着在并发过程中如何处理原子性、可见性和有序性这 3 个特征来建立的。 JMM 是通过各种操作来定义的，包括对变量的读写操作，监视器的加锁和释放操作，以及线程的启动和合并操作。 内存模型结构 Java 内存模型把 Java 虚拟机内部划分为线程栈和堆。 线程栈 每一个运行在 Java 虚拟机里的线程都拥有自己的线程栈。这个线程栈包含了这个线程调用的方法当前执行点相关的信息。一个线程仅能访问自己的线程栈。一个线程创建的本地变量对其它线程不可见，仅自己可见。即使两个线程执行同样的代码，这两个线程任然在在自己的线程栈中的代码来创建本地变量。因此，每个线程拥有每个本地变量的独有版本。 所有原始类型的本地变量都存放在线程栈上，因此对其它线程不可见。一个线程可能向另一个线程传递一个原始类型变量的拷贝，但是它不能共享这个原始类型变量自身。 堆 堆上包含在 Java 程序中创建的所有对象，无论是哪一个对象创建的。这包括原始类型的对象版本。如果一个对象被创建然后赋值给一个局部变量，或者用来作为另一个对象的成员变量，这个对象任然是存放在堆上。 一个本地变量可能是原始类型，在这种情况下，它总是在线程栈上。 一个本地变量也可能是指向一个对象的一个引用。在这种情况下，引用（这个本地变量）存放在线程栈上，但是对象本身存放在堆上。 一个对象可能包含方法，这些方法可能包含本地变量。这些本地变量任然存放在线程栈上，即使这些方法所属的对象存放在堆上。 一个对象的成员变量可能随着这个对象自身存放在堆上。不管这个成员变量是原始类型还是引用类型。 静态成员变量跟随着类定义一起也存放在堆上。 存放在堆上的对象可以被所有持有对这个对象引用的线程访问。当一个线程可以访问一个对象时，它也可以访问这个对象的成员变量。如果两个线程同时调用同一个对象上的同一个方法，它们将会都访问这个对象的成员变量，但是每一个线程都拥有这个本地变量的私有拷贝。 硬件内存架构 现代硬件内存模型与 Java 内存模型有一些不同。理解内存模型架构以及 Java 内存模型如何与它协同工作也是非常重要的。这部分描述了通用的硬件内存架构，下面的部分将会描述 Java 内存是如何与它“联手”工作的。 一个现代计算机通常由两个或者多个 CPU。其中一些 CPU 还有多核。从这一点可以看出，在一个有两个或者多个 CPU 的现代计算机上同时运行多个线程是可能的。每个 CPU 在某一时刻运行一个线程是没有问题的。这意味着，如果你的 Java 程序是多线程的，在你的 Java 程序中每个 CPU 上一个线程可能同时（并发）执行。 每个 CPU 都包含一系列的寄存器，它们是 CPU 内内存的基础。CPU 在寄存器上执行操作的速度远大于在主存上执行的速度。这是因为 CPU 访问寄存器的速度远大于主存。 每个 CPU 可能还有一个 CPU 缓存层。实际上，绝大多数的现代 CPU 都有一定大小的缓存层。CPU 访问缓存层的速度快于访问主存的速度，但通常比访问内部寄存器的速度还要慢一点。一些 CPU 还有多层缓存，但这些对理解 Java 内存模型如何和内存交互不是那么重要。只要知道 CPU 中可以有一个缓存层就可以了。 一个计算机还包含一个主存。所有的 CPU 都可以访问主存。主存通常比 CPU 中的缓存大得多。 通常情况下，当一个 CPU 需要读取主存时，它会将主存的部分读到 CPU 缓存中。它甚至可能将缓存中的部分内容读到它的内部寄存器中，然后在寄存器中执行操作。当 CPU 需要将结果写回到主存中去时，它会将内部寄存器的值刷新到缓存中，然后在某个时间点将值刷新回主存。 当 CPU 需要在缓存层存放一些东西的时候，存放在缓存中的内容通常会被刷新回主存。CPU 缓存可以在某一时刻将数据局部写到它的内存中，和在某一时刻局部刷新它的内存。它不会再某一时刻读/写整个缓存。通常，在一个被称作“cache lines”的更小的内存块中缓存被更新。一个或者多个缓存行可能被读到缓存，一个或者多个缓存行可能再被刷新回主存。 JMM 和硬件内存架构之间的桥接 上面已经提到，Java 内存模型与硬件内存架构之间存在差异。硬件内存架构没有区分线程栈和堆。对于硬件，所有的线程栈和堆都分布在主内中。部分线程栈和堆可能有时候会出现在 CPU 缓存中和 CPU 内部的寄存器中。如下图所示： 当对象和变量被存放在计算机中各种不同的内存区域中时，就可能会出现一些具体的问题。主要包括如下两个方面： 线程对共享变量修改的可见性 当读，写和检查共享变量时出现 race conditions 共享对象可见性 如果两个或者更多的线程在没有正确的使用 volatile 声明或者同步的情况下共享一个对象，一个线程更新这个共享对象可能对其它线程来说是不接见的。 想象一下，共享对象被初始化在主存中。跑在 CPU 上的一个线程将这个共享对象读到 CPU 缓存中。然后修改了这个对象。只要 CPU 缓存没有被刷新会主存，对象修改后的版本对跑在其它 CPU 上的线程都是不可见的。这种方式可能导致每个线程拥有这个共享对象的私有拷贝，每个拷贝停留在不同的 CPU 缓存中。 上图示意了这种情形。跑在左边 CPU 的线程拷贝这个共享对象到它的 CPU 缓存中，然后将 count 变量的值修改为 2。这个修改对跑在右边 CPU 上的其它线程是不可见的，因为修改后的 count 的值还没有被刷新回主存中去。 解决这个问题你可以使用 Java 中的 volatile 关键字。volatile 关键字可以保证直接从主存中读取一个变量，如果这个变量被修改后，总是会被写回到主存中去。 竞态条件 如果两个或者更多的线程共享一个对象，多个线程在这个共享对象上更新变量，就有可能发生 race conditions。 想象一下，如果线程 A 读一个共享对象的变量 count 到它的 CPU 缓存中。再想象一下，线程 B 也做了同样的事情，但是往一个不同的 CPU 缓存中。现在线程 A 将 count 加 1，线程 B 也做了同样的事情。现在 count 已经被增在了两个，每个 CPU 缓存中一次。 如果这些增加操作被顺序的执行，变量 count 应该被增加两次，然后原值+2 被写回到主存中去。 然而，两次增加都是在没有适当的同步下并发执行的。无论是线程 A 还是线程 B 将 count 修改后的版本写回到主存中取，修改后的值仅会被原值大 1，尽管增加了两次。 解决这个问题可以使用 Java 同步块。一个同步块可以保证在同一时刻仅有一个线程可以进入代码的临界区。同步块还可以保证代码块中所有被访问的变量将会从主存中读入，当线程退出同步代码块时，所有被更新的变量都会被刷新回主存中去，不管这个变量是否被声明为 volatile。 Happens-Before JMM 为程序中所有的操作定义了一个偏序关系，称之为 Happens-Before。 程序顺序规则：如果程序中操作 A 在操作 B 之前，那么在线程中操作 A 将在操作 B 之前执行。 监视器锁规则：在监视器锁上的解锁操作必须在同一个监视器锁上的加锁操作之前执行。 volatile 变量规则：对 volatile 变量的写入操作必须在对该变量的读操作之前执行。 线程启动规则：在线程上对 Thread.start 的调用必须在该线程中执行任何操作之前执行。 线程结束规则：线程中的任何操作都必须在其他线程检测到该线程已经结束之前执行，或者从 Thread.join 中成功返回，或者在调用 Thread.isAlive 时返回 false。 中断规则：当一个线程在另一个线程上调用 interrupt 时，必须在被中断线程检测到 interrupt 调用之前执行（通过抛出 InterruptException，或者调用 isInterrupted 和 interrupted）。 终结器规则：对象的构造函数必须在启动该对象的终结器之前执行完成。 传递性：如果操作 A 在操作 B 之前执行，并且操作 B 在操作 C 之前执行，那么操作 A 必须在操作 C 之前执行。 资料 Java 并发编程实战：第 16 章 Java 内存模型 Java 并发编程的艺术：第 3 章 Java 内存模型 深入理解 Java 虚拟机：第 12 章 Java 内存模型与线程 http://tutorials.jenkov.com/java-concurrency/java-memory-model.html]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>concurrent</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>memory</tag>
        <tag>concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发机制的底层实现]]></title>
    <url>%2Fblog%2F2018%2F05%2F19%2Fjava%2Fjavacore%2Fconcurrent%2F%E5%B9%B6%E5%8F%91%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[并发机制的底层实现 📓 本文已归档到：「blog」 本文内容基于 JDK1.8。 concurrent 包的实现 synchronized synchronized 的要点 synchronized 的原理 volatile volatile 的要点 volatile 的原理 volatile 的应用场景 CAS 简介 操作 应用 原理 特点 总结 资料 concurrent 包的实现 由于 Java 的 CAS 同时具有 volatile 读和 volatile 写的内存语义，因此 Java 线程之间的通信现在有了下面四种方式： A 线程写 volatile 变量，随后 B 线程读这个 volatile 变量。 A 线程写 volatile 变量，随后 B 线程用 CAS 更新这个 volatile 变量。 A 线程用 CAS 更新一个 volatile 变量，随后 B 线程用 CAS 更新这个 volatile 变量。 A 线程用 CAS 更新一个 volatile 变量，随后 B 线程读这个 volatile 变量。 同时，volatile 变量的读/写和 CAS 可以实现线程之间的通信。把这些特性整合在一起，就形成了整个 concurrent 包得以实现的基石。如果我们仔细分析 concurrent 包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为 volatile； 然后，使用 CAS 的原子条件更新来实现线程之间的同步； 同时，配合以 volatile 的读/写和 CAS 所具有的 volatile 读和写的内存语义来实现线程之间的通信。 AQS，非阻塞数据结构和原子变量类（Java.util.concurrent.atomic 包中的类），这些 concurrent 包中的基础类都是使用这种模式来实现的，而 concurrent 包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent 包的实现示意图如下： synchronized synchronized 的要点 关键字 synchronized 可以保证在同一个时刻，只有一个线程可以执行某个方法或者某个代码块。 synchronized 有 3 种应用方式： 同步实例方法 同步静态方法 同步代码块 同步实例方法 ❌ 错误示例 - 未同步的示例 @NotThreadSafepublic class SynchronizedDemo01 implements Runnable &#123; static int i = 0; public void increase() &#123; i++; &#125; @Override public void run() &#123; for (int j = 0; j &lt; 100000; j++) &#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; SynchronizedDemo01 instance = new SynchronizedDemo01(); Thread t1 = new Thread(instance); Thread t2 = new Thread(instance); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i); &#125;&#125;// 输出结果: 小于 200000 的随机数字 Java 实例方法同步是同步在拥有该方法的对象上。这样，每个实例其方法同步都同步在不同的对象上，即该方法所属的实例。只有一个线程能够在实例方法同步块中运行。如果有多个实例存在，那么一个线程一次可以在一个实例同步块中执行操作。一个实例一个线程。 @ThreadSafepublic class SynchronizedDemo02 implements Runnable &#123; static int i = 0; public synchronized void increase() &#123; i++; &#125; @Override public void run() &#123; for (int j = 0; j &lt; 100000; j++) &#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; SynchronizedDemo02 instance = new SynchronizedDemo02(); Thread t1 = new Thread(instance); Thread t2 = new Thread(instance); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i); &#125;&#125;// 输出结果:// 2000000 同步静态方法 静态方法的同步是指同步在该方法所在的类对象上。因为在 JVM 中一个类只能对应一个类对象，所以同时只允许一个线程执行同一个类中的静态同步方法。 对于不同类中的静态同步方法，一个线程可以执行每个类中的静态同步方法而无需等待。不管类中的那个静态同步方法被调用，一个类只能由一个线程同时执行。 @ThreadSafepublic class SynchronizedDemo03 implements Runnable &#123; static int i = 0; public static synchronized void increase() &#123; i++; &#125; @Override public void run() &#123; for (int j = 0; j &lt; 100000; j++) &#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(new SynchronizedDemo03()); Thread t2 = new Thread(new SynchronizedDemo03()); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i); &#125;&#125;// 输出结果:// 200000 同步代码块 有时你不需要同步整个方法，而是同步方法中的一部分。Java 可以对方法的一部分进行同步。 注意 Java 同步块构造器用括号将对象括起来。在上例中，使用了 this，即为调用 add 方法的实例本身。在同步构造器中用括号括起来的对象叫做监视器对象。上述代码使用监视器对象同步，同步实例方法使用调用方法本身的实例作为监视器对象。 一次只有一个线程能够在同步于同一个监视器对象的 Java 方法内执行。 @ThreadSafepublic class SynchronizedDemo04 implements Runnable &#123; static int i = 0; static SynchronizedDemo04 instance = new SynchronizedDemo04(); @Override public void run() &#123; synchronized (instance) &#123; for (int j = 0; j &lt; 100000; j++) &#123; i++; &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(instance); Thread t2 = new Thread(instance); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(i); &#125;&#125;// 输出结果:// 200000 synchronized 的原理 synchronized 实现同步的基础是：Java 中的每一个对象都可以作为锁。 对于普通同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前类的 Class 对象。 对于同步方法块，锁是 Synchonized 括号里配置的对象。 👉 参考阅读：Java 并发编程：synchronized 👉 参考阅读：深入理解 Java 并发之 synchronized 实现原理 volatile volatile 的要点 volatile 是轻量级的 synchronized，它在多处理器开发中保证了共享变量的“可见性”。 可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。 一旦一个共享变量（类的成员变量、类的静态成员变量）被 volatile 修饰之后，那么就具备了两层语义： 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 禁止进行指令重排序。 如果一个字段被声明成 volatile，Java 线程内存模型确保所有线程看到这个变量的值是一致的。 volatile 的原理 观察加入 volatile 关键字和没有加入 volatile 关键字时所生成的汇编代码发现，加入 volatile 关键字时，会多出一个 lock 前缀指令。 lock 前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供 3 个功能： 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他 CPU 中对应的缓存行无效。 volatile 的应用场景 如果 volatile 变量修饰符使用恰当的话，它比 synchronized 的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。 但是，volatile 无法替代 synchronized ，因为 volatile 无法保证操作的原子性。通常来说，使用 volatile 必须具备以下 2 个条件： 对变量的写操作不依赖于当前值 该变量没有包含在具有其他变量的不变式中 应用场景： 状态标记量 volatile boolean flag = false;while(!flag) &#123; doSomething();&#125;public void setFlag() &#123; flag = true;&#125; double check class Singleton &#123; private volatile static Singleton instance = null; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if(instance==null) &#123; synchronized (Singleton.class) &#123; if(instance==null) instance = new Singleton(); &#125; &#125; return instance; &#125;&#125; 👉 参考阅读：Java 并发编程：volatile 关键字解析 CAS 简介 CAS（Compare and Swap），字面意思为比较并交换。CAS 有 3 个操作数，内存值 V，旧的预期值 A，要修改的新值 B。当且仅当预期值 A 和内存值 V 相同时，将内存值 V 修改为 B，否则什么都不做。 操作 我们常常做这样的操作 if(a==b) &#123; a++;&#125; 试想一下如果在做 a之前 a 的值被改变了怎么办？a还执行吗？出现该问题的原因是在多线程环境下，a 的值处于一种不定的状态。采用锁可以解决此类问题，但 CAS 也可以解决，而且可以不加锁。 int expect = a;if(a.compareAndSet(expect,a+1)) &#123; doSomeThing1();&#125; else &#123; doSomeThing2();&#125; 这样如果 a 的值被改变了 a就不会被执行。按照上面的写法，a!=expect 之后,a就不会被执行，如果我们还是想执行 a++操作怎么办，没关系，可以采用 while 循环 while(true) &#123; int expect = a; if (a.compareAndSet(expect, a + 1)) &#123; doSomeThing1(); return; &#125; else &#123; doSomeThing2(); &#125;&#125; 采用上面的写法，在没有锁的情况下实现了 a++操作，这实际上是一种非阻塞算法。 应用 非阻塞算法 （nonblocking algorithms） 一个线程的失败或者挂起不应该影响其他线程的失败或挂起的算法。 现代的 CPU 提供了特殊的指令，可以自动更新共享数据，而且能够检测到其他线程的干扰，而 compareAndSet() 就用这些代替了锁定。 拿出 AtomicInteger 来研究在没有锁的情况下是如何做到数据正确性的。 private volatile int value; 首先毫无疑问，在没有锁的机制下可能需要借助 volatile 原语，保证线程间的数据是可见的（共享的）。 这样才获取变量的值的时候才能直接读取。 public final int get() &#123; return value;&#125; 然后来看看++i 是怎么做到的。 public final int incrementAndGet() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; &#125;&#125; 在这里采用了 CAS 操作，每次从内存中读取数据然后将此数据和+1 后的结果进行 CAS 操作，如果成功就返回结果，否则重试直到成功为止。 而 compareAndSet 利用 JNI 来完成 CPU 指令的操作。 public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update);&#125; 整体的过程就是这样子的，利用 CPU 的 CAS 指令，同时借助 JNI 来完成 Java 的非阻塞算法。其它原子操作都是利用类似的特性完成的。 其中 unsafe.compareAndSwapInt(this, valueOffset, expect, update)类似： if (this == expect) &#123; this = update return true;&#125; else &#123; return false;&#125; 那么问题就来了，成功过程中需要 2 个步骤：比较 this == expect，替换 this = update，compareAndSwapInt 如何这两个步骤的原子性呢？ 参考 CAS 的原理 原理 Java 代码如何确保处理器执行 CAS 操作？ CAS 通过调用 JNI（JNI:Java Native Interface 为 Java 本地调用，允许 Java 调用其他语言。）的代码实现的。JVM 将 CAS 操作编译为底层提供的最有效方法。在支持 CAS 的处理器上，JVM 将它们编译为相应的机器指令；在不支持 CAS 的处理器上，JVM 将使用自旋锁。 特点 优点 一般情况下，比锁性能更高。因为 CAS 是一种非阻塞算法，所以其避免了线程被阻塞时的等待时间。 缺点 ABA 问题 因为 CAS 需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是 A，变成了 B，又变成了 A，那么使用 CAS 进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA 问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么 A－B－A 就会变成 1A-2B－3A。 从 Java1.5 开始 JDK 的 atomic 包里提供了一个类 AtomicStampedReference 来解决 ABA 问题。这个类的 compareAndSet 方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 循环时间长开销大 自旋 CAS 如果长时间不成功，会给 CPU 带来非常大的执行开销。如果 JVM 能支持处理器提供的 pause 指令那么效率会有一定的提升，pause 指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使 CPU 不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起 CPU 流水线被清空（CPU pipeline flush），从而提高 CPU 的执行效率。 比较花费 CPU 资源，即使没有任何用也会做一些无用功。 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量 i ＝ 2,j=a，合并一下 ij=2a，然后用 CAS 来操作 ij。从 Java1.5 开始 JDK 提供了 AtomicReference 类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作。 总结 可以用 CAS 在无锁的情况下实现原子操作，但要明确应用场合，非常简单的操作且又不想引入锁可以考虑使用 CAS 操作，当想要非阻塞地完成某一操作也可以考虑 CAS。不推荐在复杂操作中引入 CAS，会使程序可读性变差，且难以测试，同时会出现 ABA 问题。 资料 Java 并发编程实战 Java 并发编程的艺术：第 2 章 Java 并发机制的底层实现原理 https://www.jianshu.com/p/473e14d5ab2d https://blog.csdn.net/ls5718/article/details/52563959 http://tutorials.jenkov.com/java-concurrency/non-blocking-algorithms.html]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>concurrent</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同步容器和并发容器]]></title>
    <url>%2Fblog%2F2018%2F05%2F15%2Fjava%2Fjavacore%2Fconcurrent%2F%E5%90%8C%E6%AD%A5%E5%AE%B9%E5%99%A8%E5%92%8C%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[同步容器和并发容器 📓 本文已归档到：「blog」 本文内容基于 JDK1.8。 同步容器 同步容器的缺陷 并发容器 ConcurrentHashMap CopyOnWriteArrayList 资料 同步容器 在 Java 中，同步容器主要包括 2 类： Vector、Stack、HashTable Vector 实现了 List 接口，Vector 实际上就是一个数组，和 ArrayList 类似，但是 Vector 中的方法都是 synchronized 方法，即进行了同步措施。 Stack 也是一个同步容器，它的方法也用 synchronized 进行了同步，它实际上是继承于 Vector 类。 HashTable 实现了 Map 接口，它和 HashMap 很相似，但是 HashTable 进行了同步处理，而 HashMap 没有。 Collections 类中提供的静态工厂方法创建的类（由 Collections.synchronizedXxxx 等方法） 同步容器的缺陷 同步容器的同步原理就是在方法上用 synchronized 修饰。那么，这些方法每次只允许一个线程调用执行。 性能问题 由于被 synchronized 修饰的方法，每次只允许一个线程执行，其他试图访问这个方法的线程只能等待。显然，这种方式比没有使用 synchronized 的容器性能要差。 安全问题 同步容器真的一定安全吗？ 答案是：未必。同步容器未必真的安全。在做复合操作时，仍然需要加锁来保护。 常见复合操作如下： 迭代：反复访问元素，直到遍历完全部元素； 跳转：根据指定顺序寻找当前元素的下一个（下 n 个）元素； 条件运算：例如若没有则添加等； 不安全的示例 public class Test &#123; static Vector&lt;Integer&gt; vector = new Vector&lt;Integer&gt;(); public static void main(String[] args) throws InterruptedException &#123; while(true) &#123; for(int i=0;i&lt;10;i++) vector.add(i); Thread thread1 = new Thread()&#123; public void run() &#123; for(int i=0;i&lt;vector.size();i++) vector.remove(i); &#125;; &#125;; Thread thread2 = new Thread()&#123; public void run() &#123; for(int i=0;i&lt;vector.size();i++) vector.get(i); &#125;; &#125;; thread1.start(); thread2.start(); while(Thread.activeCount()&gt;10) &#123; &#125; &#125; &#125;&#125; 执行时可能会出现数组越界错误。 Vector 是线程安全的，为什么还会报这个错？很简单，对于 Vector，虽然能保证每一个时刻只能有一个线程访问它，但是不排除这种可能： 当某个线程在某个时刻执行这句时： for(int i=0;i&lt;vector.size();i++) vector.get(i); 假若此时 vector 的 size 方法返回的是 10，i 的值为 9 然后另外一个线程执行了这句： for(int i=0;i&lt;vector.size();i++) vector.remove(i); 将下标为 9 的元素删除了。 那么通过 get 方法访问下标为 9 的元素肯定就会出问题了。 安全示例 因此为了保证线程安全，必须在方法调用端做额外的同步措施，如下面所示： public class Test &#123; static Vector&lt;Integer&gt; vector = new Vector&lt;Integer&gt;(); public static void main(String[] args) throws InterruptedException &#123; while(true) &#123; for(int i=0;i&lt;10;i++) vector.add(i); Thread thread1 = new Thread()&#123; public void run() &#123; synchronized (Test.class) &#123; //进行额外的同步 for(int i=0;i&lt;vector.size();i++) vector.remove(i); &#125; &#125;; &#125;; Thread thread2 = new Thread()&#123; public void run() &#123; synchronized (Test.class) &#123; for(int i=0;i&lt;vector.size();i++) vector.get(i); &#125; &#125;; &#125;; thread1.start(); thread2.start(); while(Thread.activeCount()&gt;10) &#123; &#125; &#125; &#125;&#125; ConcurrentModificationException 异常 在对 Vector 等容器并发地进行迭代修改时，会报 ConcurrentModificationException 异常，关于这个异常将会在后续文章中讲述。 但是在并发容器中不会出现这个问题。 并发容器 JDK 的 java.util.concurrent 包（即 juc）中提供了几个非常有用的并发容器。 CopyOnWriteArrayList - 线程安全的 ArrayList CopyOnWriteArraySet - 线程安全的 Set，它内部包含了一个 CopyOnWriteArrayList，因此本质上是由 CopyOnWriteArrayList 实现的。 ConcurrentSkipListSet - 相当于线程安全的 TreeSet。它是有序的 Set。它由 ConcurrentSkipListMap 实现。 ConcurrentHashMap - 线程安全的 HashMap。采用分段锁实现高效并发。 ConcurrentSkipListMap - 线程安全的有序 Map。使用跳表实现高效并发。 ConcurrentLinkedQueue - 线程安全的无界队列。底层采用单链表。支持 FIFO。 ConcurrentLinkedDeque - 线程安全的无界双端队列。底层采用双向链表。支持 FIFO 和 FILO。 ArrayBlockingQueue - 数组实现的阻塞队列。 LinkedBlockingQueue - 链表实现的阻塞队列。 LinkedBlockingDeque - 双向链表实现的双端阻塞队列。 ConcurrentHashMap 要点 作用：ConcurrentHashMap 是线程安全的 HashMap。 原理：JDK6 与 JDK7 中，ConcurrentHashMap 采用了分段锁机制。JDK8 中，摒弃了锁分段机制，改为利用 CAS 算法。 源码 JDK7 ConcurrentHashMap 类在 jdk1.7 中的设计，其基本结构如图所示： 每一个 segment 都是一个 HashEntry&lt;K,V&gt;[] table， table 中的每一个元素本质上都是一个 HashEntry 的单向队列。比如 table[3]为首节点，table[3]-&gt;next 为节点 1，之后为节点 2，依次类推。 public class ConcurrentHashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements ConcurrentMap&lt;K, V&gt;, Serializable &#123; // 将整个hashmap分成几个小的map，每个segment都是一个锁；与hashtable相比，这么设计的目的是对于put, remove等操作，可以减少并发冲突，对 // 不属于同一个片段的节点可以并发操作，大大提高了性能 final Segment&lt;K,V&gt;[] segments; // 本质上Segment类就是一个小的hashmap，里面table数组存储了各个节点的数据，继承了ReentrantLock, 可以作为互拆锁使用 static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; &#125; // 基本节点，存储Key， Value值 static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; &#125;&#125; JDK8 jdk8 中主要做了 2 方面的改进 取消 segments 字段，直接采用 transient volatile HashEntry&lt;K,V&gt;[] table 保存数据，采用 table 数组元素作为锁，从而实现了对每一行数据进行加锁，进一步减少并发冲突的概率。 将原先 table 数组＋单向链表的数据结构，变更为 table 数组＋单向链表＋红黑树的结构。对于 hash 表来说，最核心的能力在于将 key hash 之后能均匀的分布在数组中。如果 hash 之后散列的很均匀，那么 table 数组中的每个队列长度主要为 0 或者 1。但实际情况并非总是如此理想，虽然 ConcurrentHashMap 类默认的加载因子为 0.75，但是在数据量过大或者运气不佳的情况下，还是会存在一些队列长度过长的情况，如果还是采用单向列表方式，那么查询某个节点的时间复杂度为 O(n)；因此，对于个数超过 8(默认值)的列表，jdk1.8 中采用了红黑树的结构，那么查询的时间复杂度可以降低到 O(logN)，可以改进性能。 final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 如果table为空，初始化；否则，根据hash值计算得到数组索引i，如果tab[i]为空，直接新建节点Node即可。注：tab[i]实质为链表或者红黑树的首节点。 if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // 如果tab[i]不为空并且hash值为MOVED，说明该链表正在进行transfer操作，返回扩容完成后的table。 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; // 针对首个节点进行加锁操作，而不是segment，进一步减少线程冲突 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 如果在链表中找到值为key的节点e，直接设置e.val = value即可。 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; // 如果没有找到值为key的节点，直接新建Node并加入链表即可。 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 如果首节点为TreeBin类型，说明为红黑树结构，执行putTreeVal操作。 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; // 如果节点数&gt;＝8，那么转换链表结构为红黑树结构。 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 计数增加1，有可能触发transfer操作(扩容)。 addCount(1L, binCount); return null;&#125; 示例 public class ConcurrentHashMapDemo &#123; public static void main(String[] args) throws InterruptedException &#123; // HashMap 在并发迭代访问时会抛出 ConcurrentModificationException 异常 // Map&lt;Integer, Character&gt; map = new HashMap&lt;&gt;(); Map&lt;Integer, Character&gt; map = new ConcurrentHashMap&lt;&gt;(); Thread wthread = new Thread(() -&gt; &#123; System.out.println("写操作线程开始执行"); for (int i = 0; i &lt; 26; i++) &#123; map.put(i, (char) ('a' + i)); &#125; &#125;); Thread rthread = new Thread(() -&gt; &#123; System.out.println("读操作线程开始执行"); for (Integer key : map.keySet()) &#123; System.out.println(key + " - " + map.get(key)); &#125; &#125;); wthread.start(); rthread.start(); Thread.sleep(1000); &#125;&#125; CopyOnWriteArrayList 要点 作用：CopyOnWrite 字面意思为写入时复制。CopyOnWriteArrayList 是线程安全的 ArrayList。 原理： 在 CopyOnWriteAarrayList 中，读操作不同步，因为它们在内部数组的快照上工作，所以多个迭代器可以同时遍历而不会相互阻塞（1,2,4）。 所有的写操作都是同步的。他们在备份数组（3）的副本上工作。写操作完成后，后备阵列将被替换为复制的阵列，并释放锁定。支持数组变得易变，所以替换数组的调用是原子（5）。 写操作后创建的迭代器将能够看到修改的结构（6,7）。 写时复制集合返回的迭代器不会抛出 ConcurrentModificationException，因为它们在数组的快照上工作，并且无论后续的修改（2,4）如何，都会像迭代器创建时那样完全返回元素。 源码 重要属性 lock - 执行写时复制操作，需要使用可重入锁加锁 array - 对象数组，用于存放元素 /** The lock protecting all mutators */final transient ReentrantLock lock = new ReentrantLock();/** The array, accessed only via getArray/setArray. */private transient volatile Object[] array; 重要方法 添加操作 添加的逻辑很简单，先将原容器 copy 一份，然后在新副本上执行写操作，之后再切换引用。当然此过程是要加锁的。 public boolean add(E e) &#123; //ReentrantLock加锁，保证线程安全 final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; //拷贝原容器，长度为原容器长度加一 Object[] newElements = Arrays.copyOf(elements, len + 1); //在新副本上执行添加操作 newElements[len] = e; //将原容器引用指向新副本 setArray(newElements); return true; &#125; finally &#123; //解锁 lock.unlock(); &#125;&#125; 删除操作 删除操作同理，将除要删除元素之外的其他元素拷贝到新副本中，然后切换引用，将原容器引用指向新副本。同属写操作，需要加锁。 public E remove(int index) &#123; //加锁 final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; E oldValue = get(elements, index); int numMoved = len - index - 1; if (numMoved == 0) //如果要删除的是列表末端数据，拷贝前len-1个数据到新副本上，再切换引用 setArray(Arrays.copyOf(elements, len - 1)); else &#123; //否则，将除要删除元素之外的其他元素拷贝到新副本中，并切换引用 Object[] newElements = new Object[len - 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index + 1, newElements, index, numMoved); setArray(newElements); &#125; return oldValue; &#125; finally &#123; //解锁 lock.unlock(); &#125;&#125; 读操作 CopyOnWriteArrayList 的读操作是不用加锁的，性能很高。 public E get(int index) &#123; return get(getArray(), index);&#125;private E get(Object[] a, int index) &#123; return (E) a[index];&#125; 示例 public class CopyOnWriteArrayListDemo &#123; static class ReadTask implements Runnable &#123; List&lt;String&gt; list; ReadTask(List&lt;String&gt; list) &#123; this.list = list; &#125; public void run() &#123; for (String str : list) &#123; System.out.println(str); &#125; &#125; &#125; static class WriteTask implements Runnable &#123; List&lt;String&gt; list; int index; WriteTask(List&lt;String&gt; list, int index) &#123; this.list = list; this.index = index; &#125; public void run() &#123; list.remove(index); list.add(index, "write_" + index); &#125; &#125; public void run() &#123; final int NUM = 10; // ArrayList 在并发迭代访问时会抛出 ConcurrentModificationException 异常 // List&lt;String&gt; list = new ArrayList&lt;&gt;(); CopyOnWriteArrayList&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); for (int i = 0; i &lt; NUM; i++) &#123; list.add("main_" + i); &#125; ExecutorService executorService = Executors.newFixedThreadPool(NUM); for (int i = 0; i &lt; NUM; i++) &#123; executorService.execute(new ReadTask(list)); executorService.execute(new WriteTask(list, i)); &#125; executorService.shutdown(); &#125; public static void main(String[] args) &#123; new CopyOnWriteArrayListDemo().run(); &#125;&#125; 资料 Java 并发编程实战 Java 并发编程的艺术 https://blog.csdn.net/u010425776/article/details/54890215 https://blog.csdn.net/wangxiaotongfan/article/details/52074160 https://my.oschina.net/hosee/blog/675884 https://www.jianshu.com/p/c0642afe03e0 https://www.jianshu.com/p/f6730d5784ad http://www.javarticles.com/2012/06/copyonwritearraylist.html https://www.cnblogs.com/xrq730/p/5020760.html https://www.cnblogs.com/leesf456/p/5547853.html http://www.cnblogs.com/chengxiao/p/6881974.html http://www.cnblogs.com/dolphin0520/p/3933404.html]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>concurrent</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>container</tag>
        <tag>concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发工具类]]></title>
    <url>%2Fblog%2F2018%2F05%2F15%2Fjava%2Fjavacore%2Fconcurrent%2F%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[并发工具类 📓 本文已归档到：「blog」 本文内容基于 JDK1.8。 CountDownLatch 要点 源码 示例 CyclicBarrier 要点 源码 示例 Semaphore 要点 源码 示例 资料 JDK 的 java.util.concurrent 包（即 juc）中提供了几个非常有用的并发工具类。 CountDownLatch 要点 作用：字面意思为递减计数锁。它允许一个或多个线程等待，直到在其他线程中执行的一组操作完成。 原理：CountDownLatch 维护一个计数器 count，表示需要等待的事件数量。countDown 方法递减计数器，表示有一个事件已经发生。调用 await 方法的线程会一直阻塞直到计数器为零，或者等待中的线程中断，或者等待超时。 源码 CountDownLatch 唯一的构造方法： // 初始化计数器public CountDownLatch(int count) &#123;&#125;; CountDownLatch 的重要方法： // 调用 await() 方法的线程会被挂起，它会等待直到 count 值为 0 才继续执行public void await() throws InterruptedException &#123; &#125;;// 和 await() 类似，只不过等待一定的时间后 count 值还没变为 0 的话就会继续执行public boolean await(long timeout, TimeUnit unit) throws InterruptedException &#123; &#125;;// count 减 1public void countDown() &#123; &#125;; 示例 public class CountDownLatchDemo &#123; public static void main(String[] args) &#123; final CountDownLatch latch = new CountDownLatch(2); new Thread(() -&gt; &#123; try &#123; System.out.println("子线程" + Thread.currentThread().getName() + "正在执行"); Thread.sleep(3000); System.out.println("子线程" + Thread.currentThread().getName() + "执行完毕"); latch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); new Thread(() -&gt; &#123; try &#123; System.out.println("子线程" + Thread.currentThread().getName() + "正在执行"); Thread.sleep(3000); System.out.println("子线程" + Thread.currentThread().getName() + "执行完毕"); latch.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); try &#123; System.out.println("等待2个子线程执行完毕..."); latch.await(); System.out.println("2个子线程已经执行完毕"); System.out.println("继续执行主线程"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; CyclicBarrier 要点 作用：字面意思循环栅栏。它可以让一组线程等待至某个状态之后再全部同时执行。叫做循环是因为当所有等待线程都被释放以后，CyclicBarrier 可以被重用。 原理：CyclicBarrier 维护一个计数器 count。每次执行 await 方法之后，count 加 1，直到计数器的值和设置的值相等，等待的所有线程才会继续执行。 场景：CyclicBarrier 在并行迭代算法中非常有用。 源码 CyclicBarrier 提供了 2 个构造方法 // parties 数相当于一个屏障，当 parties 数量的线程在等待时会跳闸，并且在跳闸时不执行预定义的动作。public CyclicBarrier(int parties) &#123;&#125;// parties 数相当于一个屏障，当 parties 数量的线程在等待时会跳闸，并且在跳闸时执行给定的动作 barrierAction。public CyclicBarrier(int parties, Runnable barrierAction) &#123;&#125; CyclicBarrier 的重要方法： // 等待调用 await 的线程数达到屏障数。如果当前线程是最后一个到达的线程，并且在构造函数中提供了非空屏障操作，则当前线程在允许其他线程继续之前运行该操作。如果在屏障动作期间发生异常，那么该异常将在当前线程中传播并且屏障被置于断开状态。public int await() throws InterruptedException, BrokenBarrierException &#123;&#125;// 相比于上个方法，这个方法让这些线程等待至一定的时间，如果还有线程没有到达 barrier 状态就直接让到达 barrier 的线程执行后续任务。public int await(long timeout, TimeUnit unit) throws InterruptedException, BrokenBarrierException, TimeoutException &#123;&#125;// 将屏障重置为初始状态public void reset() &#123;&#125; 示例 public class CyclicBarrierDemo02 &#123; static class CyclicBarrierRunnable implements Runnable &#123; CyclicBarrier barrier1 = null; CyclicBarrier barrier2 = null; CyclicBarrierRunnable(CyclicBarrier barrier1, CyclicBarrier barrier2) &#123; this.barrier1 = barrier1; this.barrier2 = barrier2; &#125; public void run() &#123; try &#123; Thread.sleep(1000); System.out.println(Thread.currentThread().getName() + " waiting at barrier 1"); this.barrier1.await(); Thread.sleep(1000); System.out.println(Thread.currentThread().getName() + " waiting at barrier 2"); this.barrier2.await(); System.out.println(Thread.currentThread().getName() + " done!"); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; Runnable barrier1Action = () -&gt; System.out.println("BarrierAction 1 executed "); Runnable barrier2Action = () -&gt; System.out.println("BarrierAction 2 executed "); CyclicBarrier barrier1 = new CyclicBarrier(2, barrier1Action); CyclicBarrier barrier2 = new CyclicBarrier(2, barrier2Action); CyclicBarrierRunnable barrierRunnable1 = new CyclicBarrierRunnable(barrier1, barrier2); CyclicBarrierRunnable barrierRunnable2 = new CyclicBarrierRunnable(barrier1, barrier2); new Thread(barrierRunnable1).start(); new Thread(barrierRunnable2).start(); &#125;&#125; Semaphore 要点 作用：字面意思为信号量。Semaphore 用来控制同时访问某个特定资源的操作数量，或者同时执行某个指定操作的数量。 原理：Semaphore 管理着一组虚拟的许可（permit），permit 的初始数量可通过构造方法来指定。每次执行 acquire 方法可以获取一个 permit，如果没有就等待；而 release 方法可以释放一个 permit。 场景： Semaphore 可以用于实现资源池，如数据库连接池。 Semaphore 可以用于将任何一种容器变成有界阻塞容器。 源码 Semaphore提供了 2 个构造方法： // 初始化固定数量的 permit，并且默认为非公平模式public Semaphore(int permits) &#123;&#125;// 初始化固定数量的 permit，第二个参数设置是否为公平模式。所谓公平，是指等待久的优先获取许可public Semaphore(int permits, boolean fair) &#123;&#125; Semaphore的重要方法： // 获取 1 个许可public void acquire() throws InterruptedException &#123;&#125;//获取 permits 个许可public void acquire(int permits) throws InterruptedException &#123;&#125;// 释放 1 个许可public void release() &#123;&#125;//释放 permits 个许可public void release(int permits) &#123;&#125; 示例 public class SemaphoreDemo &#123; private static final int THREAD_COUNT = 30; private static ExecutorService threadPool = Executors.newFixedThreadPool(THREAD_COUNT); private static Semaphore s = new Semaphore(10); public static void main(String[] args) &#123; for (int i = 0; i &lt; THREAD_COUNT; i++) &#123; threadPool.execute(() -&gt; &#123; try &#123; s.acquire(); System.out.println("save data"); s.release(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; threadPool.shutdown(); &#125;&#125; 资料 Java 并发编程实战 Java 并发编程的艺术 http://www.cnblogs.com/dolphin0520/p/3920397.html]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>concurrent</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[锁]]></title>
    <url>%2Fblog%2F2018%2F05%2F15%2Fjava%2Fjavacore%2Fconcurrent%2F%E9%94%81%2F</url>
    <content type="text"><![CDATA[锁 📓 本文已归档到：「blog」 本文内容基于 JDK1.8。 概述 概念 为什么用 Lock、ReadWriteLock Lock 和 ReentrantLock 要点 源码 示例 ReadWriteLock 和 ReentrantReadWriteLock 要点 源码 示例 AQS 要点 源码 资料 概述 概念 公平锁/非公平锁 公平锁是指多个线程按照申请锁的顺序来获取锁。 非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。 对于 Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。 对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过 AQS 的来实现线程调度，所以并没有任何办法使其变成公平锁。 可重入锁 可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。 说的有点抽象，下面会有一个代码的示例。对于 Java ReentrantLock而言, 他的名字就可以看出是一个可重入锁，其名字是Re entrant Lock重新进入锁。对于Synchronized而言,也是一个可重入锁。可重入锁的一个好处是可一定程度避免死锁。 synchronized void setA() throws Exception&#123; Thread.sleep(1000); setB();&#125;synchronized void setB() throws Exception&#123; Thread.sleep(1000);&#125; 上面的代码就是一个可重入锁的一个特点，如果不是可重入锁的话，setB 可能不会被当前线程执行，可能造成死锁。 独享锁/共享锁 独享锁是指该锁一次只能被一个线程所持有。 共享锁是指该锁可被多个线程所持有。 对于 Java ReentrantLock而言，其是独享锁。但是对于 Lock 的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。读锁的共享锁可保证并发读是非常高效的，读写，写读 ，写写的过程是互斥的。独享锁与共享锁也是通过 AQS 来实现的，通过实现不同的方法，来实现独享或者共享。对于Synchronized而言，当然是独享锁。 互斥锁/读写锁 上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是具体的实现。互斥锁在 Java 中的具体实现就是ReentrantLock 读写锁在 Java 中的具体实现就是ReadWriteLock 乐观锁/悲观锁 乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作是没有事情的。 从上面的描述我们可以看出，悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。悲观锁在 Java 中的使用，就是利用各种锁。乐观锁在 Java 中的使用，是无锁编程，常常采用的是 CAS 算法，典型的例子就是原子类，通过 CAS 自旋实现原子操作的更新。 分段锁 分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来实现高效的并发操作。我们以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为 Segment，它即类似于 HashMap（JDK7 与 JDK8 中 HashMap 的实现）的结构，即内部拥有一个 Entry 数组，数组中的每个元素既是一个链表；同时又是一个 ReentrantLock（Segment 继承了 ReentrantLock)。当需要 put 元素的时候，并不是对整个 hashmap 进行加锁，而是先通过 hashcode 来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程 put 的时候，只要不是放在一个分段中，就实现了真正的并行的插入。但是，在统计 size 的时候，可就是获取 hashmap 全局信息的时候，就需要获取所有的分段锁才能统计。分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。 偏向锁/轻量级锁/重量级锁 这三种锁是指锁的状态，并且是针对Synchronized。在 Java 5 通过引入锁升级的机制来实现高效Synchronized。 这三种锁的状态是通过对象监视器在对象头中的字段来表明的。 偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁。降低获取锁的代价。 轻量级锁是指当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能。 重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。 自旋锁 在 Java 中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗 CPU。 为什么用 Lock、ReadWriteLock synchronized 的缺陷 被 synchronized 修饰的方法或代码块，只能被一个线程访问。如果这个线程被阻塞，其他线程也只能等待。 synchronized 不能响应中断。 synchronized 没有超时机制。 synchronized 只能是非公平锁。 Lock、ReadWriteLock 相较于 synchronized，解决了以上的缺陷： Lock 可以手动释放锁（synchronized 获取锁和释放锁都是自动的），以避免死锁。 Lock 可以响应中断 Lock 可以设置超时时间，避免一致等待 Lock 可以选择公平锁或非公平锁两种模式 ReadWriteLock 将读写锁分离，从而使读写操作分开，有效提高并发性。 Lock 和 ReentrantLock 要点 如果采用 Lock，必须主动去释放锁，并且在发生异常时，不会自动释放锁。因此一般来说，使用 Lock 必须在 try catch 块中进行，并且将释放锁的操作放在 finally 块中进行，以保证锁一定被被释放，防止死锁的发生。 lock() 方法的作用是获取锁。如果锁已被其他线程获取，则进行等待。 tryLock() 方法的作用是尝试获取锁，如果成功，则返回 true；如果失败（即锁已被其他线程获取），则返回 false。也就是说，这个方法无论如何都会立即返回，获取不到锁时不会一直等待。 tryLock(long time, TimeUnit unit) 方法和 tryLock() 方法是类似的，区别仅在于这个方法在获取不到锁时会等待一定的时间，在时间期限之内如果还获取不到锁，就返回 false。如果如果一开始拿到锁或者在等待期间内拿到了锁，则返回 true。 lockInterruptibly() 方法比较特殊，当通过这个方法去获取锁时，如果线程正在等待获取锁，则这个线程能够响应中断，即中断线程的等待状态。也就使说，当两个线程同时通过 lock.lockInterruptibly() 想获取某个锁时，假若此时线程 A 获取到了锁，而线程 B 只有在等待，那么对线程 B 调用 threadB.interrupt() 方法能够中断线程 B 的等待过程。由于 lockInterruptibly() 的声明中抛出了异常，所以 lock.lockInterruptibly() 必须放在 try 块中或者在调用 lockInterruptibly() 的方法外声明抛出 InterruptedException。 注意：当一个线程获取了锁之后，是不会被 interrupt() 方法中断的。因为本身在前面的文章中讲过单独调用 interrupt() 方法不能中断正在运行过程中的线程，只能中断阻塞过程中的线程。因此当通过 lockInterruptibly() 方法获取某个锁时，如果不能获取到，只有进行等待的情况下，是可以响应中断的。 unlock() 方法的作用是释放锁。 ReentrantLock 是唯一实现了 Lock 接口的类。 ReentrantLock 字面意为可重入锁。 源码 Lock 接口定义 public interface Lock &#123; void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long time, TimeUnit unit) throws InterruptedException; void unlock(); Condition newCondition();&#125; ReentrantLock 属性和方法 ReentrantLock 的核心方法当然是 Lock 中的方法（具体实现完全基于 Sync 类中提供的方法）。 此外，ReentrantLock 有两个构造方法，功能参考下面源码片段中的注释。 // 同步机制完全依赖于此private final Sync sync;// 默认初始化 sync 的实例为非公平锁（NonfairSync）public ReentrantLock() &#123;&#125;// 根据 boolean 值选择初始化 sync 的实例为公平的锁（FairSync）或不公平锁（NonfairSync）public ReentrantLock(boolean fair) &#123;&#125; Sync Sync 类是 ReentrantLock 的内部类，也是一个抽象类。 ReentrantLock 的同步机制几乎完全依赖于Sync。使用 AQS 状态来表示锁的保留数（详细介绍参见 AQS）。 Sync 是一个抽象类，有两个子类： FairSync - 公平锁版本。 NonfairSync - 非公平锁版本。 示例 public class ReentrantLockDemo &#123; private ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); private Lock lock = new ReentrantLock(); public static void main(String[] args) &#123; final ReentrantLockDemo demo = new ReentrantLockDemo(); new Thread(() -&gt; demo.insert(Thread.currentThread())).start(); new Thread(() -&gt; demo.insert(Thread.currentThread())).start(); &#125; private void insert(Thread thread) &#123; lock.lock(); try &#123; System.out.println(thread.getName() + "得到了锁"); for (int i = 0; i &lt; 5; i++) &#123; arrayList.add(i); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(thread.getName() + "释放了锁"); lock.unlock(); &#125; &#125;&#125; 👉 更多示例 ReadWriteLock 和 ReentrantReadWriteLock 要点 对于特定的资源，ReadWriteLock 允许多个线程同时对其执行读操作，但是只允许一个线程对其执行写操作。 ReadWriteLock 维护一对相关的锁。一个是读锁；一个是写锁。将读写锁分开，有利于提高并发效率。 ReentrantReadWriteLock 实现了 ReadWriteLock 接口，所以它是一个读写锁。 “读-读”线程之间不存在互斥关系。 “读-写”线程、“写-写”线程之间存在互斥关系。 源码 ReadWriteLock 接口定义 public interface ReadWriteLock &#123; /** * 返回用于读操作的锁 */ Lock readLock(); /** * 返回用于写操作的锁 */ Lock writeLock();&#125; 示例 public class ReentrantReadWriteLockDemo &#123; private ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); public static void main(String[] args) &#123; final ReentrantReadWriteLockDemo demo = new ReentrantReadWriteLockDemo(); new Thread(() -&gt; demo.get(Thread.currentThread())).start(); new Thread(() -&gt; demo.get(Thread.currentThread())).start(); &#125; public synchronized void get(Thread thread) &#123; rwl.readLock().lock(); try &#123; long start = System.currentTimeMillis(); while (System.currentTimeMillis() - start &lt;= 1) &#123; System.out.println(thread.getName() + "正在进行读操作"); &#125; System.out.println(thread.getName() + "读操作完毕"); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125;&#125; AQS AQS 作为构建锁或者其他同步组件的基础框架，有必要好好了解一下其原理。 要点 作用：AQS，AbstractQueuedSynchronizer，即队列同步器。它是构建锁或者其他同步组件的基础框架（如 ReentrantLock、ReentrantReadWriteLock、Semaphore 等）。 场景：在 LOCK 包中的相关锁(常用的有 ReentrantLock、 ReadWriteLock)都是基于 AQS 来构建。然而这些锁都没有直接来继承 AQS，而是定义了一个 Sync 类去继承 AQS。那么为什么要这样呢?because:锁面向的是使用用户，而同步器面向的则是线程控制，那么在锁的实现中聚合同步器而不是直接继承 AQS 就可以很好的隔离二者所关注的事情。 原理：AQS 在内部定义了一个 int 变量 state，用来表示同步状态。AQS 通过一个双向的 FIFO 同步队列来完成同步状态的管理，当有线程获取锁失败后，就被添加到队列末尾。 源码 AbstractQueuedSynchronizer 继承自 AbstractOwnableSynchronize。 同步队列 public abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable &#123; /** 等待队列的队头，懒加载。只能通过 setHead 方法修改。 */ private transient volatile Node head; /** 等待队列的队尾，懒加载。只能通过 enq 方法添加新的等待节点。*/ private transient volatile Node tail; /** 同步状态 */ private volatile int state;&#125; AQS 维护了一个 Node 类型双链表，通过 head 和 tail 指针进行访问。 Node static final class Node &#123; /** 该等待同步的节点处于共享模式 */ static final Node SHARED = new Node(); /** 该等待同步的节点处于独占模式 */ static final Node EXCLUSIVE = null; /** 等待状态,这个和 state 是不一样的:有 1,0,-1,-2,-3 五个值 */ volatile int waitStatus; static final int CANCELLED = 1; static final int SIGNAL = -1; static final int CONDITION = -2; static final int PROPAGATE = -3; /** 前驱节点 */ volatile Node prev; /** 后继节点 */ volatile Node next; /** 等待锁的线程 */ volatile Thread thread;&#125; 很显然，Node 是一个双链表结构。 waitStatus 5 个状态值的含义： CANCELLED（1） - 该节点的线程可能由于超时或被中断而处于被取消(作废)状态，一旦处于这个状态，节点状态将一直处于 CANCELLED(作废)，因此应该从队列中移除. SIGNAL（-1） - 当前节点为 SIGNAL 时，后继节点会被挂起，因此在当前节点释放锁或被取消之后必须被唤醒(unparking)其后继结点. CONDITION（-2） - 该节点的线程处于等待条件状态，不会被当作是同步队列上的节点,直到被唤醒(signal)，设置其值为 0,重新进入阻塞状态。 PROPAGATE（-3） - 下一个 acquireShared 应无条件传播。 0 - 非以上状态。 获取独占锁 acquire /** * 先调用 tryAcquire 查看同步状态。 * 如果成功获取同步状态，则结束方法，直接返回； * 反之，则先调用 addWaiter，再调用 acquireQueued。 */public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; addWaiter addWaiter 方法的作用是将当前线程插入等待同步队列的队尾。 private Node addWaiter(Node mode) &#123; // 1. 将当前线程构建成 Node 类型 Node node = new Node(Thread.currentThread(), mode); // 2. 判断尾指针是否为 null Node pred = tail; if (pred != null) &#123; // 2.2 将当前节点插入队列尾部 node.prev = pred; if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; // 2.1. 尾指针为 null，说明当前节点是第一个加入队列的节点 enq(node); return node;&#125; enq enq 方法的作用是通过自旋（死循环），不断尝试利用 CAS 操作将节点插入队列尾部，直到成功为止。 private Node enq(final Node node) &#123; // 设置死循环，是为了不断尝试 CAS 操作，直到成功为止 for (;;) &#123; Node t = tail; if (t == null) &#123; // 1. 构造头结点（必须初始化，需要领会双链表的精髓） if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; // 2. 通过 CAS 操作将节点插入队列尾部 node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; acquireQueued acquireQueued 方法的作用是通过自旋（死循环），不断尝试为等待队列中线程获取独占锁。 final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; // 1. 获得当前节点的上一个节点 final Node p = node.predecessor(); // 2. 当前节点能否获取独占式锁 // 2.1 如果当前节点是队列中第一个节点，并且成功获取同步状态，即可以获得独占式锁 // 说明：当前节点的上一个节点是头指针，即意味着当前节点是队列中第一个节点。 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; // 2.2 获取锁失败，线程进入等待状态等待获取独占式锁 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; acquireQueued Before setHead 方法 private void setHead(Node node) &#123; head = node; node.thread = null; node.prev = null;&#125; 将当前节点通过 setHead 方法设置为队列的头结点，然后将之前的头结点的 next 域设置为 null，并且 pre 域也为 null，即与队列断开，无任何引用方便 GC 时能够将内存进行回收。 shouldParkAfterFailedAcquire shouldParkAfterFailedAcquire 方法的作用是使用 compareAndSetWaitStatus(pred, ws, Node.SIGNAL) 将节点状态由 INITIAL 设置成 SIGNAL，表示当前线程阻塞。 当 compareAndSetWaitStatus 设置失败，则说明 shouldParkAfterFailedAcquire 方法返回 false，重新进入外部方法 acquireQueued。由于 acquireQueued 方法中是死循环，会再一次执行 shouldParkAfterFailedAcquire，直至 compareAndSetWaitStatus 设置节点状态位为 SIGNAL。 private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) return true; if (ws &gt; 0) &#123; do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; parkAndCheckInterrupt parkAndCheckInterrupt 方法的作用是调用 LookSupport.park 方法，该方法是用来阻塞当前线程的。 private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; acquire 流程 综上所述，就是 acquire 的完整流程。可以以一幅图来说明： 释放独占锁 release release 方法以独占模式发布。如果 tryRelease 返回 true，则通过解锁一个或多个线程来实现。这个方法可以用来实现 Lock.unlock 方法。 public final boolean release(int arg) &#123; // 判断同步状态释放是否成功 if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; unparkSuccessor unparkSuccessor 方法作用是唤醒 node 的下一个节点。 头指针的后继节点 private void unparkSuccessor(Node node) &#123; /* * 如果状态为负值（即可能需要信号），请尝试清除信号。 * 如果失败或状态由于等待线程而改变也是正常的。 */ int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /** * 释放后继节点的线程。 * 如果状态为 CANCELLED 放或节点明显为空， * 则从尾部向后遍历以找到状态不是 CANCELLED 的后继节点。 */ Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; // 后继节点不为 null 时唤醒该线程 if (s != null) LockSupport.unpark(s.thread);&#125; 总结 线程获取锁失败，线程被封装成 Node 进行入队操作，核心方法在于 addWaiter()和 enq()，同时 enq()完成对同步队列的头结点初始化工作以及 CAS 操作失败的重试 ; 线程获取锁是一个自旋的过程，当且仅当 当前节点的前驱节点是头结点并且成功获得同步状态时，节点出队即该节点引用的线程获得锁，否则，当不满足条件时就会调用 LookSupport.park()方法使得线程阻塞 ； 释放锁的时候会唤醒后继节点； 获取可中断的独占锁 acquireInterruptibly Lock 能响应中断，这是相较于 synchronized 的一个显著优点。 那么 Lock 响应中断的特性是如何实现的？答案就在 acquireInterruptibly 方法中。 public final void acquireInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) // 线程获取锁失败 doAcquireInterruptibly(arg);&#125; doAcquireInterruptibly 获取同步状态失败后就会调用 doAcquireInterruptibly 方法 private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; // 将节点插入到同步队列中 final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); // 获取锁出队 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) // 线程中断抛异常 throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 与 acquire 方法逻辑几乎一致，唯一的区别是当 parkAndCheckInterrupt 返回 true 时（即线程阻塞时该线程被中断），代码抛出被中断异常。 获取超时等待式的独占锁 tryAcquireNanos 通过调用 lock.tryLock(timeout,TimeUnit) 方式达到超时等待获取锁的效果，该方法会在三种情况下才会返回： 在超时时间内，当前线程成功获取了锁； 当前线程在超时时间内被中断； 超时时间结束，仍未获得锁返回 false。 我们仍然通过采取阅读源码的方式来学习底层具体是怎么实现的，该方法会调用 AQS 的方法 tryAcquireNanos public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); return tryAcquire(arg) || // 实现超时等待的效果 doAcquireNanos(arg, nanosTimeout);&#125; doAcquireNanos private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; if (nanosTimeout &lt;= 0L) return false; // 1. 根据超时时间和当前时间计算出截止时间 final long deadline = System.nanoTime() + nanosTimeout; final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); // 2. 当前线程获得锁出队列 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return true; &#125; // 3.1 重新计算超时时间 nanosTimeout = deadline - System.nanoTime(); // 3.2 超时返回 false if (nanosTimeout &lt;= 0L) return false; // 3.3 线程阻塞等待 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); // 3.4 线程被中断抛出被中断异常 if (Thread.interrupted()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 获取共享锁 acquireShared public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 尝试获取共享锁失败，调用 doAcquireShared private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 当该节点的前驱节点是头结点且成功获取同步状态 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 以上代码和 acquireQueued 的代码逻辑十分相似，区别仅在于自旋的条件以及节点出队的操作有所不同。 释放共享锁 releaseShared public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125; doReleaseShared 当成功释放同步状态之后即 tryReleaseShared 会继续执行 doReleaseShared 方法 发送后继信号并确保传播。 （注意：对于独占模式，如果需要信号，释放就相当于调用头的 unparkSuccessor。） private void doReleaseShared() &#123; for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) // 如果 CAS 失败，继续自旋 continue; &#125; // 如果头指针变化，break if (h == head) break; &#125;&#125; 获取可中断的共享锁 acquireSharedInterruptibly 方法与 acquireInterruptibly 几乎一致，不再赘述。 获取超时等待式的共享锁 tryAcquireSharedNanos 方法与 tryAcquireNanos 几乎一致，不再赘述。 资料 Java 并发编程实战 Java 并发编程的艺术 http://www.cnblogs.com/dolphin0520/p/3923167.html https://zhuanlan.zhihu.com/p/27134110 https://t.hao0.me/java/2016/04/01/aqs.html http://ju.outofmemory.cn/entry/353762 https://blog.csdn.net/u012403290/article/details/64910926?locationNum=11&amp;fps=1 https://www.cnblogs.com/qifengshi/p/6831055.html]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>concurrent</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>concurrent</tag>
        <tag>lock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发面试题总结]]></title>
    <url>%2Fblog%2F2018%2F05%2F10%2Fjava%2Fjavacore%2Fconcurrent%2FJava%E5%B9%B6%E5%8F%91%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86%2F</url>
    <content type="text"><![CDATA[Java 并发面试题总结 📓 本文已归档到：「blog」 1. 并发简介 1.1. 什么是进程？什么是线程？进程和线程的区别？ 1.2. 多线程编程的好处是什么？ 1.3. 如何让正在运行的线程暂停一段时间？ 1.4. 什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing)？ 1.5. 在多线程中，什么是上下文切换(context-switching)？ 1.6. 如何确保线程安全？ 1.7. 什么是死锁(Deadlock)？如何分析和避免死锁？ 2. 线程基础 2.1. Java 线程生命周期中有哪些状态？各状态之间如何切换？ 2.2. 创建线程有哪些方式？这些方法各自利弊是什么？ 2.3. 什么是 Callable 和 Future？什么是 FutureTask？ 2.4. start() 和 run() 有什么区别？可以直接调用 Thread 类的 run() 方法么？ 2.5. sleep()、yield()、join() 方法有什么区别？为什么 sleep() 和 yield() 方法是静态（static）的？ 2.6. Java 的线程优先级如何控制？高优先级的 Java 线程一定先执行吗？ 2.7. 什么是守护线程？为什么要用守护线程？如何创建守护线程？ 2.8. 线程间是如何通信的？ 2.9. 为什么线程通信的方法 wait(), notify() 和 notifyAll() 被定义在 Object 类里？ 2.10. 为什么 wait(), notify() 和 notifyAll() 必须在同步方法或者同步块中被调用？ 3. 并发机制的底层实现 3.1. volatile 有什么作用？它的实现原理是什么？ 3.2. synchronized 有什么作用？它的实现原理是什么？同步方法和同步块，哪个更好？ 3.3. volatile 和 synchronized 的区别？ 3.4. ThreadLocal 有什么作用？ThreadLocal 的实现原理是什么？ 4. 内存模型 4.1. 什么是 Java 内存模型 5. 同步容器和并发容器 5.1. 什么是同步容器？有哪些常见同步容器？它们是如何实现线程安全的？同步容器真的线程安全吗？ 5.2. 什么是并发容器的实现？ 6. 锁 6.1. Lock 接口(Lock interface)是什么？对比同步它有什么优势？ 6.2. 什么是阻塞队列？如何使用阻塞队列来实现生产者-消费者模型？ 7. 原子变量类 7.1. 什么是原子操作？有哪些原子类？原子类的实现原理是什么？ 8. 并发工具类 8.1. CyclicBarrier 和 CountDownLatch 有什么不同？ 9. 线程池 9.1. 什么是线程池？如何创建一个 Java 线程池？ 9.2. 什么是 Executors 框架？ 9.3. Executors 类是什么？ ThreadPoolExecutor 有哪些参数，各自有什么用？ 10. 资料 1. 并发简介 1.1. 什么是进程？什么是线程？进程和线程的区别？ 什么是进程 简言之，进程可视为一个正在运行的程序。 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动。进程是操作系统进行资源分配的基本单位。 什么是线程 线程是操作系统进行调度的基本单位。 进程 vs. 线程 一个程序至少有一个进程，一个进程至少有一个线程。 线程比进程划分更细，所以执行开销更小，并发性更高。 进程是一个实体，拥有独立的资源；而同一个进程中的多个线程共享进程的资源。 👉 参考阅读：进程和线程关系及区别 1.2. 多线程编程的好处是什么？ 更有效率的利用多处理器核心 更快的响应时间 更好的编程模型 1.3. 如何让正在运行的线程暂停一段时间？ 我们可以使用 Thread 类的 Sleep() 方法让线程暂停一段时间。 需要注意的是，这并不会让线程终止，一旦从休眠中唤醒线程，线程的状态将会被改变为 Runnable，并且根据线程调度，它将得到执行。 1.4. 什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing)？ 线程调度器是一个操作系统服务，它负责为 Runnable 状态的线程分配 CPU 时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。 时间分片是指将可用的 CPU 时间分配给可用的 Runnable 线程的过程。 分配 CPU 时间可以基于线程优先级或者线程等待的时间。线程调度并不受到 Java 虚拟机控制，所以由应用程序来控制它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。 1.5. 在多线程中，什么是上下文切换(context-switching)？ 上下文切换是存储和恢复 CPU 状态的过程，它使得线程执行能够从中断点恢复执行。上下文切换是多任务操作系统和多线程环境的基本特征。 1.6. 如何确保线程安全？ 原子类(atomic concurrent classes) 锁 volatile 关键字 不变类和线程安全类 1.7. 什么是死锁(Deadlock)？如何分析和避免死锁？ 死锁是指两个以上的线程永远相互阻塞的情况，这种情况产生至少需要两个以上的线程和两个以上的资源。 分析死锁，我们需要查看 Java 应用程序的线程转储。我们需要找出那些状态为 BLOCKED 的线程和他们等待的资源。每个资源都有一个唯一的 id，用这个 id 我们可以找出哪些线程已经拥有了它的对象锁。 避免嵌套锁，只在需要的地方使用锁和避免无限期等待是避免死锁的通常办法。 2. 线程基础 2.1. Java 线程生命周期中有哪些状态？各状态之间如何切换？ java.lang.Thread.State 中定义了 6 种不同的线程状态，在给定的一个时刻，线程只能处于其中的一个状态。 以下是各状态的说明，以及状态间的联系： 开始（New） - 还没有调用 start() 方法的线程处于此状态。 可运行（Runnable） - 已经调用了 start() 方法的线程状态。此状态意味着，线程已经准备好了，一旦被线程调度器分配了 CPU 时间片，就可以运行线程。 阻塞（Blocked） - 阻塞状态。线程阻塞的线程状态等待监视器锁定。处于阻塞状态的线程正在等待监视器锁定，以便在调用 Object.wait() 之后输入同步块/方法或重新输入同步块/方法。 等待（Waiting） - 等待状态。一个线程处于等待状态，是由于执行了 3 个方法中的任意方法： Object.wait() Thread.join() LockSupport.park() 定时等待（Timed waiting） - 等待指定时间的状态。一个线程处于定时等待状态，是由于执行了以下方法中的任意方法： Thread.sleep(sleeptime) Object.wait(timeout) Thread.join(timeout) LockSupport.parkNanos(timeout) LockSupport.parkUntil(timeout) 终止(Terminated) - 线程 run() 方法执行结束，或者因异常退出了 run() 方法，则该线程结束生命周期。死亡的线程不可再次复生。 👉 参考阅读：Java Thread Methods and Thread States 👉 参考阅读：Java 线程的 5 种状态及切换(透彻讲解) 2.2. 创建线程有哪些方式？这些方法各自利弊是什么？ 创建线程主要有三种方式： 1. 继承 Thread 类 定义 Thread 类的子类，并重写该类的 run() 方法，该 run() 方法的方法体就代表了线程要完成的任务。因此把 run() 方法称为执行体。 创建 Thread 子类的实例，即创建了线程对象。 调用线程对象的 start() 方法来启动该线程。 2. 实现 Runnable 接口 定义 Runnable 接口的实现类，并重写该接口的 run() 方法，该 run() 方法的方法体同样是该线程的线程执行体。 创建 Runnable 实现类的实例，并以此实例作为 Thread 对象，该 Thread 对象才是真正的线程对象。 调用线程对象的 start() 方法来启动该线程。 3. 通过 Callable 接口和 Future 接口 创建 Callable 接口的实现类，并实现 call() 方法，该 call() 方法将作为线程执行体，并且有返回值。 创建 Callable 实现类的实例，使用 FutureTask 类来包装 Callable 对象，该 FutureTask 对象封装了该 Callable 对象的 call() 方法的返回值。 使用 FutureTask 对象作为 Thread 对象的 target 创建并启动新线程。 调用 FutureTask 对象的 get() 方法来获得子线程执行结束后的返回值 三种创建线程方式对比 实现 Runnable 接口优于继承 Thread 类，因为根据开放封闭原则——实现接口更便于扩展； 实现 Runnable 接口的线程没有返回值；而使用 Callable / Future 方式可以让线程有返回值。 👉 参考阅读：java 创建线程的三种方式及其对比 2.3. 什么是 Callable 和 Future？什么是 FutureTask？ 什么是 Callable 和 Future？ Java 5 在 concurrency 包中引入了 Callable 接口，它和 Runnable 接口很相似，但它可以返回一个对象或者抛出一个异常。 Callable 接口使用泛型去定义它的返回类型。Executors 类提供了一些有用的方法去在线程池中执行 Callable 内的任务。由于 Callable 任务是并行的，我们必须等待它返回的结果。Future 对象为我们解决了这个问题。在线程池提交 Callable 任务后返回了一个 Future 对象，使用它我们可以知道 Callable 任务的状态和得到 Callable 返回的执行结果。Future 提供了 get() 方法让我们可以等待 Callable 结束并获取它的执行结果。 什么是 FutureTask？ FutureTask 是 Future 的一个基础实现，我们可以将它同 Executors 使用处理异步任务。通常我们不需要使用 FutureTask 类，单当我们打算重写 Future 接口的一些方法并保持原来基础的实现是，它就变得非常有用。我们可以仅仅继承于它并重写我们需要的方法。阅读 Java FutureTask 例子，学习如何使用它。 👉 参考阅读：Java 并发编程：Callable、Future 和 FutureTask 2.4. start() 和 run() 有什么区别？可以直接调用 Thread 类的 run() 方法么？ run() 方法是线程的执行体。 start() 方法负责启动线程，然后 JVM 会让这个线程去执行 run() 方法。 可以直接调用 Thread 类的 run() 方法么？ 可以。但是如果直接调用 Thread 的 run() 方法，它的行为就会和普通的方法一样。 为了在新的线程中执行我们的代码，必须使用 start() 方法。 2.5. sleep()、yield()、join() 方法有什么区别？为什么 sleep() 和 yield() 方法是静态（static）的？ yield() yield() 方法可以让当前正在执行的线程暂停，但它不会阻塞该线程，它只是将该线程从 Running 状态转入 Runnable 状态。 当某个线程调用了 yield() 方法暂停之后，只有优先级与当前线程相同，或者优先级比当前线程更高的处于就绪状态的线程才会获得执行的机会。 sleep() sleep() 方法需要指定等待的时间，它可以让当前正在执行的线程在指定的时间内暂停执行，进入 Blocked 状态。 该方法既可以让其他同优先级或者高优先级的线程得到执行的机会，也可以让低优先级的线程得到执行机会。 但是，sleep() 方法不会释放“锁标志”，也就是说如果有 synchronized 同步块，其他线程仍然不能访问共享数据。 join() join() 方法会使当前线程转入 Blocked 状态，等待调用 join() 方法的线程结束后才能继续执行。 为什么 sleep() 和 yield() 方法是静态（static）的？ Thread 类的 sleep() 和 yield() 方法将处理 Running 状态的线程。所以在其他处于非 Running 状态的线程上执行这两个方法是没有意义的。这就是为什么这些方法是静态的。它们可以在当前正在执行的线程中工作，并避免程序员错误的认为可以在其他非运行线程调用这些方法。 👉 参考阅读：Java 线程中 yield 与 join 方法的区别 👉 参考阅读：sleep()，wait()，yield()和 join()方法的区别 2.6. Java 的线程优先级如何控制？高优先级的 Java 线程一定先执行吗？ Java 中的线程优先级如何控制 Java 中的线程优先级的范围是 [1,10]，一般来说，高优先级的线程在运行时会具有优先权。可以通过 thread.setPriority(Thread.MAX_PRIORITY) 的方式设置，默认优先级为 5。 高优先级的 Java 线程一定先执行吗 即使设置了线程的优先级，也无法保证高优先级的线程一定先执行。 原因：这是因为 Java 线程优先级依赖于操作系统的支持，然而，不同的操作系统支持的线程优先级并不相同，不能很好的和 Java 中线程优先级一一对应。 结论：Java 线程优先级控制并不可靠。 2.7. 什么是守护线程？为什么要用守护线程？如何创建守护线程？ 什么是守护线程 守护线程（Daemon Thread）是在后台执行并且不会阻止 JVM 终止的线程。 与守护线程（Daemon Thread）相反的，叫用户线程（User Thread），也就是非守护线程。 为什么要用守护线程 守护线程的优先级比较低，用于为系统中的其它对象和线程提供服务。典型的应用就是垃圾回收器。 如何创建守护线程 使用 thread.setDaemon(true) 可以设置 thread 线程为守护线程。 注意点： 正在运行的用户线程无法设置为守护线程，所以 thread.setDaemon(true) 必须在 thread.start() 之前设置，否则会抛出 llegalThreadStateException 异常； 一个守护线程创建的子线程依然是守护线程。 不要认为所有的应用都可以分配给守护线程来进行服务，比如读写操作或者计算逻辑。 👉 参考阅读：Java 中守护线程的总结 2.8. 线程间是如何通信的？ 当线程间是可以共享资源时，线程间通信是协调它们的重要的手段。Object 类中 wait(), notify() 和 notifyAll() 方法可以用于线程间通信关于资源的锁的状态。 👉 参考阅读：Java 并发编程：线程间协作的两种方式：wait、notify、notifyAll 和 Condition 2.9. 为什么线程通信的方法 wait(), notify() 和 notifyAll() 被定义在 Object 类里？ Java 的每个对象中都有一个锁(monitor，也可以成为监视器) 并且 wait()、notify() 等方法用于等待对象的锁或者通知其他线程对象的监视器可用。在 Java 的线程中并没有可供任何对象使用的锁和同步器。这就是为什么这些方法是 Object 类的一部分，这样 Java 的每一个类都有用于线程间通信的基本方法 2.10. 为什么 wait(), notify() 和 notifyAll() 必须在同步方法或者同步块中被调用？ 当一个线程需要调用对象的 wait() 方法的时候，这个线程必须拥有该对象的锁，接着它就会释放这个对象锁并进入等待状态直到其他线程调用这个对象上的 notify() 方法。同样的，当一个线程需要调用对象的 notify() 方法时，它会释放这个对象的锁，以便其他在等待的线程就可以得到这个对象锁。 由于所有的这些方法都需要线程持有对象的锁，这样就只能通过同步来实现，所以他们只能在同步方法或者同步块中被调用。 3. 并发机制的底层实现 3.1. volatile 有什么作用？它的实现原理是什么？ 作用： 被 volatile 关键字修饰的变量有两层含义： 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 禁止进行指令重排序。 原理： 观察加入 volatile 关键字和没有加入 volatile 关键字时所生成的汇编代码发现，加入 volatile 关键字时，会多出一个 lock 前缀指令。lock 前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供 3 个功能： 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他 CPU 中对应的缓存行无效。 👉 参考阅读：Java 并发编程：volatile 关键字解析 3.2. synchronized 有什么作用？它的实现原理是什么？同步方法和同步块，哪个更好？ 作用： 使用 synchronized 关键字来标记一个方法或者代码块，当某个线程调用该对象的 synchronized 方法或者访问 synchronized 代码块时，这个线程便获得了该对象的锁，其他线程暂时无法访问这个方法，只有等待这个方法执行完毕或者代码块执行完毕，这个线程才会释放该对象的锁，其他线程才能执行这个方法或者代码块。 原理： synchronized 关键字会阻止其它线程获取当前对象的监控锁，这样就使得当前对象中被 synchronized 关键字保护的代码块无法被其它线程访问，也就无法并发执行。更重要的是，synchronized 还会创建一个内存屏障，内存屏障指令保证了所有 CPU 操作结果都会直接刷到主存中，从而保证了操作的内存可见性，同时也使得先获得这个锁的线程的所有操作，都 happens-before 于随后获得这个锁的线程的操作。 同步方法和同步块，哪个更好？ 同步块是更好的选择。 因为它不会锁住整个对象（当然你也可以让它锁住整个对象）。同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并需要等待获得这个对象上的锁。 👉 参考阅读：Java 并发编程：synchronized 3.3. volatile 和 synchronized 的区别？ volatile 本质是在告诉 jvm 当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取； synchronized 则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。 volatile 仅能使用在变量级别；synchronized 则可以使用在变量、方法、和类级别的。 volatile 仅能实现变量的修改可见性，不能保证原子性；而 synchronized 则可以保证变量的修改可见性和原子性 volatile 不会造成线程的阻塞；synchronized 可能会造成线程的阻塞。 volatile 标记的变量不会被编译器优化；synchronized 标记的变量可以被编译器优化。 👉 参考阅读：volatile 和 synchronized 的区别 3.4. ThreadLocal 有什么作用？ThreadLocal 的实现原理是什么？ ThreadLocal 有什么作用？ ThreadLocal 为变量在每个线程中都创建了一个副本，那么每个线程可以访问自己内部的副本变量。 ThreadLocal 的实现原理是什么？ 首先，在每个线程 Thread 内部有一个 ThreadLocal.ThreadLocalMap 类型的成员变量 threadLocals，这个 threadLocals 就是用来存储实际的变量副本的，key 为当前 ThreadLocal 变量，value 为变量副本（即 T 类型的变量）。 初始时，在 Thread 里面，threadLocals 为空。当通过 ThreadLocal 变量调用 get() 方法或者 set() 方法，就会对 Thread 类中的 threadLocals 进行初始化，并且以当前 ThreadLocal 变量为键值，以 ThreadLocal 要保存的副本变量为 value，存到 threadLocals。 然后在当前线程里面，如果要使用副本变量，就可以通过 get 方法在 threadLocals 里面查找。 需要注意的是：ThreadLocalMap.Entry 继承了 WeakReference。ThreadLocalMap 使用它的目的是：当 threadLocal 实例可以被 GC 回收时，系统可以检测到该 threadLocal 对应的 Entry 是否已经过期（根据 reference.get() == null 来判断，如果为 true 则表示过期，程序内部称为 stale slots）来自动做一些清除工作，否则如果不清除的话容易产生内存无法释放的问题：value 对应的对象即使不再使用，但由于被 threadLocalMap 所引用导致无法被 GC 回收。实际代码中，ThreadLocalMap 会在 set，get 以及 resize 等方法中对 stale slots 做自动删除（set 以及 get 不保证所有过期 slots 会在操作中会被删除，而 resize 则会删除 threadLocalMap 中所有的过期 slots）。当然将 threadLocal 对象设置为 null 并不能完全避免内存泄露对象，最安全的办法仍然是调用 ThreadLocal 的 remove 方法，来彻底避免可能的内存泄露。 👉 参考阅读：Java 并发编程：深入剖析 ThreadLocal 4. 内存模型 4.1. 什么是 Java 内存模型 Java 内存模型即 Java Memory Model，简称 JMM。JMM 定义了 JVM 在计算机内存(RAM)中的工作方式。JMM 是隶属于 JVM 的。 并发编程领域两个关键问题：线程间通信和线程间同步 线程间通信机制 共享内存 - 线程间通过写-读内存中的公共状态来隐式进行通信。 消息传递 - java 中典型的消息传递方式就是 wait()和 notify()。 线程间同步机制 在共享内存模型中，必须显示指定某个方法或某段代码在线程间互斥地执行。 在消息传递模型中，由于发送消息必须在接收消息之前，因此同步是隐式进行的。 Java 的并发采用的是共享内存模型 JMM 决定一个线程对共享变量的写入何时对另一个线程可见。 线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。 JMM 把内存分成了两部分：线程栈区和堆区 线程栈 JVM 中运行的每个线程都拥有自己的线程栈，线程栈包含了当前线程执行的方法调用相关信息，我们也把它称作调用栈。随着代码的不断执行，调用栈会不断变化。 线程栈还包含了当前方法的所有本地变量信息。线程中的本地变量对其它线程是不可见的。 堆区 堆区包含了 Java 应用创建的所有对象信息，不管对象是哪个线程创建的，其中的对象包括原始类型的封装类（如 Byte、Integer、Long 等等）。不管对象是属于一个成员变量还是方法中的本地变量，它都会被存储在堆区。 一个本地变量如果是原始类型，那么它会被完全存储到栈区。 一个本地变量也有可能是一个对象的引用，这种情况下，这个本地引用会被存储到栈中，但是对象本身仍然存储在堆区。 对于一个对象的成员方法，这些方法中包含本地变量，仍需要存储在栈区，即使它们所属的对象在堆区。 对于一个对象的成员变量，不管它是原始类型还是包装类型，都会被存储到堆区。 👉 参考阅读：全面理解 Java 内存模型 5. 同步容器和并发容器 5.1. 什么是同步容器？有哪些常见同步容器？它们是如何实现线程安全的？同步容器真的线程安全吗？ 同步容器是指 Java 中使用 synchronized 关键字修饰方法以保证方法线程安全的容器。 常见的同步容器有 Vector、HashTable、Stack，与之相对应的 ArrayList、HashMap、LinkedList 则是非线程安全的。 同步容器之所以说是线程安全的，是因为它们的方法被 synchronized 关键字修饰，从而保证了当有一个线程执行方法时，其他线程被阻塞。 同步容器中的所有自带方法都是线程安全的。但是，对这些集合类的复合操作无法保证其线程安全性。需要客户端通过主动加锁来保证。 典型场景：使用同步容器做迭代操作时，如果不对外部做同步，就可能出现 ConcurrentModificationException 异常。 结论：由于同步容器不能彻底保证线程安全，且性能不高，所以不建议使用。如果想使用线程安全的容器，可以考虑 juc 包中提供的 ConcurrentHashMap 等并发容器。 for(int i=0;i&lt;vector.size();i++) vector.remove(i); 👉 参考阅读：Java 并发编程：同步容器 5.2. 什么是并发容器的实现？ Java 集合类都是快速失败的，这就意味着当集合被改变且一个线程在使用迭代器遍历集合的时候，迭代器的 next()方法将抛出 ConcurrentModificationException 异常。 并发容器支持并发的遍历和并发的更新。 主要的类有 ConcurrentHashMap, CopyOnWriteArrayList 和 CopyOnWriteArraySet，阅读这篇文章了解如何避免 ConcurrentModificationException。 6. 锁 6.1. Lock 接口(Lock interface)是什么？对比同步它有什么优势？ Lock 接口比同步方法和同步块提供了更具扩展性的锁操作。他们允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。 它的优势有： 可以使锁更公平可以使线程在等待锁的时候响应中断可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间可以在不同的范围，以不同的顺序获取和释放锁阅读更多关于锁的例子 6.2. 什么是阻塞队列？如何使用阻塞队列来实现生产者-消费者模型？ java.util.concurrent.BlockingQueue 的特性是：当队列是空的时，从队列中获取或删除元素的操作将会被阻塞，或者当队列是满时，往队列里添加元素的操作会被阻塞。 阻塞队列不接受空值，当你尝试向队列中添加空值的时候，它会抛出 NullPointerException。 阻塞队列的实现都是线程安全的，所有的查询方法都是原子的并且使用了内部锁或者其他形式的并发控制。 BlockingQueue 接口是 java collections 框架的一部分，它主要用于实现生产者-消费者问题。 阅读这篇文章了解如何使用阻塞队列实现生产者-消费者问题。 7. 原子变量类 7.1. 什么是原子操作？有哪些原子类？原子类的实现原理是什么？ 原子操作是指一个不受其他操作影响的操作任务单元。原子操作是在多线程环境下避免数据不一致必须的手段。 int++并不是一个原子操作，所以当一个线程读取它的值并加 1 时，另外一个线程有可能会读到之前的值，这就会引发错误。 为了解决这个问题，必须保证增加操作是原子的，在 JDK1.5 之前我们可以使用同步技术来做到这一点。到 JDK1.5，java.util.concurrent.atomic 包提供了 int 和 long 类型的装类，它们可以自动的保证对于他们的操作是原子的并且不需要使用同步。可以阅读这篇文章来了解 Java 的 atomic 类。 8. 并发工具类 8.1. CyclicBarrier 和 CountDownLatch 有什么不同？ CyclicBarrier 和 CountDownLatch 都可以用来让一组线程等待其它线程。与 CyclicBarrier 不同的是，CountdownLatch 不能重用。 👉 参考阅读：Java 并发编程：CountDownLatch、CyclicBarrier 和 Semaphore 9. 线程池 9.1. 什么是线程池？如何创建一个 Java 线程池？ 一个线程池管理了一组工作线程，同时它还包括了一个用于放置等待执行的任务的队列。 java.util.concurrent.Executors 提供了一个 java.util.concurrent.Executor 接口的实现用于创建线程池。线程池例子展现了如何创建和使用线程池，或者阅读 ScheduledThreadPoolExecutor 例子，了解如何创建一个周期任务。 👉 参考阅读：Java 并发编程：线程池的使用 9.2. 什么是 Executors 框架？ Executor 框架同 java.util.concurrent.Executor 接口在 Java 5 中被引入。Executor 框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。 无限制的创建线程会引起应用程序内存溢出。所以创建一个线程池是个更好的的解决方案，因为可以限制线程的数量并且可以回收再利用这些线程。利用 Executors 框架可以非常方便的创建一个线程池，阅读这篇文章可以了解如何使用 Executor 框架创建一个线程池。 9.3. Executors 类是什么？ Executors 为 Executor，ExecutorService，ScheduledExecutorService，ThreadFactory 和 Callable 类提供了一些工具方法。 Executors 可以用于方便的创建线程池。 ThreadPoolExecutor 有哪些参数，各自有什么用？ java.uitl.concurrent.ThreadPoolExecutor 类是 Executor 框架中最核心的一个类。 ThreadPoolExecutor 有四个构造方法，前三个都是基于第四个实现。第四个构造方法定义如下： public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; 参数说明 corePoolSize：默认情况下，在创建了线程池后，线程池中的线程数为 0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到 corePoolSize 后，就会把到达的任务放到缓存队列当中。 maximumPoolSize：线程池允许创建的最大线程数。如果缓存队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。值得注意的是如果使用了无界的任务队列这个参数就没什么效果。 keepAliveTime：线程活动保持时间。线程池的工作线程空闲后，保持存活的时间。所以如果任务很多，并且每个任务执行的时间比较短，可以调大这个时间，提高线程的利用率。 unit：参数 keepAliveTime 的时间单位，有 7 种取值。可选的单位有天（DAYS），小时（HOURS），分钟（MINUTES），毫秒(MILLISECONDS)，微秒(MICROSECONDS, 千分之一毫秒)和毫微秒(NANOSECONDS, 千分之一微秒)。 workQueue：任务队列。用于保存等待执行的任务的阻塞队列。 可以选择以下几个阻塞队列。 ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按 FIFO（先进先出）原则对元素进行排序。 LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按 FIFO （先进先出） 排序元素，吞吐量通常要高于 ArrayBlockingQueue。静态工厂方法 Executors.newFixedThreadPool()使用了这个队列。 SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于 LinkedBlockingQueue，静态工厂方法 Executors.newCachedThreadPool 使用了这个队列。 PriorityBlockingQueue：一个具有优先级的无限阻塞队列。 threadFactory：创建线程的工厂。可以通过线程工厂给每个创建出来的线程设置更有意义的名字。 handler：饱和策略。当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是 AbortPolicy，表示无法处理新任务时抛出异常。以下是 JDK1.5 提供的四种策略。 AbortPolicy：直接抛出异常。 CallerRunsPolicy：只用调用者所在线程来运行任务。 DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 DiscardPolicy：不处理，丢弃掉。 当然也可以根据应用场景需要来实现 RejectedExecutionHandler 接口自定义策略。如记录日志或持久化不能处理的任务。 10. 资料 Java 线程面试题 Top 50 JAVA 多线程和并发基础面试问答 进程和线程关系及区别 Java Thread Methods and Thread States Java 线程的 5 种状态及切换(透彻讲解) Java 中守护线程的总结 java 创建线程的三种方式及其对比 Java 线程的 5 种状态及切换(透彻讲解) java 线程方法 join 的简单总结 Java 并发编程：线程间协作的两种方式：wait、notify、notifyAll 和 Condition Java 并发编程：volatile 关键字解析 Java 并发编程：Callable、Future 和 FutureTask Java 并发编程：线程池的使用]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>concurrent</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>interview</tag>
        <tag>concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 枚举]]></title>
    <url>%2Fblog%2F2018%2F04%2F24%2Fjava%2Fjavacore%2Fbasics%2FJava%E6%9E%9A%E4%B8%BE%2F</url>
    <content type="text"><![CDATA[深入理解 Java 枚举 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「javacore」 简介 枚举的本质 枚举的方法 枚举的特性 基本特性 枚举可以添加方法 枚举可以实现接口 枚举不可以继承 枚举的应用 组织常量 switch 状态机 错误码 组织枚举 策略枚举 枚举实现单例模式 枚举工具类 EnumSet EnumMap 小结 参考资料 简介 enum 的全称为 enumeration， 是 JDK5 中引入的特性。 在 Java 中，被 enum 关键字修饰的类型就是枚举类型。形式如下： enum ColorEn &#123; RED, GREEN, BLUE &#125; 枚举的好处：可以将常量组织起来，统一进行管理。 枚举的典型应用场景：错误码、状态机等。 枚举的本质 java.lang.Enum类声明 public abstract class Enum&lt;E extends Enum&lt;E&gt;&gt; implements Comparable&lt;E&gt;, Serializable &#123; ... &#125; 新建一个 ColorEn.java 文件，内容如下： package io.github.dunwu.javacore.enumeration;public enum ColorEn &#123; RED,YELLOW,BLUE&#125; 执行 javac ColorEn.java 命令，生成 ColorEn.class 文件。 然后执行 javap ColorEn.class 命令，输出如下内容： Compiled from "ColorEn.java"public final class io.github.dunwu.javacore.enumeration.ColorEn extends java.lang.Enum&lt;io.github.dunwu.javacore.enumeration.ColorEn&gt; &#123; public static final io.github.dunwu.javacore.enumeration.ColorEn RED; public static final io.github.dunwu.javacore.enumeration.ColorEn YELLOW; public static final io.github.dunwu.javacore.enumeration.ColorEn BLUE; public static io.github.dunwu.javacore.enumeration.ColorEn[] values(); public static io.github.dunwu.javacore.enumeration.ColorEn valueOf(java.lang.String); static &#123;&#125;;&#125; 💡 说明： 从上面的例子可以看出： 枚举的本质是 java.lang.Enum 的子类。 尽管 enum 看起来像是一种新的数据类型，事实上，enum 是一种受限制的类，并且具有自己的方法。枚举这种特殊的类因为被修饰为 final，所以不能继承其他类。 定义的枚举值，会被默认修饰为 public static final ，从修饰关键字，即可看出枚举值本质上是静态常量。 枚举的方法 在 enum 中，提供了一些基本方法： values()：返回 enum 实例的数组，而且该数组中的元素严格保持在 enum 中声明时的顺序。 name()：返回实例名。 ordinal()：返回实例声明时的次序，从 0 开始。 getDeclaringClass()：返回实例所属的 enum 类型。 equals() ：判断是否为同一个对象。 可以使用 == 来比较enum实例。 此外，java.lang.Enum实现了Comparable和 Serializable 接口，所以也提供 compareTo() 方法。 例：展示 enum 的基本方法 public class EnumMethodDemo &#123; enum Color &#123;RED, GREEN, BLUE;&#125; enum Size &#123;BIG, MIDDLE, SMALL;&#125; public static void main(String args[]) &#123; System.out.println("=========== Print all Color ==========="); for (Color c : Color.values()) &#123; System.out.println(c + " ordinal: " + c.ordinal()); &#125; System.out.println("=========== Print all Size ==========="); for (Size s : Size.values()) &#123; System.out.println(s + " ordinal: " + s.ordinal()); &#125; Color green = Color.GREEN; System.out.println("green name(): " + green.name()); System.out.println("green getDeclaringClass(): " + green.getDeclaringClass()); System.out.println("green hashCode(): " + green.hashCode()); System.out.println("green compareTo Color.GREEN: " + green.compareTo(Color.GREEN)); System.out.println("green equals Color.GREEN: " + green.equals(Color.GREEN)); System.out.println("green equals Size.MIDDLE: " + green.equals(Size.MIDDLE)); System.out.println("green equals 1: " + green.equals(1)); System.out.format("green == Color.BLUE: %b\n", green == Color.BLUE); &#125;&#125; 输出 =========== Print all Color ===========RED ordinal: 0GREEN ordinal: 1BLUE ordinal: 2=========== Print all Size ===========BIG ordinal: 0MIDDLE ordinal: 1SMALL ordinal: 2green name(): GREENgreen getDeclaringClass(): class org.zp.javase.enumeration.EnumDemo$Colorgreen hashCode(): 460141958green compareTo Color.GREEN: 0green equals Color.GREEN: truegreen equals Size.MIDDLE: falsegreen equals 1: falsegreen == Color.BLUE: false 枚举的特性 枚举的特性，归结起来就是一句话： 除了不能继承，基本上可以将 enum 看做一个常规的类。 但是这句话需要拆分去理解，让我们细细道来。 基本特性 如果枚举中没有定义方法，也可以在最后一个实例后面加逗号、分号或什么都不加。 如果枚举中没有定义方法，枚举值默认为从 0 开始的有序数值。以 Color 枚举类型举例，它的枚举常量依次为 RED：0，GREEN：1，BLUE：2。 枚举可以添加方法 在概念章节提到了，枚举值默认为从 0 开始的有序数值 。那么问题来了：如何为枚举显式的赋值。 （1）Java 不允许使用 = 为枚举常量赋值 如果你接触过 C/C++，你肯定会很自然的想到赋值符号 = 。在 C/C++语言中的 enum，可以用赋值符号=显式的为枚举常量赋值；但是 ，很遗憾，Java 语法中却不允许使用赋值符号 = 为枚举常量赋值。 例：C/C++ 语言中的枚举声明 typedef enum &#123; ONE = 1, TWO, THREE = 3, TEN = 10&#125; Number; （2）枚举可以添加普通方法、静态方法、抽象方法、构造方法 Java 虽然不能直接为实例赋值，但是它有更优秀的解决方案：为 enum 添加方法来间接实现显式赋值。 创建 enum 时，可以为其添加多种方法，甚至可以为其添加构造方法。 注意一个细节：如果要为 enum 定义方法，那么必须在 enum 的最后一个实例尾部添加一个分号。此外，在 enum 中，必须先定义实例，不能将字段或方法定义在实例前面。否则，编译器会报错。 例：全面展示如何在枚举中定义普通方法、静态方法、抽象方法、构造方法 public enum ErrorCodeEn &#123; OK(0) &#123; @Override public String getDescription() &#123; return "成功"; &#125; &#125;, ERROR_A(100) &#123; @Override public String getDescription() &#123; return "错误A"; &#125; &#125;, ERROR_B(200) &#123; @Override public String getDescription() &#123; return "错误B"; &#125; &#125;; private int code; // 构造方法：enum的构造方法只能被声明为private权限或不声明权限 private ErrorCodeEn(int number) &#123; // 构造方法 this.code = number; &#125; public int getCode() &#123; // 普通方法 return code; &#125; // 普通方法 public abstract String getDescription(); // 抽象方法 public static void main(String args[]) &#123; // 静态方法 for (ErrorCodeEn s : ErrorCodeEn.values()) &#123; System.out.println("code: " + s.getCode() + ", description: " + s.getDescription()); &#125; &#125;&#125;// Output:// code: 0, description: 成功// code: 100, description: 错误A// code: 200, description: 错误B 注：上面的例子并不可取，仅仅是为了展示枚举支持定义各种方法。正确的例子情况错误码示例 枚举可以实现接口 enum 可以像一般类一样实现接口。 同样是实现上一节中的错误码枚举类，通过实现接口，可以约束它的方法。 public interface INumberEnum &#123; int getCode(); String getDescription();&#125;public enum ErrorCodeEn2 implements INumberEnum &#123; OK(0, "成功"), ERROR_A(100, "错误A"), ERROR_B(200, "错误B"); ErrorCodeEn2(int number, String description) &#123; this.code = number; this.description = description; &#125; private int code; private String description; @Override public int getCode() &#123; return code; &#125; @Override public String getDescription() &#123; return description; &#125;&#125; 枚举不可以继承 enum 不可以继承另外一个类，当然，也不能继承另一个 enum 。 因为 enum 实际上都继承自 java.lang.Enum 类，而 Java 不支持多重继承，所以 enum 不能再继承其他类，当然也不能继承另一个 enum。 枚举的应用 组织常量 在 JDK5 之前，在 Java 中定义常量都是public static final TYPE a; 这样的形式。有了枚举，你可以将有关联关系的常量组织起来，使代码更加易读、安全，并且还可以使用枚举提供的方法。 下面三种声明方式是等价的： enum Color &#123; RED, GREEN, BLUE &#125;enum Color &#123; RED, GREEN, BLUE, &#125;enum Color &#123; RED, GREEN, BLUE; &#125; switch 状态机 我们经常使用 switch 语句来写状态机。JDK7 以后，switch 已经支持 int、char、String、enum 类型的参数。这几种类型的参数比较起来，使用枚举的 switch 代码更具有可读性。 public class StateMachineDemo &#123; public enum Signal &#123; GREEN, YELLOW, RED &#125; public static String getTrafficInstruct(Signal signal) &#123; String instruct = "信号灯故障"; switch (signal) &#123; case RED: instruct = "红灯停"; break; case YELLOW: instruct = "黄灯请注意"; break; case GREEN: instruct = "绿灯行"; break; default: break; &#125; return instruct; &#125; public static void main(String[] args) &#123; System.out.println(getTrafficInstruct(Signal.RED)); &#125;&#125;// Output:// 红灯停 错误码 枚举常被用于定义程序错误码。下面是一个简单示例： public class ErrorCodeEnumDemo &#123; enum ErrorCodeEn &#123; OK(0, "成功"), ERROR_A(100, "错误A"), ERROR_B(200, "错误B"); ErrorCodeEn(int number, String msg) &#123; this.code = number; this.msg = msg; &#125; private int code; private String msg; public int getCode() &#123; return code; &#125; public String getMsg() &#123; return msg; &#125; @Override public String toString() &#123; return "ErrorCodeEn&#123;" + "code=" + code + ", msg='" + msg + '\'' + '&#125;'; &#125; public static String toStringAll() &#123; StringBuilder sb = new StringBuilder(); sb.append("ErrorCodeEn All Elements: ["); for (ErrorCodeEn code : ErrorCodeEn.values()) &#123; sb.append(code.getCode()).append(", "); &#125; sb.append("]"); return sb.toString(); &#125; &#125; public static void main(String[] args) &#123; System.out.println(ErrorCodeEn.toStringAll()); for (ErrorCodeEn s : ErrorCodeEn.values()) &#123; System.out.println(s); &#125; &#125;&#125;// Output:// ErrorCodeEn All Elements: [0, 100, 200, ]// ErrorCodeEn&#123;code=0, msg='成功'&#125;// ErrorCodeEn&#123;code=100, msg='错误A'&#125;// ErrorCodeEn&#123;code=200, msg='错误B'&#125; 组织枚举 可以将类型相近的枚举通过接口或类组织起来，但是一般用接口方式进行组织。 原因是：Java 接口在编译时会自动为 enum 类型加上public static修饰符；Java 类在编译时会自动为 enum 类型加上 static 修饰符。看出差异了吗？没错，就是说，在类中组织 enum，如果你不给它修饰为 public，那么只能在本包中进行访问。 例：在接口中组织 enum public class EnumInInterfaceDemo &#123; public interface INumberEnum &#123; int getCode(); String getDescription(); &#125; public interface Plant &#123; enum Vegetable implements INumberEnum &#123; POTATO(0, "土豆"), TOMATO(0, "西红柿"); Vegetable(int number, String description) &#123; this.code = number; this.description = description; &#125; private int code; private String description; @Override public int getCode() &#123; return this.code; &#125; @Override public String getDescription() &#123; return this.description; &#125; &#125; enum Fruit implements INumberEnum &#123; APPLE(0, "苹果"), ORANGE(0, "桔子"), BANANA(0, "香蕉"); Fruit(int number, String description) &#123; this.code = number; this.description = description; &#125; private int code; private String description; @Override public int getCode() &#123; return this.code; &#125; @Override public String getDescription() &#123; return this.description; &#125; &#125; &#125; public static void main(String[] args) &#123; for (Plant.Fruit f : Plant.Fruit.values()) &#123; System.out.println(f.getDescription()); &#125; &#125;&#125;// Output:// 苹果// 桔子// 香蕉 例：在类中组织 enum 本例和上例效果相同。 public class EnumInClassDemo &#123; public interface INumberEnum &#123; int getCode(); String getDescription(); &#125; public static class Plant2 &#123; enum Vegetable implements INumberEnum &#123; // 略，与上面完全相同 &#125; enum Fruit implements INumberEnum &#123; // 略，与上面完全相同 &#125; &#125; // 略&#125;// Output:// 土豆// 西红柿 策略枚举 Effective Java 中展示了一种策略枚举。这种枚举通过枚举嵌套枚举的方式，将枚举常量分类处理。 这种做法虽然没有 switch 语句简洁，但是更加安全、灵活。 例：EffectvieJava 中的策略枚举范例 enum PayrollDay &#123; MONDAY(PayType.WEEKDAY), TUESDAY(PayType.WEEKDAY), WEDNESDAY( PayType.WEEKDAY), THURSDAY(PayType.WEEKDAY), FRIDAY(PayType.WEEKDAY), SATURDAY( PayType.WEEKEND), SUNDAY(PayType.WEEKEND); private final PayType payType; PayrollDay(PayType payType) &#123; this.payType = payType; &#125; double pay(double hoursWorked, double payRate) &#123; return payType.pay(hoursWorked, payRate); &#125; // 策略枚举 private enum PayType &#123; WEEKDAY &#123; double overtimePay(double hours, double payRate) &#123; return hours &lt;= HOURS_PER_SHIFT ? 0 : (hours - HOURS_PER_SHIFT) * payRate / 2; &#125; &#125;, WEEKEND &#123; double overtimePay(double hours, double payRate) &#123; return hours * payRate / 2; &#125; &#125;; private static final int HOURS_PER_SHIFT = 8; abstract double overtimePay(double hrs, double payRate); double pay(double hoursWorked, double payRate) &#123; double basePay = hoursWorked * payRate; return basePay + overtimePay(hoursWorked, payRate); &#125; &#125;&#125; 测试 System.out.println("时薪100的人在周五工作8小时的收入：" + PayrollDay.FRIDAY.pay(8.0, 100));System.out.println("时薪100的人在周六工作8小时的收入：" + PayrollDay.SATURDAY.pay(8.0, 100)); 枚举实现单例模式 单例模式是最常用的设计模式。 单例模式在并发环境下存在线程安全问题。 为了线程安全问题，传统做法有以下几种： 饿汉式加载 懒汉式 synchronize 和双重检查 利用 java 的静态加载机制 相比上述的方法，使用枚举也可以实现单例，而且还更加简单： public class SingleEnumDemo &#123; public enum SingleEn &#123; INSTANCE; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; public static void main(String[] args) &#123; SingleEn.INSTANCE.setName("zp"); System.out.println(SingleEn.INSTANCE.getName()); &#125;&#125; 扩展阅读：深入理解 Java 枚举类型(enum) 这篇文章对于 Java 枚举的特性讲解很仔细，其中对于枚举实现单例和传统单例实现方式说的尤为细致。 枚举工具类 Java 中提供了两个方便操作 enum 的工具类——EnumSet 和 EnumMap。 EnumSet EnumSet 是枚举类型的高性能 Set 实现。它要求放入它的枚举常量必须属于同一枚举类型。 主要接口： noneOf - 创建一个具有指定元素类型的空 EnumSet allOf - 创建一个指定元素类型并包含所有枚举值的 EnumSet range - 创建一个包括枚举值中指定范围元素的 EnumSet complementOf - 初始集合包括指定集合的补集 of - 创建一个包括参数中所有元素的 EnumSet copyOf - 创建一个包含参数容器中的所有元素的 EnumSet 示例： public class EnumSetDemo &#123; public static void main(String[] args) &#123; System.out.println("EnumSet展示"); EnumSet&lt;ErrorCodeEn&gt; errSet = EnumSet.allOf(ErrorCodeEn.class); for (ErrorCodeEn e : errSet) &#123; System.out.println(e.name() + " : " + e.ordinal()); &#125; &#125;&#125; EnumMap EnumMap 是专门为枚举类型量身定做的 Map 实现。虽然使用其它的 Map 实现（如 HashMap）也能完成枚举类型实例到值得映射，但是使用 EnumMap 会更加高效：它只能接收同一枚举类型的实例作为键值，并且由于枚举类型实例的数量相对固定并且有限，所以 EnumMap 使用数组来存放与枚举类型对应的值。这使得 EnumMap 的效率非常高。 主要接口： size - 返回键值对数 containsValue - 是否存在指定的 value containsKey - 是否存在指定的 key get - 根据指定 key 获取 value put - 取出指定的键值对 remove - 删除指定 key putAll - 批量取出键值对 clear - 清除数据 keySet - 获取 key 集合 values - 返回所有 示例： public class EnumMapDemo &#123; public enum Signal &#123; GREEN, YELLOW, RED &#125; public static void main(String[] args) &#123; System.out.println("EnumMap展示"); EnumMap&lt;Signal, String&gt; errMap = new EnumMap(Signal.class); errMap.put(Signal.RED, "红灯"); errMap.put(Signal.YELLOW, "黄灯"); errMap.put(Signal.GREEN, "绿灯"); for (Iterator&lt;Map.Entry&lt;Signal, String&gt;&gt; iter = errMap.entrySet().iterator(); iter.hasNext();) &#123; Map.Entry&lt;Signal, String&gt; entry = iter.next(); System.out.println(entry.getKey().name() + " : " + entry.getValue()); &#125; &#125;&#125; 扩展阅读：深入理解 Java 枚举类型(enum) 这篇文章中对 EnumSet 和 EnumMap 原理做了较为详细的介绍。 小结 参考资料 Java 编程思想 JAVA 核心技术（卷 1） Effective java 深入理解 Java 枚举类型(enum) https://droidyue.com/blog/2016/11/29/dive-into-enum/]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
        <tag>enum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 基本数据类型]]></title>
    <url>%2Fblog%2F2018%2F04%2F24%2Fjava%2Fjavacore%2Fbasics%2FJava%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[深入理解 Java 基本数据类型 📓 本文已归档到：「blog」 数据类型分类 值类型 值类型和引用类型的区别 数据转换 自动转换 强制转换 装箱和拆箱 包装类、装箱、拆箱 自动装箱、自动拆箱 装箱、拆箱的应用和注意点 小结 参考资料 数据类型分类 Java 中的数据类型有两类： 值类型（又叫内置数据类型，基本数据类型） 引用类型（除值类型以外，都是引用类型，包括 String、数组） 值类型 Java 语言提供了 8 种基本类型，大致分为 4 类 整数型 byte - 8 位。 short - 16 位。 int - 32 位。 long - 64 位，赋值时一般在数字后加上 l 或 L。 浮点型 float - 32 位，直接赋值时必须在数字后加上 f 或 F。 double - 64 位，赋值时一般在数字后加 d 或 D 。 字符型 char - 16 位，存储 Unicode 码，用单引号赋值。 布尔型 boolean - 只有 true 和 false 两个取值。 值类型和引用类型的区别 从概念方面来说 基本类型：变量名指向具体的数值。 引用类型：变量名指向存数据对象的内存地址。 从内存方面来说 基本类型：变量在声明之后，Java 就会立刻分配给他内存空间。 引用类型：它以特殊的方式（类似 C 指针）向对象实体（具体的值），这类变量声明时不会分配内存，只是存储了一个内存地址。 从使用方面来说 基本类型：使用时需要赋具体值,判断时使用 == 号。 引用类型：使用时可以赋 null，判断时使用 equals 方法。 👉 扩展阅读：Java 基本数据类型和引用类型 这篇文章对于基本数据类型和引用类型的内存存储讲述比较生动。 数据转换 Java 中，数据类型转换有两种方式： 自动转换 强制转换 自动转换 一般情况下，定义了某数据类型的变量，就不能再随意转换。但是 JAVA 允许用户对基本类型做有限度的类型转换。 如果符合以下条件，则 JAVA 将会自动做类型转换： 由小数据转换为大数据 显而易见的是，“小”数据类型的数值表示范围小于“大”数据类型的数值表示范围，即精度小于“大”数据类型。 所以，如果“大”数据向“小”数据转换，会丢失数据精度。比如：long 转为 int，则超出 int 表示范围的数据将会丢失，导致结果的不确定性。 反之，“小”数据向“大”数据转换，则不会存在数据丢失情况。由于这个原因，这种类型转换也称为扩大转换。 这些类型由“小”到“大”分别为：(byte，short，char) &lt; int &lt; long &lt; float &lt; double。 这里我们所说的“大”与“小”，并不是指占用字节的多少，而是指表示值的范围的大小。 转换前后的数据类型要兼容 由于 boolean 类型只能存放 true 或 false，这与整数或字符是不兼容的，因此不可以做类型转换。 整型类型和浮点型进行计算后，结果会转为浮点类型 示例： long x = 30;float y = 14.3f;System.out.println("x/y = " + x/y); 输出： x/y = 1.9607843 可见 long 虽然精度大于 float 类型，但是结果为浮点数类型。 强制转换 在不符合自动转换条件时或者根据用户的需要，可以对数据类型做强制的转换。 强制转换使用括号 () 。 引用类型也可以使用强制转换。 示例： float f = 25.5f;int x = (int)f;System.out.println("x = " + x); 装箱和拆箱 包装类、装箱、拆箱 Java 中为每一种基本数据类型提供了相应的包装类，如下： Byte &lt;-&gt; byteShort &lt;-&gt; shortInteger &lt;-&gt; intLong &lt;-&gt; longFloat &lt;-&gt; floatDouble &lt;-&gt; doubleCharacter &lt;-&gt; charBoolean &lt;-&gt; boolean 引入包装类的目的就是：提供一种机制，使得基本数据类型可以与引用类型互相转换。 基本数据类型与包装类的转换被称为装箱和拆箱。 装箱（boxing）是将值类型转换为引用类型。例如：int 转 Integer 装箱过程是通过调用包装类的 valueOf 方法实现的。 拆箱（unboxing）是将引用类型转换为值类型。例如：Integer 转 int 拆箱过程是通过调用包装类的 xxxValue 方法实现的。（xxx 代表对应的基本数据类型）。 自动装箱、自动拆箱 基本数据（Primitive）型的自动装箱（boxing）拆箱（unboxing）自 JDK 5 开始提供的功能。 自动装箱与拆箱的机制可以让我们在 Java 的变量赋值或者是方法调用等情况下使用原始类型或者对象类型更加简单直接。 因为自动装箱会隐式地创建对象，如果在一个循环体中，会创建无用的中间对象，这样会增加 GC 压力，拉低程序的性能。所以在写循环时一定要注意代码，避免引入不必要的自动装箱操作。 JDK 5 之前的形式： Integer i1 = new Integer(10); // 非自动装箱 JDK 5 之后： Integer i2 = 10; // 自动装箱 Java 对于自动装箱和拆箱的设计，依赖于一种叫做享元模式的设计模式（有兴趣的朋友可以去了解一下源码，这里不对设计模式展开详述）。 👉 扩展阅读：深入剖析 Java 中的装箱和拆箱 结合示例，一步步阐述装箱和拆箱原理。 装箱、拆箱的应用和注意点 装箱、拆箱应用场景 一种最普通的场景是：调用一个含类型为 Object 参数的方法，该 Object 可支持任意类型（因为 Object 是所有类的父类），以便通用。当你需要将一个值类型（如 int）传入时，需要使用 Integer 装箱。 另一种用法是：一个非泛型的容器，同样是为了保证通用，而将元素类型定义为 Object。于是，要将值类型数据加入容器时，需要装箱。 当 == 运算符的两个操作，一个操作数是包装类，另一个操作数是表达式（即包含算术运算）则比较的是数值（即会触发自动拆箱的过程）。 示例： Integer i1 = 10; // 自动装箱Integer i2 = new Integer(10); // 非自动装箱Integer i3 = Integer.valueOf(10); // 非自动装箱int i4 = new Integer(10); // 自动拆箱int i5 = i2.intValue(); // 非自动拆箱System.out.println("i1 = [" + i1 + "]");System.out.println("i2 = [" + i2 + "]");System.out.println("i3 = [" + i3 + "]");System.out.println("i4 = [" + i4 + "]");System.out.println("i5 = [" + i5 + "]");System.out.println("i1 == i2 is [" + (i1 == i2) + "]");System.out.println("i1 == i4 is [" + (i1 == i4) + "]"); // 自动拆箱// Output:// i1 = [10]// i2 = [10]// i3 = [10]// i4 = [10]// i5 = [10]// i1 == i2 is [false]// i1 == i4 is [true] 示例说明： 上面的例子，虽然简单，但却隐藏了自动装箱、拆箱和非自动装箱、拆箱的应用。从例子中可以看到，明明所有变量都初始化为数值 10 了，但为何会出现 i1 == i2 is [false 而 i1 == i4 is [true] ？ 原因在于： i1、i2 都是包装类，使用 == 时，Java 将它们当做两个对象，而非两个 int 值来比较，所以两个对象自然是不相等的。正确的比较操作应该使用 equals 方法。 i1 是包装类，i4 是基础数据类型，使用 == 时，Java 会将两个 i1 这个包装类对象自动拆箱为一个 int 值，再代入到 == 运算表达式中计算；最终，相当于两个 int 进行比较，由于值相同，所以结果相等。 装箱、拆箱应用注意点 装箱操作会创建对象，频繁的装箱操作会造成不必要的内存消耗，影响性能。所以应该尽量避免装箱。 基础数据类型的比较操作使用 ==，包装类的比较操作使用 equals 方法。 小结 参考资料 Java 编程思想 JAVA 核心技术（卷 1） Java 基本数据类型和引用类型 深入剖析 Java 中的装箱和拆箱]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
        <tag>datatype</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 编程规范]]></title>
    <url>%2Fblog%2F2018%2F04%2F13%2Fjava%2Fjavacore%2Fadvanced%2Fjava-code-style%2F</url>
    <content type="text"><![CDATA[Java 编程规范 编程规范就是 Java 开发的最佳实践。帮助开发人员少走弯路。 Effective Java 第 2 章 创建、销毁对象 第 1 条：考虑用静态工厂方法代替构造器 第 2 条：遇到多个构造器参数时要考虑用构建器 第 3 条：用私有构造器或者枚举类型强化 Singleton 属性 第 4 条：通过私有构造器强化不可实例化的能力 第 5 条：避免创建不必要的对象 第 6 条：消除过期的对象引用 第 7 条：避免使用终结方法 第 3 章 对于所有对象都通用的方法 第 8 条：覆盖 equals 时请遵守通用约定 第 9 条：覆盖 equals 时总要覆盖 hashCode 第 10 条：始终要覆盖 toString 第 11 条：谨慎地覆盖 clone 第 12 条：考虑实现 Comparable 接口 第 4 章 类和接口 第 13 条：使类和成员的可访问性最小化 第 14 条：在公有类中使用访问方法而非公有域 第 15 条：使可变性最小化 第 16 条：复合优先于继承 第 17 条：要么为继承而设计，并提供文档说明，要么就禁止继承 第 18 条：接口优于抽象类 第 19 条：接口只用于定义类型 第 20 条：类层次优于标签类 第 21 条：用函数对象表示策略 第 22 条：优先考虑静态成员类 第 5 章 泛型 第 23 条：请不要在新代码中使用原生态类型 第 24 条：消除非受检警告 第 25 条：列表优先于数组 第 26 条：优先考虑泛型 第 27 条：优先考虑泛型方法 第 28 条：利用有限制通配符来提升 API 的灵活性 第 29 条：优先考虑类型安全的异构容器 第 6 章 枚举和注解 第 30 条：用 enum 代替 int 常量 第 31 条：用实例域代替序数 第 32 条：用 EnumSet 代替位域 第 33 条：用 EnumMap 代替序数索引 第 34 条：用接口模拟可伸缩的枚举 第 35 条：注解优先于命名模式 第 36 条：坚持使用 Override 注解 第 37 条：用标记接口定义类型 第 7 章 方法 第 38 条：检查参数的有效性 第 39 条：必要时进行保护性拷贝 第 40 条：谨慎设计方法签名 第 41 条：慎用重载 第 42 条：慎用可变参数 第 43 条：返回零长度的数组或者集合，而不是：null 第 44 条：为所有导出的 API 元素编写文档注释 第 8 章 通用程序设计 第 45 条：将局部变量的作用域最小化 第 46 条：for-each 循环优先于传统的 for 循环 第 47 条：了解和使用类库 第 48 条：如果需要精确的答案，请避免使用 float 和 double 第 49 条：基本类型优先于装箱基本类型 第 50 条：如果其他类型更适合，则尽量避免使用字符串 第 51 条：当心字符串连接的性能 第 52 条：通过接口引用对象 第 53 条：接口优先于反射机制 第 54 条：谨慎地使用本地方法 第 55 条：谨慎地进行优化 第 56 条：遵守普遍接受的命名惯例 第 9 章 异常 第 57 条：只针对异常的情况才使用异常 第 58 条：对可恢复的情况使用受检异常，对编程错误使用运行时异常 第 59 条：避免不必要地使用受检的异常 第 60 条：优先使用标准的异常 第 61 条：抛出与抽象相对应的异常 第 62 条：每个方法抛出的异常都要有文档 第 63 条：在细节消息中包含能捕获失败的信息 第 64 条：努力使失败保持原子性 第 65 条：不要忽略异常 第 10 章 并发 第 66 条：同步访问共享的可变数据 第 67 条：避免过度同步 第 68 条：executor 和 task 优先干线程 第 69 条：并发工具优先于 wait 和 notify 第 70 条：线程安全性的文档化 第 71 条：慎用延迟初始化 第 72 条：不要依赖于线程调度器 第 73 条：避免使用线程组 第 11 章 序列化 第 74 条：谨慎地实现 Serializable 接口 第 75 条：考虑使用自定义的序列化形式 第 76 条：保护性地编写 readObject 方法 第 77 条：对于实例控制，枚举类型优先于 readResolve 第 78 条：考虑用序列化代理代替序列化实例 资源 Effective Java 阿里巴巴 Java 开发手册 Google Java 编程指南]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>advanced</tag>
        <tag>code style</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 硬件管理]]></title>
    <url>%2Fblog%2F2018%2F02%2F27%2Fos%2Flinux%2Fcli%2F08.Linux%E7%A1%AC%E4%BB%B6%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Linux 硬件管理 关键词：df, du, top, free, iotop Linux 硬件管理要点 命令常见用法 df du top free iotop Linux 硬件管理要点 查看磁盘空间 - 使用 df 查看文件或目录的磁盘空间 - 使用 du 实时查看系统整体运行状态（如：CPU、内存） - 使用 top 查看已使用和未使用的内存 - 使用 free 查看磁盘 I/O 使用状况 - 使用 iotop 命令常见用法 df df 命令用于显示磁盘分区上的可使用的磁盘空间。默认显示单位为 KB。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 参考：http://man.linuxde.net/df 示例： # 查看系统磁盘设备，默认是 KB 为单位[root@LinServ-1 ~]# df文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda2 146294492 28244432 110498708 21% //dev/sda1 1019208 62360 904240 7% /boottmpfs 1032204 0 1032204 0% /dev/shm/dev/sdb1 2884284108 218826068 2518944764 8% /data1# 使用 -h 选项以 KB 以上的单位来显示，可读性高[root@LinServ-1 ~]# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sda2 140G 27G 106G 21% //dev/sda1 996M 61M 884M 7% /boottmpfs 1009M 0 1009M 0% /dev/shm/dev/sdb1 2.7T 209G 2.4T 8% /data1# 查看全部文件系统[root@LinServ-1 ~]# df -a文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda2 146294492 28244432 110498708 21% /proc 0 0 0 - /procsysfs 0 0 0 - /sysdevpts 0 0 0 - /dev/pts/dev/sda1 1019208 62360 904240 7% /boottmpfs 1032204 0 1032204 0% /dev/shm/dev/sdb1 2884284108 218826068 2518944764 8% /data1none 0 0 0 - /proc/sys/fs/binfmt_misc du du 命令也是查看使用空间的，但是与 df 命令不同的是：du 命令是对文件和目录磁盘使用的空间的查看，还是和 df 命令有一些区别的。 参考：http://man.linuxde.net/du 示例： # 显示目录或者文件所占空间root@localhost [test]# du608 ./test6308 ./test44 ./scf/lib4 ./scf/service/deploy/product4 ./scf/service/deploy/info12 ./scf/service/deploy16 ./scf/service4 ./scf/doc4 ./scf/bin32 ./scf8 ./test31288 .# 显示指定文件所占空间[root@localhost test]# du log2012.log300 log2012.log# 查看指定目录的所占空间[root@localhost test]# du scf4 scf/lib4 scf/service/deploy/product4 scf/service/deploy/info12 scf/service/deploy16 scf/service4 scf/doc4 scf/bin32 scf# 显示多个文件所占空间[root@localhost test]# du log30.tar.gz log31.tar.gz4 log30.tar.gz4 log31.tar.gz# 只显示总和的大小[root@localhost test]# du -s1288 .[root@localhost test]# du -s scf32 scf top top 命令可以实时动态地查看系统的整体运行情况，是一个综合了多方信息监测系统性能和运行信息的实用工具。通过 top 命令所提供的互动式界面，用热键可以管理。 参考：http://man.linuxde.net/top free free 命令可以显示当前系统未使用的和已使用的内存数目，还可以显示被内核使用的内存缓冲区。 参考：http://man.linuxde.net/free 示例： free -t # 以总和的形式显示内存的使用信息free -s 10 # 周期性的查询内存使用信息，每10s 执行一次命令# 显示内存使用情况free -m total used free shared buffers cachedMem: 2016 1973 42 0 163 1497-/+ buffers/cache: 312 1703Swap: 4094 0 4094 iotop iotop 命令是一个用来监视磁盘 I/O 使用状况的 top 类工具。iotop 具有与 top 相似的 UI，其中包括 PID、用户、I/O、进程等相关信息。Linux 下的 IO 统计工具如 iostat，nmon 等大多数是只能统计到 per 设备的读写情况，如果你想知道每个进程是如何使用 IO 的就比较麻烦，使用 iotop 命令可以很方便的查看。 参考：http://man.linuxde.net/iotop 示例： Total DISK read: 0.00 B/s | Total DISK write: 0.00 B/s TID PRIO USER DISK READ DISK WRITE SWAPIN IO&gt; command 1 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % init [3] 2 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [kthreadd] 3 rt/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [migration/0] 4 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [ksoftirqd/0] 5 rt/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [watchdog/0] 6 rt/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [migration/1] 7 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [ksoftirqd/1] 8 rt/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [watchdog/1] 9 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [events/0] 10 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [events/1] 11 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [khelper]2572 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [bluetooth]]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 用户管理]]></title>
    <url>%2Fblog%2F2018%2F02%2F27%2Fos%2Flinux%2Fcli%2F05.Linux%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Linux 用户管理 关键词：groupadd, groupdel, groupmod, useradd, userdel, usermod, passwd, su, sudo Linux 用户管理要点 命令常见用法 groupadd groupdel groupmod useradd userdel usermod passwd su sudo Linux 用户管理要点 创建用户组 - 使用 groupadd 删除用户组 - 使用 groupdel 修改用户组信息 - 使用 groupmod 创建用户 - 使用 useradd 删除用户 - 使用 userdel 修改用户信息 - 使用 usermod 设置用户认证信息 - 使用 passwd 切换用户 - 使用 su 当前用户想执行没有权限执行的命令时，使用其他用户身份去执行 - 使用 sudo 命令常见用法 groupadd groupadd 命令用于创建一个新的用户组，新用户组的信息将被添加到系统文件中。 参考：http://man.linuxde.net/groupadd 示例： # 建立一个新组，并设置组 ID 加入系统$ groupadd -g 344 jsdigname groupdel groupdel 命令用于删除指定的用户组，本命令要修改的系统文件包括 /ect/group 和 /ect/gshadow。若该群组中仍包括某些用户，则必须先删除这些用户后，方能删除群组。 参考：http://man.linuxde.net/groupdel 示例： $ groupadd damon # 创建damon用户组$ groupdel damon # 删除这个用户组 groupmod groupmod 命令更改群组识别码或名称。需要更改群组的识别码或名称时，可用 groupmod 指令来完成这项工作。 参考：http://man.linuxde.net/groupmod useradd useradd 命令用于 Linux 中创建的新的系统用户。useradd 可用来建立用户帐号。帐号建好之后，再用 passwd 设定帐号的密码．而可用 userdel 删除帐号。使用 useradd 指令所建立的帐号，实际上是保存在 /etc/passwd 文本文件中。 参考：http://man.linuxde.net/useradd 示例： # 新建用户加入组$ useradd –g sales jack –G company,employees # -g：加入主要组、-G：加入次要组# 建立一个新用户账户，并设置 ID$ useradd caojh -u 544 userdel userdel 命令用于删除给定的用户，以及与用户相关的文件。若不加选项，则仅删除用户帐号，而不删除相关文件。 参考：http://man.linuxde.net/userdel 示例： userdel 命令很简单，比如我们现在有个用户 linuxde，其 home 目录位于/var目录中，现在我们来删除这个用户： $ userdel linuxde # 删除用户linuxde，但不删除其家目录及文件；$ userdel -r linuxde # 删除用户linuxde，其 home 目录及文件一并删除； usermod usermod 命令用于修改用户的基本信息。usermod 命令不允许你改变正在线上的使用者帐号名称。当 usermod 命令用来改变 user id，必须确认这名 user 没在电脑上执行任何程序。你需手动更改使用者的 crontab 档。也需手动更改使用者的 at 工作档。采用 NIS server 须在 server 上更动相关的 NIS 设定。 参考：http://man.linuxde.net/usermod 示例： # 将 newuser2 添加到组 staff 中$ usermod -G staff newuser2# 修改 newuser 的用户名为 newuser1$ usermod -l newuser1 newuser# 锁定账号 newuser1$ usermod -L newuser1# 解除对 newuser1 的锁定$ usermod -U newuser1 passwd passwd 命令用于设置用户的认证信息，包括用户密码、密码过期时间等。系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。 参考：http://man.linuxde.net/passwd 示例： # 如果是普通用户执行 passwd 只能修改自己的密码。# 如果新建用户后，要为新用户创建密码，则用 passwd 用户名，注意要以 root 用户的权限来创建。$ passwd linuxde # 更改或创建linuxde用户的密码；Changing password for user linuxde.New UNIX password: # 请输入新密码；Retype new UNIX password: # 再输入一次；passwd: all authentication tokens updated successfully. # 成功；# 普通用户如果想更改自己的密码，直接运行 passwd 即可，比如当前操作的用户是 linuxde。$ passwdChanging password for user linuxde. # 更改linuxde用户的密码；(current) UNIX password: # 请输入当前密码；New UNIX password: # 请输入新密码；Retype new UNIX password: # 确认新密码；passwd: all authentication tokens updated successfully. # 更改成功；# 比如我们让某个用户不能修改密码，可以用`-l`选项来锁定：$ passwd -l linuxde # 锁定用户linuxde不能更改密码；Locking password for user linuxde.passwd: Success # 锁定成功；$ su linuxde # 通过su切换到linuxde用户；$ passwd # linuxde来更改密码；Changing password for user linuxde.Changing password for linuxde(current) UNIX password: # 输入linuxde的当前密码；passwd: Authentication token manipulation error # 失败，不能更改密码；$ passwd -d linuxde # 清除linuxde用户密码；Removing password for user linuxde.passwd: Success # 清除成功；$ passwd -S linuxde # 查询linuxde用户密码状态；Empty password. # 空密码，也就是没有密码； su su 命令用于切换当前用户身份到其他用户身份，变更时须输入所要变更的用户帐号与密码。 参考：http://man.linuxde.net/su 示例： # 变更帐号为 root 并在执行 ls 指令后退出变回原使用者：$ su -c ls root# 变更帐号为 root 并传入`-f`选项给新执行的 shell：$ su root -f# 变更帐号为 test 并改变工作目录至 test 的家目录：$ su -test sudo sudo 命令用来以其他身份来执行命令，预设的身份为 root。在 /etc/sudoers 中设置了可执行 sudo 指令的用户。若其未经授权的用户企图使用 sudo，则会发出警告的邮件给管理员。用户使用 sudo 时，必须先输入密码，之后有 5 分钟的有效期限，超过期限则必须重新输入密码。 参考：http://man.linuxde.net/sudo 示例： # 指定用户执行命令$ sudo -u userb ls -l# 列出目前的权限$ sudo -l# 显示sudo设置$ sudo -L 给普通用户授权 sudo 假设要给普通用户 mary 配置 sudo 权限： /etc/sudoers 文件存放了 sudo 的相关用户，但是默认是没有写权限的，所以需要设为可写：chmod u+w /etc/sudoers 在该文件中添加 mary ALL=(ALL) ALL ，保存并退出，让 mary 具有 sudo 的所有权限 再将 /etc/sudoers 的权限恢复到默认状态：chmod u-w /etc/sudoers 免密码授权 sudo 与给普通用户授权 sudo 类似，区别仅在于第 2 步：mary ALL=(ALL) NOPASSWD: ALL。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 文件压缩和解压]]></title>
    <url>%2Fblog%2F2018%2F02%2F27%2Fos%2Flinux%2Fcli%2F04.Linux%E6%96%87%E4%BB%B6%E5%8E%8B%E7%BC%A9%E5%92%8C%E8%A7%A3%E5%8E%8B%2F</url>
    <content type="text"><![CDATA[Linux 文件压缩和解压 关键词：tar, gzip, zip, unzip Linux 文件压缩和解压要点 命令常见用法 tar gzip zip unzip Linux 文件压缩和解压要点 压缩和解压 tar 文件 - 使用 tar 压缩和解压 gz 文件 - 使用 gzip 压缩和解压 zip 文件 - 分别使用 zip、unzip 命令常见用法 tar tar 命令可以为 linux 的文件和目录创建档案。利用 tar，可以为某一特定文件创建档案（备份文件），也可以在档案中改变文件，或者向档案中加入新的文件。tar 最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案。利用 tar 命令，可以把一大堆的文件和目录全部打包成一个文件，这对于备份文件或将几个文件组合成为一个文件以便于网络传输是非常有用的。 参考：http://man.linuxde.net/tar 示例： tar -cvf log.tar log2012.log # 仅打包，不压缩tar -zcvf log.tar.gz log2012.log # 打包后，以 gzip 压缩tar -jcvf log.tar.bz2 log2012.log # 打包后，以 bzip2 压缩tar -ztvf log.tar.gz # 查阅上述 tar 包内有哪些文件tar -zxvf log.tar.gz # 将 tar 包解压缩tar -zxvf log30.tar.gz log2013.log # 只将 tar 内的部分文件解压出来 gzip gzip 命令用来压缩文件。gzip 是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多出“.gz”扩展名。 gzip 是在 Linux 系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。gzip 不仅可以用来压缩大的、较少使用的文件以节省磁盘空间，还可以和 tar 命令一起构成 Linux 操作系统中比较流行的压缩文件格式。据统计，gzip 命令对文本文件有 60%～ 70%的压缩率。减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。 参考：http://man.linuxde.net/gzip 示例： gzip * # 将所有文件压缩成 .gz 文件gzip -l * # 详细显示压缩文件的信息，并不解压gzip -dv * # 解压上例中的所有压缩文件，并列出详细的信息gzip -r log.tar # 压缩一个 tar 备份文件，此时压缩文件的扩展名为.tar.gzgzip -rv test/ # 递归的压缩目录gzip -dr test/ # 递归地解压目录 zip zip 命令可以用来解压缩文件，或者对文件进行打包操作。zip 是个使用广泛的压缩程序，文件经它压缩后会另外产生具有“.zip”扩展名的压缩文件。 参考：http://man.linuxde.net/zip 示例： # 将 /home/Blinux/html/ 这个目录下所有文件和文件夹打包为当前目录下的 html.zipzip -q -r html.zip /home/Blinux/html unzip unzip 命令用于解压缩由 zip 命令压缩的“.zip”压缩包。 参考：http://man.linuxde.net/unzip 示例： unzip test.zip # 解压 zip 文件unzip -n test.zip -d /tmp/ # 在指定目录下解压缩unzip -o test.zip -d /tmp/ # 在指定目录下解压缩，如果有相同文件存在则覆盖unzip -v test.zip # 查看压缩文件目录，但不解压]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 文件目录管理命令]]></title>
    <url>%2Fblog%2F2018%2F02%2F27%2Fos%2Flinux%2Fcli%2F02.Linux%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Linux 文件目录管理 关键词：cd, ls, pwd, mkdir, rmdir, tree, touch, ln, rename, stat, file, chmod, chown, locate, find, cp, scp, mv, rm Linux 文件目录工作机制 Linux 目录结构 Linux 文件属性 Linux 文件目录管理要点 目录管理 文件管理 文件和目录通用管理 命令常见用法 cd ls pwd mkdir rmdir tree touch ln rename stat file chmod chown locate find cp scp mv rm Linux 文件目录工作机制 Linux 目录结构 linux 目录结构是树形结构，其根目录是 / 。一张思维导图说明各个目录的作用： Linux 文件属性 Linux 系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux 系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。 在 Linux 中我们可以使用 ll 或者 ls –l 命令来显示一个文件的属性以及文件所属的用户和组，如： $ ls -ltotal 64dr-xr-xr-x 2 root root 4096 Dec 14 2012 bindr-xr-xr-x 4 root root 4096 Apr 19 2012 boot 实例中，bin 文件的第一个属性用 d 表示。d 在 Linux 中代表该文件是一个目录文件。 在 Linux 中第一个字符代表这个文件是目录、文件或链接文件等等。 当为 d 则是目录 当为 - 则是文件； 若是 l 则表示为链接文档(link file)； 若是 b 则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是 c 则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置)。 接下来的字符中，以三个为一组，且均为『rwx』 的三个参数的组合。其中，r 代表可读(read)、w 代表可写(write)、x 代表可执行(execute)。 要注意的是，这三个权限的位置不会改变，如果没有权限，就会出现减号 - 而已。 每个文件的属性由左边第一部分的 10 个字符来确定（如下图）。 从左至右用 0-9 这些数字来表示。 第 0 位确定文件类型 第 1-3 位确定属主（该文件的拥有者）拥有该文件的权限。 第 4-6 位确定属组（拥有者的同组用户）拥有该文件的权限。 第 7-9 位确定其他用户拥有该文件的权限。 第 1、4、7 位表示读权限，如果用&quot;r&quot;字符表示，则有读权限，如果用&quot;-&quot;字符表示，则没有读权限。 第 2、5、8 位表示写权限，如果用&quot;w&quot;字符表示，则有写权限，如果用&quot;-&quot;字符表示没有写权限。 第 3、6、9 位表示可执行权限，如果用&quot;x&quot;字符表示，则有执行权限，如果用&quot;-&quot;字符表示，则没有执行权限。 Linux 文件属主和属组 $ ls -ltotal 64dr-xr-xr-x 2 root root 4096 Dec 14 2012 bindr-xr-xr-x 4 root root 4096 Apr 19 2012 boot 对于文件来说，它都有一个特定的拥有者，也就是对该文件具有所有权的用户。 同时，在 Linux 系统中，用户是按组分类的，一个用户属于一个或多个组。 文件拥有者以外的用户又可以分为文件拥有者的同组用户和其他用户。 因此，Linux 系统按文件拥有者、文件拥有者同组用户和其他用户来规定了不同的文件访问权限。 在以上实例中，bin 文件是一个目录文件，属主和属组都为 root，属主有可读、可写、可执行的权限；与属主同组的其他用户有可读和可执行的权限；其他用户也有可读和可执行的权限。 Linux 文件目录管理要点 目录管理 切换目录 - 使用 cd 查看目录信息 - 使用 ls 显示当前目录的绝对路径 - 使用 pwd 树状显示目录的内容 - 使用 tree 创建目录 - 使用 mkdir 删除目录 - 使用 rmdir 文件管理 创建空文件 - 使用 touch 为文件创建连接 - 使用 ln 批量重命名 - 使用 rename 显示文件的详细信息 - 使用 stat 探测文件类型 - 使用 file 设置文件或目录的权限 - 使用 chmod 设置文件或目录的拥有者或所属群组 - 使用 chown 查找文件或目录 - 使用 locate 在指定目录下查找文件 - 使用 find 查找命令的绝对路径 - 使用 which 查找命令的程序、源代码等相关文件 - 使用 whereis 文件和目录通用管理 复制文件或目录 - 使用 cp 复制文件或目录到远程服务器 - 使用 scp 移动文件或目录 - 使用 mv 删除文件或目录 - 使用 rm 命令常见用法 cd cd 命令用来切换工作目录。 参考：http://man.linuxde.net/cd 示例： cd # 切换到用户主目录cd ~ # 切换到用户主目录cd - # 切换到上一个工作目录cd .. # 切换到上级目录cd ../.. # 切换到上两级目录 ls ls 命令用来显示目录信息。 参考：http://man.linuxde.net/ls 示例： ls # 列出当前目录可见文件ls -l # 列出当前目录可见文件详细信息ls -la # 列出所有文件（包括隐藏）的详细信息ls -lh # 列出详细信息并以可读大小显示文件大小ls -lt # 按时间列出文件和文件夹详细信息ls -ltr # 按修改时间列出文件和文件夹详细信息ls --color=auto # 列出文件并标记颜色分类 pwd pwd 命令用来显示当前目录的绝对路径。 参考：http://man.linuxde.net/pwd mkdir mkdir 命令用来创建目录。 参考：http://man.linuxde.net/mkdir 示例： # 在当前目录中创建 zp 和 zp 的子目录 testmkdir -p zp/test# 在当前目录中创建 zp 和 zp 的子目录 test；权限设置为文件主可读、写、执行，同组用户可读和执行，其他用户无权访问mkdir -p -m 750 zp/test rmdir rmdir 命令用来删除空目录。 参考：http://man.linuxde.net/rmdir 示例： # 删除子目录 test 和其父目录 zprmdir -p zp/test tree tree 命令以树状显示目录的内。 参考：http://man.linuxde.net/tree 示例： # 列出目录 /private 第一级文件名tree /private -L 1/private/├── etc├── tftpboot├── tmp└── var# 忽略文件夹tree -I node_modules # 忽略当前目录文件夹 node_modulestree -P node_modules # 列出当前目录文件夹 node_modules 的目录结构tree -P node_modules -L 2 # 显示目录 node_modules 两层的目录树结构tree -L 2 &gt; /home/www/tree.txt # 当前目录结果存到 tree.txt 文件中# 忽略多个文件夹tree -I 'node_modules|icon|font' -L 2 touch touch 命令有两个功能：一是用于把已存在文件的时间标签更新为系统当前的时间（默认方式），它们的数据将原封不动地保留下来；二是用来创建空文件。 参考：http://man.linuxde.net/touch 示例： touch ex2 ln ln 命令用来为文件创建连接，连接类型分为硬连接和符号连接两种，默认的连接类型是硬连接。如果要创建符号连接必须使用&quot;-s&quot;选项。 注意：符号链接文件不是一个独立的文件，它的许多属性依赖于源文件，所以给符号链接文件设置存取权限是没有意义的。 参考：http://man.linuxde.net/ln 示例： # 将目录 /usr/mengqc/mub1 下的文件 m2.c 链接到目录 /usr/liu 下的文件 a2.ccd /usr/mengqcln /mub1/m2.c /usr/liu/a2.c# 在目录 /usr/liu 下建立一个符号链接文件 abc，使它指向目录 /usr/mengqc/mub1# 执行该命令后，/usr/mengqc/mub1 代表的路径将存放在名为 /usr/liu/abc 的文件中ln -s /usr/mengqc/mub1 /usr/liu/abc rename rename 命令用字符串替换的方式批量重命名。 参考：http://man.linuxde.net/rename 示例： # 将 main1.c 重命名为 main.crename main1.c main.c main1.crename "s/AA/aa/" * # 把文件名中的 AA 替换成 aarename "s//.html//.php/" * # 把 .html 后缀的改成 .php 后缀rename "s/$//.txt/" * # 把所有的文件名都以 txt 结尾rename "s//.txt//" * # 把所有以 .txt 结尾的文件名的.txt 删掉 stat stat 命令用于显示文件的状态信息。stat 命令的输出信息比 ls 命令的输出信息要更详细。 参考：http://man.linuxde.net/stat 示例： stat myfile file file 命令用来探测给定文件的类型。file 命令对文件的检查分为文件系统、魔法幻数检查和语言检查 3 个过程。 参考：http://man.linuxde.net/file 示例： file install.log # 显示文件类型file -b install.log # 不显示文件名称file -i install.log # 显示 MIME 类型file -L /var/spool/mail # 显示符号链接的文件类型 chmod chmod 命令用来变更文件或目录的权限。在 UNIX 系统家族里，文件或目录权限的控制分别以读取、写入、执行 3 种一般权限来区分，另有 3 种特殊权限可供运用。用户可以使用 chmod 指令去变更文件与目录的权限，设置方式采用文字或数字代号皆可。符号连接的权限无法变更，如果用户对符号连接修改权限，其改变会作用在被连接的原始文件。 参考：http://man.linuxde.net/chmod 知识扩展： Linux 用 户分为：拥有者、组群(Group)、其他（other），Linux 系统中，预设的情況下，系统中所有的帐号与一般身份使用者，以及 root 的相关信 息， 都是记录在/etc/passwd文件中。每个人的密码则是记录在/etc/shadow文件下。 此外，所有的组群名称记录在/etc/group內！ linux 文件的用户权限的分析图 -rw-r--r-- 1 user staff 651 Oct 12 12:53 .gitmodules# ↑╰┬╯╰┬╯╰┬╯# ┆ ┆ ┆ ╰┈ 0 其他人# ┆ ┆ ╰┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈ g 属组# ┆ ╰┈┈┈┈ u 属组# ╰┈┈ 第一个字母 `d` 代表目录，`-` 代表普通文件 例：rwx rw- r– r=读取属性 //值＝ 4 w=写入属性 //值＝ 2 x=执行属性 //值＝ 1 示例： chmod u+x,g+w f01 # 为文件f01设置自己可以执行，组员可以写入的权限chmod u=rwx,g=rw,o=r f01chmod 764 f01chmod a+x f01 # 对文件f01的u,g,o都设置可执行属性# 将/home/wwwroot/里的所有文件和文件夹设置为755权限chmod -R 755 /home/wwwroot/* chown chown 命令改变某个文件或目录的所有者和所属的组，该命令可以向某个用户授权，使该用户变成指定文件的所有者或者改变文件所属的组。用户可以是用户或者是用户 D，用户组可以是组名或组 id。文件名可以使由空格分开的文件列表，在文件名中可以包含通配符。 只有文件拥有者和超级用户才可以便用该命令。 参考：http://man.linuxde.net/chown 示例： # 将目录/usr/meng及其下面的所有文件、子目录的文件主改成 liuchown -R liu /usr/meng locate locate 命令和 slocate 命令都用来查找文件或目录。 locate 命令其实是 find -name 的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库/var/lib/locatedb，这个数据库中含有本地所有文件信息。Linux 系统自动创建这个数据库，并且每天自动更新一次，所以使用 locate 命令查不到最新变动过的文件。为了避免这种情况，可以在使用 locate 之前，先使用 updatedb 命令，手动更新数据库。 参考：http://man.linuxde.net/locate_slocate 示例： locate pwd # 查找和 pwd 相关的所有文件locate /etc/sh # 搜索 etc 目录下所有以 sh 开头的文件 find find 命令用来在指定目录下查找文件。任何位于参数之前的字符串都将被视为欲查找的目录名。如果使用该命令时，不设置任何参数，则 find 命令将在当前目录下查找子目录与文件。并且将查找到的子目录和文件全部进行显示。 参考：http://man.linuxde.net/find # 当前目录搜索所有文件，文件内容 包含 “140.206.111.111” 的内容find . -type f -name "*" | xargs grep "140.206.111.111"# 列出当前目录及子目录下所有文件和文件夹find .# 在 /home 目录下查找以 .txt 结尾的文件名find /home -name "*.txt"# 同上，但忽略大小写find /home -iname "*.txt"# 当前目录及子目录下查找所有以 .txt 和 .pdf 结尾的文件find . -name "*.txt" -o -name "*.pdf"# 匹配文件路径或者文件find /usr/ -path "*local*"# 基于正则表达式匹配文件路径find . -regex ".*\(\.txt\|\.pdf\)$"# 同上，但忽略大小写find . -iregex ".*\(\.txt\|\.pdf\)$"# 找出 /home 下不是以 .txt 结尾的文件find /home ! -name "*.txt" cp cp 命令用来将一个或多个源文件或者目录复制到指定的目的文件或目录。它可以将单个源文件复制成一个指定文件名的具体的文件或一个已经存在的目录下。cp 命令还支持同时复制多个文件，当一次复制多个文件时，目标文件参数必须是一个已经存在的目录，否则将出现错误。 参考：http://man.linuxde.net/cp 示例： 参数 源文件：制定源文件列表。默认情况下，cp 命令不能复制目录，如果要复制目录，则必须使用-R选项； 目标文件：指定目标文件。当“源文件”为多个文件时，要求“目标文件”为指定的目录。 示例： # 将文件 file 复制到目录 /usr/men/tmp 下，并改名为 file1cp file /usr/men/tmp/file1# 将目录 /usr/men下的所有文件及其子目录复制到目录 /usr/zh 中cp -r /usr/men /usr/zh# 强行将 /usr/men下的所有文件复制到目录 /usr/zh 中，无论是否有文件重复cp -rf /usr/men/* /usr/zh# 将目录 /usr/men 中的以 m 打头的所有 .c 文件复制到目录 /usr/zh 中cp -i /usr/men m*.c /usr/zh scp scp 命令用于在 Linux 下进行远程拷贝文件的命令，和它类似的命令有 cp，不过 cp 只是在本机进行拷贝不能跨服务器，而且 scp 传输是加密的。可能会稍微影响一下速度。当你服务器硬盘变为只读 read only system 时，用 scp 可以帮你把文件移出来。另外，scp 还非常不占资源，不会提高多少系统负荷，在这一点上，rsync 就远远不及它了。虽然 rsync 比 scp 会快一点，但当小文件众多的情况下，rsync 会导致硬盘 I/O 非常高，而 scp 基本不影响系统正常使用。 示例： # 拷贝文件到远程服务器的指定目录scp &lt;file&gt; &lt;user&gt;@&lt;ip&gt;:&lt;url&gt;scp test.txt root@192.168.0.1:/opt# 拷贝目录到远程服务器的指定目录scp -r &lt;folder&gt; &lt;user&gt;@&lt;ip&gt;:&lt;url&gt;scp -r test root@192.168.0.1:/opt 免密码传输 （1）生成 ssh 公私钥对 ssh-keygen -t rsa （2）将服务器 A 的 \~/.ssh/id_rsa.pub 文件内容复制到服务器 B 的 \~/.ssh/authorized_keys 文件中。 # 服务器 A 上执行以下命令scp ~/.ssh/id_rsa.pub root@192.168.0.2:~/.ssh/id_rsa.pub.tmp# 服务器 B 上执行以下命令cat ~/.ssh/id_rsa.pub.tmp &gt;&gt; ~/.ssh/authorized_keysrm ~/.ssh/id_rsa.pub.tmp mv mv 命令用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。source 表示源文件或目录，target 表示目标文件或目录。如果将一个文件移到一个已经存在的目标文件中，则目标文件的内容将被覆盖。 参考：http://man.linuxde.net/mv 示例： mv file1.txt /home/office/ # 移动单个文件mv file2.txt file3.txt file4.txt /home/office/ # 移动多个文件mv *.txt /home/office/ # 移动所有 txt 文件mv dir1/ /home/office/ # 移动目录mv /usr/men/* . # 将指定目录中的所有文件移到当前目录中mv file1.txt file2.txt # 重命名文件mv dir1/ dir2/ # 重命名目录mv -v *.txt /home/office # 打印移动信息mv -i file1.txt /home/office # 提示是否覆盖文件mv -uv *.txt /home/office # 源文件比目标文件新时才执行更新mv -vn *.txt /home/office # 不要覆盖任何已存在的文件mv -f *.txt /home/office # 无条件覆盖已经存在的文件mv -bv *.txt /home/office # 复制时创建备份 rm rm 命令可以删除一个目录中的一个或多个文件或目录，也可以将某个目录及其下属的所有文件及其子目录均删除掉。对于链接文件，只是删除整个链接文件，而原有文件保持不变。 参考：http://man.linuxde.net/rm rm test.txt # 删除文件rm -i test.txt test2.txt # 交互式删除文件rm -r * # 删除当前目录下的所有文件和目录rm -r testdir # 删除目录下的所有文件和目录rm -rf testdir # 强制删除目录下的所有文件和目录rm -v testdir # 显示当前删除操作的详情]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 网络管理]]></title>
    <url>%2Fblog%2F2018%2F02%2F27%2Fos%2Flinux%2Fcli%2F07.Linux%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Linux 网络管理 关键词：curl, wget, telnet, ip, hostname, ifconfig, route, ssh, ssh-keygen, firewalld, iptables, host, nslookup, nc/netcat, ping, traceroute, netstat Linux 网络应用要点 命令常见用法 curl wget telnet ip hostname ifconfig route ssh ssh-keygen firewalld iptables host nslookup nc/netcat ping traceroute netstat Linux 网络应用要点 下载文件 - 使用 curl、wget telnet 方式登录远程主机，对远程主机进行管理 - 使用 telnet 查看或操纵 Linux 主机的路由、网络设备、策略路由和隧道 - 使用 ip 查看和设置系统的主机名 - 使用 hostname 查看和配置 Linux 内核中网络接口的网络参数 - 使用 ifconfig 查看和设置 Linux 内核中的网络路由表 - 使用 route ssh 方式连接远程主机 - 使用 ssh 为 ssh 生成、管理和转换认证密钥 - 使用 ssh-keygen 查看、设置防火墙（Centos7），使用 firewalld 查看、设置防火墙（Centos7 以前），使用 iptables 查看域名信息 - 使用 host, nslookup 设置路由 - 使用 nc/netcat 测试主机之间网络是否连通 - 使用 ping 追踪数据在网络上的传输时的全部路径 - 使用 traceroute 查看当前工作的端口信息 - 使用 netstat 命令常见用法 curl curl 命令是一个利用 URL 规则在命令行下工作的文件传输工具。它支持文件的上传和下载，所以是综合传输工具，但按传统，习惯称 curl 为下载工具。作为一款强力工具，curl 支持包括 HTTP、HTTPS、ftp 等众多协议，还支持 POST、cookies、认证、从指定偏移处下载部分文件、用户代理字符串、限速、文件大小、进度条等特征。做网页处理流程和数据检索自动化，curl 可以祝一臂之力。 参考：http://man.linuxde.net/curl 示例： # 下载文件$ curl http://man.linuxde.net/text.iso --silent# 下载文件，指定下载路径，并查看进度$ curl http://man.linuxde.net/test.iso -o filename.iso --progress########################################## 100.0% wget wget 命令用来从指定的 URL 下载文件。 参考：http://man.linuxde.net/wget 示例： # 使用 wget 下载单个文件$ wget http://www.linuxde.net/testfile.zip telnet telnet 命令用于登录远程主机，对远程主机进行管理。 参考：http://man.linuxde.net/telnet 示例： telnet 192.168.2.10Trying 192.168.2.10...Connected to 192.168.2.10 (192.168.2.10).Escape character is '^]'. localhost (Linux release 2.6.18-274.18.1.el5 #1 SMP Thu Feb 9 12:45:44 EST 2012) (1)login: rootPassword:Login incorrect ip ip 命令用来查看或操纵 Linux 主机的路由、网络设备、策略路由和隧道，是 Linux 下较新的功能强大的网络配置工具。 参考：http://man.linuxde.net/ip 示例： $ ip link show # 查看网络接口信息$ ip link set eth0 upi # 开启网卡$ ip link set eth0 down # 关闭网卡$ ip link set eth0 promisc on # 开启网卡的混合模式$ ip link set eth0 promisc offi # 关闭网卡的混个模式$ ip link set eth0 txqueuelen 1200 # 设置网卡队列长度$ ip link set eth0 mtu 1400 # 设置网卡最大传输单元$ ip addr show # 查看网卡IP信息$ ip addr add 192.168.0.1/24 dev eth0 # 设置eth0网卡IP地址192.168.0.1$ ip addr del 192.168.0.1/24 dev eth0 # 删除eth0网卡IP地址$ ip route show # 查看系统路由$ ip route add default via 192.168.1.254 # 设置系统默认路由$ ip route list # 查看路由信息$ ip route add 192.168.4.0/24 via 192.168.0.254 dev eth0 # 设置192.168.4.0网段的网关为192.168.0.254,数据走eth0接口$ ip route add default via 192.168.0.254 dev eth0 # 设置默认网关为192.168.0.254$ ip route del 192.168.4.0/24 # 删除192.168.4.0网段的网关$ ip route del default # 删除默认路由$ ip route delete 192.168.1.0/24 dev eth0 # 删除路由 hostname hostname 命令用于查看和设置系统的主机名称。环境变量 HOSTNAME 也保存了当前的主机名。在使用 hostname 命令设置主机名后，系统并不会永久保存新的主机名，重新启动机器之后还是原来的主机名。如果需要永久修改主机名，需要同时修改 /etc/hosts 和 /etc/sysconfig/network 的相关内容。 参考：http://man.linuxde.net/hostname 示例： $ hostnameAY1307311912260196fcZ ifconfig ifconfig 命令被用于查看和配置 Linux 内核中网络接口的网络参数。用 ifconfig 命令配置的网卡信息，在网卡重启后机器重启后，配置就不存在。要想将上述的配置信息永远的存的电脑里，那就要修改网卡的配置文件了。 参考：http://man.linuxde.net/ifconfig 示例： # 查看网络设备信息（激活状态的）[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:16:3E:00:1E:51 inet addr:10.160.7.81 Bcast:10.160.15.255 Mask:255.255.240.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:61430830 errors:0 dropped:0 overruns:0 frame:0 TX packets:88534 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:3607197869 (3.3 GiB) TX bytes:6115042 (5.8 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:56103 errors:0 dropped:0 overruns:0 frame:0 TX packets:56103 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:5079451 (4.8 MiB) TX bytes:5079451 (4.8 MiB) route route 命令用来查看和设置 Linux 内核中的网络路由表，route 命令设置的路由主要是静态路由。要实现两个不同的子网之间的通信，需要一台连接两个网络的路由器，或者同时位于两个网络的网关来实现。 参考：http://man.linuxde.net/route 示例： # 查看当前路由routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface112.124.12.0 * 255.255.252.0 U 0 0 0 eth110.160.0.0 * 255.255.240.0 U 0 0 0 eth0192.168.0.0 10.160.15.247 255.255.0.0 UG 0 0 0 eth0172.16.0.0 10.160.15.247 255.240.0.0 UG 0 0 0 eth010.0.0.0 10.160.15.247 255.0.0.0 UG 0 0 0 eth0default 112.124.15.247 0.0.0.0 UG 0 0 0 eth1route add -net 224.0.0.0 netmask 240.0.0.0 dev eth0 # 添加网关/设置网关route add -net 224.0.0.0 netmask 240.0.0.0 reject # 屏蔽一条路由route del -net 224.0.0.0 netmask 240.0.0.0 # 删除路由记录route add default gw 192.168.120.240 # 添加默认网关route del default gw 192.168.120.240 # 删除默认网关 ssh ssh 命令是 openssh 套件中的客户端连接工具，可以给予 ssh 加密协议实现安全的远程登录服务器。 参考：http://man.linuxde.net/ssh 示例： # ssh 用户名@远程服务器地址ssh user1@172.24.210.101# 指定端口ssh -p 2211 root@140.206.185.170 引申阅读：ssh 背后的故事 ssh-keygen ssh-keygen 命令用于为 ssh 生成、管理和转换认证密钥，它支持 RSA 和 DSA 两种认证密钥。 参考：http://man.linuxde.net/ssh-keygen firewalld firewalld 命令是 Linux 上的防火墙软件（Centos7 默认防火墙）。 参考：https://www.cnblogs.com/moxiaoan/p/5683743.html firewalld 的基本使用 启动 - systemctl start firewalld 关闭 - systemctl stop firewalld 查看状态 - systemctl status firewalld 开机禁用 - systemctl disable firewalld 开机启用 - systemctl enable firewalld 使用 systemctl 管理 firewalld 服务 systemctl 是 CentOS7 的服务管理工具中主要的工具，它融合之前 service 和 chkconfig 的功能于一体。 启动一个服务 - systemctl start firewalld.service 关闭一个服务 - systemctl stop firewalld.service 重启一个服务 - systemctl restart firewalld.service 显示一个服务的状态 - systemctl status firewalld.service 在开机时启用一个服务 - systemctl enable firewalld.service 在开机时禁用一个服务 - systemctl disable firewalld.service 查看服务是否开机启动 - systemctl is-enabled firewalld.service 查看已启动的服务列表 - systemctl list-unit-files|grep enabled 查看启动失败的服务列表 - systemctl --failed 配置 firewalld-cmd 查看版本 - firewall-cmd --version 查看帮助 - firewall-cmd --help 显示状态 - firewall-cmd --state 查看所有打开的端口 - firewall-cmd --zone=public --list-ports 更新防火墙规则 - firewall-cmd --reload 查看区域信息: firewall-cmd --get-active-zones 查看指定接口所属区域 - firewall-cmd --get-zone-of-interface=eth0 拒绝所有包：firewall-cmd --panic-on 取消拒绝状态 - firewall-cmd --panic-off 查看是否拒绝 - firewall-cmd --query-panic 在防火墙中开放一个端口 添加（–permanent 永久生效，没有此参数重启后失效） - firewall-cmd --zone=public --add-port=80/tcp --permanent 重新载入 - firewall-cmd --reload 查看 - firewall-cmd --zone= public --query-port=80/tcp 删除 - firewall-cmd --zone= public --remove-port=80/tcp --permanent iptables iptables 命令是 Linux 上常用的防火墙软件，是 netfilter 项目的一部分。可以直接配置，也可以通过许多前端和图形界面配置。 参考：http://man.linuxde.net/iptables 示例： # 开放指定的端口iptables -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT #允许本地回环接口(即运行本机访问本机)iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT #允许已建立的或相关连的通行iptables -A OUTPUT -j ACCEPT #允许所有本机向外的访问iptables -A INPUT -p tcp --dport 22 -j ACCEPT #允许访问22端口iptables -A INPUT -p tcp --dport 80 -j ACCEPT #允许访问80端口iptables -A INPUT -p tcp --dport 21 -j ACCEPT #允许ftp服务的21端口iptables -A INPUT -p tcp --dport 20 -j ACCEPT #允许FTP服务的20端口iptables -A INPUT -j reject #禁止其他未允许的规则访问iptables -A FORWARD -j REJECT #禁止其他未允许的规则访问# 屏蔽IPiptables -I INPUT -s 123.45.6.7 -j DROP #屏蔽单个IP的命令iptables -I INPUT -s 123.0.0.0/8 -j DROP #封整个段即从123.0.0.1到123.255.255.254的命令iptables -I INPUT -s 124.45.0.0/16 -j DROP #封IP段即从123.45.0.1到123.45.255.254的命令iptables -I INPUT -s 123.45.6.0/24 -j DROP #封IP段即从123.45.6.1到123.45.6.254的命令是# 查看已添加的iptables规则iptables -L -n -vChain INPUT (policy DROP 48106 packets, 2690K bytes) pkts bytes target prot opt in out source destination 5075 589K ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 191K 90M ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:221499K 133M ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:804364K 6351M ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED 6256 327K ACCEPT icmp -- * * 0.0.0.0/0 0.0.0.0/0Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destinationChain OUTPUT (policy ACCEPT 3382K packets, 1819M bytes) pkts bytes target prot opt in out source destination 5075 589K ACCEPT all -- * lo 0.0.0.0/0 0.0.0.0/0 host host 命令是常用的分析域名查询工具，可以用来测试域名系统工作是否正常。 参考：http://man.linuxde.net/host 示例： [root@localhost ~]# host www.jsdig.comwww.jsdig.com is an alias for host.1.jsdig.com.host.1.jsdig.com has address 100.42.212.8[root@localhost ~]# host -a www.jsdig.comTrying "www.jsdig.com";; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 34671;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0;; QUESTION SECTION:;www.jsdig.com. IN ANY;; ANSWER SECTION:www.jsdig.com. 463 IN CNAME host.1.jsdig.com.Received 54 bytes from 202.96.104.15#53 in 0 ms nslookup nslookup 命令是常用域名查询工具，就是查 DNS 信息用的命令。 参考：http://man.linuxde.net/nslookup 示例： [root@localhost ~]# nslookup www.jsdig.comServer: 202.96.104.15Address: 202.96.104.15#53Non-authoritative answer:www.jsdig.com canonical name = host.1.jsdig.com.Name: host.1.jsdig.comAddress: 100.42.212.8 nc/netcat nc 命令是 netcat 命令的简称，都是用来设置路由器。 参考：http://man.linuxde.net/nc_netcat 示例： # TCP 端口扫描[root@localhost ~]# nc -v -z -w2 192.168.0.3 1-100192.168.0.3: inverse host lookup failed: Unknown host(UNKNOWN) [192.168.0.3] 80 (http) open(UNKNOWN) [192.168.0.3] 23 (telnet) open(UNKNOWN) [192.168.0.3] 22 (ssh) open# UDP 端口扫描[root@localhost ~]# nc -u -z -w2 192.168.0.1 1-1000 # 扫描192.168.0.3 的端口 范围是 1-1000 ping ping 命令用来测试主机之间网络的连通性。执行 ping 指令会使用 ICMP 传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 参考：http://man.linuxde.net/ping 示例： [root@AY1307311912260196fcZ ~]# ping www.jsdig.comPING host.1.jsdig.com (100.42.212.8) 56(84) bytes of data.64 bytes from 100-42-212-8.static.webnx.com (100.42.212.8): icmp_seq=1 ttl=50 time=177 ms64 bytes from 100-42-212-8.static.webnx.com (100.42.212.8): icmp_seq=2 ttl=50 time=178 ms64 bytes from 100-42-212-8.static.webnx.com (100.42.212.8): icmp_seq=3 ttl=50 time=174 ms64 bytes from 100-42-212-8.static.webnx.com (100.42.212.8): icmp_seq=4 ttl=50 time=177 ms...按Ctrl+C结束--- host.1.jsdig.com ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 2998msrtt min/avg/max/mdev = 174.068/176.916/178.182/1.683 ms traceroute traceroute 命令用于追踪数据包在网络上的传输时的全部路径，它默认发送的数据包大小是 40 字节。 参考：http://man.linuxde.net/traceroute 示例： traceroute www.58.comtraceroute to www.58.com (211.151.111.30), 30 hops max, 40 byte packets 1 unknown (192.168.2.1) 3.453 ms 3.801 ms 3.937 ms 2 221.6.45.33 (221.6.45.33) 7.768 ms 7.816 ms 7.840 ms 3 221.6.0.233 (221.6.0.233) 13.784 ms 13.827 ms 221.6.9.81 (221.6.9.81) 9.758 ms 4 221.6.2.169 (221.6.2.169) 11.777 ms 122.96.66.13 (122.96.66.13) 34.952 ms 221.6.2.53 (221.6.2.53) 41.372 ms 5 219.158.96.149 (219.158.96.149) 39.167 ms 39.210 ms 39.238 ms 6 123.126.0.194 (123.126.0.194) 37.270 ms 123.126.0.66 (123.126.0.66) 37.163 ms 37.441 ms 7 124.65.57.26 (124.65.57.26) 42.787 ms 42.799 ms 42.809 ms 8 61.148.146.210 (61.148.146.210) 30.176 ms 61.148.154.98 (61.148.154.98) 32.613 ms 32.675 ms 9 202.106.42.102 (202.106.42.102) 44.563 ms 44.600 ms 44.627 ms10 210.77.139.150 (210.77.139.150) 53.302 ms 53.233 ms 53.032 ms11 211.151.104.6 (211.151.104.6) 39.585 ms 39.502 ms 39.598 ms12 211.151.111.30 (211.151.111.30) 35.161 ms 35.938 ms 36.005 ms netstat netstat 命令用来打印 Linux 中网络系统的状态信息，可让你得知整个 Linux 系统的网络情况。 参考：http://man.linuxde.net/netstat 示例： # 列出所有端口 (包括监听和未监听的)netstat -a #列出所有端口netstat -at #列出所有tcp端口netstat -au #列出所有udp端口# 列出所有处于监听状态的 Socketsnetstat -l #只显示监听端口netstat -lt #只列出所有监听 tcp 端口netstat -lu #只列出所有监听 udp 端口netstat -lx #只列出所有监听 UNIX 端口# 显示每个协议的统计信息netstat -s 显示所有端口的统计信息netstat -st 显示TCP端口的统计信息netstat -su 显示UDP端口的统计信息]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 文件内容查看编辑]]></title>
    <url>%2Fblog%2F2018%2F02%2F27%2Fos%2Flinux%2Fcli%2F03.Linux%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E6%9F%A5%E7%9C%8B%E7%BC%96%E8%BE%91%2F</url>
    <content type="text"><![CDATA[Linux 文件内容查看编辑 关键词：cat, head, tail, more, less, sed, vi, grep Linux 文件内容查看编辑要点 命令常见用法 cat head tail more less sed vi grep 参考资料 Linux 文件内容查看编辑要点 连接文件并打印到标准输出设备 - 使用 cat 显示指定文件的开头若干行 - 使用 head 显示指定文件的末尾若干行，常用于实时打印日志文件内容 - 使用 tail 显示文件内容，每次显示一屏 - 使用 more 显示文件内容，每次显示一屏 - 使用 less 自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等 - 使用 sed 文本编辑器 - 使用 vi 使用正则表达式搜索文本，并把匹配的行打印出来 - 使用 grep 命令常见用法 cat cat 命令用于连接文件并打印到标准输出设备上。 参考：http://man.linuxde.net/cat 示例： cat m1 # 在屏幕上显示文件 ml 的内容cat m1 m2 # 同时显示文件 ml 和 m2 的内容cat m1 m2 &gt; file # 将文件 ml 和 m2 合并后放入文件 file 中 head head 命令用于显示文件的开头内容。在默认情况下，head 命令显示文件的头部 10 行内容。 参考：http://man.linuxde.net/head tail tail 命令用于显示文件的尾部内容。在默认情况下，tail 命令显示文件的尾部 10 行内容。如果给定的文件不止一个，则在显示的每个文件前面加一个文件名标题。如果没有指定文件或者文件名为“-”，则读取标准输入。 参考：http://man.linuxde.net/tail 示例： tail file # 显示文件file的最后10行tail -n +20 file # 显示文件file的内容，从第20行至文件末尾tail -c 10 file # 显示文件file的最后10个字符 more more 命令是一个基于 vi 编辑器文本过滤器，它以全屏幕的方式按页显示文本文件的内容，支持 vi 中的关键字定位操作。more 名单中内置了若干快捷键，常用的有 H（获得帮助信息），Enter（向下翻滚一行），空格（向下滚动一屏），Q（退出命令）。 该命令一次显示一屏文本，满屏后停下来，并且在屏幕的底部出现一个提示信息，给出至今己显示的该文件的百分比：–More–（XX%）可以用下列不同的方法对提示做出回答： 按 Space 键：显示文本的下一屏内容。 按 Enier 键：只显示文本的下一行内容。 按斜线符|：接着输入一个模式，可以在文本中寻找下一个相匹配的模式。 按 H 键：显示帮助屏，该屏上有相关的帮助信息。 按 B 键：显示上一屏内容。 按 Q 键：退出 rnore 命令。 参考：http://man.linuxde.net/more 示例： # 显示文件 file 的内容，但在显示之前先清屏，并且在屏幕的最下方显示完核的百分比。more -dc file# 显示文件 file 的内容，每 10 行显示一次，而且在显示之前先清屏。more -c -10 file less less 命令的作用与 more 十分相似，都可以用来浏览文字档案的内容，不同的是 less 命令允许用户向前或向后浏览文件，而 more 命令只能向前浏览。用 less 命令显示文件时，用 PageUp 键向上翻页，用 PageDown 键向下翻页。要退出 less 程序，应按 Q 键。 示例： less /var/log/shadowsocks.log sed sed 是一种流编辑器，它是文本处理工具，能够完美的配合正则表达式使用，功能不同凡响。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用 sed 命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。Sed 主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。 参考：http://man.linuxde.net/sed 示例： # 替换文本中的字符串sed 's/book/books/' file# -n 选项 和 p 命令 一起使用表示只打印那些发生替换的行sed -n 's/test/TEST/p' file# 直接编辑文件选项 -i ，会匹配 file 文件中每一行的第一个 book 替换为 bookssed -i 's/book/books/g' file# 使用后缀 /g 标记会替换每一行中的所有匹配sed 's/book/books/g' file# 删除空白行sed '/^$/d' file# 删除文件的第2行sed '2d' file# 删除文件的第2行到末尾所有行sed '2,$d' file# 删除文件最后一行sed '$d' file# 删除文件中所有开头是test的行sed '/^test/'d file vi vi 命令是 UNIX 操作系统和类 UNIX 操作系统中最通用的全屏幕纯文本编辑器。Linux 中的 vi 编辑器叫 vim，它是 vi 的增强版（vi Improved），与 vi 编辑器完全兼容，而且实现了很多增强功能。 参考：http://man.linuxde.net/vi 引申阅读：Vim 快速指南 grep grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 参考：http://man.linuxde.net/grep 示例： # 在多级目录中对文本递归搜索(程序员搜代码的最爱）:$ grep "class" . -R -n# 忽略匹配样式中的字符大小写$ echo "hello world" | grep -i "HELLO"# 匹配多个模式:$ grep -e "class" -e "vitural" file# 只在目录中所有的.php和.html文件中递归搜索字符"main()"$ grep "main()" . -r --include *.&#123;php,html&#125;# 在搜索结果中排除所有README文件$ grep "main()" . -r --exclude "README"# 在搜索结果中排除filelist文件列表里的文件$ grep "main()" . -r --exclude-from filelist 参考资料 Linux 命令大全]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 系统管理]]></title>
    <url>%2Fblog%2F2018%2F02%2F27%2Fos%2Flinux%2Fcli%2F06.Linux%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Linux 系统管理 关键词：lsb_release, reboot, exit, shutdown, date, mount, umount, ps, kill, systemctl, service, crontab Linux 系统管理要点 命令常见用法 lsb_release reboot exit shutdown date mount umount ps kill systemctl service crontab Linux 系统管理要点 查看 Linux 系统发行版本 使用 lsb_release（此命令适用于所有的 Linux 发行版本） 使用 cat /etc/redhat-release（此方法只适合 Redhat 系的 Linux） 查看 CPU 信息 - 使用 cat /proc/cpuinfo 重新启动 Linux 操作系统 - 使用 reboot 退出 shell，并返回给定值 - 使用 exit 关闭系统 - 使用 shutdown 查看或设置系统时间与日期 - 使用 date 挂载文件系统 - 使用 mount 取消挂载文件系统 - 使用 umount 查看系统当前进程状态 - 使用 ps 删除当前正在运行的进程 - 使用 kill 启动、停止、重启、关闭、显示系统服务（Centos7），使用 systemctl 启动、停止、重启、关闭、显示系统服务（Centos7 以前），使用 service 管理需要周期性执行的任务，使用 crontab 命令常见用法 lsb_release lsb_release 不是 bash 默认命令，如果要使用，需要先安装。 安装方法： 执行 yum provides lsb_release，查看支持 lsb_release 命令的包。 选择合适版本，执行类似这样的安装命令：yum install -y redhat-lsb-core-4.1-27.el7.centos.1.x86_64 reboot reboot 命令用来重新启动正在运行的 Linux 操作系统。 参考：http://man.linuxde.net/reboot 示例： reboot # 重开机。reboot -w # 做个重开机的模拟（只有纪录并不会真的重开机）。 exit exit 命令同于退出 shell，并返回给定值。在 shell 脚本中可以终止当前脚本执行。执行 exit 可使 shell 以指定的状态值退出。若不设置状态值参数，则 shell 以预设值退出。状态值 0 代表执行成功，其他值代表执行失败。 参考：http://man.linuxde.net/exit 示例： # 退出当前 shell[root@localhost ~]# exitlogout# 在脚本中，进入脚本所在目录，否则退出cd $(dirname $0) || exit 1# 在脚本中，判断参数数量，不匹配就打印使用方式，退出if [ "$#" -ne "2" ]; then echo "usage: $0 &lt;area&gt; &lt;hours&gt;" exit 2fi# 在脚本中，退出时删除临时文件trap "rm -f tmpfile; echo Bye." EXIT# 检查上一命令的退出码./mycommand.shEXCODE=$?if [ "$EXCODE" == "0" ]; then echo "O.K"fi shutdown shutdown 命令用来系统关机命令。shutdown 指令可以关闭所有程序，并依用户的需要，进行重新开机或关机的动作。 参考：http://man.linuxde.net/shutdown 示例： # 指定现在立即关机shutdown -h now# 指定 5 分钟后关机，同时送出警告信息给登入用户shutdown +5 "System will shutdown after 5 minutes" date date 命令是显示或设置系统时间与日期。 参考：http://man.linuxde.net/date 示例： # 格式化输出date +"%Y-%m-%d"2009-12-07# 输出昨天日期date -d "1 day ago" +"%Y-%m-%d"2012-11-19# 2 秒后输出date -d "2 second" +"%Y-%m-%d %H:%M.%S"2012-11-20 14:21.31# 传说中的 1234567890 秒date -d "1970-01-01 1234567890 seconds" +"%Y-%m-%d %H:%m:%S"2009-02-13 23:02:30# 普通转格式date -d "2009-12-12" +"%Y/%m/%d %H:%M.%S"2009/12/12 00:00.00# apache 格式转换date -d "Dec 5, 2009 12:00:37 AM" +"%Y-%m-%d %H:%M.%S"2009-12-05 00:00.37# 格式转换后时间游走date -d "Dec 5, 2009 12:00:37 AM 2 year ago" +"%Y-%m-%d %H:%M.%S"2007-12-05 00:00.37# 加减操作date +%Y%m%d # 显示前天年月日date -d "+1 day" +%Y%m%d # 显示前一天的日期date -d "-1 day" +%Y%m%d # 显示后一天的日期date -d "-1 month" +%Y%m%d # 显示上一月的日期date -d "+1 month" +%Y%m%d # 显示下一月的日期date -d "-1 year" +%Y%m%d # 显示前一年的日期date -d "+1 year" +%Y%m%d # 显示下一年的日期# 设定时间date -s # 设置当前时间，只有root权限才能设置，其他只能查看date -s 20120523 # 设置成20120523，这样会把具体时间设置成空00:00:00date -s 01:01:01 # 设置具体时间，不会对日期做更改date -s "01:01:01 2012-05-23" # 这样可以设置全部时间date -s "01:01:01 20120523" # 这样可以设置全部时间date -s "2012-05-23 01:01:01" # 这样可以设置全部时间date -s "20120523 01:01:01" # 这样可以设置全部时间# 有时需要检查一组命令花费的时间#!/bin/bashstart=$(date +%s)nmap man.linuxde.net &amp;&gt; /dev/nullend=$(date +%s)difference=$(( end - start ))echo $difference seconds. mount mount 命令用于挂载文件系统到指定的挂载点。此命令的最常用于挂载 cdrom，使我们可以访问 cdrom 中的数据，因为你将光盘插入 cdrom 中，Linux 并不会自动挂载，必须使用 Linux mount 命令来手动完成挂载。 参考：http://man.linuxde.net/mount &gt; https://blog.csdn.net/weishujie000/article/details/76531924 示例： # 将 /dev/hda1 挂在 /mnt 之下mount /dev/hda1 /mnt# 将 /dev/hda1 用唯读模式挂在 /mnt 之下mount -o ro /dev/hda1 /mnt# 将 /tmp/image.iso 这个光碟的 image 档使用 loop 模式挂在 /mnt/cdrom 之下# 用这种方法可以将一般网络上可以找到的 Linux ISO 在不烧录成光碟的情况下检视其内容mount -o loop /tmp/image.iso /mnt/cdrom umount umount 命令用于卸载已经挂载的文件系统。利用设备名或挂载点都能 umount 文件系统，不过最好还是通过挂载点卸载，以免使用绑定挂载（一个设备，多个挂载点）时产生混乱。 参考：http://man.linuxde.net/umount 示例： # 通过设备名卸载umount -v /dev/sda1/dev/sda1 umounted# 通过挂载点卸载umount -v /mnt/mymount//tmp/diskboot.img umounted ps ps 命令用于报告当前系统的进程状态。可以搭配 kill 指令随时中断、删除不必要的程序。ps 命令是最基本同时也是非常强大的进程查看命令，使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等，总之大部分信息都是可以通过执行该命令得到的。 参考：http://man.linuxde.net/ps 示例： # 按内存资源的使用量对进程进行排序ps aux | sort -rnk 4# 按 CPU 资源的使用量对进程进行排序ps aux | sort -nk 3 kill kill 命令用来删除执行中的程序或工作。kill 可将指定的信息送至程序。预设的信息为 SIGTERM(15),可将指定程序终止。若仍无法终止该程序，可使用 SIGKILL(9) 信息尝试强制删除程序。程序或工作的编号可利用 ps 指令或 job 指令查看。 参考：http://man.linuxde.net/kill 示例： # 列出所有信号名称 kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR213) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+439) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+1247) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-1451) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-1055) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-659) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX# 先用 ps 查找进程，然后用 kill 杀掉ps -ef | grep vimroot 3268 2884 0 16:21 pts/1 00:00:00 vim install.logroot 3370 2822 0 16:21 pts/0 00:00:00 grep vimkill 3268kill 3268-bash: kill: (3268) - 没有那个进程 systemctl systemctl 命令是系统服务管理器指令，它实际上将 service 和 chkconfig 这两个命令组合到一起。 参考：http://man.linuxde.net/systemctl 示例： # 1.启动 nfs 服务systemctl start nfs-server.service# 2.设置开机自启动systemctl enable nfs-server.service# 3.停止开机自启动systemctl disable nfs-server.service# 4.查看服务当前状态systemctl status nfs-server.service# 5.重新启动某服务systemctl restart nfs-server.service# 6.查看所有已启动的服务systemctl list -units --type=service# 7. 开启防火墙 22 端口iptables -I INPUT -p tcp --dport 22 -j accept# 8. 彻底关闭防火墙sudo systemctl status firewalld.servicesudo systemctl stop firewalld.servicesudo systemctl disable firewalld.service service service 命令是 Redhat Linux 兼容的发行版中用来控制系统服务的实用工具，它以启动、停止、重新启动和关闭系统服务，还可以显示所有系统服务的当前状态。 参考：http://man.linuxde.net/service 示例： service network status配置设备：lo eth0当前的活跃设备：lo eth0service network restart正在关闭接口 eth0： [ 确定 ]关闭环回接口： [ 确定 ]设置网络参数： [ 确定 ]弹出环回接口： [ 确定 ]弹出界面 eth0： [ 确定 ] crontab crontab 命令被用来提交和管理用户的需要周期性执行的任务，与 windows 下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动 crond 进程，crond 进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 参考：http://man.linuxde.net/crontab 示例： # 每 1 分钟执行一次 command* * * * * command# 每小时的第 3 和第 15 分钟执行3,15 * * * * command# 在上午 8 点到 11 点的第 3 和第 15 分钟执行3,15 8-11 * * * command# 每隔两天的上午 8 点到 11 点的第 3 和第 15 分钟执行3,15 8-11 */2 * * command# 每个星期一的上午 8 点到 11 点的第 3 和第 15 分钟执行3,15 8-11 * * 1 command# 每晚的 21:30 重启 smb30 21 * * * /etc/init.d/smb restart# 每月 1、10、22 日的 4 : 45 重启 smb45 4 1,10,22 * * /etc/init.d/smb restart# 每周六、周日的 1:10 重启 smb10 1 * * 6,0 /etc/init.d/smb restart# 每天 18 : 00 至 23 : 00 之间每隔 30 分钟重启 smb0,30 18-23 * * * /etc/init.d/smb restart# 每星期六的晚上 11:00 pm 重启 smb0 23 * * 6 /etc/init.d/smb restart# 每一小时重启 smb* */1 * * * /etc/init.d/smb restart# 晚上 11 点到早上 7 点之间，每隔一小时重启 smb* 23-7/1 * * * /etc/init.d/smb restart# 每月的 4 号与每周一到周三的 11 点重启 smb0 11 4 * mon-wed /etc/init.d/smb restart# 一月一号的 4 点重启 smb0 4 1 jan * /etc/init.d/smb restart# 每小时执行`/etc/cron.hourly`目录内的脚本01 * * * * root run-parts /etc/cron.hourly]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 软件管理]]></title>
    <url>%2Fblog%2F2018%2F02%2F26%2Fos%2Flinux%2Fcli%2F09.Linux%E8%BD%AF%E4%BB%B6%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Linux 软件管理 关键词：rpm, yum, apt-get rpm yum yum 源 apt-get 参考资料 rpm rpm 命令是 RPM 软件包的管理工具。rpm 原本是 Red Hat Linux 发行版专门用来管理 Linux 各项套件的程序，由于它遵循 GPL 规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM 套件管理方式的出现，让 Linux 易于安装，升级，间接提升了 Linux 的适用度。 参考：http://man.linuxde.net/rpm 示例： （1）安装 rpm 包 rpm -ivh xxx.rpm （2）安装.src.rpm 软件包 这类软件包是包含了源代码的 rpm 包，在安装时需要进行编译 rpm -i xxx.src.rpmcd /usr/src/redhat/SPECSrpmbuild -bp xxx.specs #一个和你的软件包同名的specs文件cd /usr/src/redhat/BUILD/xxx/ #一个和你的软件包同名的目录./configure #这一步和编译普通的源码软件一样，可以加上参数makemake install （3）卸载 rpm 软件包 使用命令 rpm -e 包名，包名可以包含版本号等信息，但是不可以有后缀.rpm，比如卸载软件包 proftpd-1.2.8-1，可以使用下列格式： rpm -e proftpd-1.2.8-1rpm -e proftpd-1.2.8rpm -e proftpd-rpm -e proftpd 不可以是下列格式： rpm -e proftpd-1.2.8-1.i386.rpmrpm -e proftpd-1.2.8-1.i386rpm -e proftpd-1.2rpm -e proftpd-1 有时会出现一些错误或者警告： ... is needed by ... 这说明这个软件被其他软件需要，不能随便卸载，可以用 rpm -e --nodeps 强制卸载 （4）查看与 rpm 包相关的文件和其他信息 rpm -qa # 列出所有安装过的包 yum yum 命令是在 Fedora 和 RedHat 以及 SUSE 中基于 rpm 的软件包管理器，它可以使系统管理人员交互和自动化地更细与管理 RPM 软件包，能够从指定的服务器自动下载 RPM 包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。 参考：http://man.linuxde.net/yum 示例： 部分常用的命令包括： 自动搜索最快镜像插件：yum install yum-fastestmirror 安装 yum 图形窗口插件：yum install yumex 查看可能批量安装的列表：yum grouplist 安装 yum install #全部安装yum install package1 #安装指定的安装包package1yum groupinsall group1 #安装程序组group1 更新和升级 yum update #全部更新yum update package1 #更新指定程序包package1yum check-update #检查可更新的程序yum upgrade package1 #升级指定程序包package1yum groupupdate group1 #升级程序组group1 查找和显示 yum info package1 #显示安装包信息package1yum list #显示所有已经安装和可以安装的程序包yum list package1 #显示指定程序包安装情况package1yum groupinfo group1 #显示程序组group1信息yum search string 根据关键字string查找安装包yum search &lt;keyword&gt; #查找软件包 删除程序 yum remove &lt;package_name&gt; #删除程序包package_nameyum groupremove group1 #删除程序组group1yum deplist package1 #查看程序package1依赖情况 清除缓存 yum clean packages #清除缓存目录下的软件包yum clean headers #清除缓存目录下的 headersyum clean oldheaders #清除缓存目录下旧的 headers yum 源 yum 的默认源是国外的，下载速度比较慢，所以最好替换为一个国内的 yum 源。 推荐 yum 国内源 源地址 http://mirrors.163.com/ Centos6：http://mirrors.aliyun.com/repo/Centos-6.repoCentos7：http://mirrors.aliyun.com/repo/Centos-7.repo http://mirrors.aliyun.com/ Centos6：http://mirrors.163.com/.help/CentOS6-Base-163.repoCentos7：http://mirrors.163.com/.help/CentOS7-Base-163.repo 注意：Cento5 已废弃，只能使用 http://vault.centos.org/ 替换，但由于是国外镜像，速度较慢。 替换方法，以 aliyun CentOS7 为例： cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bakwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum clean allyum makecache apt-get apt-get 命令是 Debian Linux 发行版中的 APT 软件包管理工具。所有基于 Debian 的发行都使用这个包管理系统。deb 包可以把一个应用的文件包在一起，大体就如同 Windows 上的安装文件。 参考：http://man.linuxde.net/apt-get 示例： 使用 apt-get 命令的第一步就是引入必需的软件库，Debian 的软件库也就是所有 Debian 软件包的集合，它们存在互联网上的一些公共站点上。把它们的地址加入，apt-get 就能搜索到我们想要的软件。/etc/apt/sources.list 是存放这些地址列表的配置文件，其格式如下： deb [web 或 ftp 地址][发行版名字] [main/contrib/non-free] 我们常用的 Ubuntu 就是一个基于 Debian 的发行，我们使用 apt-get 命令获取这个列表，以下是我整理的常用命令： 在修改 /etc/apt/sources.list 或者 /etc/apt/preferences 之后运行该命令。 # 更新 apt-getapt-get update# 安装一个软件包apt-get install packagename# 卸载一个已安装的软件包（保留配置文件）apt-get remove packagename# 卸载一个已安装的软件包（删除配置文件）apt-get –purge remove packagename# 如果需要空间的话，可以让这个命令来删除你已经删掉的软件apt-get autoclean apt# 把安装的软件的备份也删除，不过这样不会影响软件的使用的apt-get clean# 更新所有已安装的软件包apt-get upgrade# 将系统升级到新版本apt-get dist-upgrade 参考资料 http://man.linuxde.net/rpm http://man.linuxde.net/yum http://man.linuxde.net/apt-get http://www.runoob.com/linux/linux-yum.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat 快速入门]]></title>
    <url>%2Fblog%2F2018%2F01%2F08%2Fjava%2Fjavaweb%2Ftools%2Ftomcat%2F</url>
    <content type="text"><![CDATA[Tomcat 快速入门 版本说明 本文使用 Tomcat 版本为 Tomcat 8.5.24。 Tomcat 8.5 要求 JDK 版本为 1.7 以上。 简介 Tomcat 是什么 Tomcat 重要目录 web 工程发布目录结构 QuickStart 安装 配置 启动 Tomcat 工作原理 Tomcat 主要组件 Tomcat 生命周期 Connector Comet 异步 Servlet 资料 官方 第三方 简介 Tomcat 是什么 Tomcat 是由 Apache 开发的一个 Servlet 容器，实现了对 Servlet 和 JSP 的支持，并提供了作为 Web 服务器的一些特有功能，如 Tomcat 管理和控制平台、安全域管理和 Tomcat 阀等。 由于 Tomcat 本身也内含了一个 HTTP 服务器，它也可以被视作一个单独的 Web 服务器。但是，不能将 Tomcat 和 Apache HTTP 服务器混淆，Apache HTTP 服务器是一个用 C 语言实现的 HTTP Web 服务器；这两个 HTTP web server 不是捆绑在一起的。Tomcat 包含了一个配置管理工具，也可以通过编辑 XML 格式的配置文件来进行配置。 Tomcat 重要目录 /bin - Tomcat 脚本存放目录（如启动、关闭脚本）。 *.sh 文件用于 Unix 系统； *.bat 文件用于 Windows 系统。 /conf - Tomcat 配置文件目录。 /logs - Tomcat 默认日志目录。 /webapps - webapp 运行的目录。 web 工程发布目录结构 一般 web 项目路径结构 |-- webapp # 站点根目录 |-- META-INF # META-INF 目录 | `-- MANIFEST.MF # 配置清单文件 |-- WEB-INF # WEB-INF 目录 | |-- classes # class文件目录 | | |-- *.class # 程序需要的 class 文件 | | `-- *.xml # 程序需要的 xml 文件 | |-- lib # 库文件夹 | | `-- *.jar # 程序需要的 jar 包 | `-- web.xml # Web应用程序的部署描述文件 |-- &lt;userdir&gt; # 自定义的目录 |-- &lt;userfiles&gt; # 自定义的资源文件 webapp：工程发布文件夹。其实每个 war 包都可以视为 webapp 的压缩包。 META-INF：META-INF 目录用于存放工程自身相关的一些信息，元文件信息，通常由开发工具，环境自动生成。 WEB-INF：Java web 应用的安全目录。所谓安全就是客户端无法访问，只有服务端可以访问的目录。 /WEB-INF/classes：存放程序所需要的所有 Java class 文件。 /WEB-INF/lib：存放程序所需要的所有 jar 文件。 /WEB-INF/web.xml：web 应用的部署配置文件。它是工程中最重要的配置文件，它描述了 servlet 和组成应用的其它组件，以及应用初始化参数、安全管理约束等。 QuickStart 安装 前提条件 Tomcat 8.5 要求 JDK 版本为 1.7 以上。 进入 Tomcat 官方下载地址 选择合适版本下载，并解压到本地。 Windows 添加环境变量 CATALINA_HOME ，值为 Tomcat 的安装路径。 进入安装目录下的 bin 目录，运行 startup.bat 文件，启动 Tomcat Linux / Unix 下面的示例以 8.5.24 版本为例，包含了下载、解压、启动操作。 # 下载解压到本地wget http://mirrors.hust.edu.cn/apache/tomcat/tomcat-8/v8.5.24/bin/apache-tomcat-8.5.24.tar.gztar -zxf apache-tomcat-8.5.24.tar.gz# 启动 Tomcat./apache-tomcat-8.5.24/bin/startup.sh 启动后，访问 http://localhost:8080 ，可以看到 Tomcat 安装成功的测试页面。 配置 本节将列举一些重要、常见的配置项。详细的 Tomcat8 配置可以参考 Tomcat 8 配置官方参考文档 。 Server Server 元素表示整个 Catalina servlet 容器。 因此，它必须是 conf/server.xml 配置文件中的根元素。它的属性代表了整个 servlet 容器的特性。 属性表 属性 描述 备注 className 这个类必须实现 org.apache.catalina.Server 接口。 默认 org.apache.catalina.core.StandardServer address 服务器等待关机命令的 TCP / IP 地址。如果没有指定地址，则使用 localhost。 port 服务器等待关机命令的 TCP / IP 端口号。设置为-1 以禁用关闭端口。 shutdown 必须通过 TCP / IP 连接接收到指定端口号的命令字符串，以关闭 Tomcat。 Service Service 元素表示一个或多个连接器组件的组合，这些组件共享一个用于处理传入请求的引擎组件。Server 中可以有多个 Service。 属性表 属性 描述 备注 className 这个类必须实现org.apache.catalina.Service接口。 默认 org.apache.catalina.core.StandardService name 此服务的显示名称，如果您使用标准 Catalina 组件，将包含在日志消息中。与特定服务器关联的每个服务的名称必须是唯一的。 实例 - conf/server.xml 配置文件示例 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Server port="8080" shutdown="SHUTDOWN"&gt; &lt;Service name="xxx"&gt; ... &lt;/Service&gt;&lt;/Server&gt; Executor Executor 表示可以在 Tomcat 中的组件之间共享的线程池。 属性表 属性 描述 备注 className 这个类必须实现org.apache.catalina.Executor接口。 默认 org.apache.catalina.core.StandardThreadExecutor name 线程池名称。 要求唯一, 供 Connector 元素的 executor 属性使用 namePrefix 线程名称前缀。 maxThreads 最大活跃线程数。 默认 200 minSpareThreads 最小活跃线程数。 默认 25 maxIdleTime 当前活跃线程大于 minSpareThreads 时,空闲线程关闭的等待最大时间。 默认 60000ms maxQueueSize 线程池满情况下的请求排队大小。 默认 Integer.MAX_VALUE &lt;Service name="xxx"&gt; &lt;Executor name="tomcatThreadPool" namePrefix="catalina-exec-" maxThreads="300" minSpareThreads="25"/&gt;&lt;/Service&gt; Connector Connector 代表连接组件。Tomcat 支持三种协议：HTTP/1.1、HTTP/2.0、AJP。 属性表 属性 说明 备注 asyncTimeout Servlet3.0 规范中的异步请求超时 默认 30s port 请求连接的 TCP Port 设置为 0,则会随机选取一个未占用的端口号 protocol 协议. 一般情况下设置为 HTTP/1.1,这种情况下连接模型会在 NIO 和 APR/native 中自动根据配置选择 URIEncoding 对 URI 的编码方式. 如果设置系统变量 org.apache.catalina.STRICT_SERVLET_COMPLIANCE 为 true,使用 ISO-8859-1 编码;如果未设置此系统变量且未设置此属性, 使用 UTF-8 编码 useBodyEncodingForURI 是否采用指定的 contentType 而不是 URIEncoding 来编码 URI 中的请求参数 以下属性在标准的 Connector(NIO, NIO2 和 APR/native)中有效: 属性 说明 备注 acceptCount 当最大请求连接 maxConnections 满时的最大排队大小 默认 100,注意此属性和 Executor 中属性 maxQueueSize 的区别.这个指的是请求连接满时的堆栈大小,Executor 的 maxQueueSize 指的是处理线程满时的堆栈大小 connectionTimeout 请求连接超时 默认 60000ms executor 指定配置的线程池名称 keepAliveTimeout keeAlive 超时时间 默认值为 connectionTimeout 配置值.-1 表示不超时 maxConnections 最大连接数 连接满时后续连接放入最大为 acceptCount 的队列中. 对 NIO 和 NIO2 连接,默认值为 10000;对 APR/native,默认值为 8192 maxThreads 如果指定了 Executor, 此属性忽略;否则为 Connector 创建的内部线程池最大值 默认 200 minSpareThreads 如果指定了 Executor, 此属性忽略;否则为 Connector 创建线程池的最小活跃线程数 默认 10 processorCache 协议处理器缓存 Processor 对象的大小 -1 表示不限制.当不使用 servlet3.0 的异步处理情况下: 如果配置 Executor,配置为 Executor 的 maxThreads;否则配置为 Connnector 的 maxThreads. 如果使用 Serlvet3.0 异步处理, 取 maxThreads 和 maxConnections 的最大值 Context Context 元素表示一个 Web 应用程序，它在特定的虚拟主机中运行。每个 Web 应用程序都基于 Web 应用程序存档（WAR）文件，或者包含相应的解包内容的相应目录，如 Servlet 规范中所述。 属性表 属性 说明 备注 altDDName web.xml 部署描述符路径 默认 /WEB-INF/web.xml docBase Context 的 Root 路径 和 Host 的 appBase 相结合, 可确定 web 应用的实际目录 failCtxIfServletStartFails 同 Host 中的 failCtxIfServletStartFails, 只对当前 Context 有效 默认为 false logEffectiveWebXml 是否日志打印 web.xml 内容(web.xml 由默认的 web.xml 和应用中的 web.xml 组成) 默认为 false path web 应用的 context path 如果为根路径,则配置为空字符串(&quot;&quot;), 不能不配置 privileged 是否使用 Tomcat 提供的 manager servlet reloadable /WEB-INF/classes/ 和/WEB-INF/lib/ 目录中 class 文件发生变化是否自动重新加载 默认为 false swallowOutput true 情况下, System.out 和 System.err 输出将被定向到 web 应用日志中 默认为 false Engine Engine 元素表示与特定的 Catalina 服务相关联的整个请求处理机器。它接收并处理来自一个或多个连接器的所有请求，并将完成的响应返回给连接器，以便最终传输回客户端。 属性表 属性 描述 备注 defaultHost 默认主机名，用于标识将处理指向此服务器上主机名称但未在此配置文件中配置的请求的主机。 这个名字必须匹配其中一个嵌套的主机元素的名字属性。 name 此引擎的逻辑名称，用于日志和错误消息。 在同一服务器中使用多个服务元素时，每个引擎必须分配一个唯一的名称。 Host Host 元素表示一个虚拟主机，它是一个服务器的网络名称（如“www.mycompany.com”）与运行 Tomcat 的特定服务器的关联。 属性表 属性 说明 备注 name 名称 用于日志输出 appBase 虚拟主机对应的应用基础路径 可以是个绝对路径, 或${CATALINA_BASE}相对路径 xmlBase 虚拟主机 XML 基础路径,里面应该有 Context xml 配置文件 可以是个绝对路径, 或${CATALINA_BASE}相对路径 createDirs 当 appBase 和 xmlBase 不存在时,是否创建目录 默认为 true autoDeploy 是否周期性的检查 appBase 和 xmlBase 并 deploy web 应用和 context 描述符 默认为 true deployIgnore 忽略 deploy 的正则 deployOnStartup Tomcat 启动时是否自动 deploy 默认为 true failCtxIfServletStartFails 配置为 true 情况下,任何 load-on-startup &gt;=0 的 servlet 启动失败,则其对应的 Contxt 也启动失败 默认为 false Cluster 由于在实际开发中，我从未用过 Tomcat 集群配置，所以没研究。 启动 部署方式 这种方式要求本地必须安装 Tomcat 。 将打包好的 war 包放在 Tomcat 安装目录下的 webapps 目录下，然后在 bin 目录下执行 startup.bat 或 startup.sh ，Tomcat 会自动解压 webapps 目录下的 war 包。 成功后，可以访问 http://localhost:8080/xxx （xxx 是 war 包文件名）。 注意 以上步骤是最简单的示例。步骤中的 war 包解压路径、启动端口以及一些更多的功能都可以修改配置文件来定制 （主要是 server.xml 或 context.xml 文件）。 嵌入式 API 方式 在 pom.xml 中添加依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt; &lt;version&gt;8.5.24&lt;/version&gt;&lt;/dependency&gt; 添加 SimpleEmbedTomcatServer.java 文件，内容如下： import java.util.Optional;import org.apache.catalina.startup.Tomcat;public class SimpleTomcatServer &#123; private static final int PORT = 8080; private static final String CONTEXT_PATH = "/javatool-server"; public static void main(String[] args) throws Exception &#123; // 设定 profile Optional&lt;String&gt; profile = Optional.ofNullable(System.getProperty("spring.profiles.active")); System.setProperty("spring.profiles.active", profile.orElse("develop")); Tomcat tomcat = new Tomcat(); tomcat.setPort(PORT); tomcat.getHost().setAppBase("."); tomcat.addWebapp(CONTEXT_PATH, getAbsolutePath() + "src/main/webapp"); tomcat.start(); tomcat.getServer().await(); &#125; private static String getAbsolutePath() &#123; String path = null; String folderPath = SimpleEmbedTomcatServer.class.getProtectionDomain().getCodeSource().getLocation().getPath() .substring(1); if (folderPath.indexOf("target") &gt; 0) &#123; path = folderPath.substring(0, folderPath.indexOf("target")); &#125; return path; &#125;&#125; 成功后，可以访问 http://localhost:8080/javatool-server 。 说明 本示例是使用 org.apache.tomcat.embed 启动嵌入式 Tomcat 的最简示例。 这个示例中使用的是 Tomcat 默认的配置，但通常，我们需要对 Tomcat 配置进行一些定制和调优。为了加载配置文件，启动类就要稍微再复杂一些。这里不想再贴代码，有兴趣的同学可以参考： 示例项目 使用 maven 插件启动（不推荐） 不推荐理由：这种方式启动 maven 虽然最简单，但是有一个很大的问题是，真的很久很久没发布新版本了（最新版本发布时间：2013-11-11）。且貌似只能找到 Tomcat6 、Tomcat7 插件。 使用方法 在 pom.xml 中引入插件 &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;8080&lt;/port&gt; &lt;path&gt;/$&#123;project.artifactId&#125;&lt;/path&gt; &lt;uriEncoding&gt;UTF-8&lt;/uriEncoding&gt; &lt;/configuration&gt;&lt;/plugin&gt; 运行 mvn tomcat7:run 命令，启动 Tomcat。 成功后，可以访问 http://localhost:8080/xxx （xxx 是 ${project.artifactId} 指定的项目名）。 IDE 插件 常见 Java IDE 一般都有对 Tomcat 的支持。 以 Intellij IDEA 为例，提供了 Tomcat and TomEE Integration 插件（一般默认会安装）。 使用步骤 点击 Run/Debug Configurations &gt; New Tomcat Server &gt; local ，打开 Tomcat 配置页面。 点击 Confiure… 按钮，设置 Tomcat 安装路径。 点击 Deployment 标签页，设置要启动的应用。 设置启动应用的端口、JVM 参数、启动浏览器等。 成功后，可以访问 http://localhost:8080/（当然，你也可以在 url 中设置上下文名称）。 说明 个人认为这个插件不如 Eclipse 的 Tomcat 插件好用，Eclipse 的 Tomcat 插件支持对 Tomcat xml 配置文件进行配置。而这里，你只能自己去 Tomcat 安装路径下修改配置文件。 文中的嵌入式启动示例可以参考我的示例项目 Tomcat 工作原理 Tomcat 主要组件 Server - 指的就是整个 Tomcat 服 务器，包含多组服务，负责管理和 启动各个 Service，同时监听 8005 端口发过来的 shutdown 命令，用 于关闭整个容器。 Service - Tomcat 封装的、对外提 供完整的、基于组件的 web 服务， 包含 Connectors、Container 两个核心组件，以及多个功能组件，各 个 Service 之间是独立的，但是共享 同一 JVM 的资源。 Connector - Tomcat 与外部世界的连接器，监听固定端口接收外部请求，传递给 Container，并 将 Container 处理的结果返回给外部； Container - Catalina，Servlet 容器，内部有多层容器组成，用于管理 Servlet 生命周期，调用 servlet 相关方法。 Loader - 封装了 Java ClassLoader，用于 Container 加载类文件； Realm - Tomcat 中为 web 应用程序提供访问认证和角色管理的机制。 JMX - Java SE 中定义技术规范，是一个为应用程序、设备、系统等植入管理功能的框架，通过 JMX 可以远程监控 Tomcat 的运行状态。 Jasper - Tomcat 的 Jsp 解析引擎，用于将 Jsp 转换成 Java 文件，并编译成 class 文件。 Session - 负责管理和创建 session，以及 Session 的持久化(可自定义)，支持 session 的集 群。 Pipeline - 在容器中充当管道的作用，管道中可以设置各种 valve(阀门)，请求和响应在经由管 道中各个阀门处理，提供了一种灵活可配置的处理请求和响应的机制。 Naming - 命名服务，JNDI， Java 命名和目录接口，是一组在 Java 应用中访问命名和目录服务的 API。命名服务将名称和对象联系起来，使得我们可以用名称访问对象，目录服务也是一种命名 服务，对象不但有名称，还有属性。Tomcat 中可以使用 JNDI 定义数据源、配置信息，用于开发 与部署的分离。 Container 组件 Engine - Servlet 的顶层容器，包含一 个或多个 Host 子容器； Host - 虚拟主机，负责 web 应用的部 署和 Context 的创建； Context - Web 应用上下文，包含多个 Wrapper，负责 web 配置的解析、管 理所有的 Web 资源； Wrapper - 最底层的容器，是对 Servlet 的封装，负责 Servlet 实例的创 建、执行和销毁。 Tomcat 生命周期 Tomcat 生命周期管理 Tomcat 为了方便管理组件和容器的生命周期，定义了从创建、启动、到停止、销毁共 12 中状态，tomcat 生命周期管理了内部状态变化的规则控制，组件和容器只需实现相应的生命周期 方法即可完成各生命周期内的操作(initInternal、startInternal、stopInternal、 destroyInternal)； 比如执行初始化操作时，会判断当前状态是否 New，如果不是则抛出生命周期异常；是的 话则设置当前状态为 Initializing，并执行 initInternal 方法，由子类实现，方法执行成功则设置当 前状态为 Initialized，执行失败则设置为 Failed 状态； Tomcat 的生命周期管理引入了事件机制，在组件或容器的生命周期状态发生变化时会通 知事件监听器，监听器通过判断事件的类型来进行相应的操作。 事件监听器的添加可以在 server.xml 文件中进行配置; Tomcat 各类容器的配置过程就是通过添加 listener 的方式来进行的，从而达到配置逻辑与 容器的解耦。如 EngineConfig、HostConfig、ContextConfig。 EngineConfig - 主要打印启动和停止日志 HostConfig - 主要处理部署应用，解析应用 META-INF/context.xml 并创建应用的 Context。 ContextConfig - 主要解析并合并 web.xml，扫描应用的各类 web 资源 (filter、servlet、listener)。 Tomcat 的启动过程 启动从 Tomcat 提供的 start.sh 脚本开始，shell 脚本会调用 Bootstrap 的 main 方法，实际 调用了 Catalina 相应的 load、start 方法。 load 方法会通过 Digester 进行 config/server.xml 的解析，在解析的过程中会根据 xml 中的关系 和配置信息来创建容器，并设置相关的属性。接着 Catalina 会调用 StandardServer 的 init 和 start 方法进行容器的初始化和启动。 按照 xml 的配置关系，server 的子元素是 service，service 的子元素是顶层容器 Engine，每层容器有持有自己的子容器，而这些元素都实现了生命周期管理 的各个方法，因此就很容易的完成整个容器的启动、关闭等生命周期的管理。 StandardServer 完成 init 和 start 方法调用后，会一直监听来自 8005 端口(可配置)，如果接收 到 shutdown 命令，则会退出循环监听，执行后续的 stop 和 destroy 方法，完成 Tomcat 容器的 关闭。同时也会调用 JVM 的 Runtime.getRuntime()﴿.addShutdownHook 方法，在虚拟机意外退 出的时候来关闭容器。 所有容器都是继承自 ContainerBase，基类中封装了容器中的重复工作，负责启动容器相关的组 件 Loader、Logger、Manager、Cluster、Pipeline，启动子容器(线程池并发启动子容器，通过 线程池 submit 多个线程，调用后返回 Future 对象，线程内部启动子容器，接着调用 Future 对象 的 get 方法来等待执行结果)。 List&lt;Future&lt;Void&gt;&gt; results = new ArrayList&lt;Future&lt;Void&gt;&gt;();for (int i = 0; i &lt; children.length; i++) &#123; results.add(startStopExecutor.submit(new StartChild(children[i])));&#125;boolean fail = false;for (Future&lt;Void&gt; result ： results) &#123; try &#123; result.get(); &#125; catch (Exception e) &#123; log.error(sm.getString("containerBase.threadedStartFailed")， e); fail = true; &#125;&#125; Web 应用的部署方式 注：catalina.home：安装目录;catalina.base：工作目录;默认值 user.dir Server.xml 配置 Host 元素，指定 appBase 属性，默认$catalina.base/webapps/ Server.xml 配置 Context 元素，指定 docBase，元素，指定 web 应用的路径 自定义配置：在$catalina.base/EngineName/HostName/XXX.xml 配置 Context 元素 HostConfig 监听了 StandardHost 容器的事件，在 start 方法中解析上述配置文件： 扫描 appbase 路径下的所有文件夹和 war 包，解析各个应用的 META-INF/context.xml，并 创建 StandardContext，并将 Context 加入到 Host 的子容器中。 解析$catalina.base/EngineName/HostName/下的所有 Context 配置，找到相应 web 应 用的位置，解析各个应用的 META-INF/context.xml，并创建 StandardContext，并将 Context 加入到 Host 的子容器中。 注： HostConfig 并没有实际解析 Context.xml，而是在 ContextConfig 中进行的。 HostConfig 中会定期检查 watched 资源文件(context.xml 配置文件) ContextConfig 解析 context.xml 顺序： 先解析全局的配置 config/context.xml 然后解析 Host 的默认配置 EngineName/HostName/context.xml.default 最后解析应用的 META-INF/context.xml ContextConfig 解析 web.xml 顺序： 先解析全局的配置 config/web.xml 然后解析 Host 的默认配置 EngineName/HostName/web.xml.default 接着解析应用的 MEB-INF/web.xml 扫描应用 WEB-INF/lib/下的 jar 文件，解析其中的 META-INF/web-fragment.xml 最后合并 xml 封装成 WebXml，并设置 Context 注： 扫描 web 应用和 jar 中的注解(Filter、Listener、Servlet)就是上述步骤中进行的。 容器的定期执行：backgroundProcess，由 ContainerBase 来实现的，并且只有在顶层容器 中才会开启线程。(backgroundProcessorDelay=10 标志位来控制) 请求处理过程 根据 server.xml 配置的指定的 connector 以及端口监听 http、或者 ajp 请求 请求到来时建立连接,解析请求参数,创建 Request 和 Response 对象,调用顶层容器 pipeline 的 invoke 方法 容器之间层层调用,最终调用业务 servlet 的 service 方法 Connector 将 response 流中的数据写到 socket 中 Pipeline 与 Valve Pipeline 可以理解为现实中的管道,Valve 为管道中的阀门,Request 和 Response 对象在管道中 经过各个阀门的处理和控制。 每个容器的管道中都有一个必不可少的 basic valve,其他的都是可选的,basic valve 在管道中最 后调用,同时负责调用子容器的第一个 valve。 Valve 中主要的三个方法:setNext、getNext、invoke;valve 之间的关系是单向链式结构,本身 invoke 方法中会调用下一个 valve 的 invoke 方法。 各层容器对应的 basic valve 分别是 StandardEngineValve、StandardHostValve、 StandardContextValve、StandardWrapperValve。 Connector 阻塞 IO 非阻塞 IO IO 多路复用 阻塞与非阻塞的区别在于进行读操作和写操作的系统调用时，如果此时内核态没有数据可读或者没有缓冲空间可写时，是否阻塞。 IO 多路复用的好处在于可同时监听多个 socket 的可读和可写事件，这样就能使得应用可以同时监听多个 socket，释放了应用线程资源。 Tomcat 各类 Connector 对比 JIO：用 java.io 编写的 TCP 模块，阻塞 IO NIO：用 java.nio 编写的 TCP 模块，非阻塞 IO，（IO 多路复用） APR：全称 Apache Portable Runtime，使用 JNI 的方式来进行读取文件以及进行网络传输 Apache Portable Runtime 是一个高度可移植的库，它是 Apache HTTP Server 2.x 的核心。 APR 具有许多用途，包括访问高级 IO 功能（如 sendfile，epoll 和 OpenSSL），操作系统级功能（随机数生成，系统状态等）和本地进程处理（共享内存，NT 管道和 Unix 套接字）。 表格中字段含义说明： Support Polling - 是否支持基于 IO 多路复用的 socket 事件轮询 Polling Size - 轮询的最大连接数 Wait for next Request - 在等待下一个请求时，处理线程是否释放，BIO 是没有释放的，所以在 keep-alive=true 的情况下处理的并发连接数有限 Read Request Headers - 由于 request header 数据较少，可以由容器提前解析完毕，不需要阻塞 Read Request Body - 读取 request body 的数据是应用业务逻辑的事情，同时 Servlet 的限制，是需要阻塞读取的 Write Response - 跟读取 request body 的逻辑类似，同样需要阻塞写 NIO 处理相关类 Poller 线程从 EventQueue 获取 PollerEvent，并执行 PollerEvent 的 run 方法，调用 Selector 的 select 方法，如果有可读的 Socket 则创建 Http11NioProcessor，放入到线程池中执行； CoyoteAdapter 是 Connector 到 Container 的适配器，Http11NioProcessor 调用其提供的 service 方法，内部创建 Request 和 Response 对象，并调用最顶层容器的 Pipeline 中的第一个 Valve 的 invoke 方法 Mapper 主要处理 http url 到 servlet 的映射规则的解析，对外提供 map 方法 Comet Comet 是一种用于 web 的推送技术，能使服务器实时地将更新的信息传送到客户端，而无须客户端发出请求 在 WebSocket 出来之前，如果不适用 comet，只能通过浏览器端轮询 Server 来模拟实现服务器端推送。 Comet 支持 servlet 异步处理 IO，当连接上数据可读时触发事件，并异步写数据(阻塞) Tomcat 要实现 Comet，只需继承 HttpServlet 同时，实现 CometProcessor 接口 Begin：新的请求连接接入调用，可进行与 Request 和 Response 相关的对象初始化操作，并保存 response 对象，用于后续写入数据 Read：请求连接有数据可读时调用 End：当数据可用时，如果读取到文件结束或者 response 被关闭时则被调用 Error：在连接上发生异常时调用，数据读取异常、连接断开、处理异常、socket 超时 Note： Read：在 post 请求有数据，但在 begin 事件中没有处理，则会调用 read，如果 read 没有读取数据，在会触发 Error 回调，关闭 socket End：当 socket 超时，并且 response 被关闭时也会调用；server 被关闭时调用 Error：除了 socket 超时不会关闭 socket，其他都会关闭 socket End 和 Error 时间触发时应关闭当前 comet 会话，即调用 CometEvent 的 close 方法 Note：在事件触发时要做好线程安全的操作 异步 Servlet 传统流程： 首先，Servlet 接收到请求之后，request 数据解析； 接着，调用业务接口的某些方法，以完成业务处理； 最后，根据处理的结果提交响应，Servlet 线程结束 异步处理流程： 客户端发送一个请求 Servlet 容器分配一个线程来处理容器中的一个 servlet servlet 调用 request.startAsync()，保存 AsyncContext, 然后返回 任何方式存在的容器线程都将退出，但是 response 仍然保持开放 业务线程使用保存的 AsyncContext 来完成响应（线程池） 客户端收到响应 Servlet 线程将请求转交给一个异步线程来执行业务处理，线程本身返回至容器，此时 Servlet 还没有生成响应数据，异步线程处理完业务以后，可以直接生成响应数据（异步线程拥有 ServletRequest 和 ServletResponse 对象的引用） 为什么 web 应用中支持异步？ 推出异步，主要是针对那些比较耗时的请求：比如一次缓慢的数据库查询，一次外部 REST API 调用, 或者是其他一些 I/O 密集型操作。这种耗时的请求会很快的耗光 Servlet 容器的线程池，继而影响可扩展性。 Note：从客户端的角度来看，request 仍然像任何其他的 HTTP 的 request-response 交互一样，只是耗费了更长的时间而已 异步事件监听 onStartAsync：Request 调用 startAsync 方法时触发 onComplete：syncContext 调用 complete 方法时触发 onError：处理请求的过程出现异常时触发 onTimeout：socket 超时触发 Note : onError/ onTimeout 触发后，会紧接着回调 onComplete onComplete 执行后，就不可再操作 request 和 response 资料 官方 Tomcat 官方网站 Tomcat Wiki Tomee 官方网站 第三方 Creating a Web App with Bootstrap and Tomcat Embedded Tomcat 组成与工作原理 Tomcat 工作原理 Tomcat 设计模式分析]]></content>
      <categories>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何学习一门编程语言]]></title>
    <url>%2Fblog%2F2018%2F01%2F05%2Fprogramming%2Fprogramming-guide%2F</url>
    <content type="text"><![CDATA[如何学习一门编程语言 📓 本文已归档到：「blog」 前言 学习编程语言的步骤 基本语法 数组、枚举、集合 常用类 语言特性 代码组织、模块加载、库管理 容错处理 输入输出和文件处理 回调机制 序列化和反序列化 进阶特性 库和框架 小结 前言 很多人喜欢争论什么什么编程语言好，我认为这个话题如果不限定应用范围，就毫无意义。 每种编程语言必然有其优点和缺点，这也决定了它有适合的应用场景和不适合的应用场景。现代软件行业，想一门编程语言包打天下是不现实的。这中现状也造成了一种现象，一个程序员往往要掌握多种编程语言。 学习任何一门编程语言，都会面临的第一个问题都是：如何学习 XX 语言？ 我不想说什么多看、多学、多写、多练之类的废话。世上事有难易乎？无他，唯手熟尔。谁不知道熟能生巧的道理？ 我觉得有必要谈谈的是：如何由浅入深的学习一门编程语言？学习所有编程语言有没有一个相对统一的学习方法？ 曾几何时，当我还是一名小菜鸟时，总是叹服那些大神掌握多门编程语言。后来，在多年编程工作和学习中，我陆陆续续也接触过不少编程语言：C、C++、Java、C#、Javascript、shell 等等。每次学习一门新的编程语言，掌握程度或深或浅，但是学习的曲线却大抵相似。 下面，我按照个人的学习经验总结一下，学习编程语言的基本步骤。 学习编程语言的步骤 基本语法 首先当然是了解语言的最基本语法。 控制台输出，如 C 的 printf，Java 的 System.out.println 等。 普通程序员的第一行代码一般都是输出 “Hello World” 吧。 基本数据类型 不同编程语言的基本数据类型不同。基本数据类型是的申请内存空间变得方便、规范化。 变量 不同编程语言的声明变量方式有很大不同。有的如 Java 、C++ 需要明确指定变量数据类型，这种叫强类型定义语言。有的语言（主要是脚本语言），如 Javascript、Shell 等，不需要明确指定数据类型，这种叫若类型定义语言。 还需要注意的一点是变量的作用域范围和生命周期。不同语言变量的作用域范围和生命周期不一定一样，这个需要在代码中细细体会，有时会为此埋雷。 逻辑控制语句 编程语言都会有逻辑控制语句，哪怕是汇编语言。 掌握条件语句、循环语句、中断循环语句（break、continue）、选择语句。一般区别仅仅在于关键字、语法格式略有不同。 运算符 掌握基本运算符，如算术运算符、关系运算符、逻辑运算符、赋值运算符等。 有些语言还提供位运算符、特殊运算符，视情节掌握。 注释（没啥好说的） 函数 编程语言基本都有函数。注意语法格式：是否支持出参；支持哪些数据作为入参，有些语言允许将函数作为参数传入另一个参数（即回调）；返回值；如何退出函数（如 Java、C++的 return，）。 数组、枚举、集合 枚举只有部分编程语言有，如 Java、C++、C#。 但是数组和集合（有些语言叫容器）一般编程语言都有，只是有的编程语言提供的集合比较丰富。使用方法基本类似。 常用类 比较常用的类（当然有些语言中不叫类，叫对象或者其他什么，这个不重要，领会精神）请了解其 API 用法，如：字符串、日期、数学计算等等。 语言特性 语言特性这个特字反映的就是各个编程语言自身的&quot;独特个性&quot;，这涉及的点比较多，简单列举一些。 编程模式 比较流行的编程模式大概有： 面向对象编程，主要是封装、继承、多态；函数式编程，主要是应用 Lambda；过程式编程，可以理解为实现需求功能的特定步骤。 每种编程模式都有一定的道理，我从不认为只有面向对象编程才是王道。 Java 是面向对象语言，从 Java8 开始也支持函数编程（引入 Lambda 表达式）；C++ 可以算是半面向对象，半面向过程式语言。 语言自身特性 每个语言自身都有一些重要特性需要了解。例如，学习 C、C++，你必须了解内存的申请和释放，了解指针、引用。而学习 Java，你需要了解 JVM，垃圾回收机制。学习 Javascript，你需要了解 DOM 操作等。 代码组织、模块加载、库管理 一个程序一般都有很多个源代码文件。这就会引入这些问题：如何将代码文件组织起来？如何根据业务需要，选择将部分模块启动时进行加载，部分模块使用懒加载（或者热加载）？ 最基本的引用文件就不提了，如 C、C++的#include，Java 的 import 等。 针对代码组织、模块加载、库管理这些问题，不同语言会有不同的解决方案。 如 Java 可以用 maven、gradle 管理项目依赖、组织代码结构；Javascript （包括 Nodejs、jquery、react 等等库）可以用 npm、yarn 管理依赖，用 webpack 等工具管理模块加载。 容错处理 程序总难免会有 bug。 所以为了代码健壮性也好，为了方便定位问题也好，代码中需要有容错处理。常见的手段有： 异常 断言 日志 调试 单元测试 输入输出和文件处理 这块知识比较繁杂。建议提纲挈领的学习一下，理解基本概念，比如输入输出流、管道等等。至于 API，用到的时候再查一下即可。 回调机制 每种语言实现回调的方式有所不同，如 .Net 的 delegate （大量被用于 WinForm 程序）；Javascript 中函数天然支持回调：Javascript 函数允许传入另一个函数作为入参，然后在方法中调用它。其它语言的回调方式不一一列举。 序列化和反序列化 首先需要了解的是，序列化和反序列化的作用是为了在不同平台之间传输对象。 其次，要知道序列化存在多种方式，不同编程语言可能有多种方案。根据应用的序列化方式，选择性了解即可。 进阶特性 以下学习内容属于进阶性内容。可以根据开发需要去学习、掌握。需要注意的是，学习这些特性的态度应该是不学则已，学则死磕。因为半懂半不懂，特别容易引入问题。 对于半桶水的同学，我想说：放过自己，也放过别人，活着不好吗？ **并发编程：**好处多多，十分重要，但是并发代码容易出错，且出错难以定位。要学习还是要花很大力气的，需要了解大量知识，如：进程、线程、同步、异步、读写锁等等。 反射 - 让你可以动态编程（慎用）。 泛型 - 集合（或者叫容器）的基石。精通泛型，能大大提高你的代码效率。 元数据 - 描述数据的数据。Java 中叫做注解。 库和框架 学习一门编程语言，难免需要用到围绕它构建的技术生态圈——库和框架。这方面知识范围太庞大，根据实际应用领域去学习吧。比如搞 JavaWeb，你多多少少肯定要用到 Spring、Mybatis、Hibernate、Shiro 等大量开发框架；如果做 Javascript 前端，你可能会用到 React、Vue、Angular 、jQuery 等库或框架。 小结 总结以上，编程语言学习的道路是任重而道远的，未来是光明的。 最后一句话与君共勉：路漫漫兮其修远，吾将上下而求索。]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic 技术栈之 Logstash 基础]]></title>
    <url>%2Fblog%2F2017%2F12%2F26%2Fos%2Flinux%2Fops%2Fservice%2Felastic%2Felastic-logstash%2F</url>
    <content type="text"><![CDATA[Elastic 技术栈之 Logstash 基础 本文是 Elastic 技术栈（ELK）的 Logstash 应用。 如果不了解 Elastic 的安装、配置、部署，可以参考：Elastic 技术栈之快速入门 简介 Logstash 可以传输和处理你的日志、事务或其他数据。 功能 Logstash 是 Elasticsearch 的最佳数据管道。 Logstash 是插件式管理模式，在输入、过滤、输出以及编码过程中都可以使用插件进行定制。Logstash 社区有超过 200 种可用插件。 工作原理 Logstash 有两个必要元素：input 和 output ，一个可选元素：filter。 这三个元素，分别代表 Logstash 事件处理的三个阶段：输入 &gt; 过滤器 &gt; 输出。 input 负责从数据源采集数据。 filter 将数据修改为你指定的格式或内容。 output 将数据传输到目的地。 在实际应用场景中，通常输入、输出、过滤器不止一个。Logstash 的这三个元素都使用插件式管理方式，用户可以根据应用需要，灵活的选用各阶段需要的插件，并组合使用。 后面将对插件展开讲解，暂且不表。 设置 设置文件 logstash.yml：logstash 的默认启动配置文件 jvm.options：logstash 的 JVM 配置文件。 startup.options (Linux)：包含系统安装脚本在 /usr/share/logstash/bin 中使用的选项为您的系统构建适当的启动脚本。安装 Logstash 软件包时，系统安装脚本将在安装过程结束时执行，并使用 startup.options 中指定的设置来设置用户，组，服务名称和服务描述等选项。 logstash.yml 设置项 节选部分设置项，更多项请参考：https://www.elastic.co/guide/en/logstash/current/logstash-settings-file.html 参数 描述 默认值 node.name 节点名 机器的主机名 path.data Logstash及其插件用于任何持久性需求的目录。 LOGSTASH_HOME/data pipeline.workers 同时执行管道的过滤器和输出阶段的工作任务数量。如果发现事件正在备份，或CPU未饱和，请考虑增加此数字以更好地利用机器处理能力。 Number of the host’s CPU cores pipeline.batch.size 尝试执行过滤器和输出之前，单个工作线程从输入收集的最大事件数量。较大的批量处理大小一般来说效率更高，但是以增加的内存开销为代价。您可能必须通过设置 LS_HEAP_SIZE 变量来有效使用该选项来增加JVM堆大小。 125 pipeline.batch.delay 创建管道事件批处理时，在将一个尺寸过小的批次发送给管道工作任务之前，等待每个事件需要多长时间（毫秒）。 5 pipeline.unsafe_shutdown 如果设置为true，则即使在内存中仍存在inflight事件时，也会强制Logstash在关闭期间退出。默认情况下，Logstash将拒绝退出，直到所有接收到的事件都被推送到输出。启用此选项可能会导致关机期间数据丢失。 false path.config 主管道的Logstash配置路径。如果您指定一个目录或通配符，配置文件将按字母顺序从目录中读取。 Platform-specific. See [dir-layout]. config.string 包含用于主管道的管道配置的字符串。使用与配置文件相同的语法。 None config.test_and_exit 设置为true时，检查配置是否有效，然后退出。请注意，使用此设置不会检查grok模式的正确性。 Logstash可以从目录中读取多个配置文件。如果将此设置与log.level：debug结合使用，则Logstash将记录组合的配置文件，并注掉其源文件的配置块。 false config.reload.automatic 设置为true时，定期检查配置是否已更改，并在配置更改时重新加载配置。这也可以通过SIGHUP信号手动触发。 false config.reload.interval Logstash 检查配置文件更改的时间间隔。 3s config.debug 设置为true时，将完全编译的配置显示为调试日志消息。您还必须设置log.level：debug。警告：日志消息将包括任何传递给插件配置作为明文的“密码”选项，并可能导致明文密码出现在您的日志！ false config.support_escapes 当设置为true时，带引号的字符串将处理转义字符。 false modules 配置时，模块必须处于上表所述的嵌套YAML结构中。 None http.host 绑定地址 &quot;127.0.0.1&quot; http.port 绑定端口 9600 log.level 日志级别。有效选项：fatal &gt; error &gt; warn &gt; info &gt; debug &gt; trace info log.format 日志格式。json （JSON 格式）或 plain （原对象） plain path.logs Logstash 自身日志的存储路径 LOGSTASH_HOME/logs path.plugins 在哪里可以找到自定义的插件。您可以多次指定此设置以包含多个路径。 启动 命令行 通过命令行启动 logstash 的方式如下： bin/logstash [options] 其中 [options] 是您可以指定用于控制 Logstash 执行的命令行标志。 在命令行上设置的任何标志都会覆盖 Logstash 设置文件（logstash.yml）中的相应设置，但设置文件本身不会更改。 注 虽然可以通过指定命令行参数的方式，来控制 logstash 的运行方式，但显然这么做很麻烦。 建议通过指定配置文件的方式，来控制 logstash 运行，启动命令如下： &gt; bin/logstash -f logstash.conf&gt; 若想了解更多的命令行参数细节，请参考：https://www.elastic.co/guide/en/logstash/current/running-logstash-command-line.html 配置文件 上节，我们了解到，logstash 可以执行 bin/logstash -f logstash.conf ，按照配置文件中的参数去覆盖默认设置文件（logstash.yml）中的设置。 这节，我们就来学习一下这个配置文件如何配置参数。 配置文件结构 在工作原理一节中，我们已经知道了 Logstash 主要有三个工作阶段 input 、filter、output。而 logstash 配置文件文件结构也与之相对应： input &#123;&#125;filter &#123;&#125;output &#123;&#125; 每个部分都包含一个或多个插件的配置选项。如果指定了多个过滤器，则会按照它们在配置文件中的显示顺序应用它们。 插件配置 插件的配置由插件名称和插件的一个设置块组成。 下面的例子中配置了两个输入文件配置： input &#123; file &#123; path =&gt; "/var/log/messages" type =&gt; "syslog" &#125; file &#123; path =&gt; "/var/log/apache/access.log" type =&gt; "apache" &#125;&#125; 您可以配置的设置因插件类型而异。你可以参考： Input Plugins, Output Plugins, Filter Plugins, 和 Codec Plugins 。 值类型 一个插件可以要求设置的值是一个特定的类型，比如布尔值，列表或哈希值。以下值类型受支持。 Array users =&gt; [ &#123;id =&gt; 1, name =&gt; bob&#125;, &#123;id =&gt; 2, name =&gt; jane&#125; ] Lists path =&gt; [ "/var/log/messages", "/var/log/*.log" ]uris =&gt; [ "http://elastic.co", "http://example.net" ] Boolean ssl_enable =&gt; true Bytes my_bytes =&gt; "1113" # 1113 bytesmy_bytes =&gt; "10MiB" # 10485760 bytesmy_bytes =&gt; "100kib" # 102400 bytesmy_bytes =&gt; "180 mb" # 180000000 bytes Codec codec =&gt; "json" Hash match =&gt; &#123; "field1" =&gt; "value1" "field2" =&gt; "value2" ...&#125; Number port =&gt; 33 Password my_password =&gt; "password" URI my_uri =&gt; "http://foo:bar@example.net" Path my_path =&gt; "/tmp/logstash" String 转义字符 插件 input Logstash 支持各种输入选择 ，可以在同一时间从众多常用来源捕捉事件。能够以连续的流式传输方式，轻松地从您的日志、指标、Web 应用、数据存储以及各种 AWS 服务采集数据。 常用 input 插件 file：从文件系统上的文件读取，就像UNIX命令 tail -0F 一样 **syslog：**在众所周知的端口514上侦听系统日志消息，并根据RFC3164格式进行解析 **redis：**从redis服务器读取，使用redis通道和redis列表。 Redis经常用作集中式Logstash安装中的“代理”，它将来自远程Logstash“托运人”的Logstash事件排队。 **beats：**处理由Filebeat发送的事件。 更多详情请见：Input Plugins filter 过滤器是Logstash管道中的中间处理设备。如果符合特定条件，您可以将条件过滤器组合在一起，对事件执行操作。 常用 filter 插件 **grok：**解析和结构任意文本。 Grok目前是Logstash中将非结构化日志数据解析为结构化和可查询的最佳方法。 **mutate：**对事件字段执行一般转换。您可以重命名，删除，替换和修改事件中的字段。 **drop：**完全放弃一个事件，例如调试事件。 **clone：**制作一个事件的副本，可能会添加或删除字段。 **geoip：**添加有关IP地址的地理位置的信息（也可以在Kibana中显示惊人的图表！） 更多详情请见：Filter Plugins output 输出是Logstash管道的最后阶段。一个事件可以通过多个输出，但是一旦所有输出处理完成，事件就完成了执行。 常用 output 插件 **elasticsearch：**将事件数据发送给 Elasticsearch（推荐模式）。 **file：**将事件数据写入文件或磁盘。 **graphite：**将事件数据发送给 graphite（一个流行的开源工具，存储和绘制指标。 http://graphite.readthedocs.io/en/latest/）。 **statsd：**将事件数据发送到 statsd （这是一种侦听统计数据的服务，如计数器和定时器，通过UDP发送并将聚合发送到一个或多个可插入的后端服务）。 更多详情请见：Output Plugins codec 用于格式化对应的内容。 常用 codec 插件 **json：**以JSON格式对数据进行编码或解码。 **multiline：**将多行文本事件（如java异常和堆栈跟踪消息）合并为单个事件。 更多插件请见：Codec Plugins 实战 前面的内容都是对 Logstash 的介绍和原理说明。接下来，我们来实战一些常见的应用场景。 传输控制台数据 stdin input 插件从标准输入读取事件。这是最简单的 input 插件，一般用于测试场景。 应用 （1）创建 logstash-input-stdin.conf ： input &#123; stdin &#123; &#125; &#125;output &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-stdin.html （2）执行 logstash，使用 -f 来指定你的配置文件： bin/logstash -f logstash-input-stdin.conf 传输 logback 日志 elk 默认使用的 Java 日志工具是 log4j2 ，并不支持 logback 和 log4j。 想使用 logback + logstash ，可以使用 logstash-logback-encoder 。logstash-logback-encoder 提供了 UDP / TCP / 异步方式来传输日志数据到 logstash。 如果你使用的是 log4j ，也不是不可以用这种方式，只要引入桥接 jar 包即可。如果你对 log4j 、logback ，或是桥接 jar 包不太了解，可以参考我的这篇博文：细说 Java 主流日志工具库 。 TCP 应用 logstash 配置 （1）创建 logstash-input-tcp.conf ： input &#123;tcp &#123; port =&gt; 9251 codec =&gt; json_lines mode =&gt; server&#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-tcp.html （2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-udp.conf java 应用配置 （1）在 Java 应用的 pom.xml 中引入 jar 包： &lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt;&lt;/dependency&gt;&lt;!-- logback 依赖包 --&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-access&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt; （2）接着，在 logback.xml 中添加 appender &lt;appender name="ELK-TCP" class="net.logstash.logback.appender.LogstashTcpSocketAppender"&gt; &lt;!-- destination 是 logstash 服务的 host:port， 相当于和 logstash 建立了管道，将日志数据定向传输到 logstash --&gt; &lt;destination&gt;192.168.28.32:9251&lt;/destination&gt; &lt;encoder charset="UTF-8" class="net.logstash.logback.encoder.LogstashEncoder"/&gt;&lt;/appender&gt;&lt;logger name="io.github.dunwu.spring" level="TRACE" additivity="false"&gt; &lt;appender-ref ref="ELK-TCP" /&gt;&lt;/logger&gt; （3）接下来，就是 logback 的具体使用 ，如果对此不了解，不妨参考一下我的这篇博文：细说 Java 主流日志工具库 。 实例：我的logback.xml UDP 应用 UDP 和 TCP 的使用方式大同小异。 logstash 配置 （1）创建 logstash-input-udp.conf ： input &#123;udp &#123; port =&gt; 9250 codec =&gt; json&#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-udp.html （2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-udp.conf java 应用配置 （1）在 Java 应用的 pom.xml 中引入 jar 包： 与 TCP 应用 一节中的引入依赖包完全相同。 （2）接着，在 logback.xml 中添加 appender &lt;appender name="ELK-UDP" class="net.logstash.logback.appender.LogstashSocketAppender"&gt; &lt;host&gt;192.168.28.32&lt;/host&gt; &lt;port&gt;9250&lt;/port&gt;&lt;/appender&gt;&lt;logger name="io.github.dunwu.spring" level="TRACE" additivity="false"&gt; &lt;appender-ref ref="ELK-UDP" /&gt;&lt;/logger&gt; （3）接下来，就是 logback 的具体使用 ，如果对此不了解，不妨参考一下我的这篇博文：细说 Java 主流日志工具库 。 实例：我的logback.xml 传输文件 在 Java Web 领域，需要用到一些重要的工具，例如 Tomcat 、Nginx 、Mysql 等。这些不属于业务应用，但是它们的日志数据对于定位问题、分析统计同样很重要。这时无法使用 logback 方式将它们的日志传输到 logstash。 如何采集这些日志文件呢？别急，你可以使用 logstash 的 file input 插件。 需要注意的是，传输文件这种方式，必须在日志所在的机器上部署 logstash 。 应用 logstash 配置 （1）创建 logstash-input-file.conf ： input &#123; file &#123; path =&gt; ["/var/log/nginx/access.log"] type =&gt; "nginx-access-log" start_position =&gt; "beginning" &#125;&#125;output &#123; if [type] == "nginx-access-log" &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] index =&gt; "nginx-access-log" &#125; &#125;&#125; （2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-file.conf 更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html 小技巧 启动、终止应用 如果你的 logstash 每次都是通过指定配置文件方式启动。不妨建立一个启动脚本。 # cd xxx 进入 logstash 安装目录下的 bin 目录logstash -f logstash.conf 如果你的 logstash 运行在 linux 系统下，不妨使用 nohup 来启动一个守护进程。这样做的好处在于，即使关闭终端，应用仍会运行。 创建 startup.sh nohup ./logstash -f logstash.conf &gt;&gt; nohup.out 2&gt;&amp;1 &amp; 终止应用没有什么好方法，你只能使用 ps -ef | grep logstash ，查出进程，将其kill 。不过，我们可以写一个脚本来干这件事： 创建 shutdown.sh 脚本不多解释，请自行领会作用。 PID=`ps -ef | grep logstash | awk '&#123; print $2&#125;' | head -n 1`kill -9 $&#123;PID&#125; 资料 Logstash 官方文档 logstash-logback-encoder ELK Stack权威指南 ELK（Elasticsearch、Logstash、Kibana）安装和配置 推荐阅读 Elastic 技术栈 JavaStack]]></content>
      <categories>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>log</tag>
        <tag>elastic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring 4 升级踩雷指南]]></title>
    <url>%2Fblog%2F2017%2F12%2F15%2Fjava%2Fjavaweb%2Fspring%2Fappendix%2Fspring4-upgrade%2F</url>
    <content type="text"><![CDATA[spring 4 升级踩雷指南 前言 最近，一直在为公司老项目做核心库升级工作。本来只是想升级一下 JDK8 ，却因为兼容性问题而不得不升级一些其他的库，而其他库本身依赖的一些库可能也要同步升级。这是一系列连锁问题，你很难一一识别，往往只有在编译时、运行时才能发现问题。 总之，这是个费劲的活啊。 本文小结一下升级 Spring4 的连锁问题。 为什么升级 spring4 升级 Spring4 的原因是：Spring 4 以前的版本不兼容 JDK8。当你的项目同时使用 Spring3 和 JDK8，如果代码中有使用 JDK8 字节码或 Lambada 表达式，那么会出问题。 也许你会问，为什么不使用最新的 Spring 5 呢？因为作为企业软件，一般更倾向使用稳定的版本（bug 少），而不是最新的版本，尤其是一些核心库。 更多细节可以参考： https://spring.io/blog/2013/05/21/spring-framework-4-0-m1-3-2-3-available/ spring 4 重要新特性 Spring 4 相比 Spring 3，引入许多新特性，这里列举几条较为重要的： 支持 JDK8 （这个是最主要的）。 Groovy Bean Definition DSL 风格配置。 支持 WebSocket、SockJS、STOMP 消息 移除 Deprecated 包和方法 一些功能加强，如：核心容器、Web、Test 等等，不一一列举。 更多 Spring 4 新特性可以参考： https://docs.spring.io/spring/docs/4.3.14.BUILD-SNAPSHOT/spring-framework-reference/htmlsingle/#spring-whats-new http://jinnianshilongnian.iteye.com/blog/1995111 升级 spring 4 步骤 了解了前面内容，我们知道了升级 Spring 4 带来的好处。现在开始真刀真枪的升级了。 不要以为升级一下 Spring 4，仅仅是改一下版本号，那么简单，细节处多着呢。 下面，结合我在公司项目升级 Spring4 时遇到的一系列坑，希望能帮助各位少走弯路。 注 下文内容基于假设你的项目是用 maven 管理这一前提。如果不满足这一前提，那么这篇文章对你没什么太大帮助。 修改 spring 版本 第一步，当然是修改 pom.xml 中的 spring 版本。 3.x.x.RELEASE &gt; 4.x.x.RELEASE 实例：升级 spring-core 其它 spring 库的升级也如此： &lt;properties&gt; &lt;spring.version&gt;4.3.13.RELEASE&lt;/spring.version&gt;&lt;/properties&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt; 修改 spring xml 文件的 xsd 用过 spring 的都知道，spring 通常依赖于大量的 xml 配置。 spring 的 xml 解析器在解析 xml 时，需要读取 xml schema，schema 定义了 xml 的命名空间。它的好处在于可以避免命名冲突，有点像 Java 中的 package。 实例：一个 spring xml 的 schema &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:util="http://www.springframework.org/schema/util" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-3.1.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.1.xsd"&gt; 说明 xmlns=&quot;http://www.springframework.org/schema/beans&quot; 声明 xml 文件默认的命名空间，表示未使用其他命名空间的所有标签的默认命名空间。 xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; 声明XML Schema 实例，声明后就可以使用 schemaLocation 属性了。 xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; 声明前缀为 mvc 的命名空间，后面的 URL 用于标示命名空间的地址不会被解析器用于查找信息。其惟一的作用是赋予命名空间一个惟一的名称。当命名空间被定义在元素的开始标签中时，所有带有相同前缀的子元素都会与同一个命名空间相关联。 其它的类似 xmlns:context 、xmlns:jdbc 等等同样如此。 &gt; xsi:schemaLocation="http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd&gt; ..."&gt; 这个从命名可以看出个大概，指定 schema 位置这个属性必须结合命名空间使用。这个属性有两个值，第一个值表示需要使用的命名空间。第二个值表示供命名空间使用的 xml schema 的位置。 上面示例中的 xsd 版本是 3.1.xsd ，表示 spring 的 xml 解析器会将其视为 3.1 版本的 xml 文件来处理。 现在，我们使用了 Spring 4，3.1.xsd 版本显然就不正确了，我们可以根据自己引入的 Spring 4 的子版本号将其改为 4.x.xsd 。 但是，还有一种更好的做法：把这个指定 xsd 版本的关键字干掉，类似这样：http://www.springframework.org/schema/tx/spring-tx.xsd 。 这么做的原因如下： Spring 默认在启动时要加载 xsd 文件来验证 xml 文件。 如果没有提供 schemaLocation，那么 spring 的 xml 解析器会从 namespace 的 uri 里加载 xsd 文件。 schemaLocation 提供了一个 xml namespace 到对应的 xsd 文件的一个映射。 如果不指定 spring xsd 的版本号，spring 取的就是当前本地 jar 里的 xsd 文件，减少了各种风险（比如 xsd 与实际 spring jar 版本不一致）。 更多详细内容可以参考这篇文章：为什么在Spring的配置里，最好不要配置xsd文件的版本号 修改 spring xml 文件 spring 4 对 xml 做了一些改动。这里说一个最常用的改动： ref local spring 不再支持 ref 元素的 local 属性，如果你的项目中使用了，需要改为 bean。 shi spring 4 以前： &lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource"&gt; &lt;ref local="dataSource" /&gt; &lt;/property&gt;&lt;/bean&gt; spring 4 以后： &lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource"&gt; &lt;ref bean="dataSource" /&gt; &lt;/property&gt;&lt;/bean&gt; 如果不改启动会报错： Caused by: org.xml.sax.SAXParseException: cvc-complex-type.3.2.2: Attribute 'local' is not allowed to appear in element 'ref'. 当然，可能还有一些其他配置改动，这个只能说兵来将挡水来土掩，遇到了再去查官方文档吧。 加入 spring support spring 3 中很多的扩展内容不需要引入support 。但是 spring 4 中分离的更彻底了，如果不分离，会有很多ClassNotFound 。 &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.2.3.RELEASE&lt;/version&gt;&lt;/dependency&gt; 更换 spring-mvc jackson spring mvc 中如果返回结果为 json 需要依赖 jackson 的jar包，但是他升级到了2, 以前是 codehaus.jackson，现在换成了 fasterxml.jackson &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt;&lt;/dependency&gt; 同时修改spring mvc的配置文件： &lt;bean class="org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter"&gt; &lt;property name="messageConverters"&gt; &lt;list&gt; &lt;ref bean="stringHttpMessageConverter" /&gt; &lt;bean class="org.springframework.http.converter.json.MappingJackson2HttpMessageConverter"&gt; &lt;/bean&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id="stringHttpMessageConverter" class="org.springframework.http.converter.StringHttpMessageConverter"&gt; &lt;property name="supportedMediaTypes"&gt; &lt;list&gt; &lt;value&gt;text/plain;charset=UTF-8&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 解决 ibatis 兼容问题 问题 如果你的项目中使用了 ibatis (mybatis 的前身)这个 orm 框架，当 spring3 升级 spring4 后，会出现兼容性问题，编译都不能通过。 这是因为 Spring4 官方已经不再支持 ibatis。 解决方案 添加兼容性 jar 包 &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-2-spring&lt;/artifactId&gt; &lt;version&gt;1.0.1&lt;/version&gt;&lt;/dependency&gt; 更多内容可参考：https://stackoverflow.com/questions/32353286/no-support-for-ibatis-in-spring4-2-0 升级 Dubbo 我们的项目中使用了 soa 框架 Dubbo 。由于 Dubbo 是老版本的，具体来说是（2013年的 2.4.10），而老版本中使用的 spirng 版本为2.x，有兼容性问题。 Dubbo 项目从今年开始恢复维护了，首先把一些落后的库升级到较新版本，比如 jdk8，spring4 等，并修复了一些 bug。所以，我们可以通过升级一下 Dubbo 版本来解决问题。 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.5.8&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.javassist&lt;/groupId&gt; &lt;artifactId&gt;javassist&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 升级 Jedis 升级 Dubbo 为当前最新的 2.5.8 版本后，运行时报错： JedisPoolConfig 配置错误 Caused by: java.lang.ClassNotFoundException: org.apache.commons.pool2.impl.GenericObjectPoolConfig 由于项目中使用了 redis，版本为 2.0.0 ，这个问题是由于 jedis 需要升级： &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; jedis 2.4.1 以上版本的 JedisPoolConfig 已经没有了maxActive 和 maxWait 属性。 修改方法如下： maxActive &gt; maxTotal maxWait &gt; maxWaitMillis &lt;bean id="jedisPoolConfig" class="redis.clients.jedis.JedisPoolConfig"&gt; &lt;property name="maxTotal" value="200" /&gt; &lt;property name="maxIdle" value="10" /&gt; &lt;property name="maxWaitMillis" value="1000" /&gt; &lt;property name="testOnBorrow" value="true" /&gt;&lt;/bean&gt; JedisPool 配置错误 InvalidURIException: Cannot open Redis connection due invalid URI 原来的配置如下： &lt;bean id="jedisPool" class="redis.clients.jedis.JedisPool" destroy-method="destroy" depends-on="jedisPoolConfig"&gt; &lt;constructor-arg ref="jedisPoolConfig" /&gt; &lt;constructor-arg type="java.lang.String" value="$&#123;redis.host&#125;" /&gt; &lt;constructor-arg type="int" value="$&#123;redis.port&#125;" /&gt;&lt;/bean&gt; 查看源码可以发现，初始化 JedisPool 时未指定结构方法参数的类型，导致 host 字符串值被视为 URI 类型，当然类型不匹配。 解决方法是修改上面的host 配置，为：&lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;${redis.host}&quot; /&gt; 至此，spring 4 升级结束。后面如果遇到其他升级问题再补充。 资料 https://spring.io/blog/2013/05/21/spring-framework-4-0-m1-3-2-3-available/ https://docs.spring.io/spring/docs/4.3.14.BUILD-SNAPSHOT/spring-framework-reference/htmlsingle/#spring-whats-new Spring 3.x 升级到Spring 4.x 注意事项和步骤，错误解决方法 http://jinnianshilongnian.iteye.com/blog/1995111 为什么在Spring的配置里，最好不要配置xsd文件的版本号 https://stackoverflow.com/questions/32353286/no-support-for-ibatis-in-spring4-2-0]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>upgrade</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 从入门到精通]]></title>
    <url>%2Fblog%2F2017%2F12%2F09%2Ftools%2Fgit%2F</url>
    <content type="text"><![CDATA[Git 从入门到精通 📓 本文已归档到：「blog」 简介 Git 是什么？ 什么是版本控制？ 什么是分布式版本控制系统？ 为什么使用 Git？ 安装 配置 用户信息 .gitignore 原理 版本库 哈希值 文件状态 工作区域 命令 创建仓库 添加修改 撤销修改 更新与推送 查看信息 分支 标签 合并与重置 Github 最佳实践 Git Flow 常见问题 编辑提交(editting commits) 暂存(Staging) 未暂存(Unstaged)的内容 分支(Branches) Rebasing 和合并(Merging) 杂项(Miscellaneous Objects) 跟踪文件(Tracking Files) 配置(Configuration) 我不知道我做错了些什么 小结 参考资料 简介 Git 是什么？ Git 是一个开源的分布式版本控制系统。 什么是版本控制？ 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 什么是分布式版本控制系统？ 介绍分布式版本控制系统前，有必要先了解一下传统的集中式版本控制系统。 集中化的版本控制系统，诸如 CVS，Subversion 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。 这么做最显而易见的缺点是中央服务器的单点故障。如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。要是中央服务器的磁盘发生故障，碰巧没做备份，或者备份不够及时，就会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录。 分布式版本控制系统的客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。因为每一次的提取操作，实际上都是一次对代码仓库的完整备份。 为什么使用 Git？ Git 是分布式的。这是 Git 和其它非分布式的版本控制系统，例如 svn，cvs 等，最核心的区别。分布式带来以下好处： 工作时不需要联网 首先，分布式版本控制系统根本没有“中央服务器”，每个人的电脑上都是一个完整的版本库，这样，你工作的时候，就不需要联网了，因为版本库就在你自己的电脑上。既然每个人电脑上都有一个完整的版本库，那多个人如何协作呢？比方说你在自己电脑上改了文件 A，你的同事也在他的电脑上改了文件 A，这时，你们俩之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。 更加安全 集中式版本控制系统，一旦中央服务器出了问题，所有人都无法工作。 分布式版本控制系统，每个人电脑中都有完整的版本库，所以某人的机器挂了，并不影响其它人。 安装 Debian/Ubuntu 环境安装 如果你使用的系统是 Debian/Ubuntu ， 安装命令为： $ apt-get install libcurl4-gnutls-dev libexpat1-dev gettext \&gt; libz-dev libssl-dev$ apt-get install git-core$ git --versiongit version 1.8.1.2 Centos/RedHat 环境安装 如果你使用的系统是 Centos/RedHat ，安装命令为： $ yum install curl-devel expat-devel gettext-devel \&gt; openssl-devel zlib-devel$ yum -y install git-core$ git --versiongit version 1.7.1 Windows 环境安装 在Git 官方下载地址下载 exe 安装包。按照安装向导安装即可。 建议安装 Git Bash 这个 git 的命令行工具。 Mac 环境安装 在Git 官方下载地址下载 mac 安装包。按照安装向导安装即可。 配置 Git 自带一个 git config 的工具来帮助设置控制 Git 外观和行为的配置变量。 这些变量存储在三个不同的位置： /etc/gitconfig 文件: 包含系统上每一个用户及他们仓库的通用配置。 如果使用带有 --system 选项的 git config 时，它会从此文件读写配置变量。 \~/.gitconfig 或 \~/.config/git/config 文件：只针对当前用户。 可以传递 --global 选项让 Git 读写此文件。 当前使用仓库的 Git 目录中的 config 文件（就是 .git/config）：针对该仓库。 每一个级别覆盖上一级别的配置，所以 .git/config 的配置变量会覆盖 /etc/gitconfig 中的配置变量。 在 Windows 系统中，Git 会查找 $HOME 目录下（一般情况下是 C:\Users\$USER）的 .gitconfig 文件。 Git 同样也会寻找 /etc/gitconfig 文件，但只限于 MSys 的根目录下，即安装 Git 时所选的目标位置。 用户信息 当安装完 Git 应该做的第一件事就是设置你的用户名称与邮件地址。 这样做很重要，因为每一个 Git 的提交都会使用这些信息，并且它会写入到你的每一次提交中，不可更改： $ git config --global user.name "John Doe"$ git config --global user.email johndoe@example.com 再次强调，如果使用了 --global 选项，那么该命令只需要运行一次，因为之后无论你在该系统上做任何事情， Git 都会使用那些信息。 当你想针对特定项目使用不同的用户名称与邮件地址时，可以在那个项目目录下运行没有 --global 选项的命令来配置。 很多 GUI 工具都会在第一次运行时帮助你配置这些信息。 .gitignore .gitignore 文件可能从字面含义也不难猜出：这个文件里配置的文件或目录，会自动被 git 所忽略，不纳入版本控制。 在日常开发中，我们的项目经常会产生一些临时文件，如编译 Java 产生的 *.class 文件，又或是 IDE 自动生成的隐藏目录（Intellij 的 .idea 目录、Eclipse 的 .settings 目录等）等等。这些文件或目录实在没必要纳入版本管理。在这种场景下，你就需要用到 .gitignore 配置来过滤这些文件或目录。 配置的规则很简单，也没什么可说的，看几个例子，自然就明白了。 这里推荐一下 Github 的开源项目：https://github.com/github/gitignore 在这里，你可以找到很多常用的模板，如：Java、Nodejs、C++ 的 .gitignore 模板等等。 原理 个人认为，对于 Git 这个版本工具，再不了解原理的情况下，直接去学习命令行，可能会一头雾水。所以，本文特意将原理放在命令使用章节之前讲解。 版本库 当你一个项目到本地或创建一个 git 项目，项目目录下会有一个隐藏的 .git 子目录。这个目录是 git 用来跟踪管理版本库的，千万不要手动修改。 哈希值 Git 中所有数据在存储前都计算校验和，然后以校验和来引用。 这意味着不可能在 Git 不知情时更改任何文件内容或目录内容。 这个功能建构在 Git 底层，是构成 Git 哲学不可或缺的部分。 若你在传送过程中丢失信息或损坏文件，Git 就能发现。 Git 用以计算校验和的机制叫做 SHA-1 散列（hash，哈希）。 这是一个由 40 个十六进制字符（0-9 和 a-f）组成字符串，基于 Git 中文件的内容或目录结构计算出来。 SHA-1 哈希看起来是这样： 24b9da6552252987aa493b52f8696cd6d3b00373 Git 中使用这种哈希值的情况很多，你将经常看到这种哈希值。 实际上，Git 数据库中保存的信息都是以文件内容的哈希值来索引，而不是文件名。 文件状态 在 GIt 中，你的文件可能会处于三种状态之一： 已修改（modified） - 已修改表示修改了文件，但还没保存到数据库中。 已暂存（staged） - 已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 已提交（committed） - 已提交表示数据已经安全的保存在本地数据库中。 工作区域 与文件状态对应的，不同状态的文件在 Git 中处于不同的工作区域。 工作区（working） - 当你 git clone 一个项目到本地，相当于在本地克隆了项目的一个副本。工作区是对项目的某个版本独立提取出来的内容。 这些从 Git 仓库的压缩数据库中提取出来的文件，放在磁盘上供你使用或修改。 暂存区（staging） - 暂存区是一个文件，保存了下次将提交的文件列表信息，一般在 Git 仓库目录中。 有时候也被称作`‘索引’’，不过一般说法还是叫暂存区。 本地仓库（local） - 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 本地仓库。 远程仓库（remote） - 以上几个工作区都是在本地。为了让别人可以看到你的修改，你需要将你的更新推送到远程仓库。同理，如果你想同步别人的修改，你需要从远程仓库拉取更新。 命令 国外网友制作了一张 Git Cheat Sheet，总结很精炼，各位不妨收藏一下。 本节选择性介绍 git 中比较常用的命令行场景。 创建仓库 克隆一个已创建的仓库： # 通过 SSH$ git clone ssh://user@domain.com/repo.git#通过 HTTP$ git clone http://domain.com/user/repo.git 创建一个新的本地仓库： $ git init 添加修改 添加修改到暂存区： # 把指定文件添加到暂存区$ git add xxx# 把当前所有修改添加到暂存区$ git add .# 把所有修改添加到暂存区$ git add -A 提交修改到本地仓库： # 提交本地的所有修改$ git commit -a# 提交之前已标记的变化$ git commit# 附加消息提交$ git commit -m 'commit message' 储藏 有时，我们需要在同一个项目的不同分支上工作。当需要切换分支时，偏偏本地的工作还没有完成，此时，提交修改显得不严谨，但是不提交代码又无法切换分支。这时，你可以使用 git stash 将本地的修改内容作为草稿储藏起来。 官方称之为储藏，但我个人更喜欢称之为存草稿。 # 1. 将修改作为当前分支的草稿保存$ git stash# 2. 查看草稿列表$ git stash liststash@&#123;0&#125;: WIP on master: 6fae349 :memo: Writing docs.# 3.1 删除草稿$ git stash drop stash@&#123;0&#125;# 3.2 读取草稿$ git stash apply stash@&#123;0&#125; 撤销修改 撤销本地修改： # 移除缓存区的所有文件（i.e. 撤销上次git add）$ git reset HEAD# 将HEAD重置到上一次提交的版本，并将之后的修改标记为未添加到缓存区的修改$ git reset &lt;commit&gt;# 将HEAD重置到上一次提交的版本，并保留未提交的本地修改$ git reset --keep &lt;commit&gt;# 放弃工作目录下的所有修改$ git reset --hard HEAD# 将HEAD重置到指定的版本，并抛弃该版本之后的所有修改$ git reset --hard &lt;commit-hash&gt;# 用远端分支强制覆盖本地分支$ git reset --hard &lt;remote/branch&gt; e.g., upstream/master, origin/my-feature# 放弃某个文件的所有本地修改$ git checkout HEAD &lt;file&gt; 删除添加.gitignore文件前错误提交的文件： $ git rm -r --cached .$ git add .$ git commit -m "remove xyz file" 撤销远程修改（创建一个新的提交，并回滚到指定版本）： $ git revert &lt;commit-hash&gt; 彻底删除指定版本： # 执行下面命令后，commit-hash 提交后的记录都会被彻底删除，使用需谨慎$ git reset --hard &lt;commit-hash&gt;$ git push -f 更新与推送 更新： # 下载远程端版本，但不合并到HEAD中$ git fetch &lt;remote&gt;# 将远程端版本合并到本地版本中$ git pull origin master# 以rebase方式将远端分支与本地合并$ git pull --rebase &lt;remote&gt; &lt;branch&gt; 推送： # 将本地版本推送到远程端$ git push remote &lt;remote&gt; &lt;branch&gt;# 删除远程端分支$ git push &lt;remote&gt; :&lt;branch&gt; (since Git v1.5.0)$ git push &lt;remote&gt; --delete &lt;branch&gt; (since Git v1.7.0)# 发布标签$ git push --tags 查看信息 显示工作路径下已修改的文件： $ git status 显示与上次提交版本文件的不同： $ git diff 显示提交历史： # 从最新提交开始，显示所有的提交记录（显示hash， 作者信息，提交的标题和时间）$ git log# 显示某个用户的所有提交$ git log --author="username"# 显示某个文件的所有修改$ git log -p &lt;file&gt; 显示搜索内容： # 从当前目录的所有文件中查找文本内容$ git grep "Hello"# 在某一版本中搜索文本$ git grep "Hello" v2.5 分支 增删查分支： # 列出所有的分支$ git branch# 列出所有的远端分支$ git branch -r# 基于当前分支创建新分支$ git branch &lt;new-branch&gt;# 基于远程分支创建新的可追溯的分支$ git branch --track &lt;new-branch&gt; &lt;remote-branch&gt;# 删除本地分支$ git branch -d &lt;branch&gt;# 强制删除本地分支，将会丢失未合并的修改$ git branch -D &lt;branch&gt; 切换分支： # 切换分支$ git checkout &lt;branch&gt;# 创建并切换到新分支$ git checkout -b &lt;branch&gt; 标签 # 给当前版本打标签$ git tag &lt;tag-name&gt;# 给当前版本打标签并附加消息$ git tag -a &lt;tag-name&gt; 合并与重置 merge 与 rebase 虽然是 git 常用功能，但是强烈建议不要使用 git 命令来完成这项工作。 因为如果出现代码冲突，在没有代码比对工具的情况下，实在太艰难了。 你可以考虑使用各种 Git GUI 工具。 合并： # 将分支合并到当前HEAD中$ git merge &lt;branch&gt; 重置： # 将当前HEAD版本重置到分支中，请勿重置已发布的提交$ git rebase &lt;branch&gt; Github Github 作为最著名的代码开源协作社区，在程序员圈想必无人不知，无人不晓。 这里不赘述 Github 的用法，确实有不会用的新手同学，可以参考官方教程：https://guides.github.com/ clone 方式 Git 支持三种协议：HTTPS / SSH / GIT 而 Github 上支持 HTTPS 和 SSH。 HTTPS 这种方式要求你每次 push 时都要输入用户名、密码，有些繁琐。 而 SSH 要求你本地生成证书，然后在你的 Github 账户中注册。第一次配置麻烦是麻烦了点，但是以后就免去了每次 push 需要输入用户名、密码的繁琐。 以下介绍以下，如何生成证书，以及在 Github 中注册。 生成 SSH 公钥 如前所述，许多 Git 服务器都使用 SSH 公钥进行认证。 为了向 Git 服务器提供 SSH 公钥，如果某系统用户尚未拥有密钥，必须事先为其生成一份。 这个过程在所有操作系统上都是相似的。 首先，你需要确认自己是否已经拥有密钥。 默认情况下，用户的 SSH 密钥存储在其 \~/.ssh 目录下。 进入该目录并列出其中内容，你便可以快速确认自己是否已拥有密钥： $ cd ~/.ssh$ lsauthorized_keys2 id_dsa known_hostsconfig id_dsa.pub 我们需要寻找一对以 id_dsa 或 id_rsa 命名的文件，其中一个带有 .pub 扩展名。 .pub 文件是你的公钥，另一个则是私钥。 如果找不到这样的文件（或者根本没有 .ssh 目录），你可以通过运行 ssh-keygen 程序来创建它们。在 Linux/Mac 系统中，ssh-keygen 随 SSH 软件包提供；在 Windows 上，该程序包含于 MSysGit 软件包中。 $ ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/home/schacon/.ssh/id_rsa):Created directory '/home/schacon/.ssh'.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /home/schacon/.ssh/id_rsa.Your public key has been saved in /home/schacon/.ssh/id_rsa.pub.The key fingerprint is:d0:82:24:8e:d7:f1:bb:9b:33:53:96:93:49:da:9b:e3 schacon@mylaptop.local 首先 ssh-keygen 会确认密钥的存储位置（默认是 .ssh/id_rsa），然后它会要求你输入两次密钥口令。如果你不想在使用密钥时输入口令，将其留空即可。 现在，进行了上述操作的用户需要将各自的公钥发送给任意一个 Git 服务器管理员（假设服务器正在使用基于公钥的 SSH 验证设置）。 他们所要做的就是复制各自的 .pub 文件内容，并将其通过邮件发送。 公钥看起来是这样的： $ cat ~/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAklOUpkDHrfHY17SbrmTIpNLTGK9Tjom/BWDSUGPl+nafzlHDTYW7hdI4yZ5ew18JH4JW9jbhUFrviQzM7xlELEVf4h9lFX5QVkbPppSwg0cda3Pbv7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8V6RjsNAQwdsdMFvSlVK/7XAt3FaoJoAsncM1Q9x5+3V0Ww68/eIFmb1zuUFljQJKprrX88XypNDvjYNby6vw/Pb0rwert/EnmZ+AW4OZPnTPI89ZPmVMLuayrD2cE86Z/il8b+gw3r3+1nKatmIkjn2so1d01QraTlMqVSsbxNrRFi9wrf+M7Q== schacon@mylaptop.local 在你的 Github 账户中，依次点击 Settings &gt; SSH and GPG keys &gt; New SSH key 然后，将上面生成的公钥内容粘贴到 Key 编辑框并保存。至此大功告成。 后面，你在克隆你的 Github 项目时使用 SSH 方式即可。 如果觉得我的讲解还不够细致，可以参考：https://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account/ 最佳实践 Git Flow 详细内容，可以参考这篇文章：Git 在团队中的最佳实践–如何正确使用 Git Flow Git 在实际开发中的最佳实践策略 Git Flow 可以归纳为以下： master 分支 - 也就是我们经常使用的主线分支，这个分支是最近发布到生产环境的代码，这个分支只能从其他分支合并，不能在这个分支直接修改。 develop 分支 - 这个分支是我们的主开发分支，包含所有要发布到下一个 release 的代码，这个分支主要是从其他分支合并代码过来，比如 feature 分支。 feature 分支 - 这个分支主要是用来开发一个新的功能，一旦开发完成，我们合并回 develop 分支进入下一个 release。 release 分支 - 当你需要一个发布一个新 release 的时候，我们基于 Develop 分支创建一个 release 分支，完成 release 后，我们合并到 master 和 develop 分支。 hotfix 分支 - 当我们在 master 发现新的 Bug 时候，我们需要创建一个 hotfix, 完成 hotfix 后，我们合并回 master 和 develop 分支，所以 hotfix 的改动会进入下一个 release。 常见问题 编辑提交(editting commits) 我刚才提交了什么 如果你用 git commit -a 提交了一次变化(changes)，而你又不确定到底这次提交了哪些内容。 你就可以用下面的命令显示当前HEAD上的最近一次的提交(commit): (master)$ git show 或者 $ git log -n1 -p 我的提交信息(commit message)写错了 如果你的提交信息(commit message)写错了且这次提交(commit)还没有推(push), 你可以通过下面的方法来修改提交信息(commit message): $ git commit --amend 这会打开你的默认编辑器, 在这里你可以编辑信息. 另一方面, 你也可以用一条命令一次完成: $ git commit --amend -m 'xxxxxxx' 如果你已经推(push)了这次提交(commit), 你可以修改这次提交(commit)然后强推(force push), 但是不推荐这么做。 我提交(commit)里的用户名和邮箱不对 如果这只是单个提交(commit)，修改它： $ git commit --amend --author "New Authorname &lt;authoremail@mydomain.com&gt;" 如果你需要修改所有历史, 参考 'git filter-branch’的指南页. 我想从一个提交(commit)里移除一个文件 通过下面的方法，从一个提交(commit)里移除一个文件: $ git checkout HEAD^ myfile$ git add -A$ git commit --amend 这将非常有用，当你有一个开放的补丁(open patch)，你往上面提交了一个不必要的文件，你需要强推(force push)去更新这个远程补丁。 我想删除我的的最后一次提交(commit) 如果你需要删除推了的提交(pushed commits)，你可以使用下面的方法。可是，这会不可逆的改变你的历史，也会搞乱那些已经从该仓库拉取(pulled)了的人的历史。简而言之，如果你不是很确定，千万不要这么做。 $ git reset HEAD^ --hard$ git push -f [remote] [branch] 如果你还没有推到远程, 把 Git 重置(reset)到你最后一次提交前的状态就可以了(同时保存暂存的变化): (my-branch*)$ git reset --soft HEAD@&#123;1&#125; 这只能在没有推送之前有用. 如果你已经推了, 唯一安全能做的是 git revert SHAofBadCommit， 那会创建一个新的提交(commit)用于撤消前一个提交的所有变化(changes)； 或者, 如果你推的这个分支是 rebase-safe 的 (例如： 其它开发者不会从这个分支拉), 只需要使用 git push -f； 更多, 请参考 the above section。 删除任意提交(commit) 同样的警告：不到万不得已的时候不要这么做. $ git rebase --onto SHA1_OF_BAD_COMMIT^ SHA1_OF_BAD_COMMIT$ git push -f [remote] [branch] 或者做一个 交互式 rebase 删除那些你想要删除的提交(commit)里所对应的行。 我尝试推一个修正后的提交(amended commit)到远程，但是报错： To https://github.com/yourusername/repo.git! [rejected] mybranch -&gt; mybranch (non-fast-forward)error: failed to push some refs to 'https://github.com/tanay1337/webmaker.org.git'hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Integrate the remote changes (e.g.hint: 'git pull ...') before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details. 注意, rebasing(见下面)和修正(amending)会用一个新的提交(commit)代替旧的, 所以如果之前你已经往远程仓库上推过一次修正前的提交(commit)，那你现在就必须强推(force push) (-f)。 注意 – 总是 确保你指明一个分支! (my-branch)$ git push origin mybranch -f 一般来说, 要避免强推. 最好是创建和推(push)一个新的提交(commit)，而不是强推一个修正后的提交。后者会使那些与该分支或该分支的子分支工作的开发者，在源历史中产生冲突。 我意外的做了一次硬重置(hard reset)，我想找回我的内容 如果你意外的做了 git reset --hard, 你通常能找回你的提交(commit), 因为 Git 对每件事都会有日志，且都会保存几天。 (master)$ git reflog 你将会看到一个你过去提交(commit)的列表, 和一个重置的提交。 选择你想要回到的提交(commit)的 SHA，再重置一次: (master)$ git reset --hard SHA1234 这样就完成了。 暂存(Staging) 我需要把暂存的内容添加到上一次的提交(commit) (my-branch*)$ git commit --amend 我想要暂存一个新文件的一部分，而不是这个文件的全部 一般来说, 如果你想暂存一个文件的一部分, 你可这样做: $ git add --patch filename.x -p 简写。这会打开交互模式， 你将能够用 s 选项来分隔提交(commit)； 然而, 如果这个文件是新的, 会没有这个选择， 添加一个新文件时, 这样做: $ git add -N filename.x 然后, 你需要用 e 选项来手动选择需要添加的行，执行 git diff --cached 将会显示哪些行暂存了哪些行只是保存在本地了。 我想把在一个文件里的变化(changes)加到两个提交(commit)里 git add 会把整个文件加入到一个提交. git add -p 允许交互式的选择你想要提交的部分. 我想把暂存的内容变成未暂存，把未暂存的内容暂存起来 这个有点困难， 我能想到的最好的方法是先 stash 未暂存的内容， 然后重置(reset)，再 pop 第一步 stashed 的内容, 最后再 add 它们。 $ git stash -k$ git reset --hard$ git stash pop$ git add -A 未暂存(Unstaged)的内容 我想把未暂存的内容移动到一个新分支 $ git checkout -b my-branch 我想把未暂存的内容移动到另一个已存在的分支 $ git stash$ git checkout my-branch$ git stash pop 我想丢弃本地未提交的变化(uncommitted changes) 如果你只是想重置源(origin)和你本地(local)之间的一些提交(commit)，你可以： ## one commit(my-branch)$ git reset --hard HEAD^## two commits(my-branch)$ git reset --hard HEAD^^## four commits(my-branch)$ git reset --hard HEAD~4## or(master)$ git checkout -f 重置某个特殊的文件, 你可以用文件名做为参数: $ git reset filename 我想丢弃某些未暂存的内容 如果你想丢弃工作拷贝中的一部分内容，而不是全部。 签出(checkout)不需要的内容，保留需要的。 $ git checkout -p## Answer y to all of the snippets you want to drop 另外一个方法是使用 stash， Stash 所有要保留下的内容, 重置工作拷贝, 重新应用保留的部分。 $ git stash -p## Select all of the snippets you want to save$ git reset --hard$ git stash pop 或者, stash 你不需要的部分, 然后 stash drop。 $ git stash -p## Select all of the snippets you don't want to save$ git stash drop 分支(Branches) 我从错误的分支拉取了内容，或把内容拉取到了错误的分支 这是另外一种使用 git reflog 情况，找到在这次错误拉(pull) 之前 HEAD 的指向。 (master)$ git reflogab7555f HEAD@&#123;0&#125;: pull origin wrong-branch: Fast-forwardc5bc55a HEAD@&#123;1&#125;: checkout: checkout message goes here 重置分支到你所需的提交(desired commit): $ git reset --hard c5bc55a 完成。 我想扔掉本地的提交(commit)，以便我的分支与远程的保持一致 先确认你没有推(push)你的内容到远程。 git status 会显示你领先(ahead)源(origin)多少个提交: (my-branch)$ git status## On branch my-branch## Your branch is ahead of 'origin/my-branch' by 2 commits.## (use "git push" to publish your local commits)# 一种方法是: (master)$ git reset --hard origin/my-branch 我需要提交到一个新分支，但错误的提交到了 master 在 master 下创建一个新分支，不切换到新分支,仍在 master 下: (master)$ git branch my-branch 把 master 分支重置到前一个提交: (master)$ git reset --hard HEAD^ HEAD^ 是 HEAD^1 的简写，你可以通过指定要设置的HEAD来进一步重置。 或者, 如果你不想使用 HEAD^, 找到你想重置到的提交(commit)的 hash(git log 能够完成)， 然后重置到这个 hash。 使用git push 同步内容到远程。 例如, master 分支想重置到的提交的 hash 为a13b85e: (master)$ git reset --hard a13b85eHEAD is now at a13b85e 签出(checkout)刚才新建的分支继续工作: (master)$ git checkout my-branch 我想保留来自另外一个 ref-ish 的整个文件 假设你正在做一个原型方案(原文为 working spike (see note)), 有成百的内容，每个都工作得很好。现在, 你提交到了一个分支，保存工作内容: (solution)$ git add -A &amp;&amp; git commit -m "Adding all changes from this spike into one big commit." 当你想要把它放到一个分支里 (可能是feature, 或者 develop), 你关心是保持整个文件的完整，你想要一个大的提交分隔成比较小。 假设你有: 分支 solution, 拥有原型方案， 领先 develop 分支。 分支 develop, 在这里你应用原型方案的一些内容。 我去可以通过把内容拿到你的分支里，来解决这个问题: (develop)$ git checkout solution -- file1.txt 这会把这个文件内容从分支 solution 拿到分支 develop 里来: ## On branch develop## Your branch is up-to-date with 'origin/develop'.## Changes to be committed:## (use "git reset HEAD &lt;file&gt;..." to unstage)### modified: file1.txt 然后, 正常提交。 Note: Spike solutions are made to analyze or solve the problem. These solutions are used for estimation and discarded once everyone gets clear visualization of the problem. ~ Wikipedia. 我把几个提交(commit)提交到了同一个分支，而这些提交应该分布在不同的分支里 假设你有一个master分支， 执行git log, 你看到你做过两次提交: (master)$ git logcommit e3851e817c451cc36f2e6f3049db528415e3c114Author: Alex Lee &lt;alexlee@example.com&gt;Date: Tue Jul 22 15:39:27 2014 -0400 Bug #21 - Added CSRF protectioncommit 5ea51731d150f7ddc4a365437931cd8be3bf3131Author: Alex Lee &lt;alexlee@example.com&gt;Date: Tue Jul 22 15:39:12 2014 -0400 Bug #14 - Fixed spacing on titlecommit a13b85e984171c6e2a1729bb061994525f626d14Author: Aki Rose &lt;akirose@example.com&gt;Date: Tue Jul 21 01:12:48 2014 -0400 First commit 让我们用提交 hash(commit hash)标记 bug (e3851e8 for #21, 5ea5173 for #14). 首先, 我们把master分支重置到正确的提交(a13b85e): (master)$ git reset --hard a13b85eHEAD is now at a13b85e 现在, 我们对 bug #21 创建一个新的分支: (master)$ git checkout -b 21(21)$ 接着, 我们用 cherry-pick 把对 bug #21 的提交放入当前分支。 这意味着我们将应用(apply)这个提交(commit)，仅仅这一个提交(commit)，直接在 HEAD 上面。 (21)$ git cherry-pick e3851e8 这时候, 这里可能会产生冲突， 参见交互式 rebasing 章 冲突节 解决冲突. 再者， 我们为 bug #14 创建一个新的分支, 也基于master分支 (21)$ git checkout master(master)$ git checkout -b 14(14)$ 最后, 为 bug #14 执行 cherry-pick: (14)$ git cherry-pick 5ea5173 我想删除上游(upstream)分支被删除了的本地分支 一旦你在 github 上面合并(merge)了一个 pull request, 你就可以删除你 fork 里被合并的分支。 如果你不准备继续在这个分支里工作, 删除这个分支的本地拷贝会更干净，使你不会陷入工作分支和一堆陈旧分支的混乱之中。 $ git fetch -p 我不小心删除了我的分支 如果你定期推送到远程, 多数情况下应该是安全的，但有些时候还是可能删除了还没有推到远程的分支。 让我们先创建一个分支和一个新的文件: (master)$ git checkout -b my-branch(my-branch)$ git branch(my-branch)$ touch foo.txt(my-branch)$ lsREADME.md foo.txt 添加文件并做一次提交 (my-branch)$ git add .(my-branch)$ git commit -m 'foo.txt added'(my-branch)$ foo.txt added 1 files changed, 1 insertions(+) create mode 100644 foo.txt(my-branch)$ git logcommit 4e3cd85a670ced7cc17a2b5d8d3d809ac88d5012Author: siemiatj &lt;siemiatj@example.com&gt;Date: Wed Jul 30 00:34:10 2014 +0200 foo.txt addedcommit 69204cdf0acbab201619d95ad8295928e7f411d5Author: Kate Hudson &lt;katehudson@example.com&gt;Date: Tue Jul 29 13:14:46 2014 -0400 Fixes #6: Force pushing after amending commits 现在我们切回到主(master)分支，‘不小心的’删除my-branch分支 (my-branch)$ git checkout masterSwitched to branch 'master'Your branch is up-to-date with 'origin/master'.(master)$ git branch -D my-branchDeleted branch my-branch (was 4e3cd85).(master)$ echo oh noes, deleted my branch!oh noes, deleted my branch! 在这时候你应该想起了reflog, 一个升级版的日志，它存储了仓库(repo)里面所有动作的历史。 (master)$ git reflog69204cd HEAD@&#123;0&#125;: checkout: moving from my-branch to master4e3cd85 HEAD@&#123;1&#125;: commit: foo.txt added69204cd HEAD@&#123;2&#125;: checkout: moving from master to my-branch 正如你所见，我们有一个来自删除分支的提交 hash(commit hash)，接下来看看是否能恢复删除了的分支。 (master)$ git checkout -b my-branch-helpSwitched to a new branch 'my-branch-help'(my-branch-help)$ git reset --hard 4e3cd85HEAD is now at 4e3cd85 foo.txt added(my-branch-help)$ lsREADME.md foo.txt 看! 我们把删除的文件找回来了。 Git 的 reflog 在 rebasing 出错的时候也是同样有用的。 我想删除一个分支 删除一个远程分支: (master)$ git push origin --delete my-branch 你也可以: (master)$ git push origin :my-branch 删除一个本地分支: (master)$ git branch -D my-branch 我想从别人正在工作的远程分支签出(checkout)一个分支 首先, 从远程拉取(fetch) 所有分支: (master)$ git fetch --all 假设你想要从远程的daves分支签出到本地的daves (master)$ git checkout --track origin/davesBranch daves set up to track remote branch daves from origin.Switched to a new branch 'daves' (--track 是 git checkout -b [branch] [remotename]/[branch] 的简写) 这样就得到了一个daves分支的本地拷贝, 任何推过(pushed)的更新，远程都能看到. Rebasing 和合并(Merging) 我想撤销 rebase/merge 你可以合并(merge)或 rebase 了一个错误的分支, 或者完成不了一个进行中的 rebase/merge。 Git 在进行危险操作的时候会把原始的 HEAD 保存在一个叫 ORIG_HEAD 的变量里, 所以要把分支恢复到 rebase/merge 前的状态是很容易的。 (my-branch)$ git reset --hard ORIG_HEAD 我已经 rebase 过, 但是我不想强推(force push) 不幸的是，如果你想把这些变化(changes)反应到远程分支上，你就必须得强推(force push)。 是因你快进(Fast forward)了提交，改变了 Git 历史, 远程分支不会接受变化(changes)，除非强推(force push)。这就是许多人使用 merge 工作流, 而不是 rebasing 工作流的主要原因之一， 开发者的强推(force push)会使大的团队陷入麻烦。使用时需要注意，一种安全使用 rebase 的方法是，不要把你的变化(changes)反映到远程分支上, 而是按下面的做: (master)$ git checkout my-branch(my-branch)$ git rebase -i master(my-branch)$ git checkout master(master)$ git merge --ff-only my-branch 更多, 参见 this SO thread. 我需要组合(combine)几个提交(commit) 假设你的工作分支将会做对于 master 的 pull-request。 一般情况下你不关心提交(commit)的时间戳，只想组合 所有 提交(commit) 到一个单独的里面, 然后重置(reset)重提交(recommit)。 确保主(master)分支是最新的和你的变化都已经提交了, 然后: (my-branch)$ git reset --soft master(my-branch)$ git commit -am "New awesome feature" 如果你想要更多的控制, 想要保留时间戳, 你需要做交互式 rebase (interactive rebase): (my-branch)$ git rebase -i master 如果没有相对的其它分支， 你将不得不相对自己的HEAD 进行 rebase。 例如：你想组合最近的两次提交(commit), 你将相对于HEAD\~2 进行 rebase， 组合最近 3 次提交(commit), 相对于HEAD\~3, 等等。 (master)$ git rebase -i HEAD~2 在你执行了交互式 rebase 的命令(interactive rebase command)后, 你将在你的编辑器里看到类似下面的内容: pick a9c8a1d Some refactoringpick 01b2fd8 New awesome featurepick b729ad5 fixuppick e3851e8 another fix## Rebase 8074d12..b729ad5 onto 8074d12### Commands:## p, pick = use commit## r, reword = use commit, but edit the commit message## e, edit = use commit, but stop for amending## s, squash = use commit, but meld into previous commit## f, fixup = like "squash", but discard this commit's log message## x, exec = run command (the rest of the line) using shell### These lines can be re-ordered; they are executed from top to bottom.### If you remove a line here THAT COMMIT WILL BE LOST.### However, if you remove everything, the rebase will be aborted.### Note that empty commits are commented out 所有以 # 开头的行都是注释, 不会影响 rebase. 然后，你可以用任何上面命令列表的命令替换 pick, 你也可以通过删除对应的行来删除一个提交(commit)。 例如, 如果你想 单独保留最旧(first)的提交(commit),组合所有剩下的到第二个里面, 你就应该编辑第二个提交(commit)后面的每个提交(commit) 前的单词为 f: pick a9c8a1d Some refactoringpick 01b2fd8 New awesome featuref b729ad5 fixupf e3851e8 another fix 如果你想组合这些提交(commit) 并重命名这个提交(commit), 你应该在第二个提交(commit)旁边添加一个r，或者更简单的用s 替代 f: pick a9c8a1d Some refactoringpick 01b2fd8 New awesome features b729ad5 fixups e3851e8 another fix 你可以在接下来弹出的文本提示框里重命名提交(commit)。 Newer, awesomer features## Please enter the commit message for your changes. Lines starting## with '#' will be ignored, and an empty message aborts the commit.## rebase in progress; onto 8074d12## You are currently editing a commit while rebasing branch 'master' on '8074d12'.### Changes to be committed:# modified: README.md# 如果成功了, 你应该看到类似下面的内容: (master)$ Successfully rebased and updated refs/heads/master. 安全合并(merging)策略 --no-commit 执行合并(merge)但不自动提交, 给用户在做提交前检查和修改的机会。 no-ff 会为特性分支(feature branch)的存在过留下证据, 保持项目历史一致。 (master)$ git merge --no-ff --no-commit my-branch 我需要将一个分支合并成一个提交(commit) (master)$ git merge --squash my-branch 我只想组合(combine)未推的提交(unpushed commit) 有时候，在将数据推向上游之前，你有几个正在进行的工作提交(commit)。这时候不希望把已经推(push)过的组合进来，因为其他人可能已经有提交(commit)引用它们了。 (master)$ git rebase -i @&#123;u&#125; 这会产生一次交互式的 rebase(interactive rebase), 只会列出没有推(push)的提交(commit)， 在这个列表时进行 reorder/fix/squash 都是安全的。 检查是否分支上的所有提交(commit)都合并(merge)过了 检查一个分支上的所有提交(commit)是否都已经合并(merge)到了其它分支, 你应该在这些分支的 head(或任何 commits)之间做一次 diff: (master)$ git log --graph --left-right --cherry-pick --oneline HEAD...feature/120-on-scroll 这会告诉你在一个分支里有而另一个分支没有的所有提交(commit), 和分支之间不共享的提交(commit)的列表。 另一个做法可以是: (master)$ git log master ^feature/120-on-scroll --no-merges 交互式 rebase(interactive rebase)可能出现的问题 这个 rebase 编辑屏幕出现’noop’ 如果你看到的是这样: noop 这意味着你 rebase 的分支和当前分支在同一个提交(commit)上, 或者 领先(ahead) 当前分支。 你可以尝试: 检查确保主(master)分支没有问题 rebase HEAD\~2 或者更早 有冲突的情况 如果你不能成功的完成 rebase, 你可能必须要解决冲突。 首先执行 git status 找出哪些文件有冲突: (my-branch)$ git statusOn branch my-branchChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: README.md 在这个例子里面, README.md 有冲突。 打开这个文件找到类似下面的内容: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADsome code=========some code&gt;&gt;&gt;&gt;&gt;&gt;&gt; new-commit 你需要解决新提交的代码(示例里, 从中间==线到new-commit的地方)与HEAD 之间不一样的地方. 有时候这些合并非常复杂，你应该使用可视化的差异编辑器(visual diff editor): (master*)$ git mergetool -t opendiff 在你解决完所有冲突和测试过后, git add 变化了的(changed)文件, 然后用git rebase --continue 继续 rebase。 (my-branch)$ git add README.md(my-branch)$ git rebase --continue 如果在解决完所有的冲突过后，得到了与提交前一样的结果, 可以执行git rebase --skip。 任何时候你想结束整个 rebase 过程，回来 rebase 前的分支状态, 你可以做: (my-branch)$ git rebase --abort 杂项(Miscellaneous Objects) 克隆所有子模块 $ git clone --recursive git://github.com/foo/bar.git 如果已经克隆了: $ git submodule update --init --recursive 删除标签(tag) $ git tag -d &lt;tag_name&gt;$ git push &lt;remote&gt; :refs/tags/&lt;tag_name&gt; 恢复已删除标签(tag) 如果你想恢复一个已删除标签(tag), 可以按照下面的步骤: 首先, 需要找到无法访问的标签(unreachable tag): $ git fsck --unreachable | grep tag 记下这个标签(tag)的 hash，然后用 Git 的 update-ref: $ git update-ref refs/tags/&lt;tag_name&gt; &lt;hash&gt; 这时你的标签(tag)应该已经恢复了。 已删除补丁(patch) 如果某人在 GitHub 上给你发了一个 pull request, 但是然后他删除了他自己的原始 fork, 你将没法克隆他们的提交(commit)或使用 git am。在这种情况下, 最好手动的查看他们的提交(commit)，并把它们拷贝到一个本地新分支，然后做提交。 做完提交后, 再修改作者，参见变更作者。 然后, 应用变化, 再发起一个新的 pull request。 跟踪文件(Tracking Files) 我只想改变一个文件名字的大小写，而不修改内容 (master)$ git mv --force myfile MyFile 我想从 Git 删除一个文件，但保留该文件 (master)$ git rm --cached log.txt 配置(Configuration) 我想给一些 Git 命令添加别名(alias) 在 OS X 和 Linux 下, 你的 Git 的配置文件储存在 \~/.gitconfig。我在[alias] 部分添加了一些快捷别名(和一些我容易拼写错误的)，如下: [alias] a = add amend = commit --amend c = commit ca = commit --amend ci = commit -a co = checkout d = diff dc = diff --changed ds = diff --staged f = fetch loll = log --graph --decorate --pretty=oneline --abbrev-commit m = merge one = log --pretty=oneline outstanding = rebase -i @&#123;u&#125; s = status unpushed = log @&#123;u&#125; wc = whatchanged wip = rebase -i @&#123;u&#125; zap = fetch -p 我想缓存一个仓库(repository)的用户名和密码 你可能有一个仓库需要授权，这时你可以缓存用户名和密码，而不用每次推/拉(push/pull)的时候都输入，Credential helper 能帮你。 $ git config --global credential.helper cache## Set git to use the credential memory cache $ git config --global credential.helper 'cache --timeout=3600'## Set the cache to timeout after 1 hour (setting is in seconds) 我不知道我做错了些什么 你把事情搞砸了：你 重置(reset) 了一些东西, 或者你合并了错误的分支, 亦或你强推了后找不到你自己的提交(commit)了。有些时候, 你一直都做得很好, 但你想回到以前的某个状态。 这就是 git reflog 的目的， reflog 记录对分支顶端(the tip of a branch)的任何改变, 即使那个顶端没有被任何分支或标签引用。基本上, 每次 HEAD 的改变, 一条新的记录就会增加到reflog。遗憾的是，这只对本地分支起作用，且它只跟踪动作 (例如，不会跟踪一个没有被记录的文件的任何改变)。 (master)$ git reflog0a2e358 HEAD@&#123;0&#125;: reset: moving to HEAD\~20254ea7 HEAD@&#123;1&#125;: checkout: moving from 2.2 to masterc10f740 HEAD@&#123;2&#125;: checkout: moving from master to 2.2 上面的 reflog 展示了从 master 分支签出(checkout)到 2.2 分支，然后再签回。 那里，还有一个硬重置(hard reset)到一个较旧的提交。最新的动作出现在最上面以 HEAD@{0}标识. 如果事实证明你不小心回移(move back)了提交(commit), reflog 会包含你不小心回移前 master 上指向的提交(0254ea7)。 $ git reset --hard 0254ea7 然后使用 git reset 就可以把 master 改回到之前的 commit，这提供了一个在历史被意外更改情况下的安全网。 小结 最后，放一张我总结的脑图总结一下以上的知识点。 参考资料 官方资源 Git 官网 Git Github 模板 gitignore 模板 - .gitignore 文件模板 gitattributes 模板 - .gitattributes 文件模板 github-cheat-sheet - git 命令简略图表 Git 书 Git 官方推荐教程 - Scott Chacon 的 Git 书。 Git 教程 Git 中文教程 廖雪峰的 Git 教程 有关 git 的学习资源 文章 Git Cookbook Git 奇技淫巧 Git 风格指南 Git 在团队中的最佳实践–如何正确使用 Git Flow Git 工具 guis - Git 官网展示的客户端工具列表。 gogs - 极易搭建的自助 Git 服务。 gitflow - 应用 fit-flow 模型的工具。 firstaidgit.io 一个可搜索的最常被问到的 Git 的问题 git-extra-commands - 一堆有用的额外的 Git 脚本 git-extras - GIT 工具集 – repo summary, repl, changelog population, author commit percentages and more git-fire - git-fire 是一个 Git 插件，用于帮助在紧急情况下添加所有当前文件, 做提交(committing), 和推(push)到一个新分支(阻止合并冲突)。 git-tips - Git 小提示 git-town - 通用，高级 Git 工作流支持！ http://www.git-town.com GUI 客户端(GUI Clients) GitKraken - 豪华的 Git 客户端 Windows, Mac &amp; Linux git-cola - 另外一个 Git 客户端 Windows &amp; OS X GitUp - 一个新的 Git 客户端，在处理 Git 的复杂性上有自己的特点 gitx-dev - 图形化的 Git 客户端 OS X Source Tree - 免费的图形化 Git 客户端 Windows &amp; OS X Tower - 图形化 Git 客户端 OS X(付费) git cheat sheet github-git-cheat-sheet]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic 技术栈之快速入门]]></title>
    <url>%2Fblog%2F2017%2F12%2F06%2Fos%2Flinux%2Fops%2Fservice%2Felastic%2Felastic-quickstart%2F</url>
    <content type="text"><![CDATA[Elastic 技术栈之快速入门 概念 ELK 是什么 ELK 是 elastic 公司旗下三款产品 ElasticSearch 、Logstash 、Kibana 的首字母组合。 ElasticSearch 是一个基于 Lucene 构建的开源，分布式，RESTful 搜索引擎。 Logstash 传输和处理你的日志、事务或其他数据。 Kibana 将 Elasticsearch 的数据分析并渲染为可视化的报表。 为什么使用 ELK ？ 对于有一定规模的公司来说，通常会很多个应用，并部署在大量的服务器上。运维和开发人员常常需要通过查看日志来定位问题。如果应用是集群化部署，试想如果登录一台台服务器去查看日志，是多么费时费力。 而通过 ELK 这套解决方案，可以同时实现日志收集、日志搜索和日志分析的功能。 Elastic 架构 说明 以上是 ELK 技术栈的一个架构图。从图中可以清楚的看到数据流向。 Beats 是单一用途的数据传输平台，它可以将多台机器的数据发送到 Logstash 或 ElasticSearch。但 Beats 并不是不可或缺的一环，所以本文中暂不介绍。 Logstash 是一个动态数据收集管道。支持以 TCP/UDP/HTTP 多种方式收集数据（也可以接受 Beats 传输来的数据），并对数据做进一步丰富或提取字段处理。 ElasticSearch 是一个基于 JSON 的分布式的搜索和分析引擎。作为 ELK 的核心，它集中存储数据。 Kibana 是 ELK 的用户界面。它将收集的数据进行可视化展示（各种报表、图形化数据），并提供配置、管理 ELK 的界面。 安装 准备 ELK 要求本地环境中安装了 JDK 。如果不确定是否已安装，可使用下面的命令检查： java -version 注意 本文使用的 ELK 是 6.0.0，要求 jdk 版本不低于 JDK8。 友情提示：安装 ELK 时，三个应用请选择统一的版本，避免出现一些莫名其妙的问题。例如：由于版本不统一，导致三个应用间的通讯异常。 Elasticsearch 安装步骤如下： elasticsearch 官方下载地址下载所需版本包并解压到本地。 运行 bin/elasticsearch （Windows 上运行 bin\elasticsearch.bat） 验证运行成功：linux 上可以执行 curl http://localhost:9200/ ；windows 上可以用访问 REST 接口的方式来访问 http://localhost:9200/ 说明 Linux 上可以执行下面的命令来下载压缩包： &gt; curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.0.0.tar.gz&gt; Mac 上可以执行以下命令来进行安装： &gt; brew install elasticsearch&gt; Windows 上可以选择 MSI 可执行安装程序，将应用安装到本地。 Logstash 安装步骤如下： 在 logstash 官方下载地址下载所需版本包并解压到本地。 添加一个 logstash.conf 文件，指定要使用的插件以及每个插件的设置。举个简单的例子： input &#123; stdin &#123; &#125; &#125;output &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 运行 bin/logstash -f logstash.conf （Windows 上运行bin/logstash.bat -f logstash.conf） Kibana 安装步骤如下： 在 kibana 官方下载地址下载所需版本包并解压到本地。 修改 config/kibana.yml 配置文件，设置 elasticsearch.url 指向 Elasticsearch 实例。 运行 bin/kibana （Windows 上运行 bin\kibana.bat） 在浏览器上访问 http://localhost:5601 安装 FAQ elasticsearch 不允许以 root 权限来运行 **问题：**在 Linux 环境中，elasticsearch 不允许以 root 权限来运行。 如果以 root 身份运行 elasticsearch，会提示这样的错误： can not run elasticsearch as root **解决方法：**使用非 root 权限账号运行 elasticsearch # 创建用户组groupadd elk# 创建新用户，-g elk 设置其用户组为 elk，-p elk 设置其密码为 elkuseradd elk -g elk -p elk# 更改 /opt 文件夹及内部文件的所属用户及组为 elk:elkchown -R elk:elk /opt # 假设你的 elasticsearch 安装在 opt 目录下# 切换账号su elk vm.max_map_count 不低于 262144 问题：vm.max_map_count 表示虚拟内存大小，它是一个内核参数。elasticsearch 默认要求 vm.max_map_count 不低于 262144。 max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 解决方法： 你可以执行以下命令，设置 vm.max_map_count ，但是重启后又会恢复为原值。 sysctl -w vm.max_map_count=262144 持久性的做法是在 /etc/sysctl.conf 文件中修改 vm.max_map_count 参数： echo "vm.max_map_count=262144" &gt; /etc/sysctl.confsysctl -p 注意 如果运行环境为 docker 容器，可能会限制执行 sysctl 来修改内核参数。 这种情况下，你只能选择直接修改宿主机上的参数了。 nofile 不低于 65536 问题： nofile 表示进程允许打开的最大文件数。elasticsearch 进程要求可以打开的最大文件数不低于 65536。 max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536] 解决方法： 在 /etc/security/limits.conf 文件中修改 nofile 参数： echo "* soft nofile 65536" &gt; /etc/security/limits.confecho "* hard nofile 131072" &gt; /etc/security/limits.conf nproc 不低于 2048 问题： nproc 表示最大线程数。elasticsearch 要求最大线程数不低于 2048。 max number of threads [1024] for user [user] is too low, increase to at least [2048] 解决方法： 在 /etc/security/limits.conf 文件中修改 nproc 参数： echo "* soft nproc 2048" &gt; /etc/security/limits.confecho "* hard nproc 4096" &gt; /etc/security/limits.conf Kibana No Default Index Pattern Warning **问题：**安装 ELK 后，访问 kibana 页面时，提示以下错误信息： Warning No default index pattern. You must select or create one to continue....Unable to fetch mapping. Do you have indices matching the pattern? 这就说明 logstash 没有把日志写入到 elasticsearch。 解决方法： 检查 logstash 与 elasticsearch 之间的通讯是否有问题，一般问题就出在这。 使用 本人使用的 Java 日志方案为 slf4j + logback，所以这里以 logback 来讲解。 Java 应用输出日志到 ELK 修改 logstash.conf 配置 首先，我们需要修改一下 logstash 服务端 logstash.conf 中的配置 input &#123; # stdin &#123; &#125; tcp &#123; # host:port就是上面appender中的 destination， # 这里其实把logstash作为服务，开启9250端口接收logback发出的消息 host =&gt; "127.0.0.1" port =&gt; 9250 mode =&gt; "server" tags =&gt; ["tags"] codec =&gt; json_lines &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 说明 这个 input 中的配置其实是 logstash 服务端监听 9250 端口，接收传递来的日志数据。 然后，在 Java 应用的 pom.xml 中引入 jar 包： &lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt;&lt;/dependency&gt; 接着，在 logback.xml 中添加 appender &lt;appender name="LOGSTASH" class="net.logstash.logback.appender.LogstashTcpSocketAppender"&gt; &lt;!-- destination 是 logstash 服务的 host:port， 相当于和 logstash 建立了管道，将日志数据定向传输到 logstash --&gt; &lt;destination&gt;127.0.0.1:9250&lt;/destination&gt; &lt;encoder charset="UTF-8" class="net.logstash.logback.encoder.LogstashEncoder"/&gt;&lt;/appender&gt;&lt;logger name="io.github.dunwu.spring" level="TRACE" additivity="false"&gt; &lt;appender-ref ref="LOGSTASH" /&gt;&lt;/logger&gt; 大功告成，此后，io.github.dunwu.spring 包中的 TRACE 及以上级别的日志信息都会被定向输出到 logstash 服务。 资料 elastic 官方文档 elasticsearch github logstash github kibana github]]></content>
      <categories>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>log</tag>
        <tag>elastic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池]]></title>
    <url>%2Fblog%2F2017%2F12%2F05%2Fjava%2Fjavacore%2Fconcurrent%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[线程池 📓 本文已归档到：「blog」 概述 什么是线程池？ 为什么要用线程池？ Executor 框架 简介 ThreadPoolExecutor Executors 源码 线程池状态 任务的执行 线程池中的线程初始化 任务缓存队列及排队策略 任务拒绝策略 线程池的关闭 线程池容量的动态调整 资料 概述 什么是线程池？ 线程池是一种多线程处理形式，处理过程中将任务添加到队列，然后在创建线程后自动启动这些任务。 为什么要用线程池？ 降低资源消耗 通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度 当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性 线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。但是要做到合理的利用线程池，必须对其原理了如指掌。 Executor 框架 简介 Executor：一个接口，其定义了一个接收 Runnable 对象的方法 executor，其方法签名为 executor(Runnable command), ExecutorService：是一个比 Executor 使用更广泛的子类接口，其提供了生命周期管理的方法，以及可跟踪一个或多个异步任务执行状况返回 Future 的方法。 AbstractExecutorService：ExecutorService 执行方法的默认实现。 ScheduledExecutorService：一个可定时调度任务的接口。 ScheduledThreadPoolExecutor：ScheduledExecutorService 的实现，一个可定时调度任务的线程池。 ThreadPoolExecutor：线程池，可以通过调用 Executors 以下静态工厂方法来创建线程池并返回一个 ExecutorService 对象。 ThreadPoolExecutor java.uitl.concurrent.ThreadPoolExecutor 类是 Executor 框架中最核心的一个类。 ThreadPoolExecutor 有四个构造方法，前三个都是基于第四个实现。第四个构造方法定义如下： public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; 参数说明 corePoolSize：线程池的基本线程数。这个参数跟后面讲述的线程池的实现原理有非常大的关系。在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了 prestartAllCoreThreads()或者 prestartCoreThread()方法，从这 2 个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建 corePoolSize 个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为 0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到 corePoolSize 后，就会把到达的任务放到缓存队列当中。 maximumPoolSize：线程池允许创建的最大线程数。如果队列满了，并且已创建的线程数小于最大线程数，则线程池会再创建新的线程执行任务。值得注意的是如果使用了无界的任务队列这个参数就没什么效果。 keepAliveTime：线程活动保持时间。线程池的工作线程空闲后，保持存活的时间。所以如果任务很多，并且每个任务执行的时间比较短，可以调大这个时间，提高线程的利用率。 unit：参数 keepAliveTime 的时间单位，有 7 种取值。可选的单位有天（DAYS），小时（HOURS），分钟（MINUTES），毫秒(MILLISECONDS)，微秒(MICROSECONDS, 千分之一毫秒)和毫微秒(NANOSECONDS, 千分之一微秒)。 workQueue：任务队列。用于保存等待执行的任务的阻塞队列。 可以选择以下几个阻塞队列。 ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按 FIFO（先进先出）原则对元素进行排序。 LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按 FIFO （先进先出） 排序元素，吞吐量通常要高于 ArrayBlockingQueue。静态工厂方法 Executors.newFixedThreadPool()使用了这个队列。 SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于 LinkedBlockingQueue，静态工厂方法 Executors.newCachedThreadPool 使用了这个队列。 PriorityBlockingQueue：一个具有优先级的无限阻塞队列。 threadFactory：创建线程的工厂。可以通过线程工厂给每个创建出来的线程设置更有意义的名字。 handler：饱和策略。当队列和线程池都满了，说明线程池处于饱和状态，那么必须采取一种策略处理提交的新任务。这个策略默认情况下是 AbortPolicy，表示无法处理新任务时抛出异常。以下是 JDK1.5 提供的四种策略。 AbortPolicy：直接抛出异常。 CallerRunsPolicy：只用调用者所在线程来运行任务。 DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务。 DiscardPolicy：不处理，丢弃掉。 当然也可以根据应用场景需要来实现 RejectedExecutionHandler 接口自定义策略。如记录日志或持久化不能处理的任务。 重要方法 在 ThreadPoolExecutor 类中有几个非常重要的方法： execute() 方法实际上是 Executor 中声明的方法，在 ThreadPoolExecutor 进行了具体的实现，这个方法是 ThreadPoolExecutor 的核心方法，通过这个方法可以向线程池提交一个任务，交由线程池去执行。 submit() 方法是在 ExecutorService 中声明的方法，在 AbstractExecutorService 就已经有了具体的实现，在 ThreadPoolExecutor 中并没有对其进行重写，这个方法也是用来向线程池提交任务的，但是它和 execute()方法不同，它能够返回任务执行的结果，去看 submit()方法的实现，会发现它实际上还是调用的 execute()方法，只不过它利用了 Future 来获取任务执行结果（Future 相关内容将在下一篇讲述）。 shutdown() 和 shutdownNow() 是用来关闭线程池的。 向线程池提交任务 我们可以使用 execute 提交任务，但是 execute 方法没有返回值，所以无法判断任务是否被线程池执行成功。 通过以下代码可知 execute 方法输入的任务是一个 Runnable 实例。 threadsPool.execute(new Runnable() &#123; @Override public void run() &#123; // TODO Auto-generated method stub &#125; &#125;); 我们也可以使用 submit 方法来提交任务，它会返回一个 Future ，那么我们可以通过这个 Future 来判断任务是否执行成功。 通过 Future 的 get 方法来获取返回值，get 方法会阻塞住直到任务完成。而使用 get(long timeout, TimeUnit unit) 方法则会阻塞一段时间后立即返回，这时有可能任务没有执行完。 Future&lt;Object&gt; future = executor.submit(harReturnValuetask);try &#123; Object s = future.get();&#125; catch (InterruptedException e) &#123; // 处理中断异常&#125; catch (ExecutionException e) &#123; // 处理无法执行任务异常&#125; finally &#123; // 关闭线程池 executor.shutdown();&#125; 线程池的关闭 我们可以通过调用线程池的 shutdown 或 shutdownNow 方法来关闭线程池，它们的原理是遍历线程池中的工作线程，然后逐个调用线程的 interrupt 方法来中断线程，所以无法响应中断的任务可能永远无法终止。但是它们存在一定的区别，shutdownNow 首先将线程池的状态设置成 STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表，而 shutdown 只是将线程池的状态设置成 SHUTDOWN 状态，然后中断所有没有正在执行任务的线程。 只要调用了这两个关闭方法的其中一个，isShutdown 方法就会返回 true。当所有的任务都已关闭后,才表示线程池关闭成功，这时调用 isTerminaed 方法会返回 true。至于我们应该调用哪一种方法来关闭线程池，应该由提交到线程池的任务特性决定，通常调用 shutdown 来关闭线程池，如果任务不一定要执行完，则可以调用 shutdownNow。 Executors JDK 中提供了几种具有代表性的线程池，这些线程池是基于 ThreadPoolExecutor 的定制化实现。 在实际使用线程池的场景中，我们往往不是直接使用 ThreadPoolExecutor ，而是使用 JDK 中提供的具有代表性的线程池实例。 newCachedThreadPool 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 这种类型的线程池特点是： 工作线程的创建数量几乎没有限制（其实也有限制的,数目为 Interger.MAX_VALUE）, 这样可灵活的往线程池中添加线程。 如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间（默认为 1 分钟），则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。 在使用 CachedThreadPool 时，一定要注意控制任务的数量，否则，由于大量线程同时运行，很有会造成系统瘫痪。 示例： public class CachedThreadPoolDemo &#123; public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; try &#123; Thread.sleep(index * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; executorService.execute(() -&gt; System.out.println(Thread.currentThread().getName() + " 执行，i = " + index)); &#125; &#125;&#125; newFixedThreadPool 创建一个指定工作线程数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。 FixedThreadPool 是一个典型且优秀的线程池，它具有线程池提高程序效率和节省创建线程时所耗的开销的优点。但是，在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。 示例： public class FixedThreadPoolDemo &#123; public static void main(String[] args) &#123; ExecutorService executorService = Executors.newFixedThreadPool(3); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; executorService.execute(() -&gt; &#123; try &#123; System.out.println(Thread.currentThread().getName() + " 执行，i = " + index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; &#125;&#125; newSingleThreadExecutor 创建一个单线程化的 Executor，即只创建唯一的工作者线程来执行任务，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。如果这个线程异常结束，会有另一个取代它，保证顺序执行。单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。 示例： public class SingleThreadExecutorDemo &#123; public static void main(String[] args) &#123; ExecutorService executorService = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; executorService.execute(() -&gt; &#123; try &#123; System.out.println(Thread.currentThread().getName() + " 执行，i = " + index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; &#125;&#125; newScheduleThreadPool 创建一个线程池，可以安排任务在给定延迟后运行，或定期执行。 public class ScheduledThreadPoolDemo &#123; private static void delay() &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.schedule(() -&gt; System.out.println(Thread.currentThread().getName() + " 延迟 3 秒"), 3, TimeUnit.SECONDS); &#125; private static void cycle() &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.scheduleAtFixedRate( () -&gt; System.out.println(Thread.currentThread().getName() + " 延迟 1 秒，每 3 秒执行一次"), 1, 3, TimeUnit.SECONDS); &#125; public static void main(String[] args) &#123; delay(); cycle(); &#125;&#125; 源码 线程池的具体实现原理，大致从以下几个方面讲解： 线程池状态 任务的执行 线程池中的线程初始化 任务缓存队列及排队策略 任务拒绝策略 线程池的关闭 线程池容量的动态调整 线程池状态 // runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// Packing and unpacking ctlprivate static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125; runState 表示当前线程池的状态，它是一个 volatile 变量用来保证线程之间的可见性； 下面的几个 static final 变量表示 runState 可能的几个取值。 当创建线程池后，初始时，线程池处于 RUNNING 状态； RUNNING -&gt; SHUTDOWN 如果调用了 shutdown()方法，则线程池处于 SHUTDOWN 状态，此时线程池不能够接受新的任务，它会等待所有任务执行完毕。 (RUNNING or SHUTDOWN) -&gt; STOP 如果调用了 shutdownNow()方法，则线程池处于 STOP 状态，此时线程池不能接受新的任务，并且会去尝试终止正在执行的任务。 SHUTDOWN -&gt; TIDYING 当线程池和队列都为空时，则线程池处于 TIDYING 状态。 STOP -&gt; TIDYING 当线程池为空时，则线程池处于 TIDYING 状态。 TIDYING -&gt; TERMINATED 当 terminated() 回调方法完成时，线程池处于 TERMINATED 状态。 任务的执行 任务执行的核心方法是 execute() 方法。执行步骤如下： 如果少于 corePoolSize 个线程正在运行，尝试使用给定命令作为第一个任务启动一个新线程。对 addWorker 的调用会自动检查 runState 和 workerCount，从而防止在不应该的情况下添加线程。 如果任务排队成功，仍然需要仔细检查是否应该添加一个线程（因为现有的线程自上次检查以来已经死亡）或者自从进入方法后，线程池就关闭了。所以我们重新检查状态，如果有必要的话，在线程池停止状态时回滚队列，如果没有线程的话，就开始一个新的线程。 如果任务排队失败，那么我们尝试添加一个新的线程。如果失败了，说明线程池已经关闭了，或者已经饱和了，所以拒绝这个任务。 public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; else if (!addWorker(command, false)) reject(command);&#125; 线程池中的线程初始化 默认情况下，创建线程池之后，线程池中是没有线程的，需要提交任务之后才会创建线程。 在实际中如果需要线程池创建之后立即创建线程，可以通过以下两个方法办到： prestartCoreThread()：初始化一个核心线程； prestartAllCoreThreads()：初始化所有核心线程 public boolean prestartCoreThread() &#123; return addIfUnderCorePoolSize(null); //注意传进去的参数是null&#125;public int prestartAllCoreThreads() &#123; int n = 0; while (addIfUnderCorePoolSize(null))//注意传进去的参数是null ++n; return n;&#125; 任务缓存队列及排队策略 在前面我们多次提到了任务缓存队列，即 workQueue，它用来存放等待执行的任务。 workQueue 的类型为 BlockingQueue，通常可以取下面三种类型： ArrayBlockingQueue：基于数组的先进先出队列，此队列创建时必须指定大小； LinkedBlockingQueue：基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为 Integer.MAX_VALUE； SynchronousQueue：这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务。 任务拒绝策略 当线程池的任务缓存队列已满并且线程池中的线程数目达到 maximumPoolSize，如果还有任务到来就会采取任务拒绝策略，通常有以下四种策略 ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出 RejectedExecutionException 异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 线程池的关闭 ThreadPoolExecutor 提供了两个方法，用于线程池的关闭，分别是 shutdown()和 shutdownNow()，其中： shutdown()：不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务 shutdownNow()：立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务 线程池容量的动态调整 ThreadPoolExecutor 提供了动态调整线程池容量大小的方法：setCorePoolSize()和 setMaximumPoolSize()， setCorePoolSize：设置核心池大小 setMaximumPoolSize：设置线程池最大能创建的线程数目大小 当上述参数从小变大时，ThreadPoolExecutor 进行线程赋值，还可能立即创建新的线程来执行任务。 资料 Java 并发编程实战 Java 并发编程的艺术 https://www.cnblogs.com/MOBIN/p/5436482.html]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
        <category>concurrent</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>concurrent</tag>
        <tag>ThreadPool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 快速入门]]></title>
    <url>%2Fblog%2F2017%2F12%2F01%2Fdatabase%2Fnosql%2Fredis%2Fredis-quickstart%2F</url>
    <content type="text"><![CDATA[Redis 快速入门 1. 概述 1.1. 什么是 Redis 1.2. 为什么用 Redis 2. 安装 2.1. Window 下安装 2.2. Linux 下安装 2.3. Ubuntu 下安装 2.4. 启动 Redis 2.5. 查看 redis 是否启动？ 3. Redis 命令 1. 概述 1.1. 什么是 Redis Redis 是一个高性能的 key-value 数据库，也可用于缓存和消息代理。 1.2. 为什么用 Redis 与其它 key - value 数据库产品相比，具有以下优势： 支持数据持久化——可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 丰富的数据类型——Redis 支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 支持数据的备份——数据备份采用 master-slave 模式。 性能极高——Redis 能读的速度是 110000 次/s，写的速度是 81000 次/s 。 原子性——Redis 的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性，通过 MULTI 和 EXEC 指令包起来。 2. 安装 2.1. Window 下安装 下载地址：https://github.com/MSOpenTech/redis/releases。 Redis 支持 32 位和 64 位。这个需要根据你系统平台的实际情况选择，这里我们下载 Redis-x64-xxx.zip压缩包到 C 盘，解压后，将文件夹重新命名为 redis。 打开一个 cmd 窗口 使用 cd 命令切换目录到 C:\redis 运行 redis-server.exe redis.windows.conf 。 如果想方便的话，可以把 redis 的路径加到系统的环境变量里，这样就省得再输路径了，后面的那个 redis.windows.conf 可以省略，如果省略，会启用默认的。输入之后，会显示如下界面： 这时候另启一个 cmd 窗口，原来的不要关闭，不然就无法访问服务端了。 切换到 redis 目录下运行 redis-cli.exe -h 127.0.0.1 -p 6379 。 设置键值对 set myKey abc 取出键值对 get myKey 2.2. Linux 下安装 下载地址：http://redis.io/download，下载最新文档版本。 本教程使用的最新文档版本为 2.8.17，下载并安装： $ wget http://download.redis.io/releases/redis-2.8.17.tar.gz$ tar xzf redis-2.8.17.tar.gz$ cd redis-2.8.17$ make make 完后 redis-2.8.17 目录下会出现编译后的 redis 服务程序 redis-server,还有用于测试的客户端程序 redis-cli,两个程序位于安装目录 src 目录下： 下面启动 redis 服务. $ cd src$ ./redis-server 注意这种方式启动 redis 使用的是默认配置。也可以通过启动参数告诉 redis 使用指定配置文件使用下面命令启动。 $ cd src$ ./redis-server redis.conf redis.conf 是一个默认的配置文件。我们可以根据需要使用自己的配置文件。 启动 redis 服务进程后，就可以使用测试客户端程序 redis-cli 和 redis 服务交互了。 比如： $ cd src$ ./redis-cliredis&gt; set foo barOKredis&gt; get foo"bar" 2.3. Ubuntu 下安装 在 Ubuntu 系统安装 Redi 可以使用以下命令: $sudo apt-get update$sudo apt-get install redis-server 2.4. 启动 Redis $ redis-server 2.5. 查看 redis 是否启动？ $ redis-cli 以上命令将打开以下终端： redis 127.0.0.1:6379&gt; 127.0.0.1 是本机 IP ，6379 是 redis 服务端口。现在我们输入 PING 命令。 redis 127.0.0.1:6379&gt; pingPONG 以上说明我们已经成功安装了 redis。 3. Redis 命令 Redis 命令用于在 redis 服务上执行操作。 要在 redis 服务上执行命令，需要先进入 redis 客户端。 进入 redis 客户端的方法： $ redis-cli 远程进入 redis 客户端的方法： $ redis-cli -h host -p port -a password 实例 以下实例演示了如何连接到主机为 127.0.0.1，端口为 6379 ，密码为 pass 的 redis 服务上： $redis-cli -h 127.0.0.1 -p 6379 -a "pass"redis 127.0.0.1:6379&gt;redis 127.0.0.1:6379&gt; PINGPONG 更多命令行可以参考：Redis 官方命令行字典]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>key-value</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一篇文章让你彻底掌握 shell 语言]]></title>
    <url>%2Fblog%2F2017%2F11%2F20%2Fprogramming%2Fshell%2F</url>
    <content type="text"><![CDATA[一篇文章让你彻底掌握 shell 语言 由于 bash 是 Linux 标准默认的 shell 解释器，可以说 bash 是 shell 编程的基础。 本文主要介绍 bash 的语法，对于 linux 指令不做任何介绍。 📓 本文已归档到：「blog」 ⌨️ 本文的源码已归档到 os-tutorial ███████╗██╗ ██╗███████╗██╗ ██╗██╔════╝██║ ██║██╔════╝██║ ██║███████╗███████║█████╗ ██║ ██║╚════██║██╔══██║██╔══╝ ██║ ██║███████║██║ ██║███████╗███████╗███████╗ 1. 简介 1.1. 什么是 shell 1.2. 什么是 shell 脚本 1.3. Shell 环境 1.4. 模式 2. 基本语法 2.1. 解释器 2.2. 注释 2.3. echo 2.4. printf 3. 变量 3.1. 变量命名原则 3.2. 声明变量 3.3. 只读变量 3.4. 删除变量 3.5. 变量类型 3.6. 变量示例源码 4. 字符串 4.1. 单引号和双引号 4.2. 拼接字符串 4.3. 获取字符串长度 4.4. 截取子字符串 4.5. 查找子字符串 4.6. 字符串示例源码 5. 数组 5.1. 创建数组 5.2. 访问数组元素 5.3. 访问数组长度 5.4. 向数组中添加元素 5.5. 从数组中删除元素 5.6. 数组示例源码 6. 运算符 6.1. 算术运算符 6.2. 关系运算符 6.3. 布尔运算符 6.4. 逻辑运算符 6.5. 字符串运算符 6.6. 文件测试运算符 7. 控制语句 7.1. 条件语句 7.2. 循环语句 8. 函数 8.1. 位置参数 8.2. 函数处理参数 9. Shell 扩展 10. 流和重定向 10.1. 输入、输出流 10.2. 重定向 10.3. /dev/null 文件 11. Debug 12. 更多内容 1. 简介 1.1. 什么是 shell Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。 Shell 既是一种命令语言，又是一种程序设计语言。 Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问 Linux 内核的服务。 Ken Thompson 的 sh 是第一种 Unix Shell，Windows Explorer 是一个典型的图形界面 Shell。 1.2. 什么是 shell 脚本 Shell 脚本（shell script），是一种为 shell 编写的脚本程序，一般文件后缀为 .sh。 业界所说的 shell 通常都是指 shell 脚本，但 shell 和 shell script 是两个不同的概念。 1.3. Shell 环境 Shell 编程跟 java、php 编程一样，只要有一个能编写代码的文本编辑器和一个能解释执行的脚本解释器就可以了。 Shell 的解释器种类众多，常见的有： sh - 即 Bourne Shell。sh 是 Unix 标准默认的 shell。 bash - 即 Bourne Again Shell。bash 是 Linux 标准默认的 shell。 fish - 智能和用户友好的命令行 shell。 xiki - 使 shell 控制台更友好，更强大。 zsh - 功能强大的 shell 与脚本语言。 指定脚本解释器 在 shell 脚本，#! 告诉系统其后路径所指定的程序即是解释此脚本文件的 Shell 解释器。#! 被称作shebang（也称为 Hashbang ）。 所以，你应该会在 shell 中，见到诸如以下的注释： 指定 sh 解释器 #!/bin/sh 指定 bash 解释器 #!/bin/bash 注意 上面的指定解释器的方式是比较常见的，但有时候，你可能也会看到下面的方式： &gt; #!/usr/bin/env bash&gt; 这样做的好处是，系统会自动在 PATH 环境变量中查找你指定的程序（本例中的bash）。相比第一种写法，你应该尽量用这种写法，因为程序的路径是不确定的。这样写还有一个好处，操作系统的PATH变量有可能被配置为指向程序的另一个版本。比如，安装完新版本的bash，我们可能将其路径添加到PATH中，来“隐藏”老版本。如果直接用#!/bin/bash，那么系统会选择老版本的bash来执行脚本，如果用#!/usr/bin/env bash，则会使用新版本。 1.4. 模式 shell 有交互和非交互两种模式。 交互模式 简单来说，你可以将 shell 的交互模式理解为执行命令行。 看到形如下面的东西，说明 shell 处于交互模式下： user@host:~$ 接着，便可以输入一系列 Linux 命令，比如 ls，grep，cd，mkdir，rm 等等。 非交互模式 简单来说，你可以将 shell 的非交互模式理解为执行 shell 脚本。 在非交互模式下，shell 从文件或者管道中读取命令并执行。 当 shell 解释器执行完文件中的最后一个命令，shell 进程终止，并回到父进程。 可以使用下面的命令让 shell 以非交互模式运行： sh /path/to/script.shbash /path/to/script.shsource /path/to/script.sh./path/to/script.sh 上面的例子中，script.sh是一个包含 shell 解释器可以识别并执行的命令的普通文本文件，sh和bash是 shell 解释器程序。你可以使用任何喜欢的编辑器创建script.sh（vim，nano，Sublime Text, Atom 等等）。 其中，source /path/to/script.sh 和 ./path/to/script.sh 是等价的。 除此之外，你还可以通过chmod命令给文件添加可执行的权限，来直接执行脚本文件： chmod +x /path/to/script.sh #使脚本具有执行权限/path/to/test.sh 这种方式要求脚本文件的第一行必须指明运行该脚本的程序，比如： ⌨️ 『示例源码』 helloworld.sh #!/usr/bin/env bashecho "Hello, world!" 上面的例子中，我们使用了一个很有用的命令echo来输出字符串到屏幕上。 2. 基本语法 2.1. 解释器 前面虽然两次提到了#! ，但是本着重要的事情说三遍的精神，这里再强调一遍： 在 shell 脚本，#! 告诉系统其后路径所指定的程序即是解释此脚本文件的 Shell 解释器。#! 被称作shebang（也称为 Hashbang ）。 #! 决定了脚本可以像一个独立的可执行文件一样执行，而不用在终端之前输入sh, bash, python, php等。 # 以下两种方式都可以指定 shell 解释器为 bash，第二种方式更好#!/bin/bash#!/usr/bin/env bash 2.2. 注释 注释可以说明你的代码是什么作用，以及为什么这样写。 shell 语法中，注释是特殊的语句，会被 shell 解释器忽略。 单行注释 - 以 # 开头，到行尾结束。 多行注释 - 以 :&lt;&lt;EOF 开头，到 EOF 结束。 ⌨️ 『示例源码』 comment-demo.sh #--------------------------------------------# shell 注释示例# author：zp#--------------------------------------------# echo '这是单行注释'########## 这是分割线 ##########:&lt;&lt;EOFecho '这是多行注释'echo '这是多行注释'echo '这是多行注释'EOF 2.3. echo echo 用于字符串的输出。 输出普通字符串： echo "hello, world"# Output: hello, world 输出含变量的字符串： echo "hello, \"zp\""# Output: hello, "zp" 输出含变量的字符串： name=zpecho "hello, \"$&#123;name&#125;\""# Output: hello, "zp" 输出含换行符的字符串： # 输出含换行符的字符串echo "YES\nNO"# Output: YES\nNOecho -e "YES\nNO" # -e 开启转义# Output:# YES# NO 输出含不换行符的字符串： echo "YES"echo "NO"# Output:# YES# NOecho -e "YES\c" # -e 开启转义 \c 不换行echo "NO"# Output:# YESNO 输出重定向至文件 echo "test" &gt; test.txt 输出执行结果 echo `pwd`# Output:(当前目录路径) ⌨️ 『示例源码』 echo-demo.sh 2.4. printf printf 用于格式化输出字符串。 默认，printf 不会像 echo 一样自动添加换行符，如果需要换行可以手动添加 \n。 ⌨️ 『示例源码』 printf-demo.sh # 单引号printf '%d %s\n' 1 "abc"# Output:1 abc# 双引号printf "%d %s\n" 1 "abc"# Output:1 abc# 无引号printf %s abcdef# Output: abcdef(并不会换行)# 格式只指定了一个参数，但多出的参数仍然会按照该格式输出printf "%s\n" abc def# Output:# abc# defprintf "%s %s %s\n" a b c d e f g h i j# Output:# a b c# d e f# g h i# j# 如果没有参数，那么 %s 用 NULL 代替，%d 用 0 代替printf "%s and %d \n"# Output:# and 0# 格式化输出printf "%-10s %-8s %-4s\n" 姓名 性别 体重kgprintf "%-10s %-8s %-4.2f\n" 郭靖 男 66.1234printf "%-10s %-8s %-4.2f\n" 杨过 男 48.6543printf "%-10s %-8s %-4.2f\n" 郭芙 女 47.9876# Output:# 姓名 性别 体重kg# 郭靖 男 66.12# 杨过 男 48.65# 郭芙 女 47.99 printf 的转义符 序列 说明 \a 警告字符，通常为 ASCII 的 BEL 字符 \b 后退 \c 抑制（不显示）输出结果中任何结尾的换行字符（只在%b 格式指示符控制下的参数字符串中有效），而且，任何留在参数里的字符、任何接下来的参数以及任何留在格式字符串中的字符，都被忽略 \f 换页（formfeed） \n 换行 \r 回车（Carriage return） \t 水平制表符 \v 垂直制表符 \\ 一个字面上的反斜杠字符 \ddd 表示 1 到 3 位数八进制值的字符。仅在格式字符串中有效 \0ddd 表示 1 到 3 位的八进制值字符 3. 变量 跟许多程序设计语言一样，你可以在 bash 中创建变量。 Bash 中没有数据类型，bash 中的变量可以保存一个数字、一个字符、一个字符串等等。同时无需提前声明变量，给变量赋值会直接创建变量。 3.1. 变量命名原则 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用 bash 里的关键字（可用 help 命令查看保留关键字）。 3.2. 声明变量 访问变量的语法形式为：${var} 和 $var 。 变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，所以推荐加花括号。 word="hello"echo $&#123;word&#125;# Output: hello 3.3. 只读变量 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。 rword="hello"echo $&#123;rword&#125;readonly rword# rword="bye" # 如果放开注释，执行时会报错 3.4. 删除变量 使用 unset 命令可以删除变量。变量被删除后不能再次使用。unset 命令不能删除只读变量。 dword="hello" # 声明变量echo $&#123;dword&#125; # 输出变量值# Output: hellounset dword # 删除变量echo $&#123;dword&#125;# Output: （空） 3.5. 变量类型 局部变量 - 局部变量是仅在某个脚本内部有效的变量。它们不能被其他的程序和脚本访问。 环境变量 - 环境变量是对当前 shell 会话内所有的程序或脚本都可见的变量。创建它们跟创建局部变量类似，但使用的是 export 关键字，shell 脚本也可以定义环境变量。 常见的环境变量： 变量 描述 $HOME 当前用户的用户目录 $PATH 用分号分隔的目录列表，shell 会到这些目录中查找命令 $PWD 当前工作目录 $RANDOM 0 到 32767 之间的整数 $UID 数值类型，当前用户的用户 ID $PS1 主要系统输入提示符 $PS2 次要系统输入提示符 这里 有一张更全面的 Bash 环境变量列表。 3.6. 变量示例源码 ⌨️ 『示例源码』 variable-demo.sh 4. 字符串 4.1. 单引号和双引号 shell 字符串可以用单引号 ''，也可以用双引号 “”，也可以不用引号。 单引号的特点 单引号里不识别变量 单引号里不能出现单独的单引号（使用转义符也不行），但可成对出现，作为字符串拼接使用。 双引号的特点 双引号里识别变量 双引号里可以出现转义字符 综上，推荐使用双引号。 4.2. 拼接字符串 # 使用单引号拼接name1='white'str1='hello, '$&#123;name1&#125;''str2='hello, $&#123;name1&#125;'echo $&#123;str1&#125;_$&#123;str2&#125;# Output:# hello, white_hello, $&#123;name1&#125;# 使用双引号拼接name2="black"str3="hello, "$&#123;name2&#125;""str4="hello, $&#123;name2&#125;"echo $&#123;str3&#125;_$&#123;str4&#125;# Output:# hello, black_hello, black 4.3. 获取字符串长度 text="12345"echo $&#123;#text&#125;# Output:# 5 4.4. 截取子字符串 text="12345"echo $&#123;text:2:2&#125;# Output:# 34 从第 3 个字符开始，截取 2 个字符 4.5. 查找子字符串 #!/usr/bin/env bashtext="hello"echo `expr index "$&#123;text&#125;" ll`# Execute: ./str-demo5.sh# Output:# 3 查找 ll 子字符在 hello 字符串中的起始位置。 4.6. 字符串示例源码 ⌨️ 『示例源码』 string-demo.sh 5. 数组 bash 只支持一维数组。 数组下标从 0 开始，下标可以是整数或算术表达式，其值应大于或等于 0。 5.1. 创建数组 # 创建数组的不同方式nums=([2]=2 [0]=0 [1]=1)colors=(red yellow "dark blue") 5.2. 访问数组元素 访问数组的单个元素： echo $&#123;nums[1]&#125;# Output: 1 访问数组的所有元素： echo $&#123;colors[*]&#125;# Output: red yellow dark blueecho $&#123;colors[@]&#125;# Output: red yellow dark blue 上面两行有很重要（也很微妙）的区别： 为了将数组中每个元素单独一行输出，我们用 printf 命令： printf "+ %s\n" $&#123;colors[*]&#125;# Output:# + red# + yellow# + dark# + blue 为什么dark和blue各占了一行？尝试用引号包起来： printf "+ %s\n" "$&#123;colors[*]&#125;"# Output:# + red yellow dark blue 现在所有的元素都在一行输出 —— 这不是我们想要的！让我们试试${colors[@]} printf "+ %s\n" "$&#123;colors[@]&#125;"# Output:# + red# + yellow# + dark blue 在引号内，${colors[@]}将数组中的每个元素扩展为一个单独的参数；数组元素中的空格得以保留。 访问数组的部分元素： echo $&#123;nums[@]:0:2&#125;# Output:# 0 1 在上面的例子中，${array[@]} 扩展为整个数组，:0:2取出了数组中从 0 开始，长度为 2 的元素。 5.3. 访问数组长度 echo $&#123;#nums[*]&#125;# Output:# 3 5.4. 向数组中添加元素 向数组中添加元素也非常简单： colors=(white "$&#123;colors[@]&#125;" green black)echo $&#123;colors[@]&#125;# Output:# white red yellow dark blue green black 上面的例子中，${colors[@]} 扩展为整个数组，并被置换到复合赋值语句中，接着，对数组colors的赋值覆盖了它原来的值。 5.5. 从数组中删除元素 用unset命令来从数组中删除一个元素： unset nums[0]echo $&#123;nums[@]&#125;# Output:# 1 2 5.6. 数组示例源码 ⌨️ 『示例源码』 array-demo.sh 6. 运算符 6.1. 算术运算符 下表列出了常用的算术运算符，假定变量 x 为 10，变量 y 为 20： 运算符 说明 举例 + 加法 expr $x + $y 结果为 30。 - 减法 expr $x - $y 结果为 -10。 * 乘法 expr $x * $y 结果为 200。 / 除法 expr $y / $x 结果为 2。 % 取余 expr $y % $x 结果为 0。 = 赋值 x=$y 将把变量 y 的值赋给 x。 == 相等。用于比较两个数字，相同则返回 true。 [ $x == $y ] 返回 false。 != 不相等。用于比较两个数字，不相同则返回 true。 [ $x != $y ] 返回 true。 **注意：**条件表达式要放在方括号之间，并且要有空格，例如: [$x==$y] 是错误的，必须写成 [ $x == $y ]。 ⌨️ 『示例源码』 operator-demo.sh x=10y=20echo "x=$&#123;x&#125;, y=$&#123;y&#125;"val=`expr $&#123;x&#125; + $&#123;y&#125;`echo "$&#123;x&#125; + $&#123;y&#125; = $val"val=`expr $&#123;x&#125; - $&#123;y&#125;`echo "$&#123;x&#125; - $&#123;y&#125; = $val"val=`expr $&#123;x&#125; \* $&#123;y&#125;`echo "$&#123;x&#125; * $&#123;y&#125; = $val"val=`expr $&#123;y&#125; / $&#123;x&#125;`echo "$&#123;y&#125; / $&#123;x&#125; = $val"val=`expr $&#123;y&#125; % $&#123;x&#125;`echo "$&#123;y&#125; % $&#123;x&#125; = $val"if [[ $&#123;x&#125; == $&#123;y&#125; ]]then echo "$&#123;x&#125; = $&#123;y&#125;"fiif [[ $&#123;x&#125; != $&#123;y&#125; ]]then echo "$&#123;x&#125; != $&#123;y&#125;"fi# Execute: ./operator-demo.sh# Output:# x=10, y=20# 10 + 20 = 30# 10 - 20 = -10# 10 * 20 = 200# 20 / 10 = 2# 20 % 10 = 0# 10 != 20 6.2. 关系运算符 关系运算符只支持数字，不支持字符串，除非字符串的值是数字。 下表列出了常用的关系运算符，假定变量 x 为 10，变量 y 为 20： 运算符 说明 举例 -eq 检测两个数是否相等，相等返回 true。 [ $a -eq $b ]返回 false。 -ne 检测两个数是否相等，不相等返回 true。 [ $a -ne $b ] 返回 true。 -gt 检测左边的数是否大于右边的，如果是，则返回 true。 [ $a -gt $b ] 返回 false。 -lt 检测左边的数是否小于右边的，如果是，则返回 true。 [ $a -lt $b ] 返回 true。 -ge 检测左边的数是否大于等于右边的，如果是，则返回 true。 [ $a -ge $b ] 返回 false。 -le 检测左边的数是否小于等于右边的，如果是，则返回 true。 [ $a -le $b ]返回 true。 ⌨️ 『示例源码』 operator-demo2.sh x=10y=20echo "x=$&#123;x&#125;, y=$&#123;y&#125;"if [[ $&#123;x&#125; -eq $&#123;y&#125; ]]; then echo "$&#123;x&#125; -eq $&#123;y&#125; : x 等于 y"else echo "$&#123;x&#125; -eq $&#123;y&#125;: x 不等于 y"fiif [[ $&#123;x&#125; -ne $&#123;y&#125; ]]; then echo "$&#123;x&#125; -ne $&#123;y&#125;: x 不等于 y"else echo "$&#123;x&#125; -ne $&#123;y&#125;: x 等于 y"fiif [[ $&#123;x&#125; -gt $&#123;y&#125; ]]; then echo "$&#123;x&#125; -gt $&#123;y&#125;: x 大于 y"else echo "$&#123;x&#125; -gt $&#123;y&#125;: x 不大于 y"fiif [[ $&#123;x&#125; -lt $&#123;y&#125; ]]; then echo "$&#123;x&#125; -lt $&#123;y&#125;: x 小于 y"else echo "$&#123;x&#125; -lt $&#123;y&#125;: x 不小于 y"fiif [[ $&#123;x&#125; -ge $&#123;y&#125; ]]; then echo "$&#123;x&#125; -ge $&#123;y&#125;: x 大于或等于 y"else echo "$&#123;x&#125; -ge $&#123;y&#125;: x 小于 y"fiif [[ $&#123;x&#125; -le $&#123;y&#125; ]]; then echo "$&#123;x&#125; -le $&#123;y&#125;: x 小于或等于 y"else echo "$&#123;x&#125; -le $&#123;y&#125;: x 大于 y"fi# Execute: ./operator-demo2.sh# Output:# x=10, y=20# 10 -eq 20: x 不等于 y# 10 -ne 20: x 不等于 y# 10 -gt 20: x 不大于 y# 10 -lt 20: x 小于 y# 10 -ge 20: x 小于 y# 10 -le 20: x 小于或等于 y 6.3. 布尔运算符 下表列出了常用的布尔运算符，假定变量 x 为 10，变量 y 为 20： 运算符 说明 举例 ! 非运算，表达式为 true 则返回 false，否则返回 true。 [ ! false ] 返回 true。 -o 或运算，有一个表达式为 true 则返回 true。 [ $a -lt 20 -o $b -gt 100 ] 返回 true。 -a 与运算，两个表达式都为 true 才返回 true。 [ $a -lt 20 -a $b -gt 100 ] 返回 false。 ⌨️ 『示例源码』 operator-demo3.sh x=10y=20echo "x=$&#123;x&#125;, y=$&#123;y&#125;"if [[ $&#123;x&#125; != $&#123;y&#125; ]]; then echo "$&#123;x&#125; != $&#123;y&#125; : x 不等于 y"else echo "$&#123;x&#125; != $&#123;y&#125;: x 等于 y"fiif [[ $&#123;x&#125; -lt 100 &amp;&amp; $&#123;y&#125; -gt 15 ]]; then echo "$&#123;x&#125; 小于 100 且 $&#123;y&#125; 大于 15 : 返回 true"else echo "$&#123;x&#125; 小于 100 且 $&#123;y&#125; 大于 15 : 返回 false"fiif [[ $&#123;x&#125; -lt 100 || $&#123;y&#125; -gt 100 ]]; then echo "$&#123;x&#125; 小于 100 或 $&#123;y&#125; 大于 100 : 返回 true"else echo "$&#123;x&#125; 小于 100 或 $&#123;y&#125; 大于 100 : 返回 false"fiif [[ $&#123;x&#125; -lt 5 || $&#123;y&#125; -gt 100 ]]; then echo "$&#123;x&#125; 小于 5 或 $&#123;y&#125; 大于 100 : 返回 true"else echo "$&#123;x&#125; 小于 5 或 $&#123;y&#125; 大于 100 : 返回 false"fi# Execute: ./operator-demo3.sh# Output:# x=10, y=20# 10 != 20 : x 不等于 y# 10 小于 100 且 20 大于 15 : 返回 true# 10 小于 100 或 20 大于 100 : 返回 true# 10 小于 5 或 20 大于 100 : 返回 false 6.4. 逻辑运算符 以下介绍 Shell 的逻辑运算符，假定变量 x 为 10，变量 y 为 20: 运算符 说明 举例 &amp;&amp; 逻辑的 AND [[ ${x} -lt 100 &amp;&amp; ${y} -gt 100 ]] 返回 false || 逻辑的 OR [[ ${x} -lt 100 || ${y} -gt 100 ]] 返回 true ⌨️ 『示例源码』 operator-demo4.sh x=10y=20echo "x=$&#123;x&#125;, y=$&#123;y&#125;"if [[ $&#123;x&#125; -lt 100 &amp;&amp; $&#123;y&#125; -gt 100 ]]then echo "$&#123;x&#125; -lt 100 &amp;&amp; $&#123;y&#125; -gt 100 返回 true"else echo "$&#123;x&#125; -lt 100 &amp;&amp; $&#123;y&#125; -gt 100 返回 false"fiif [[ $&#123;x&#125; -lt 100 || $&#123;y&#125; -gt 100 ]]then echo "$&#123;x&#125; -lt 100 || $&#123;y&#125; -gt 100 返回 true"else echo "$&#123;x&#125; -lt 100 || $&#123;y&#125; -gt 100 返回 false"fi# Execute: ./operator-demo4.sh# Output:# x=10, y=20# 10 -lt 100 &amp;&amp; 20 -gt 100 返回 false# 10 -lt 100 || 20 -gt 100 返回 true 6.5. 字符串运算符 下表列出了常用的字符串运算符，假定变量 a 为 “abc”，变量 b 为 “efg”： 运算符 说明 举例 = 检测两个字符串是否相等，相等返回 true。 [ $a = $b ] 返回 false。 != 检测两个字符串是否相等，不相等返回 true。 [ $a != $b ] 返回 true。 -z 检测字符串长度是否为 0，为 0 返回 true。 [ -z $a ] 返回 false。 -n 检测字符串长度是否为 0，不为 0 返回 true。 [ -n $a ] 返回 true。 str 检测字符串是否为空，不为空返回 true。 [ $a ] 返回 true。 ⌨️ 『示例源码』 operator-demo5.sh x="abc"y="xyz"echo "x=$&#123;x&#125;, y=$&#123;y&#125;"if [[ $&#123;x&#125; = $&#123;y&#125; ]]; then echo "$&#123;x&#125; = $&#123;y&#125; : x 等于 y"else echo "$&#123;x&#125; = $&#123;y&#125;: x 不等于 y"fiif [[ $&#123;x&#125; != $&#123;y&#125; ]]; then echo "$&#123;x&#125; != $&#123;y&#125; : x 不等于 y"else echo "$&#123;x&#125; != $&#123;y&#125;: x 等于 y"fiif [[ -z $&#123;x&#125; ]]; then echo "-z $&#123;x&#125; : 字符串长度为 0"else echo "-z $&#123;x&#125; : 字符串长度不为 0"fiif [[ -n "$&#123;x&#125;" ]]; then echo "-n $&#123;x&#125; : 字符串长度不为 0"else echo "-n $&#123;x&#125; : 字符串长度为 0"fiif [[ $&#123;x&#125; ]]; then echo "$&#123;x&#125; : 字符串不为空"else echo "$&#123;x&#125; : 字符串为空"fi# Execute: ./operator-demo5.sh# Output:# x=abc, y=xyz# abc = xyz: x 不等于 y# abc != xyz : x 不等于 y# -z abc : 字符串长度不为 0# -n abc : 字符串长度不为 0# abc : 字符串不为空 6.6. 文件测试运算符 文件测试运算符用于检测 Unix 文件的各种属性。 属性检测描述如下： 操作符 说明 举例 -b file 检测文件是否是块设备文件，如果是，则返回 true。 [ -b $file ] 返回 false。 -c file 检测文件是否是字符设备文件，如果是，则返回 true。 [ -c $file ] 返回 false。 -d file 检测文件是否是目录，如果是，则返回 true。 [ -d $file ] 返回 false。 -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。 [ -f $file ] 返回 true。 -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 [ -g $file ] 返回 false。 -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。 [ -k $file ]返回 false。 -p file 检测文件是否是有名管道，如果是，则返回 true。 [ -p $file ] 返回 false。 -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 [ -u $file ] 返回 false。 -r file 检测文件是否可读，如果是，则返回 true。 [ -r $file ] 返回 true。 -w file 检测文件是否可写，如果是，则返回 true。 [ -w $file ] 返回 true。 -x file 检测文件是否可执行，如果是，则返回 true。 [ -x $file ] 返回 true。 -s file 检测文件是否为空（文件大小是否大于 0），不为空返回 true。 [ -s $file ] 返回 true。 -e file 检测文件（包括目录）是否存在，如果是，则返回 true。 [ -e $file ] 返回 true。 ⌨️ 『示例源码』 operator-demo6.sh file="/etc/hosts"if [[ -r $&#123;file&#125; ]]; then echo "$&#123;file&#125; 文件可读"else echo "$&#123;file&#125; 文件不可读"fiif [[ -w $&#123;file&#125; ]]; then echo "$&#123;file&#125; 文件可写"else echo "$&#123;file&#125; 文件不可写"fiif [[ -x $&#123;file&#125; ]]; then echo "$&#123;file&#125; 文件可执行"else echo "$&#123;file&#125; 文件不可执行"fiif [[ -f $&#123;file&#125; ]]; then echo "$&#123;file&#125; 文件为普通文件"else echo "$&#123;file&#125; 文件为特殊文件"fiif [[ -d $&#123;file&#125; ]]; then echo "$&#123;file&#125; 文件是个目录"else echo "$&#123;file&#125; 文件不是个目录"fiif [[ -s $&#123;file&#125; ]]; then echo "$&#123;file&#125; 文件不为空"else echo "$&#123;file&#125; 文件为空"fiif [[ -e $&#123;file&#125; ]]; then echo "$&#123;file&#125; 文件存在"else echo "$&#123;file&#125; 文件不存在"fi# Execute: ./operator-demo6.sh# Output:(根据文件的实际情况，输出结果可能不同)# /etc/hosts 文件可读# /etc/hosts 文件可写# /etc/hosts 文件不可执行# /etc/hosts 文件为普通文件# /etc/hosts 文件不是个目录# /etc/hosts 文件不为空# /etc/hosts 文件存在 7. 控制语句 7.1. 条件语句 跟其它程序设计语言一样，Bash 中的条件语句让我们可以决定一个操作是否被执行。结果取决于一个包在[[ ]]里的表达式。 由[[ ]]（sh中是[ ]）包起来的表达式被称作 检测命令 或 基元。这些表达式帮助我们检测一个条件的结果。这里可以找到有关bash 中单双中括号区别的答案。 共有两个不同的条件表达式：if和case。 if （1）if 语句 if在使用上跟其它语言相同。如果中括号里的表达式为真，那么then和fi之间的代码会被执行。fi标志着条件代码块的结束。 # 写成一行if [[ 1 -eq 1 ]]; then echo "1 -eq 1 result is: true"; fi# Output: 1 -eq 1 result is: true# 写成多行if [[ "abc" -eq "abc" ]]then echo ""abc" -eq "abc" result is: true"fi# Output: abc -eq abc result is: true （2）if else 语句 同样，我们可以使用if..else语句，例如： if [[ 2 -ne 1 ]]; then echo "true"else echo "false"fi# Output: true （3）if elif else 语句 有些时候，if..else不能满足我们的要求。别忘了if..elif..else，使用起来也很方便。 x=10y=20if [[ $&#123;x&#125; &gt; $&#123;y&#125; ]]; then echo "$&#123;x&#125; &gt; $&#123;y&#125;"elif [[ $&#123;x&#125; &lt; $&#123;y&#125; ]]; then echo "$&#123;x&#125; &lt; $&#123;y&#125;"else echo "$&#123;x&#125; = $&#123;y&#125;"fi# Output: 10 &lt; 20 ⌨️ 『示例源码』 if-demo.sh case 如果你需要面对很多情况，分别要采取不同的措施，那么使用case会比嵌套的if更有用。使用case来解决复杂的条件判断，看起来像下面这样： ⌨️ 『示例源码』 case-demo.sh execcase $&#123;oper&#125; in "+") val=`expr $&#123;x&#125; + $&#123;y&#125;` echo "$&#123;x&#125; + $&#123;y&#125; = $&#123;val&#125;" ;; "-") val=`expr $&#123;x&#125; - $&#123;y&#125;` echo "$&#123;x&#125; - $&#123;y&#125; = $&#123;val&#125;" ;; "*") val=`expr $&#123;x&#125; \* $&#123;y&#125;` echo "$&#123;x&#125; * $&#123;y&#125; = $&#123;val&#125;" ;; "/") val=`expr $&#123;x&#125; / $&#123;y&#125;` echo "$&#123;x&#125; / $&#123;y&#125; = $&#123;val&#125;" ;; *) echo "Unknown oper!" ;;esac 每种情况都是匹配了某个模式的表达式。|用来分割多个模式，)用来结束一个模式序列。第一个匹配上的模式对应的命令将会被执行。*代表任何不匹配以上给定模式的模式。命令块儿之间要用;;分隔。 7.2. 循环语句 循环其实不足为奇。跟其它程序设计语言一样，bash 中的循环也是只要控制条件为真就一直迭代执行的代码块。 Bash 中有四种循环：for，while，until和select。 for循环 for与它在 C 语言中的姊妹非常像。看起来是这样： for arg in elem1 elem2 ... elemNdo ### 语句done 在每次循环的过程中，arg依次被赋值为从elem1到elemN。这些值还可以是通配符或者大括号扩展。 当然，我们还可以把for循环写在一行，但这要求do之前要有一个分号，就像下面这样： for i in &#123;1..5&#125;; do echo $i; done 还有，如果你觉得for..in..do对你来说有点奇怪，那么你也可以像 C 语言那样使用for，比如： for (( i = 0; i &lt; 10; i++ )); do echo $idone 当我们想对一个目录下的所有文件做同样的操作时，for就很方便了。举个例子，如果我们想把所有的.bash文件移动到script文件夹中，并给它们可执行权限，我们的脚本可以这样写： DIR=/home/zpfor FILE in $&#123;DIR&#125;/*.sh; do mv "$FILE" "$&#123;DIR&#125;/scripts"done# 将 /home/zp 目录下所有 sh 文件拷贝到 /home/zp/scripts ⌨️ 『示例源码』 for-demo.sh while循环 while循环检测一个条件，只要这个条件为 真，就执行一段命令。被检测的条件跟if..then中使用的基元并无二异。因此一个while循环看起来会是这样： while [[ condition ]]do ### 语句done 跟for循环一样，如果我们把do和被检测的条件写到一行，那么必须要在do之前加一个分号。 比如下面这个例子： ### 0到9之间每个数的平方x=0while [[ $&#123;x&#125; -lt 10 ]]; do echo $((x * x)) x=$((x + 1))done# Output:# 0# 1# 4# 9# 16# 25# 36# 49# 64# 81 ⌨️ 『示例源码』 while-demo.sh until循环 until循环跟while循环正好相反。它跟while一样也需要检测一个测试条件，但不同的是，只要该条件为 假 就一直执行循环： x=0until [[ $&#123;x&#125; -ge 5 ]]; do echo $&#123;x&#125; x=`expr $&#123;x&#125; + 1`done# Output:# 0# 1# 2# 3# 4 ⌨️ 『示例源码』 until-demo.sh select循环 select循环帮助我们组织一个用户菜单。它的语法几乎跟for循环一致： select answer in elem1 elem2 ... elemNdo ### 语句done select会打印elem1..elemN以及它们的序列号到屏幕上，之后会提示用户输入。通常看到的是$?（PS3变量）。用户的选择结果会被保存到answer中。如果answer是一个在1..N之间的数字，那么语句会被执行，紧接着会进行下一次迭代 —— 如果不想这样的话我们可以使用break语句。 一个可能的实例可能会是这样： #!/usr/bin/env bashPS3="Choose the package manager: "select ITEM in bower npm gem pipdoecho -n "Enter the package name: " &amp;&amp; read PACKAGEcase $&#123;ITEM&#125; in bower) bower install $&#123;PACKAGE&#125; ;; npm) npm install $&#123;PACKAGE&#125; ;; gem) gem install $&#123;PACKAGE&#125; ;; pip) pip install $&#123;PACKAGE&#125; ;;esacbreak # 避免无限循环done 这个例子，先询问用户他想使用什么包管理器。接着，又询问了想安装什么包，最后执行安装操作。 运行这个脚本，会得到如下输出： $ ./my_script1) bower2) npm3) gem4) pipChoose the package manager: 2Enter the package name: gitbook-cli ⌨️ 『示例源码』 select-demo.sh break 和 continue 如果想提前结束一个循环或跳过某次循环执行，可以使用 shell 的break和continue语句来实现。它们可以在任何循环中使用。 break语句用来提前结束当前循环。 continue语句用来跳过某次迭代。 ⌨️ 『示例源码』 break-demo.sh # 查找 10 以内第一个能整除 2 和 3 的正整数i=1while [[ $&#123;i&#125; -lt 10 ]]; do if [[ $((i % 3)) -eq 0 ]] &amp;&amp; [[ $((i % 2)) -eq 0 ]]; then echo $&#123;i&#125; break; fi i=`expr $&#123;i&#125; + 1`done# Output: 6 ⌨️ 『示例源码』 continue-demo.sh # 打印10以内的奇数for (( i = 0; i &lt; 10; i ++ )); do if [[ $((i % 2)) -eq 0 ]]; then continue; fi echo $&#123;i&#125;done# Output:# 1# 3# 5# 7# 9 8. 函数 bash 函数定义语法如下： [ function ] funname [()] &#123; action; [return int;]&#125; 💡 说明： 函数定义时，function 关键字可有可无。 函数返回值 - return 返回函数返回值，返回值类型只能为整数（0-255）。如果不加 return 语句，shell 默认将以最后一条命令的运行结果，作为函数返回值。 函数返回值在调用该函数后通过 $? 来获得。 所有函数在使用前必须定义。这意味着必须将函数放在脚本开始部分，直至 shell 解释器首次发现它时，才可以使用。调用函数仅使用其函数名即可。 ⌨️ 『示例源码』 function-demo.sh #!/usr/bin/env bashcalc()&#123; PS3="choose the oper: " select oper in + - \* / # 生成操作符选择菜单 do echo -n "enter first num: " &amp;&amp; read x # 读取输入参数 echo -n "enter second num: " &amp;&amp; read y # 读取输入参数 exec case $&#123;oper&#125; in "+") return $(($&#123;x&#125; + $&#123;y&#125;)) ;; "-") return $(($&#123;x&#125; - $&#123;y&#125;)) ;; "*") return $(($&#123;x&#125; * $&#123;y&#125;)) ;; "/") return $(($&#123;x&#125; / $&#123;y&#125;)) ;; *) echo "$&#123;oper&#125; is not support!" return 0 ;; esac break done&#125;calcecho "the result is: $?" # $? 获取 calc 函数返回值 执行结果： $ ./function-demo.sh1) +2) -3) *4) /choose the oper: 3enter first num: 10enter second num: 10the result is: 100 8.1. 位置参数 位置参数是在调用一个函数并传给它参数时创建的变量。 位置参数变量表： 变量 描述 $0 脚本名称 $1 … $9 第 1 个到第 9 个参数列表 ${10} … ${N} 第 10 个到 N 个参数列表 $* or $@ 除了$0外的所有位置参数 $# 不包括$0在内的位置参数的个数 $FUNCNAME 函数名称（仅在函数内部有值） ⌨️ 『示例源码』 function-demo2.sh #!/usr/bin/env bashx=0if [[ -n $1 ]]; then echo "第一个参数为：$1" x=$1else echo "第一个参数为空"fiy=0if [[ -n $2 ]]; then echo "第二个参数为：$2" y=$2else echo "第二个参数为空"fiparamsFunction()&#123; echo "函数第一个入参：$1" echo "函数第二个入参：$2"&#125;paramsFunction $&#123;x&#125; $&#123;y&#125; 执行结果： $ ./function-demo2.sh第一个参数为空第二个参数为空函数第一个入参：0函数第二个入参：0$ ./function-demo2.sh 10 20第一个参数为：10第二个参数为：20函数第一个入参：10函数第二个入参：20 执行 ./variable-demo4.sh hello world ，然后在脚本中通过 $1、$2 … 读取第 1 个参数、第 2 个参数。。。 8.2. 函数处理参数 另外，还有几个特殊字符用来处理参数： 参数处理 说明 $# 返回参数个数 $* 返回所有参数 $$ 脚本运行的当前进程 ID 号 $! 后台运行的最后一个进程的 ID 号 $@ 返回所有参数 $- 返回 Shell 使用的当前选项，与 set 命令功能相同。 $? 函数返回值 ⌨️ 『示例源码』 function-demo3.sh runner() &#123; return 0&#125;name=zpparamsFunction()&#123; echo "函数第一个入参：$1" echo "函数第二个入参：$2" echo "传递到脚本的参数个数：$#" echo "所有参数：" printf "+ %s\n" "$*" echo "脚本运行的当前进程 ID 号：$$" echo "后台运行的最后一个进程的 ID 号：$!" echo "所有参数：" printf "+ %s\n" "$@" echo "Shell 使用的当前选项：$-" runner echo "runner 函数的返回值：$?"&#125;paramsFunction 1 "abc" "hello, \"zp\""# Output:# 函数第一个入参：1# 函数第二个入参：abc# 传递到脚本的参数个数：3# 所有参数：# + 1 abc hello, "zp"# 脚本运行的当前进程 ID 号：26400# 后台运行的最后一个进程的 ID 号：# 所有参数：# + 1# + abc# + hello, "zp"# Shell 使用的当前选项：hB# runner 函数的返回值：0 9. Shell 扩展 扩展 发生在一行命令被分成一个个的 记号（tokens） 之后。换言之，扩展是一种执行数学运算的机制，还可以用来保存命令的执行结果，等等。 感兴趣的话可以阅读关于 shell 扩展的更多细节。 大括号扩展 大括号扩展让生成任意的字符串成为可能。它跟 文件名扩展 很类似，举个例子： echo beg&#123;i,a,u&#125;n ### begin began begun 大括号扩展还可以用来创建一个可被循环迭代的区间。 echo &#123;0..5&#125; ### 0 1 2 3 4 5echo &#123;00..8..2&#125; ### 00 02 04 06 08 命令置换 命令置换允许我们对一个命令求值，并将其值置换到另一个命令或者变量赋值表达式中。当一个命令被``或$()包围时，命令置换将会执行。举个例子： now=`date +%T`### ornow=$(date +%T)echo $now ### 19:08:26 算数扩展 在 bash 中，执行算数运算是非常方便的。算数表达式必须包在$(( ))中。算数扩展的格式为： result=$(( ((10 + 5*3) - 7) / 2 ))echo $result ### 9 在算数表达式中，使用变量无需带上$前缀： x=4y=7echo $(( x + y )) ### 11echo $(( ++x + y++ )) ### 12echo $(( x + y )) ### 13 单引号和双引号 单引号和双引号之间有很重要的区别。在双引号中，变量引用或者命令置换是会被展开的。在单引号中是不会的。举个例子： echo "Your home: $HOME" ### Your home: /Users/&lt;username&gt;echo 'Your home: $HOME' ### Your home: $HOME 当局部变量和环境变量包含空格时，它们在引号中的扩展要格外注意。随便举个例子，假如我们用echo来输出用户的输入： INPUT="A string with strange whitespace."echo $INPUT ### A string with strange whitespace.echo "$INPUT" ### A string with strange whitespace. 调用第一个echo时给了它 5 个单独的参数 —— $INPUT 被分成了单独的词，echo在每个词之间打印了一个空格。第二种情况，调用echo时只给了它一个参数（整个$INPUT 的值，包括其中的空格）。 来看一个更严肃的例子： FILE="Favorite Things.txt"cat $FILE ### 尝试输出两个文件: `Favorite` 和 `Things.txt`cat "$FILE" ### 输出一个文件: `Favorite Things.txt` 尽管这个问题可以通过把 FILE 重命名成Favorite-Things.txt来解决，但是，假如这个值来自某个环境变量，来自一个位置参数，或者来自其它命令（find, cat, 等等）呢。因此，如果输入 可能 包含空格，务必要用引号把表达式包起来。 10. 流和重定向 Bash 有很强大的工具来处理程序之间的协同工作。使用流，我们能将一个程序的输出发送到另一个程序或文件，因此，我们能方便地记录日志或做一些其它我们想做的事。 管道给了我们创建传送带的机会，控制程序的执行成为可能。 学习如何使用这些强大的、高级的工具是非常非常重要的。 10.1. 输入、输出流 Bash 接收输入，并以字符序列或 字符流 的形式产生输出。这些流能被重定向到文件或另一个流中。 有三个文件描述符： 代码 描述符 描述 0 stdin 标准输入 1 stdout 标准输出 2 stderr 标准错误输出 10.2. 重定向 重定向让我们可以控制一个命令的输入来自哪里，输出结果到什么地方。这些运算符在控制流的重定向时会被用到： Operator Description &gt; 重定向输出 &amp;&gt; 重定向输出和错误输出 &amp;&gt;&gt; 以附加的形式重定向输出和错误输出 &lt; 重定向输入 &lt;&lt; Here 文档 语法 &lt;&lt;&lt; Here 字符串 以下是一些使用重定向的例子： ### ls的结果将会被写到list.txt中ls -l &gt; list.txt### 将输出附加到list.txt中ls -a &gt;&gt; list.txt### 所有的错误信息会被写到errors.txt中grep da * 2&gt; errors.txt### 从errors.txt中读取输入less &lt; errors.txt 10.3. /dev/null 文件 如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null： $ command &gt; /dev/null /dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到&quot;禁止输出&quot;的效果。 如果希望屏蔽 stdout 和 stderr，可以这样写： $ command &gt; /dev/null 2&gt;&amp;1 11. Debug shell 提供了用于 debug 脚本的工具。 如果想采用 debug 模式运行某脚本，可以在其 shebang 中使用一个特殊的选项： #!/bin/bash options options 是一些可以改变 shell 行为的选项。下表是一些可能对你有用的选项： Short Name Description -f noglob 禁止文件名展开（globbing） -i interactive 让脚本以 交互 模式运行 -n noexec 读取命令，但不执行（语法检查） -t — 执行完第一条命令后退出 -v verbose 在执行每条命令前，向stderr输出该命令 -x xtrace 在执行每条命令前，向stderr输出该命令以及该命令的扩展参数 举个例子，如果我们在脚本中指定了-x例如： #!/bin/bash -xfor (( i = 0; i &lt; 3; i++ )); do echo $idone 这会向stdout打印出变量的值和一些其它有用的信息： $ ./my_script+ (( i = 0 ))+ (( i &lt; 3 ))+ echo 00+ (( i++ ))+ (( i &lt; 3 ))+ echo 11+ (( i++ ))+ (( i &lt; 3 ))+ echo 22+ (( i++ ))+ (( i &lt; 3 )) 有时我们值需要 debug 脚本的一部分。这种情况下，使用set命令会很方便。这个命令可以启用或禁用选项。使用-启用选项，+禁用选项： ⌨️ 『示例源码』 debug-demo.sh # 开启 debugset -xfor (( i = 0; i &lt; 3; i++ )); do printf $&#123;i&#125;done# 关闭 debugset +x# Output:# + (( i = 0 ))# + (( i &lt; 3 ))# + printf 0# 0+ (( i++ ))# + (( i &lt; 3 ))# + printf 1# 1+ (( i++ ))# + (( i &lt; 3 ))# + printf 2# 2+ (( i++ ))# + (( i &lt; 3 ))# + set +xfor i in &#123;1..5&#125;; do printf $&#123;i&#125;; doneprintf "\n"# Output: 12345 12. 更多内容 📓 本文已归档到：「blog」 awesome-shell，shell 资源列表 awesome-bash，bash 资源列表 bash-handbook bash-guide ，bash 基本用法指南 bash-it，为你日常使用，开发以及维护 shell 脚本和自定义命令提供了一个可靠的框架 dotfiles.github.io，上面有 bash 和其它 shell 的各种 dotfiles 集合以及 shell 框架的链接 Runoob Shell 教程 shellcheck 一个静态 shell 脚本分析工具，本质上是 bash／sh／zsh 的 lint。 最后，Stack Overflow 上 bash 标签下有很多你可以学习的问题，当你遇到问题时，也是一个提问的好地方。]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>programming</tag>
        <tag>shell</tag>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件工程与项目管理]]></title>
    <url>%2Fblog%2F2017%2F11%2F20%2Fmethod%2Fsoftware-engineering%2F</url>
    <content type="text"><![CDATA[软件工程与项目管理 📓 本文已归档到：「blog」 软件工程是一门研究用工程化方法构建和维护有效的、实用的和高质量的软件的学科。它涉及程序设计语言、数据库、软件开发工具、系统平台、标准、设计模式等方面。 软件工程的目标 软件工程的原理 软件工程的方法 软件需求 软件需求说明书（ SRS ） 软件生命周期 软件生命周期模型 瀑布模型 螺旋模型 软件工程术语 更多内容 书 文章 工具 文档模板 软件工程的目标 软件工程的目标是：在给定成本、进度的前提下，开发出具有适用性、有效性、可修改性、可靠性、可理解性、可维护性、可重用性、可移植性、可追踪性、可互操作性和满足用户需求的软件产品。 适用性 - 软件在不同的系统约束条件下，使用户需求得到满足的难易程度。 有效性 - 软件系统能最有效的利用计算机的时间和空间资源。各种软件无不把系统的时/空开销作为衡量软件质量的一项重要技术指标。很多场合，在追求时间有效性和空间有效性时会发生矛盾，这时不得不牺牲时间有效性换取空间有效性或牺牲空间有效性换取时间有效性。时/空折衷是经常采用的技巧。 可修改性 - 允许对系统进行修改而不增加原系统的复杂性。它支持软件的调试和维护，是一个难以达到的目标。 可靠性 - 能防止因概念、设计和结构等方面的不完善造成的软件系统失效，具有挽回因操作不当造成软件系统失效的能力。 可理解性 - 系统具有清晰的结构，能直接反映问题的需求。可理解性有助于控制系统软件复杂性，并支持软件的维护、移植或重用。 可维护性 - 软件交付使用后，能够对它进行修改，以改正潜伏的错误，改进性能和其它属性，使软件产品适应环境的变化等。软件维护费用在软件开发费用中占有很大的比重。可维护性是软件工程中一项十分重要的目标。 可重用性 - 把概念或功能相对独立的一个或一组相关模块定义为一个软部件。可组装在系统的任何位置，降低工作量。 可移植性 - 软件从一个计算机系统或环境搬到另一个计算机系统或环境的难易程度。 可追踪性 - 根据软件需求对软件设计、程序进行正向追踪，或根据软件设计、程序对软件需求的逆向追踪的能力。 可互操作性 - 多个软件元素相互通信并协同完成任务的能力。 软件工程的原理 软件工程的七条基本原理： 用分阶段的生存周期计划进行严格的管理。 坚持进行阶段评审。 实行严格的产品控制。 采用现代程序设计技术。 软件工程结果应能清楚地审查。 开发小组的人员应该少而精。 承认不断改进软件工程实践的必要性。 软件工程的方法 著名的重量级开发方法： ISO9000 - ISO 9000 系列标准是国际标准化组织设立的标准，与品质管理系统有关。 能力成熟度模型（CMM） - CMM 涵盖一个成熟的软件发展组织所应具备的重要功能与项目，它描述了软件发展的演进过程，从毫无章法、不成熟的软件开发阶段到成熟软件开发阶段的过程。 统一软件开发过程（RUP） - RUP 是一种软件工程方法，为迭代式软件开发流程。 著名的轻量级开发方法： 敏捷开发（Agile Development） - 是一种应对快速变化的需求的一种软件开发能力。它们的具体名称、理念、过程、术语都不尽相同，相对于“非敏捷”，更强调程序员团队与业务专家之间的紧密协作、面对面的沟通（认为比书面的文档更有效）、频繁交付新的软件版本、紧凑而自我组织型的团队、能够很好地适应需求变化的代码编写和团队组织方法，也更注重软件开发过程中人的作用。 极限编程（XP） - 极限编程是敏捷软件开发中最有成效的方法学之一。极限编程技术以沟通（Communication）、简单（Simplicity）、反馈（Feedback）、勇气（Courage）和尊重（Respect）为价值标准。 软件需求 软件需求包括三个不同的层次：业务需求、用户需求和功能需求。 **业务需求（Business requirement）**表示组织或客户高层次的目标。业务需求通常来自项目投资人、购买产品的客户、实际用户的管理者、市场营销部门或产品策划部门。业务需求描述了组织为什么要开发一个系统，即组织希望达到的目标。使用前景和范围（ vision and scope ）文档来记录业务需求，这份文档有时也被称作项目轮廓图或市场需求（ project charter 或 market requirement ）文档。 **用户需求（user requirement）**描述的是用户的目标，或用户要求系统必须能完成的任务。用例、场景描述和事件――响应表都是表达用户需求的有效途径。也就是说用户需求描述了用户能使用系统来做些什么。 **功能需求（functional requirement）**规定开发人员必须在产品中实现的软件功能，用户利用这些功能来完成任务，满足业务需求。功能需求有时也被称作行为需求（ behavioral requirement ），因为习惯上总是用“应该”对其进行描述：“系统应该发送电子邮件来通知用户已接受其预定”。功能需求描述是开发人员需要实现什么。 **系统需求（system requirement）**用于描述包含多个子系统的产品（即系统）的顶级需求。系统可以只包含软件系统，也可以既包含软件又包含硬件子系统。人也可以是系统的一部分，因此某些系统功能可能要由人来承担。 软件需求说明书（ SRS ） 软件需求说明书（ SRS ）完整地描述了软件系统的预期特性。开发、测试、质量保证、项目管理和其他相关的项目功能都要用到 SRS 。 除了功能需求外， SRS 中还包含非功能需求，包括性能指标和对质量属性的描述。 **质量属性（quality attribute）**对产品的功能描述作了补充，它从不同方面描述了产品的各种特性。这些特性包括可用性、可移植性、完整性、效率和健壮性，它们对用户或开发人员都很重要。其他的非功能需求包括系统与外部世界的外部界面，以及对设计与实现的约束。 **约束（constraint）**限制了开发人员设计和构建系统时的选择范围。 软件生命周期 软件生命周期（Software Life Cycle,SLC）是软件的产生直到报废或停止使用的生命周期。 问题定义 - 要求系统分析员与用户进行交流，弄清“用户需要计算机解决什么问题”然后提出关于“系统目标与范围的说明”，提交用户审查和确认。 可行性研究 - 一方面在于把待开发的系统的目标以明确的语言描述出来；另一方面从经济、技术、法律等多方面进行可行性分析。 需求分析 - 弄清用户对软件系统的全部需求，编写需求规格说明书和初步的用户手册，提交评审。 开发阶段 概要设计 详细设计 编码实现 软件测试 - 测试的过程分单元测试、组装测试以及系统测试三个阶段进行。测试的方法主要有白盒测试和黑盒测试两种。 维护 软件生命周期模型 瀑布模型 瀑布模型（Waterfall Model）强调系统开发应有完整的周期，且必须完整的经历周期的每一开发阶段，并系统化的考量分析与设计的技术、时间与资源之投入等。 核心思想 瀑布模型核心思想是按工序将问题拆分，将功能的实现与设计分开，便于分工协作，即采用结构化的分析与设计方法将逻辑实现与物理实现分开。将软件生命周期划分为制定计划、需求分析、软件设计、程序编写、软件测试和运行维护等六个基本活动，并且规定了它们自上而下、相互衔接的固定次序，如同瀑布流水，逐级下落。 优缺点 优点 为项目提供了按阶段划分的检查点。 当前一阶段完成后，您只需要去关注后续阶段。 可在迭代模型中应用瀑布模型。 它提供了一个模板，这个模板使得分析、设计、编码、测试和支持的方法可以在该模板下有一个共同的指导。 缺点 各个阶段的划分完全固定，阶段之间产生大量的文档，极大地增加了工作量。 由于开发模型是线性的，用户只有等到整个过程的末期才能见到开发成果，从而增加了开发风险。 通过过多的强制完成日期和里程碑来跟踪各个项目阶段。 瀑布模型的突出缺点是不适应用户需求的变化。 适用场景 是否使用这一模型主要取决于是否能理解客户的需求以及在项目的进程中这些需求的变化程度。对于需求经常变化的项目，不要适用瀑布模型。 螺旋模型 螺旋模型基本做法是在“瀑布模型”的每一个开发阶段前引入一个非常严格的风险识别、风险分析和风险控制，它把软件项目分解成一个个小项目。每个小项目都标识一个或多个主要风险，直到所有的主要风险因素都被确定。 核心思想 螺旋模型沿着螺线进行若干次迭代，图中的四个象限代表了以下活动： 制定计划 - 确定软件目标，选定实施方案，弄清项目开发的限制条件； 风险分析 - 分析评估所选方案，考虑如何识别和消除风险； 实施工程 - 实施软件开发和验证； 客户评估 - 评价开发工作，提出修正建议，制定下一步计划。 螺旋模型由风险驱动，强调可选方案和约束条件从而支持软件的重用，有助于将软件质量作为特殊目标融入产品开发之中。 优缺点 优点 设计上的灵活性,可以在项目的各个阶段进行变更。 以小的分段来构建大型系统,使成本计算变得简单容易。 客户始终参与每个阶段的开发,保证了项目不偏离正确方向以及项目的可控性。 随着项目推进,客户始终掌握项目的最新信息, 从而他或她能够和管理层有效地交互。 客户认可这种公司内部的开发方式带来的良好的沟通和高质量的产品。 缺点 很难让用户确信这种演化方法的结果是可以控制的。建设周期长，而软件技术发展比较快，所以经常出现软件开发完毕后，和当前的技术水平有了较大的差距，无法满足当前用户需求。 适用场景 对于新项目，需求不明确的情况下，适合用螺旋模型进行开发，便于风险控制和需求变更。 软件工程术语 里程碑（Milestone） - 在制定项目进度计划时，在进度时间表上设立一些重要的时间检查点，这样一来，就可以在项目执行过程中利用这些重要的时间检查点来对项目的进程进行检查和控制。这些重要的时间检查点被称作项目的里程碑。 人月 - 软件开发的工作量单位。如 200 人月，10 个人开发，那算来就是花 20 个月就可完工。 基线 - 基线是项目储存库中每个工件版本在特定时期的一个“快照”。它提供一个正式标准，随后的工作基于此标准，并且只有经过授权后才能变更这个标准。建立一个初始基线后，以后每次对其进行的变更都将记录为一个差值，直到建成下一个基线。 更多内容 书 人月神话 代码大全 文章 使用甘特图做项目管理 使用燃尽图监控项目整体进度 工具 10 大开源免费的项目管理软件推荐 http://www.ruanyifeng.com/blog/2017/08/issue.html 文档模板 软件工程文档标准模板百度网盘下载 - 下载密码：uu1f]]></content>
      <categories>
        <category>method</category>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>method</tag>
        <tag>软件工程</tag>
        <tag>项目管理</tag>
        <tag>方法论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 快速指南]]></title>
    <url>%2Fblog%2F2017%2F11%2F17%2Fos%2Flinux%2Ftool%2Fvim%2F</url>
    <content type="text"><![CDATA[Vim 快速指南 1. 概念 1.1. 什么是 vim 1.2. Vim 的模式 2. Vim 渐进学习 2.1. 存活 2.2. 感觉良好 2.3. 更好，更强，更快 2.4. Vim 超能力 3. Vim Cheat Sheet 3.1. 经典版 3.2. 入门版 3.3. 进阶版 3.4. 增强版 3.5. 文字版 4. 资料 1. 概念 1.1. 什么是 vim Vim 是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。和 Emacs 并列成为类 Unix 系统用户最喜欢的编辑器。 1.2. Vim 的模式 基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），插入模式（Insert mode）和底线命令模式（Last line mode）。 命令模式 用户刚刚启动 vi/vim，便进入了命令模式。 此状态下敲击键盘动作会被 Vim 识别为命令，而非输入字符。 插入模式 在命令模式下按下 i 就进入了输入模式。 在输入模式下，你可以输入文本内容。 底线命令模式 在命令模式下按下 :（英文冒号）就进入了底线命令模式。 底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。 2. Vim 渐进学习 2.1. 存活 安装 vim 启动 vim **什么也别干！**请先阅读 当你安装好一个编辑器后，你一定会想在其中输入点什么东西，然后看看这个编辑器是什么样子。但 vim 不是这样的，请按照下面的命令操作： 启 动 Vim 后，vim 在 Normal 模式下。 让我们进入 Insert 模式，请按下键 i 。(注：你会看到 vim 左下角有一个–insert–字样，表示，你可以以插入的方式输入了） 此时，你可以输入文本了，就像你用“记事本”一样。 如果你想返回 Normal 模式，请按 ESC 键。 现在，你知道如何在 Insert 和 Normal 模式下切换了。下面是一些命令，可以让你在 Normal 模式下幸存下来： i → Insert 模式，按 ESC 回到 Normal 模式. x → 删当前光标所在的一个字符。 :wq → 存盘 + 退出 (:w 存盘, :q 退出) （注：:w 后可以跟文件名） dd → 删除当前行，并把删除的行存到剪贴板里 p → 粘贴剪贴板 推荐 hjkl (强例推荐使用其移动光标，但不必需) → 你也可以使用光标键 (←↓↑→). 注: j 就像下箭头。 :help &lt;command&gt; → 显示相关命令的帮助。你也可以就输入 :help 而不跟命令。（注：退出帮助需要输入:q） 你能在 vim 幸存下来只需要上述的那 5 个命令，你就可以编辑文本了，你一定要把这些命令练成一种下意识的状态。于是你就可以开始进阶到第二级了。 当是，在你进入第二级时，需要再说一下 Normal 模式。在一般的编辑器下，当你需要 copy 一段文字的时候，你需要使用 Ctrl 键，比如：Ctrl-C。也就是说，Ctrl 键就好像功能键一样，当你按下了功能键 Ctrl 后，C 就不在是 C 了，而且就是一个命令或是一个快键键了，在 vim 的 Normal 模式下，所有的键都是功能键。这个你需要知道。 标记 下面的文字中，如果是 Ctrl-λ我会写成 &lt;C-λ&gt;. 以 : 开始的命令你需要输入 &lt;enter&gt;回车，例如 — 如果我写成 :q 也就是说你要输入 :q&lt;enter&gt;. 2.2. 感觉良好 上面的那些命令只能让你存活下来，现在是时候学习一些更多的命令了，下面是我的建议：（注：所有的命令都需要在 Normal 模式下使用，如果你不知道现在在什么样的模式，你就狂按几次 ESC 键） 各种插入模式 a → 在光标后插入 o → 在当前行后插入一个新行 O → 在当前行前插入一个新行 cw → 替换从光标所在位置后到一个单词结尾的字符 简单的移动光标 0 → 数字零，到行头 ^ → 到本行第一个不是 blank 字符的位置（所谓 blank 字符就是空格，tab，换行，回车等） $ → 到本行行尾 g_ → 到本行最后一个不是 blank 字符的位置。 /pattern → 搜索 pattern 的字符串（注：如果搜索出多个匹配，可按 n 键到下一个） 拷贝/粘贴 （注：p/P 都可以，p 是表示在当前位置之后，P 表示在当前位置之前） P → 粘贴 yy → 拷贝当前行当行于 ddP Undo/Redo u → undo &lt;C-r&gt; → redo 打开/保存/退出/改变文件 (Buffer) :e &lt;path/to/file&gt; → 打开一个文件 :w → 存盘 :saveas &lt;path/to/file&gt; → 另存为 &lt;path/to/file&gt; :x， ZZ 或 :wq → 保存并退出 (:x 表示仅在需要时保存，ZZ 不需要输入冒号并回车) :q! → 退出不保存 :qa! 强行退出所有的正在编辑的文件，就算别的文件有更改。 :bn 和 :bp → 你可以同时打开很多文件，使用这两个命令来切换下一个或上一个文件。（注：我喜欢使用:n 到下一个文件） 花点时间熟悉一下上面的命令，一旦你掌握他们了，你就几乎可以干其它编辑器都能干的事了。但是到现在为止，你还是觉得使用 vim 还是有点笨拙，不过没关系，你可以进阶到第三级了。 2.3. 更好，更强，更快 先恭喜你！你干的很不错。我们可以开始一些更为有趣的事了。在第三级，我们只谈那些和 vi 可以兼容的命令。 更好 下面，让我们看一下 vim 是怎么重复自己的：1515G . → (小数点) 可以重复上一次的命令 N&lt;command&gt; → 重复某个命令 N 次 下面是一个示例，找开一个文件你可以试试下面的命令： 2dd → 删除 2 行 3p → 粘贴文本 3 次 100idesu [ESC] → 会写下 “desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu desu “ . → 重复上一个命令—— 100 “desu “. 3. → 重复 3 次 “desu” (注意：不是 300，你看，VIM 多聪明啊). 更强 你要让你的光标移动更有效率，你一定要了解下面的这些命令，千万别跳过。 NG → 到第 N 行 （注：注意命令中的 G 是大写的，另我一般使用 : N 到第 N 行，如 :137 到第 137 行） gg → 到第一行。（注：相当于 1G，或 :1） G → 到最后一行。 按单词移动： w → 到下一个单词的开头。 e → 到下一个单词的结尾。 &gt; 如果你认为单词是由默认方式，那么就用小写的 e 和 w。默认上来说，一个单词由字母，数字和下划线组成（注：程序变量） &gt; 如果你认为单词是由 blank 字符分隔符，那么你需要使用大写的 E 和 W。（注：程序语句） 下面，让我来说说最强的光标移动： % : 匹配括号移动，包括 (, {, [. （注：你需要把光标先移到括号上） * 和 #: 匹配光标当前所在的单词，移动光标到下一个（或上一个）匹配单词（*是下一个，#是上一个） 相信我，上面这三个命令对程序员来说是相当强大的。 更快 你一定要记住光标的移动，因为很多命令都可以和这些移动光标的命令连动。很多命令都可以如下来干： &lt;start position&gt;&lt;command&gt;&lt;end position&gt; 例如 0y$ 命令意味着： 0 → 先到行头 y → 从这里开始拷贝 $ → 拷贝到本行最后一个字符 你可可以输入 ye，从当前位置拷贝到本单词的最后一个字符。 你也可以输入 y2/foo 来拷贝 2 个 “foo” 之间的字符串。 还有很多时间并不一定你就一定要按 y 才会拷贝，下面的命令也会被拷贝： d (删除 ) v (可视化的选择) gU (变大写) gu (变小写) 等等 （注：可视化选择是一个很有意思的命令，你可以先按 v，然后移动光标，你就会看到文本被选择，然后，你可能 d，也可 y，也可以变大写等） 2.4. Vim 超能力 你只需要掌握前面的命令，你就可以很舒服的使用 VIM 了。但是，现在，我们向你介绍的是 VIM 杀手级的功能。下面这些功能是我只用 vim 的原因。 在当前行上移动光标: 0 ^ ####fFtT,``;` 0 → 到行头 ^ → 到本行的第一个非 blank 字符 $ → 到行尾 g_ → 到本行最后一个不是 blank 字符的位置。 fa → 到下一个为 a 的字符处，你也可以 fs 到下一个为 s 的字符。 t, → 到逗号前的第一个字符。逗号可以变成其它字符。 3fa → 在当前行查找第三个出现的 a。 F 和 T → 和 f 和 t 一样，只不过是相反方向。 还有一个很有用的命令是 dt&quot; → 删除所有的内容，直到遇到双引号—— &quot;。 区域选择 &lt;action&gt;a&lt;object&gt; 或 &lt;action&gt;i&lt;object&gt; 在 visual 模式下，这些命令很强大，其命令格式为 &lt;action&gt;a&lt;object&gt; 和 &lt;action&gt;i&lt;object&gt; action 可以是任何的命令，如 d (删除), y (拷贝), v (可以视模式选择)。 object 可能是： w 一个单词， W 一个以空格为分隔的单词， s 一个句字， p 一个段落。也可以是一个特别的字符：&quot;、 '、 )、 }、 ]。 假设你有一个字符串 (map (+) (&quot;foo&quot;)).而光标键在第一个 o的位置。 vi&quot; → 会选择 foo. va&quot; → 会选择 &quot;foo&quot;. vi) → 会选择 &quot;foo&quot;. va) → 会选择(&quot;foo&quot;). v2i) → 会选择 map (+) (&quot;foo&quot;) v2a) → 会选择 (map (+) (&quot;foo&quot;)) 块操作: &lt;C-v&gt; 块操作，典型的操作： 0 &lt;C-v&gt; &lt;C-d&gt; I-- [ESC] ^ → 到行头 &lt;C-v&gt; → 开始块操作 &lt;C-d&gt; → 向下移动 (你也可以使用 hjkl 来移动光标，或是使用%，或是别的) I-- [ESC] → I 是插入，插入“--”，按 ESC 键来为每一行生效。 在 Windows 下的 vim，你需要使用 &lt;C-q&gt; 而不是 &lt;C-v&gt; ，&lt;C-v&gt; 是拷贝剪贴板。 自动提示： &lt;C-n&gt; 和 &lt;C-p&gt; 在 Insert 模式下，你可以输入一个词的开头，然后按 &lt;C-p&gt;或是&lt;C-n&gt;，自动补齐功能就出现了…… 宏录制： qa 操作序列 q, @a, @@ qa 把你的操作记录在寄存器 a。 于是 @a 会 replay 被录制的宏。 @@ 是一个快捷键用来 replay 最新录制的宏。 示例 在一个只有一行且这一行只有“1”的文本中，键入如下命令： &gt; qaYp&lt;C-a&gt;q&gt; → qa 开始录制 Yp 复制行. &lt;C-a&gt; 增加 1. q 停止录制. @a → 在 1 下面写下 2 @@ → 在 2 正面写下 3 现在做 100@@ 会创建新的 100 行，并把数据增加到 103. 可视化选择： v,V,&lt;C-v&gt; 前面，我们看到了 &lt;C-v&gt;的示例 （在 Windows 下应该是），我们可以使用 v 和 V。一但被选好了，你可以做下面的事： J → 把所有的行连接起来（变成一行） &lt; 或 &gt; → 左右缩进 = → 自动给缩进 （注：这个功能相当强大，我太喜欢了） 在所有被选择的行后加上点东西： &lt;C-v&gt; 选中相关的行 (可使用 j 或 &lt;C-d&gt; 或是 /pattern 或是 % 等……) $ 到行最后 A, 输入字符串，按 ESC。 分屏: :split 和 vsplit. 下面是主要的命令，你可以使用 VIM 的帮助 :help split. 你可以参考本站以前的一篇文章VIM 分屏。 :split → 创建分屏 (:vsplit创建垂直分屏) &lt;C-w&gt;&lt;dir&gt; : dir 就是方向，可以是 hjkl 或是 ←↓↑→ 中的一个，其用来切换分屏。 &lt;C-w&gt;_ (或 &lt;C-w&gt;|) : 最大化尺寸 (| 垂直分屏) &lt;C-w&gt;+ (或 &lt;C-w&gt;-) : 增加尺寸 3. Vim Cheat Sheet 本节内容的原文地址：http://cenalulu.github.io/linux/all-vim-cheatsheat/ 3.1. 经典版 下面这个键位图应该是大家最常看见的经典版了。其实这个版本是一系列的入门教程键位图的组合结果。要查看不同编辑模式下的键位图，可以看这里打包下载 此外，这里还有简体中文版。 3.2. 入门版 基本操作的入门版。原版出处还有 keynote 版本可供 DIY 以及其他相关有用的 cheatsheet。 3.3. 进阶版 下图是 300DPI 的超清大图，另外查看原文还有更多版本：黑白，低分辨率，色盲等 3.4. 增强版 下图是一个更新时间较新的现代版，含有的信息也更丰富。原文链接 3.5. 文字版 原文链接 4. 资料 简明 VIM 练级攻略 ，Vim 渐进学习内容来源于这篇文章，作为 Vim 新手，我觉得入门效果很好。 vim 官方文档 vim-galore Vim 入门基础]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8 快速指南]]></title>
    <url>%2Fblog%2F2017%2F11%2F14%2Fjava%2Fjavacore%2Fadvanced%2FJava8%2F</url>
    <content type="text"><![CDATA[JDK8 快速指南 JDK8 升级常见问题章节是我个人的经验整理。其他内容基本翻译自 java8-tutorial 📓 本文已归档到：「blog」 Default Methods for Interfaces(接口的默认方法) Lambda expressions(Lambda 表达式) Functional Interfaces(函数接口) Method and Constructor References(方法和构造器引用) Lambda Scopes(Lambda 作用域) Built-in Functional Interfaces(内置函数接口) Optionals Streams Parallel Streams Maps Date API Annotations JDK8 升级常见问题 参考资料 Default Methods for Interfaces(接口的默认方法) Java 8 使我们能够通过使用 default 关键字将非抽象方法实现添加到接口。这个功能也被称为虚拟扩展方法。 这是我们的第一个例子： interface Formula &#123; double calculate(int a); default double sqrt(int a) &#123; return Math.sqrt(a); &#125;&#125; 除了抽象方法 calculate ，接口 Formula 还定义了默认方法 sqrt。具体类只需要执行抽象方法计算。默认的方法 sqrt 可以用于开箱即用。 Formula formula = new Formula() &#123; @Override public double calculate(int a) &#123; return sqrt(a * 100); &#125;&#125;;formula.calculate(100); // 100.0formula.sqrt(16); // 4.0 Formula 被实现为一个匿名对象。代码非常冗长：用于 sqrt(a * 100) 这样简单的计算的 6 行代码。正如我们将在下一节中看到的，在 Java 8 中实现单个方法对象有更好的方法。 Lambda expressions(Lambda 表达式) 让我们从一个简单的例子来说明如何在以前版本的 Java 中对字符串列表进行排序： List&lt;String&gt; names = Arrays.asList("peter", "anna", "mike", "xenia");Collections.sort(names, new Comparator&lt;String&gt;() &#123; @Override public int compare(String a, String b) &#123; return b.compareTo(a); &#125;&#125;); 静态工具方法 Collections.sort 为了对指定的列表进行排序，接受一个列表和一个比较器。您会发现自己经常需要创建匿名比较器并将其传递给排序方法。 Java 8 使用更简短的 lambda 表达式来避免常常创建匿名对象的问题： Collections.sort(names, (String a, String b) -&gt; &#123; return b.compareTo(a);&#125;); 如您缩减，这段代码比上段代码简洁很多。但是，还可以更加简洁： Collections.sort(names, (String a, String b) -&gt; b.compareTo(a)); 这行代码中，你省去了花括号 {} 和 return 关键字。但是，这还不算完，它还可以再进一步简洁： names.sort((a, b) -&gt; b.compareTo(a)); 列表现在有一个 sort 方法。此外，java 编译器知道参数类型，所以你可以不指定入参的数据类型。让我们深入探讨如何使用 lambda 表达式。 Functional Interfaces(函数接口) lambda 表达式如何适应 Java 的类型系统？每个 lambda 对应一个由接口指定的类型。一个所谓的函数接口必须包含一个抽象方法声明。该类型的每个 lambda 表达式都将与此抽象方法匹配。由于默认方法不是抽象的，所以你可以自由地添加默认方法到你的函数接口。 只要保证接口仅包含一个抽象方法，就可以使用任意的接口作为 lambda 表达式。为确保您的接口符合要求，您应该添加 @FunctionalInterface 注解。编译器注意到这个注解后，一旦您尝试在接口中添加第二个抽象方法声明，编译器就会抛出编译器错误。 示例： @FunctionalInterfaceinterface Converter&lt;F, T&gt; &#123; T convert(F from);&#125; Converter&lt;String, Integer&gt; converter = (from) -&gt; Integer.valueOf(from);Integer converted = converter.convert("123");System.out.println(converted); // 123 请记住，如果 @FunctionalInterface 注解被省略，代码也是有效的。 Method and Constructor References(方法和构造器引用) 上面的示例代码可以通过使用静态方法引用进一步简化： Converter&lt;String, Integer&gt; converter = Integer::valueOf;Integer converted = converter.convert("123");System.out.println(converted); // 123 Java 8 允许您通过 :: 关键字传递方法或构造函数的引用。上面的例子展示了如何引用一个静态方法。但是我们也可以引用对象方法： class Something &#123; String startsWith(String s) &#123; return String.valueOf(s.charAt(0)); &#125;&#125; Something something = new Something();Converter&lt;String, String&gt; converter = something::startsWith;String converted = converter.convert("Java");System.out.println(converted); // "J" 我们来观察一下 :: 关键字是如何作用于构造器的。首先，我们定义一个有多个构造器的示例类。 class Person &#123; String firstName; String lastName; Person() &#123;&#125; Person(String firstName, String lastName) &#123; this.firstName = firstName; this.lastName = lastName; &#125;&#125; 接着，我们指定一个用于创建 Person 对象的 PersonFactory 接口。 interface PersonFactory&lt;P extends Person&gt; &#123; P create(String firstName, String lastName);&#125; 我们不是手动实现工厂，而是通过构造引用将所有东西粘合在一起： PersonFactory&lt;Person&gt; personFactory = Person::new;Person person = personFactory.create("Peter", "Parker"); 我们通过 Person::new 来创建一个 Person 构造器的引用。Java 编译器会根据PersonFactory.create 的签名自动匹配正确的构造器。 Lambda Scopes(Lambda 作用域) 从 lambda 表达式访问外部作用域变量与匿名对象非常相似。您可以访问本地外部作用域的常量以及实例的成员变量和静态变量。 Accessing local variables(访问本地变量) 我们可以访问 lambda 表达式作用域外部的常量： final int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);stringConverter.convert(2); // 3 不同于匿名对象的是：这个变量 num 不是一定要被 final 修饰。下面的代码一样合法： int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);stringConverter.convert(2); // 3 但是，num 必须是隐式常量的。下面的代码不能编译通过： int num = 1;Converter&lt;Integer, String&gt; stringConverter = (from) -&gt; String.valueOf(from + num);num = 3; 此外，在 lambda 表达式中对 num 做写操作也是被禁止的。 Accessing fields and static variables(访问成员变量和静态变量) 与局部变量相比，我们既可以在 lambda 表达式中读写实例的成员变量，也可以读写实例的静态变量。这种行为在匿名对象中是众所周知的。 class Lambda4 &#123; static int outerStaticNum; int outerNum; void testScopes() &#123; Converter&lt;Integer, String&gt; stringConverter1 = (from) -&gt; &#123; outerNum = 23; return String.valueOf(from); &#125;; Converter&lt;Integer, String&gt; stringConverter2 = (from) -&gt; &#123; outerStaticNum = 72; return String.valueOf(from); &#125;; &#125;&#125; Accessing Default Interface Methods（访问默认的接口方法） 还记得第一节的 formula 例子吗？ Formula 接口定义了一个默认方法 sqrt，它可以被每个 formula 实例（包括匿名对象）访问。这个特性不适用于 lambda 表达式。 默认方法不能被 lambda 表达式访问。下面的代码不能编译通过： Formula formula = (a) -&gt; sqrt(a * 100); Built-in Functional Interfaces(内置函数接口) JDK 1.8 API 包含许多内置的功能接口。它们中的一些在较早的 Java 版本（比如 Comparator 或 Runnable）中是众所周知的。这些现有的接口通过 @FunctionalInterfaceannotation 注解被扩展为支持 Lambda。 但是，Java 8 API 也提供了不少新的函数接口。其中一些新接口在 Google Guava 库中是众所周知的。即使您熟悉这个库，也应该密切关注如何通过一些有用的方法扩展来扩展这些接口。 Predicates Predicate 是只有一个参数的布尔值函数。该接口包含各种默认方法，用于将谓词组合成复杂的逻辑术语（与、或、非） Predicate&lt;String&gt; predicate = (s) -&gt; s.length() &gt; 0;predicate.test("foo"); // truepredicate.negate().test("foo"); // falsePredicate&lt;Boolean&gt; nonNull = Objects::nonNull;Predicate&lt;Boolean&gt; isNull = Objects::isNull;Predicate&lt;String&gt; isEmpty = String::isEmpty;Predicate&lt;String&gt; isNotEmpty = isEmpty.negate(); Functions Function 接受一个参数并产生一个结果。可以使用默认方法将多个函数链接在一起（compose、andThen）。 Function&lt;String, Integer&gt; toInteger = Integer::valueOf;Function&lt;String, String&gt; backToString = toInteger.andThen(String::valueOf);backToString.apply("123"); // "123" Suppliers Supplier 产生一个泛型结果。与 Function 不同，Supplier 不接受参数。 Supplier&lt;Person&gt; personSupplier = Person::new;personSupplier.get(); // new Person Consumers Consumer 表示要在一个输入参数上执行的操作。 Consumer&lt;Person&gt; greeter = (p) -&gt; System.out.println("Hello, " + p.firstName);greeter.accept(new Person("Luke", "Skywalker")); Comparators 比较器在老版本的 Java 中是众所周知的。 Java 8 为接口添加了各种默认方法。 Comparator&lt;Person&gt; comparator = (p1, p2) -&gt; p1.firstName.compareTo(p2.firstName);Person p1 = new Person("John", "Doe");Person p2 = new Person("Alice", "Wonderland");comparator.compare(p1, p2); // &gt; 0comparator.reversed().compare(p1, p2); // &lt; 0 Optionals Optional 不是功能性接口，而是防止 NullPointerException 的好工具。这是下一节的一个重要概念，所以让我们快速看看 Optional 是如何工作的。 可选是一个简单的容器，其值可以是 null 或非 null。想想一个可能返回一个非空结果的方法，但有时候什么都不返回。不是返回 null，而是返回 Java 8 中的 Optional。 Optional&lt;String&gt; optional = Optional.of("bam");optional.isPresent(); // trueoptional.get(); // "bam"optional.orElse("fallback"); // "bam"optional.ifPresent((s) -&gt; System.out.println(s.charAt(0))); // "b" Streams java.util.Stream 表示可以在其上执行一个或多个操作的元素序列。流操作是中间或终端。当终端操作返回一个特定类型的结果时，中间操作返回流本身，所以你可以链接多个方法调用。流在源上创建，例如一个 java.util.Collection 像列表或集合（不支持映射）。流操作既可以按顺序执行，也可以并行执行。 流是非常强大的，所以，我写了一个独立的 Java 8 Streams 教程 。您还应该查看 Sequent，将其作为 Web 的类似库。 我们先来看看顺序流如何工作。首先，我们以字符串列表的形式创建一个示例源代码： List&lt;String&gt; stringCollection = new ArrayList&lt;&gt;();stringCollection.add("ddd2");stringCollection.add("aaa2");stringCollection.add("bbb1");stringCollection.add("aaa1");stringCollection.add("bbb3");stringCollection.add("ccc");stringCollection.add("bbb2");stringCollection.add("ddd1"); Java 8 中的集合已被扩展，因此您可以通过调用 Collection.stream() 或Collection.parallelStream() 来简单地创建流。以下各节介绍最常见的流操作。 Filter 过滤器接受一个谓词来过滤流的所有元素。这个操作是中间的，使我们能够调用另一个流操作（forEach）的结果。 ForEach 接受一个消费者被执行的过滤流中的每个元素。 ForEach 是一个终端操作。它是无效的，所以我们不能调用另一个流操作。 stringCollection .stream() .filter((s) -&gt; s.startsWith("a")) .forEach(System.out::println);// "aaa2", "aaa1" Sorted 排序是一个中间操作，返回流的排序视图。元素按自然顺序排序，除非您传递自定义比较器。 stringCollection .stream() .sorted() .filter((s) -&gt; s.startsWith("a")) .forEach(System.out::println);// "aaa1", "aaa2" 请记住，排序只会创建流的排序视图，而不会操纵支持的集合的排序。 stringCollection 的排序是不变的： System.out.println(stringCollection);// ddd2, aaa2, bbb1, aaa1, bbb3, ccc, bbb2, ddd1 Map 中间操作映射通过给定函数将每个元素转换为另一个对象。以下示例将每个字符串转换为大写字母字符串。但是您也可以使用 map 将每个对象转换为另一种类型。结果流的泛型类型取决于您传递给 map 的函数的泛型类型。 stringCollection .stream() .map(String::toUpperCase) .sorted((a, b) -&gt; b.compareTo(a)) .forEach(System.out::println);// "DDD2", "DDD1", "CCC", "BBB3", "BBB2", "AAA2", "AAA1" Match 可以使用各种匹配操作来检查某个谓词是否与流匹配。所有这些操作都是终端并返回布尔结果。 boolean anyStartsWithA = stringCollection .stream() .anyMatch((s) -&gt; s.startsWith("a"));System.out.println(anyStartsWithA); // trueboolean allStartsWithA = stringCollection .stream() .allMatch((s) -&gt; s.startsWith("a"));System.out.println(allStartsWithA); // falseboolean noneStartsWithZ = stringCollection .stream() .noneMatch((s) -&gt; s.startsWith("z"));System.out.println(noneStartsWithZ); // true Count Count 是一个终端操作，返回流中元素的个数。 long startsWithB = stringCollection .stream() .filter((s) -&gt; s.startsWith("b")) .count();System.out.println(startsWithB); // 3 Reduce 该终端操作使用给定的功能对流的元素进行缩减。结果是一个 Optional 持有缩小后的值。 Optional&lt;String&gt; reduced = stringCollection .stream() .sorted() .reduce((s1, s2) -&gt; s1 + "#" + s2);reduced.ifPresent(System.out::println);// "aaa1##aaa2##bbb1##bbb2##bbb3##ccc##ddd1##ddd2" Parallel Streams 如上所述，流可以是顺序的也可以是并行的。顺序流上的操作在单个线程上执行，而并行流上的操作在多个线程上同时执行。 以下示例演示了通过使用并行流提高性能是多么容易。 首先，我们创建一个较大的独特元素的列表： int max = 1000000;List&lt;String&gt; values = new ArrayList&lt;&gt;(max);for (int i = 0; i &lt; max; i++) &#123; UUID uuid = UUID.randomUUID(); values.add(uuid.toString());&#125; 现在我们测量对这个集合进行排序所花费的时间。 Sequential Sort long t0 = System.nanoTime();long count = values.stream().sorted().count();System.out.println(count);long t1 = System.nanoTime();long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format("sequential sort took: %d ms", millis));// sequential sort took: 899 ms Parallel Sort long t0 = System.nanoTime();long count = values.parallelStream().sorted().count();System.out.println(count);long t1 = System.nanoTime();long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0);System.out.println(String.format("parallel sort took: %d ms", millis));// parallel sort took: 472 ms 如你所见，两个代码段差不多，但是并行排序快了近 50%。你所需做的仅仅是将 stream() 改为 parallelStream() 。 Maps 如前所述，map 不直接支持流。Map 接口本身没有可用的 stream() 方法，但是你可以通过 map.keySet().stream() 、 map.values().stream() 和 map.entrySet().stream() 创建指定的流。 此外，map 支持各种新的、有用的方法来处理常见任务。 Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;();for (int i = 0; i &lt; 10; i++) &#123; map.putIfAbsent(i, "val" + i);&#125;map.forEach((id, val) -&gt; System.out.println(val)); 上面的代码应该是自我解释的：putIfAbsent 阻止我们写入额外的空值检查；forEach 接受消费者为 map 的每个值实现操作。 这个例子展示了如何利用函数来计算 map 上的代码： map.computeIfPresent(3, (num, val) -&gt; val + num);map.get(3); // val33map.computeIfPresent(9, (num, val) -&gt; null);map.containsKey(9); // falsemap.computeIfAbsent(23, num -&gt; "val" + num);map.containsKey(23); // truemap.computeIfAbsent(3, num -&gt; "bam");map.get(3); // val33 接下来，我们学习如何删除给定键的条目，只有当前键映射到给定值时： map.remove(3, "val3");map.get(3); // val33map.remove(3, "val33");map.get(3); // null 另一个有用方法： map.getOrDefault(42, "not found"); // not found 合并一个 map 的 entry 很简单： map.merge(9, "val9", (value, newValue) -&gt; value.concat(newValue));map.get(9); // val9map.merge(9, "concat", (value, newValue) -&gt; value.concat(newValue));map.get(9); // val9concat 如果不存在该键的条目，合并或者将键/值放入 map 中；否则将调用合并函数来更改现有值。 Date API Java 8 在 java.time 包下新增了一个全新的日期和时间 API。新的日期 API 与 Joda-Time 库相似，但不一样。以下示例涵盖了此新 API 的最重要部分。 Clock Clock 提供对当前日期和时间的访问。Clock 知道一个时区，可以使用它来代替 System.currentTimeMillis() ，获取从 Unix EPOCH 开始的以毫秒为单位的当前时间。时间线上的某一时刻也由类 Instant 表示。 Instants 可以用来创建遗留的 java.util.Date 对象。 Clock clock = Clock.systemDefaultZone();long millis = clock.millis();Instant instant = clock.instant();Date legacyDate = Date.from(instant); // legacy java.util.Date Timezones 时区由 ZoneId 表示。他们可以很容易地通过静态工厂方法访问。时区定义了某一时刻和当地日期、时间之间转换的重要偏移量。 System.out.println(ZoneId.getAvailableZoneIds());// prints all available timezone idsZoneId zone1 = ZoneId.of("Europe/Berlin");ZoneId zone2 = ZoneId.of("Brazil/East");System.out.println(zone1.getRules());System.out.println(zone2.getRules());// ZoneRules[currentStandardOffset=+01:00]// ZoneRules[currentStandardOffset=-03:00] LocalTime LocalTime 代表没有时区的时间，例如晚上 10 点或 17:30:15。以下示例为上面定义的时区创建两个本地时间。然后我们比较两次，并计算两次之间的小时和分钟的差异。 LocalTime now1 = LocalTime.now(zone1);LocalTime now2 = LocalTime.now(zone2);System.out.println(now1.isBefore(now2)); // falselong hoursBetween = ChronoUnit.HOURS.between(now1, now2);long minutesBetween = ChronoUnit.MINUTES.between(now1, now2);System.out.println(hoursBetween); // -3System.out.println(minutesBetween); // -239 LocalTime 带有各种工厂方法，以简化新实例的创建，包括解析时间字符串。 LocalTime late = LocalTime.of(23, 59, 59);System.out.println(late); // 23:59:59DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedTime(FormatStyle.SHORT) .withLocale(Locale.GERMAN);LocalTime leetTime = LocalTime.parse("13:37", germanFormatter);System.out.println(leetTime); // 13:37 LocalDate LocalDate 表示不同的日期，例如：2014 年 3 月 11 日。它是不可变的，并且与 LocalTime 完全类似。该示例演示如何通过加减日、月或年来计算新日期。请记住，每个操作都会返回一个新的实例。 LocalDate today = LocalDate.now();LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);LocalDate yesterday = tomorrow.minusDays(2);LocalDate independenceDay = LocalDate.of(2014, Month.JULY, 4);DayOfWeek dayOfWeek = independenceDay.getDayOfWeek();System.out.println(dayOfWeek); // FRIDAY 从一个字符串中解析出 LocalDate 对象，和解析 LocalTime 一样的简单： DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedDate(FormatStyle.MEDIUM) .withLocale(Locale.GERMAN);LocalDate xmas = LocalDate.parse("24.12.2014", germanFormatter);System.out.println(xmas); // 2014-12-24 LocalDateTime LocalDateTime 表示日期时间。它将日期和时间组合成一个实例。 LocalDateTime 是不可变的，其作用类似于 LocalTime 和 LocalDate。我们可以利用方法去获取日期时间中某个单位的值。 LocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59);DayOfWeek dayOfWeek = sylvester.getDayOfWeek();System.out.println(dayOfWeek); // WEDNESDAYMonth month = sylvester.getMonth();System.out.println(month); // DECEMBERlong minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY);System.out.println(minuteOfDay); // 1439 通过一个时区的附加信息可以转为一个实例。这个实例很容易转为java.util.Date 类型。 Instant instant = sylvester .atZone(ZoneId.systemDefault()) .toInstant();Date legacyDate = Date.from(instant);System.out.println(legacyDate); // Wed Dec 31 23:59:59 CET 2014 日期时间的格式化类似于 Date 或 Time。我们可以使用自定义模式创建格式化程序，而不是使用预定义的格式。 DateTimeFormatter formatter = DateTimeFormatter .ofPattern("MMM dd, yyyy - HH:mm");LocalDateTime parsed = LocalDateTime.parse("Nov 03, 2014 - 07:13", formatter);String string = formatter.format(parsed);System.out.println(string); // Nov 03, 2014 - 07:13 不同于 java.text.NumberFormat ， DateTimeFormatter 是不可变且线程安全的 。 更多关于日期格式化的内容可以参考这里. Annotations Java 8 中的注释是可重复的。让我们直接看一个例子来解决这个问题。 首先，我们定义一个包含实际注释数组的外层注释： @interface Hints &#123; Hint[] value();&#125;@Repeatable(Hints.class)@interface Hint &#123; String value();&#125; Java8 允许我们通过使用 @Repeatable 注解来引入多个同类型的注解。 Variant 1: 使用容器注解 (老套路) @Hints(&#123;@Hint("hint1"), @Hint("hint2")&#125;)class Person &#123;&#125; Variant 2: 使用 repeatable 注解 (新套路) @Hint("hint1")@Hint("hint2")class Person &#123;&#125; 使用场景 2，Java 编译器隐式地设置了 @Hints 注解。 这对于通过反射来读取注解信息很重要。 Hint hint = Person.class.getAnnotation(Hint.class);System.out.println(hint); // nullHints hints1 = Person.class.getAnnotation(Hints.class);System.out.println(hints1.value().length); // 2Hint[] hints2 = Person.class.getAnnotationsByType(Hint.class);System.out.println(hints2.length); // 2 尽管，我门从没有在 Person 类上声明 @Hints 注解，但是仍可以通过getAnnotation(Hints.class) 读取它。然而，更便利的方式是 getAnnotationsByType ，它可以直接访问所有 @Hint 注解。 此外，Java 8 中的注释使用扩展了两个新的目标： @Target(&#123;ElementType.TYPE_PARAMETER, ElementType.TYPE_USE&#125;)@interface MyAnnotation &#123;&#125; JDK8 升级常见问题 JDK8 发布很久了，它提供了许多吸引人的新特性，能够提高编程效率。 如果是新的项目，使用 JDK8 当然是最好的选择。但是，对于一些老的项目，升级到 JDK8 则存在一些兼容性问题，是否升级需要酌情考虑。 近期，我在工作中遇到一个任务，将部门所有项目的 JDK 版本升级到 1.8 （老版本大多是 1.6）。在这个过程中，遇到一些问题点，并结合在网上看到的坑，在这里总结一下。 Intellij 中的 JDK 环境设置 Settings 点击 File &gt; Settings &gt; Java Compiler Project bytecode version 选择 1.8 点击 File &gt; Settings &gt; Build Tools &gt; Maven &gt; Importing 选择 JDK for importer 为 1.8 Projcet Settings Project SDK 选择 1.8 Application 如果 web 应用的启动方式为 Application ，需要修改 JRE 点击 Run/Debug Configurations &gt; Configuration 选择 JRE 为 1.8 Linux 环境修改 修改环境变量 修改 /etc/profile 中的 JAVA_HOME，设置 为 jdk8 所在路径。 修改后，执行 source /etc/profile 生效。 编译、发布脚本中如果有 export JAVA_HOME ，需要注意，需要使用 jdk8 的路径。 修改 maven settings.xml 中 profile 的激活条件如果是 jdk，需要修改一下 jdk 版本 &lt;activation&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;!-- 修改为 1.8 --&gt;&lt;/activation&gt; 修改 server 修改 server 中的 javac 版本，以 resin 为例： 修改 resin 配置文件中的 javac 参数。 &lt;javac compiler="internal" args="-source 1.8"/&gt; sun.* 包缺失问题 JDK8 不再提供 sun.* 包供开发者使用，因为这些接口不是公共接口，不能保证在所有 Java 兼容的平台上工作。 使用了这些 API 的程序如果要升级到 JDK 1.8 需要寻求替代方案。 虽然，也可以自己导入包含 sun.* 接口 jar 包到 classpath 目录，但这不是一个好的做法。 需要详细了解为什么不要使用 sun.* ，可以参考官方文档：Why Developers Should Not Write Programs That Call ‘sun’ Packages 默认安全策略修改 升级后估计有些小伙伴在使用不安全算法时可能会发生错误，so，支持不安全算法还是有必要的 找到$JAVA_HOME 下 jre/lib/security/java.security ，将禁用的算法设置为空：jdk.certpath.disabledAlgorithms= 。 JVM 参数调整 在 jdk8 中，PermSize 相关的参数已经不被使用： -XX:MaxPermSize=sizeSets the maximum permanent generation space size (in bytes). This option was deprecated in JDK 8, and superseded by the -XX:MaxMetaspaceSize option.-XX:PermSize=sizeSets the space (in bytes) allocated to the permanent generation that triggers a garbage collection if it is exceeded. This option was deprecated un JDK 8, and superseded by the -XX:MetaspaceSize option. JDK8 中再也没有 PermGen 了。其中的某些部分，如被 intern 的字符串，在 JDK7 中已经移到了普通堆里。其余结构在 JDK8 中会被移到称作“Metaspace”的本机内存区中，该区域在默认情况下会自动生长，也会被垃圾回收。它有两个标记：MetaspaceSize 和 MaxMetaspaceSize。 -XX:MetaspaceSize=size Sets the size of the allocated class metadata space that will trigger a garbage collection the first time it is exceeded. This threshold for a garbage collection is increased or decreased depending on the amount of metadata used. The default size depends on the platform. -XX:MaxMetaspaceSize=size Sets the maximum amount of native memory that can be allocated for class metadata. By default, the size is not limited. The amount of metadata for an application depends on the application itself, other running applications, and the amount of memory available on the system. 以下示例显示如何将类类元数据的上限设置为 256 MB： XX:MaxMetaspaceSize=256m 字节码问题 ASM 5.0 beta 开始支持 JDK8 字节码错误 Caused by: java.io.IOException: invalid constant type: 15 at javassist.bytecode.ConstPool.readOne(ConstPool.java:1113) 查找组件用到了 mvel，mvel 为了提高效率进行了字节码优化，正好碰上 JDK8 死穴，所以需要升级。 &lt;dependency&gt; &lt;groupId&gt;org.mvel&lt;/groupId&gt; &lt;artifactId&gt;mvel2&lt;/artifactId&gt; &lt;version&gt;2.2.7.Final&lt;/version&gt;&lt;/dependency&gt; javassist &lt;dependency&gt; &lt;groupId&gt;org.javassist&lt;/groupId&gt; &lt;artifactId&gt;javassist&lt;/artifactId&gt; &lt;version&gt;3.18.1-GA&lt;/version&gt;&lt;/dependency&gt; 注意 有些部署工具不会删除旧版本 jar 包，所以可以尝试手动删除老版本 jar 包。 http://asm.ow2.org/history.html Java 连接 redis 启动报错 Error redis clients jedis HostAndPort cant resolve localhost address 错误环境: 本地 window 开发环境没有问题。上到 Linux 环境,启动出现问题。 错误信息: Error redis clients jedis HostAndPort cant resolve localhost address 解决办法: 查看 Linux 系统的主机名 # hostnametemplate 查看/etc/hosts 文件中是否有 127.0.0.1 对应主机名，如果没有则添加 Resin 容器指定 JDK 1.8 如果 resin 容器原来版本低于 JDK1.8，运行 JDK 1.8 编译的 web app 时，可能会提示错误： java.lang.UnsupportedClassVersionError: PR/Sort : Unsupported major.minor version 52.0 解决方法就是，使用 JDK 1.8 要重新编译一下。然后，我在部署时出现过编译后仍报错的情况，重启一下服务器后，问题解决，不知是什么原因。 ./configure --prefix=/usr/local/resin --with-java=/usr/local/jdk1.8.0_121make &amp; make install 参考资料 java8-tutorial Compatibility Guide for JDK 8 Compatibility Guide for JDK 8 中文翻译 Why Developers Should Not Write Programs That Call ‘sun’ Packages]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>advanced</tag>
        <tag>jdk8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lombok 使用小结]]></title>
    <url>%2Fblog%2F2017%2F11%2F09%2Fjava%2Fjavalib%2Flombok%2F</url>
    <content type="text"><![CDATA[Lombok 使用小结 Lombok 简介 Lombok 是一种 Java 实用工具，可用来帮助开发人员消除 Java 的冗长，尤其是对于简单的 Java 对象（POJO）。它通过注释实现这一目的。通过在开发环境中实现 Lombok，开发人员可以节省构建诸如 hashCode() 和 equals() 、getter / setter 这样的方法以及以往用来分类各种 accessor 和 mutator 的大量时间。 Lombok 安装 使 IntelliJ IDEA 支持 Lombok 方式如下： Intellij 设置支持注解处理 点击 File &gt; Settings &gt; Build &gt; Annotation Processors 勾选 Enable annotation processing 安装插件 点击 Settings &gt; Plugins &gt; Browse repositories 查找 Lombok Plugin 并进行安装 重启 IntelliJ IDEA 将 lombok 添加到 pom 文件 &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.8&lt;/version&gt;&lt;/dependency&gt; Lombok 使用 API Lombok 提供注解API 来修饰指定的类： @Getter and @Setter @Getter and @Setter Lombok 代码： @Getter @Setter private boolean employed = true;@Setter(AccessLevel.PROTECTED) private String name; 等价于 Java 源码： private boolean employed = true;private String name;public boolean isEmployed() &#123; return employed;&#125;public void setEmployed(final boolean employed) &#123; this.employed = employed;&#125;protected void setName(final String name) &#123; this.name = name;&#125; @NonNull @NonNull Lombok 代码： @Getter @Setter @NonNullprivate List&lt;Person&gt; members; 等价于 Java 源码： @NonNullprivate List&lt;Person&gt; members;public Family(@NonNull final List&lt;Person&gt; members) &#123; if (members == null) throw new java.lang.NullPointerException("members"); this.members = members;&#125; @NonNullpublic List&lt;Person&gt; getMembers() &#123; return members;&#125;public void setMembers(@NonNull final List&lt;Person&gt; members) &#123; if (members == null) throw new java.lang.NullPointerException("members"); this.members = members;&#125; @ToString @ToString Lombok 代码： @ToString(callSuper=true,exclude="someExcludedField")public class Foo extends Bar &#123; private boolean someBoolean = true; private String someStringField; private float someExcludedField;&#125; 等价于 Java 源码： public class Foo extends Bar &#123; private boolean someBoolean = true; private String someStringField; private float someExcludedField; @java.lang.Override public java.lang.String toString() &#123; return "Foo(super=" + super.toString() + ", someBoolean=" + someBoolean + ", someStringField=" + someStringField + ")"; &#125;&#125; @EqualsAndHashCode @EqualsAndHashCode Lombok 代码： @EqualsAndHashCode(callSuper=true,exclude=&#123;"address","city","state","zip"&#125;)public class Person extends SentientBeing &#123; enum Gender &#123; Male, Female &#125; @NonNull private String name; @NonNull private Gender gender; private String ssn; private String address; private String city; private String state; private String zip;&#125; 等价于 Java 源码： public class Person extends SentientBeing &#123; enum Gender &#123; /*public static final*/ Male /* = new Gender() */, /*public static final*/ Female /* = new Gender() */; &#125; @NonNull private String name; @NonNull private Gender gender; private String ssn; private String address; private String city; private String state; private String zip; @java.lang.Override public boolean equals(final java.lang.Object o) &#123; if (o == this) return true; if (o == null) return false; if (o.getClass() != this.getClass()) return false; if (!super.equals(o)) return false; final Person other = (Person)o; if (this.name == null ? other.name != null : !this.name.equals(other.name)) return false; if (this.gender == null ? other.gender != null : !this.gender.equals(other.gender)) return false; if (this.ssn == null ? other.ssn != null : !this.ssn.equals(other.ssn)) return false; return true; &#125; @java.lang.Override public int hashCode() &#123; final int PRIME = 31; int result = 1; result = result * PRIME + super.hashCode(); result = result * PRIME + (this.name == null ? 0 : this.name.hashCode()); result = result * PRIME + (this.gender == null ? 0 : this.gender.hashCode()); result = result * PRIME + (this.ssn == null ? 0 : this.ssn.hashCode()); return result; &#125;&#125; @Data @Data Lombok 代码： @Data(staticConstructor="of")public class Company &#123; private final Person founder; private String name; private List&lt;Person&gt; employees;&#125; 等价于 Java 源码： public class Company &#123; private final Person founder; private String name; private List&lt;Person&gt; employees; private Company(final Person founder) &#123; this.founder = founder; &#125; public static Company of(final Person founder) &#123; return new Company(founder); &#125; public Person getFounder() &#123; return founder; &#125; public String getName() &#123; return name; &#125; public void setName(final String name) &#123; this.name = name; &#125; public List&lt;Person&gt; getEmployees() &#123; return employees; &#125; public void setEmployees(final List&lt;Person&gt; employees) &#123; this.employees = employees; &#125; @java.lang.Override public boolean equals(final java.lang.Object o) &#123; if (o == this) return true; if (o == null) return false; if (o.getClass() != this.getClass()) return false; final Company other = (Company)o; if (this.founder == null ? other.founder != null : !this.founder.equals(other.founder)) return false; if (this.name == null ? other.name != null : !this.name.equals(other.name)) return false; if (this.employees == null ? other.employees != null : !this.employees.equals(other.employees)) return false; return true; &#125; @java.lang.Override public int hashCode() &#123; final int PRIME = 31; int result = 1; result = result * PRIME + (this.founder == null ? 0 : this.founder.hashCode()); result = result * PRIME + (this.name == null ? 0 : this.name.hashCode()); result = result * PRIME + (this.employees == null ? 0 : this.employees.hashCode()); return result; &#125; @java.lang.Override public java.lang.String toString() &#123; return "Company(founder=" + founder + ", name=" + name + ", employees=" + employees + ")"; &#125;&#125; @Cleanup @Cleanup Lombok 代码： public void testCleanUp() &#123; try &#123; @Cleanup ByteArrayOutputStream baos = new ByteArrayOutputStream(); baos.write(new byte[] &#123;'Y','e','s'&#125;); System.out.println(baos.toString()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 等价于 Java 源码： public void testCleanUp() &#123; try &#123; ByteArrayOutputStream baos = new ByteArrayOutputStream(); try &#123; baos.write(new byte[]&#123;'Y', 'e', 's'&#125;); System.out.println(baos.toString()); &#125; finally &#123; baos.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; @Synchronized @Synchronized Lombok 代码： private DateFormat format = new SimpleDateFormat("MM-dd-YYYY");@Synchronizedpublic String synchronizedFormat(Date date) &#123; return format.format(date);&#125; 等价于 Java 源码： private final java.lang.Object $lock = new java.lang.Object[0];private DateFormat format = new SimpleDateFormat("MM-dd-YYYY");public String synchronizedFormat(Date date) &#123; synchronized ($lock) &#123; return format.format(date); &#125;&#125; @SneakyThrows @SneakyThrows Lombok 代码： @SneakyThrowspublic void testSneakyThrows() &#123; throw new IllegalAccessException();&#125; 等价于 Java 源码： public void testSneakyThrows() &#123; try &#123; throw new IllegalAccessException(); &#125; catch (java.lang.Throwable $ex) &#123; throw lombok.Lombok.sneakyThrow($ex); &#125;&#125; 示例 使用 Lombok 定义一个 Java Bean import lombok.Data;import lombok.ToString;@Data@ToString(exclude = "age")public class Person &#123; private String name; private Integer age; private String sex;&#125; 测试 Person person = new Person();person.setName("张三");person.setAge(20);person.setSex("男");System.out.println(person.toString());// output: Person(name=张三, sex=男) 资料 Lombok 官网 | Lombok Github IntelliJ IDEA - Lombok Plugin]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>bean</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 库]]></title>
    <url>%2Fblog%2F2017%2F11%2F09%2Fjava%2Fjavalib%2FREADME%2F</url>
    <content type="text"><![CDATA[Java 库 通用库 apache commons 工具包组 commons-collections - Apache 容器工具包 commons-io - Apache IO 工具包 commons-lang - Apache 语言扩展包 guava - google 工具包 日志 slf4j commons-logging log4j logback JSON 主流 json 工具包： fastjson - 阿里巴巴 json 工具包（国内广泛使用） jackson - spring 默认 json 工具包 gson - google json 工具包 测试 junit4 junit5 mockito JavaBean lombok – Lombok 提供了简单的注解的形式，来帮助我们消除一些必须有，但显得很臃肿的 Java 样板代码。 DOM jsoup – jsoup 是一款 Java 的 HTML 解析器，可直接解析某个 URL 地址、HTML 文本内容。它提供了一套非常省力的 API，可通过 DOM，CSS 以及类似于 jQuery 的操作方法来取出和操作数据。 dom4j – 处理 XML 的开源框架。它集成了 XPath 并提供全力支持 DOM,JAXP 和 Java 平台。 iText – iText 是一个非常著名的能够快速产生 PDF 文件的 Java 类库。支持文本，表格，图形的操作，可以方便的跟 Servlet 进行结合。 网络 netty – Netty 是一个基于 NIO 的客户、服务器端编程框架，使用 Netty 可以确保你快速和简单的开发出一个网络应用。]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 集成调度器]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fspring%2Fintegration%2Fspring-and-scheduler%2F</url>
    <content type="text"><![CDATA[概述 如果想在Spring中使用任务调度功能，除了集成调度框架Quartz这种方式，也可以使用Spring自己的调度任务框架。 使用Spring的调度框架，优点是：支持注解@Scheduler，可以省去大量的配置。 实时触发调度任务 TaskScheduler接口 Spring3引入了TaskScheduler接口，这个接口定义了调度任务的抽象方法。 TaskScheduler接口的声明： public interface TaskScheduler &#123; ScheduledFuture schedule(Runnable task, Trigger trigger); ScheduledFuture schedule(Runnable task, Date startTime); ScheduledFuture scheduleAtFixedRate(Runnable task, Date startTime, long period); ScheduledFuture scheduleAtFixedRate(Runnable task, long period); ScheduledFuture scheduleWithFixedDelay(Runnable task, Date startTime, long delay); ScheduledFuture scheduleWithFixedDelay(Runnable task, long delay);&#125; 从以上方法可以看出TaskScheduler有两类重要参数： 一个是要调度的方法，即一个实现了Runnable接口的线程类的run()方法； 另一个就是触发条件。 TaskScheduler接口的实现类 它有三个实现类：DefaultManagedTaskScheduler、ThreadPoolTaskScheduler、TimerManagerTaskScheduler。 DefaultManagedTaskScheduler：基于JNDI的调度器。 TimerManagerTaskScheduler：托管commonj.timers.TimerManager实例的调度器。 ThreadPoolTaskScheduler：提供线程池管理的调度器，它也实现了TaskExecutor接口，从而使的单一的实例可以尽可能快地异步执行。 Trigger接口 Trigger接口抽象了触发条件的方法。 Trigger接口的声明： public interface Trigger &#123; Date nextExecutionTime(TriggerContext triggerContext);&#125; Trigger接口的实现类 CronTrigger：实现了cron规则的触发器类（和Quartz的cron规则相同）。 PeriodicTrigger：实现了一个周期性规则的触发器类（例如：定义触发起始时间、间隔时间等）。 完整范例 实现一个调度任务的功能有以下几个关键点： (1) 定义调度器 在spring-bean.xml中进行配置 使用task:scheduler标签定义一个大小为10的线程池调度器，spring会实例化一个ThreadPoolTaskScheduler。 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:task="http://www.springframework.org/schema/task" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-3.1.xsd"&gt; &lt;mvc:annotation-driven/&gt; &lt;task:scheduler id="myScheduler" pool-size="10"/&gt;&lt;/beans&gt; 注：不要忘记引入xsd： http://www.springframework.org/schema/taskhttp://www.springframework.org/schema/task/spring-task-3.1.xsd (2) 定义调度任务 定义实现Runnable接口的线程类。 import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class DemoTask implements Runnable &#123; final Logger logger = LoggerFactory.getLogger(this.getClass()); @Override public void run() &#123; logger.info("call DemoTask.run"); &#125;&#125; (3) 装配调度器，并执行调度任务 在一个Controller类中用@Autowired注解装配TaskScheduler。 然后调动TaskScheduler对象的schedule方法启动调度器，就可以执行调度任务了。 import org.springframework.beans.factory.annotation.Autowired;import org.springframework.scheduling.TaskScheduler;import org.springframework.scheduling.support.CronTrigger;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;@Controller@RequestMapping("/scheduler")public class SchedulerController &#123; @Autowired TaskScheduler scheduler; @RequestMapping(value = "/start", method = RequestMethod.POST) public void start() &#123; scheduler.schedule(new DemoTask(), new CronTrigger("0/5 * * * * *")); &#125;&#125; 访问/scheduler/start接口，启动调度器，可以看到如下日志内容： 13:53:15.010 myScheduler-1 o.zp.notes.spring.scheduler.DemoTask.run - call DemoTask.run13:53:20.003 myScheduler-1 o.zp.notes.spring.scheduler.DemoTask.run - call DemoTask.run13:53:25.004 myScheduler-2 o.zp.notes.spring.scheduler.DemoTask.run - call DemoTask.run13:53:30.005 myScheduler-1 o.zp.notes.spring.scheduler.DemoTask.run - call DemoTask.run @Scheduler的使用方法 Spring的调度器一个很大的亮点在于@Scheduler注解，这可以省去很多繁琐的配置。 启动注解 使用@Scheduler注解先要使用&lt;task:annotation-driven&gt;启动注解开关。 例： &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:task="http://www.springframework.org/schema/task" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-3.1.xsd"&gt; &lt;mvc:annotation-driven/&gt; &lt;task:annotation-driven executor="myExecutor" scheduler="myScheduler"/&gt; &lt;task:executor id="myExecutor" pool-size="5"/&gt; &lt;task:scheduler id="myScheduler" pool-size="10"/&gt;&lt;/beans&gt; @Scheduler定义触发条件 例：使用fixedDelay指定触发条件为每5000毫秒执行一次。注意：必须在上一次调度成功后的5000秒才能执行。 @Scheduled(fixedDelay=5000)public void doSomething() &#123; // something that should execute periodically&#125; 例：使用fixedRate指定触发条件为每5000毫秒执行一次。注意：无论上一次调度是否成功，5000秒后必然执行。 @Scheduled(fixedRate=5000)public void doSomething() &#123; // something that should execute periodically&#125; 例：使用initialDelay指定方法在初始化1000毫秒后才开始调度。 @Scheduled(initialDelay=1000, fixedRate=5000)public void doSomething() &#123; // something that should execute periodically&#125; 例：使用cron表达式指定触发条件为每5000毫秒执行一次。cron规则和Quartz中的cron规则一致。 @Scheduled(cron="*/5 * * * * MON-FRI")public void doSomething() &#123; // something that should execute on weekdays only&#125; 完整范例 (1) 启动注解开关，并定义调度器和执行器 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:task="http://www.springframework.org/schema/task" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-3.1.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-3.1.xsd"&gt; &lt;mvc:annotation-driven/&gt; &lt;task:annotation-driven executor="myExecutor" scheduler="myScheduler"/&gt; &lt;task:executor id="myExecutor" pool-size="5"/&gt; &lt;task:scheduler id="myScheduler" pool-size="10"/&gt;&lt;/beans&gt; (2) 使用@Scheduler注解来修饰一个要调度的方法 下面的例子展示了@Scheduler注解定义触发条件的不同方式。 import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.scheduling.annotation.Scheduled;import org.springframework.stereotype.Component;import java.text.SimpleDateFormat;import java.util.Date;/** * @description 使用@Scheduler注解调度任务范例 * @author Vicotr Zhang * @date 2016年8月31日 */@Componentpublic class ScheduledMgr &#123; private final SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); final Logger logger = LoggerFactory.getLogger(this.getClass()); /** * 构造函数中打印初始化时间 */ public ScheduledMgr() &#123; logger.info("Current time: &#123;&#125;", dateFormat.format(new Date())); &#125; /** * fixedDelay属性定义调度间隔时间。调度需要等待上一次调度执行完成。 */ @Scheduled(fixedDelay = 5000) public void testFixedDelay() throws Exception &#123; Thread.sleep(6000); logger.info("Current time: &#123;&#125;", dateFormat.format(new Date())); &#125; /** * fixedRate属性定义调度间隔时间。调度不等待上一次调度执行完成。 */ @Scheduled(fixedRate = 5000) public void testFixedRate() throws Exception &#123; Thread.sleep(6000); logger.info("Current time: &#123;&#125;", dateFormat.format(new Date())); &#125; /** * initialDelay属性定义初始化后的启动延迟时间 */ @Scheduled(initialDelay = 1000, fixedRate = 5000) public void testInitialDelay() throws Exception &#123; Thread.sleep(6000); logger.info("Current time: &#123;&#125;", dateFormat.format(new Date())); &#125; /** * cron属性支持使用cron表达式定义触发条件 */ @Scheduled(cron = "0/5 * * * * ?") public void testCron() throws Exception &#123; Thread.sleep(6000); logger.info("Current time: &#123;&#125;", dateFormat.format(new Date())); &#125;&#125; 我刻意设置触发方式的间隔都是5s，且方法中均有Thread.sleep(6000);语句。从而确保方法在下一次调度触发时间点前无法完成执行，来看一看各种方式的表现吧。 启动spring项目后，spring会扫描@Component注解，然后初始化ScheduledMgr。 接着，spring会扫描@Scheduler注解，初始化调度器。调度器在触发条件匹配的情况下开始工作，输出日志。 截取部分打印日志来进行分析。 10:58:46.479 localhost-startStop-1 o.z.n.s.scheduler.ScheduledTasks.&lt;init&gt; - Current time: 2016-08-31 10:58:4610:58:52.523 myScheduler-1 o.z.n.s.scheduler.ScheduledTasks.testFixedRate - Current time: 2016-08-31 10:58:5210:58:52.523 myScheduler-3 o.z.n.s.scheduler.ScheduledTasks.testFixedDelay - Current time: 2016-08-31 10:58:5210:58:53.524 myScheduler-2 o.z.n.s.scheduler.ScheduledTasks.testInitialDelay - Current time: 2016-08-31 10:58:5310:58:55.993 myScheduler-4 o.z.n.s.scheduler.ScheduledTasks.testCron - Current time: 2016-08-31 10:58:5510:58:58.507 myScheduler-1 o.z.n.s.scheduler.ScheduledTasks.testFixedRate - Current time: 2016-08-31 10:58:5810:58:59.525 myScheduler-5 o.z.n.s.scheduler.ScheduledTasks.testInitialDelay - Current time: 2016-08-31 10:58:5910:59:03.536 myScheduler-3 o.z.n.s.scheduler.ScheduledTasks.testFixedDelay - Current time: 2016-08-31 10:59:0310:59:04.527 myScheduler-1 o.z.n.s.scheduler.ScheduledTasks.testFixedRate - Current time: 2016-08-31 10:59:0410:59:05.527 myScheduler-4 o.z.n.s.scheduler.ScheduledTasks.testInitialDelay - Current time: 2016-08-31 10:59:0510:59:06.032 myScheduler-2 o.z.n.s.scheduler.ScheduledTasks.testCron - Current time: 2016-08-31 10:59:0610:59:10.534 myScheduler-9 o.z.n.s.scheduler.ScheduledTasks.testFixedRate - Current time: 2016-08-31 10:59:1010:59:11.527 myScheduler-10 o.z.n.s.scheduler.ScheduledTasks.testInitialDelay - Current time: 2016-08-31 10:59:1110:59:14.524 myScheduler-4 o.z.n.s.scheduler.ScheduledTasks.testFixedDelay - Current time: 2016-08-31 10:59:1410:59:15.987 myScheduler-6 o.z.n.s.scheduler.ScheduledTasks.testCron - Current time: 2016-08-31 10:59:15 构造方法打印一次，时间点在10:58:46。 testFixedRate打印四次，每次间隔6秒。说明，fixedRate不等待上一次调度执行完成，在间隔时间达到时立即执行。 testFixedDelay打印三次，每次间隔大于6秒，且时间不固定。说明，fixedDelay等待上一次调度执行成功后，开始计算间隔时间，再执行。 testInitialDelay第一次调度时间和构造方法调度时间相隔7秒。说明，initialDelay在初始化后等待指定的延迟时间才开始调度。 testCron打印三次，时间间隔并非5秒或6秒，显然，cron等待上一次调度执行成功后，开始计算间隔时间，再执行。 此外，可以从日志中看出，打印日志的线程最多只有10个，说明2.1中的调度器线程池配置生效。 参考 Spring Framework官方文档]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>integration</tag>
        <tag>scheduler</tag>
        <tag>quartz</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JSP 教程]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fjavaee%2Fjsp%2F</url>
    <content type="text"><![CDATA[JSP 教程 简介 什么是 Java Server Pages? 为什么使用 JSP？ JSP 的优势 JSP 工作原理 JSP 工作流程 JSP 生命周期 语法 脚本 JSP 声明 JSP 表达式 JSP 注释 控制语句 JSP 字面量 指令 Page 指令 Include 指令 Taglib 指令 JSP 动作元素 常见的属性 &lt;jsp:include&gt; &lt;jsp:useBean&gt; &lt;jsp:setProperty&gt; &lt;jsp:getProperty&gt; &lt;jsp:forward&gt; &lt;jsp:plugin&gt; &lt;jsp:element&gt; 、 &lt;jsp:attribute&gt;、&lt;jsp:body&gt; &lt;jsp:text&gt; JSP 隐式对象 request 对象 response 对象 out 对象 session 对象 application 对象 config 对象 pageContext 对象 page 对象 exception 对象 EL 表达式 一个简单的语法 EL 中的基础操作符 JSP EL 中的函数 JSP EL 隐含对象 pageContext 对象 Scope 对象 param 和 paramValues 对象 header 和 headerValues 对象 JSTL JSTL 库安装 核心标签 格式化标签 SQL 标签 XML 标签 JSTL 函数 Taglib JSP 自定义标签 创建&quot;Hello&quot;标签 访问标签体 自定义标签属性 简介 什么是 Java Server Pages? JSP全称Java Server Pages，是一种动态网页开发技术。 它使用 JSP 标签在 HTML 网页中插入 Java 代码。标签通常以&lt;%开头以%&gt;结束。 JSP 是一种 Java servlet，主要用于实现 Java web 应用程序的用户界面部分。网页开发者们通过结合 HTML 代码、XHTML 代码、XML 元素以及嵌入 JSP 操作和命令来编写 JSP。 JSP 通过网页表单获取用户输入数据、访问数据库及其他数据源，然后动态地创建网页。 JSP 标签有多种功能，比如访问数据库、记录用户选择信息、访问 JavaBeans 组件等，还可以在不同的网页中传递控制信息和共享信息。 为什么使用 JSP？ JSP 也是一种 Servlet，因此 JSP 能够完成 Servlet 能完成的任何工作。 JSP 程序与 CGI 程序有着相似的功能，但和 CGI 程序相比，JSP 程序有如下优势： 性能更加优越，因为 JSP 可以直接在 HTML 网页中动态嵌入元素而不需要单独引用 CGI 文件。 服务器调用的是已经编译好的 JSP 文件，而不像 CGI/Perl 那样必须先载入解释器和目标脚本。 JSP 基于 Java Servlets API，因此，JSP 拥有各种强大的企业级 Java API，包括 JDBC，JNDI，EJB，JAXP 等等。 JSP 页面可以与处理业务逻辑的 servlets 一起使用，这种模式被 Java servlet 模板引擎所支持。 最后，JSP 是 Java EE 不可或缺的一部分，是一个完整的企业级应用平台。这意味着 JSP 可以用最简单的方式来实现最复杂的应用。 JSP 的优势 以下列出了使用 JSP 带来的其他好处： 与 ASP 相比：JSP 有两大优势。首先，动态部分用 Java 编写，而不是 VB 或其他 MS 专用语言，所以更加强大与易用。第二点就是 JSP 易于移植到非 MS 平台上。 与纯 Servlets 相比：JSP 可以很方便的编写或者修改 HTML 网页而不用去面对大量的 println 语句。 与 SSI 相比：SSI 无法使用表单数据、无法进行数据库链接。 与 JavaScript 相比：虽然 JavaScript 可以在客户端动态生成 HTML，但是很难与服务器交互，因此不能提供复杂的服务，比如访问数据库和图像处理等等。 与静态 HTML 相比：静态 HTML 不包含动态信息。 JSP 工作原理 JSP 是一种 Servlet，但工作方式和 Servlet 有所差别。 Servlet 是先将源代码编译为 class 文件后部署到服务器下的，先编译后部署。 Jsp 是先将源代码部署到服务器再编译，先部署后编译。 Jsp 会在客户端第一次请求 Jsp 文件时被编译为 HttpJspPage 类（Servlet 的一个子类）。该类会被服务器临时存放在服务器工作目录里。所以，第一次请求 Jsp 后，访问速度会变快就是这个道理。 JSP 工作流程 网络服务器需要一个 JSP 引擎，也就是一个容器来处理 JSP 页面。容器负责截获对 JSP 页面的请求。本教程使用内嵌 JSP 容器的 Apache 来支持 JSP 开发。 JSP 容器与 Web 服务器协同合作，为 JSP 的正常运行提供必要的运行环境和其他服务，并且能够正确识别专属于 JSP 网页的特殊元素。 下图显示了 JSP 容器和 JSP 文件在 Web 应用中所处的位置。 工作步骤 以下步骤表明了 Web 服务器是如何使用 JSP 来创建网页的： 就像其他普通的网页一样，您的浏览器发送一个 HTTP 请求给服务器。 Web 服务器识别出这是一个对 JSP 网页的请求，并且将该请求传递给 JSP 引擎。通过使用 URL 或者.jsp 文件来完成。 JSP 引擎从磁盘中载入 JSP 文件，然后将它们转化为 servlet。这种转化只是简单地将所有模板文本改用 println()语句，并且将所有的 JSP 元素转化成 Java 代码。 JSP 引擎将 servlet 编译成可执行类，并且将原始请求传递给 servlet 引擎。 Web 服务器的某组件将会调用 servlet 引擎，然后载入并执行 servlet 类。在执行过程中，servlet 产生 HTML 格式的输出并将其内嵌于 HTTP response 中上交给 Web 服务器。 Web 服务器以静态 HTML 网页的形式将 HTTP response 返回到您的浏览器中。 最终，Web 浏览器处理 HTTP response 中动态产生的 HTML 网页，就好像在处理静态网页一样。 以上提及到的步骤可以用下图来表示： 一般情况下，JSP 引擎会检查 JSP 文件对应的 servlet 是否已经存在，并且检查 JSP 文件的修改日期是否早于 servlet。如果 JSP 文件的修改日期早于对应的 servlet，那么容器就可以确定 JSP 文件没有被修改过并且 servlet 有效。这使得整个流程与其他脚本语言（比如 PHP）相比要高效快捷一些。 JSP 生命周期 理解 JSP 底层功能的关键就是去理解它们所遵守的生命周期。 JSP 生命周期就是从创建到销毁的整个过程，类似于 servlet 生命周期，区别在于 JSP 生命周期还包括将 JSP 文件编译成 servlet。 以下是 JSP 生命周期中所走过的几个阶段： **编译阶段：**servlet 容器编译 servlet 源文件，生成 servlet 类 **初始化阶段：**加载与 JSP 对应的 servlet 类，创建其实例，并调用它的初始化方法 **执行阶段：**调用与 JSP 对应的 servlet 实例的服务方法 **销毁阶段：**调用与 JSP 对应的 servlet 实例的销毁方法，然后销毁 servlet 实例 很明显，JSP 生命周期的四个主要阶段和 servlet 生命周期非常相似，下面给出图示： JSP 编译 当浏览器请求 JSP 页面时，JSP 引擎会首先去检查是否需要编译这个文件。如果这个文件没有被编译过，或者在上次编译后被更改过，则编译这个 JSP 文件。 编译的过程包括三个步骤： 解析 JSP 文件。 将 JSP 文件转为 servlet。 编译 servlet。 JSP 初始化 容器载入 JSP 文件后，它会在为请求提供任何服务前调用 jspInit()方法。如果您需要执行自定义的 JSP 初始化任务，复写 jspInit()方法就行了，就像下面这样： public void jspInit()&#123; // 初始化代码&#125; 一般来讲程序只初始化一次，servlet 也是如此。通常情况下您可以在 jspInit()方法中初始化数据库连接、打开文件和创建查询表。 JSP 执行 这一阶段描述了 JSP 生命周期中一切与请求相关的交互行为，直到被销毁。 当 JSP 网页完成初始化后，JSP 引擎将会调用_jspService()方法。 _jspService()方法需要一个 HttpServletRequest 对象和一个 HttpServletResponse 对象作为它的参数，就像下面这样： void _jspService(HttpServletRequest request, HttpServletResponse response)&#123; // 服务端处理代码&#125; _jspService() 方法在每个 request 中被调用一次并且负责产生与之相对应的 response，并且它还负责产生所有 7 个 HTTP 方法的回应，比如 GET、POST、DELETE 等等。 JSP 清理 JSP 生命周期的销毁阶段描述了当一个 JSP 网页从容器中被移除时所发生的一切。 jspDestroy()方法在 JSP 中等价于 servlet 中的销毁方法。当您需要执行任何清理工作时复写 jspDestroy()方法，比如释放数据库连接或者关闭文件夹等等。 jspDestroy()方法的格式如下： public void jspDestroy()&#123; // 清理代码&#125; 语法 脚本 脚本程序可以包含任意量的 Java 语句、变量、方法或表达式，只要它们在脚本语言中是有效的。 脚本程序的语法格式： &lt;% 代码片段 %&gt; 或者，您也可以编写与其等价的 XML 语句，就像下面这样： &lt;jsp:scriptlet&gt; 代码片段&lt;/jsp:scriptlet&gt; 任何文本、HTML 标签、JSP 元素必须写在脚本程序的外面。 下面给出一个示例，同时也是本教程的第一个 JSP 示例： &lt;html&gt;&lt;head&gt;&lt;title&gt;Hello World&lt;/title&gt;&lt;/head&gt;&lt;body&gt;Hello World!&lt;br/&gt;&lt;%out.println("Your IP address is " + request.getRemoteAddr());%&gt;&lt;/body&gt;&lt;/html&gt; **注意：**请确保 Apache Tomcat 已经安装在 C:\apache-tomcat-7.0.2 目录下并且运行环境已经正确设置。 将以上代码保存在 hello.jsp 中，然后将它放置在 C:\apache-tomcat-7.0.2\webapps\ROOT 目录下，打开浏览器并在地址栏中输入http://localhost:8080/hello.jsp。运行后得到以下结果： 中文编码问题 如果我们要在页面正常显示中文，我们需要在 JSP 文件头部添加以下代码：&lt;&gt; &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt; 接下来我们将以上程序修改为： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;Hello World!&lt;br/&gt;&lt;%out.println("你的 IP 地址 " + request.getRemoteAddr());%&gt;&lt;/body&gt;&lt;/html&gt; 这样中文就可以正常显示了。 JSP 声明 一个声明语句可以声明一个或多个变量、方法，供后面的 Java 代码使用。在 JSP 文件中，您必须先声明这些变量和方法然后才能使用它们。 JSP 声明的语法格式： &lt;%! declaration; [ declaration; ]+ ... %&gt; 或者，您也可以编写与其等价的 XML 语句，就像下面这样： &lt;jsp:declaration&gt; 代码片段&lt;/jsp:declaration&gt; 程序示例： &lt;%! int i = 0; %&gt;&lt;%! int a, b, c; %&gt;&lt;%! Circle a = new Circle(2.0); %&gt; JSP 表达式 一个 JSP 表达式中包含的脚本语言表达式，先被转化成 String，然后插入到表达式出现的地方。 由于表达式的值会被转化成 String，所以您可以在一个文本行中使用表达式而不用去管它是否是 HTML 标签。 表达式元素中可以包含任何符合 Java 语言规范的表达式，但是不能使用分号来结束表达式。 JSP 表达式的语法格式： &lt;%= 表达式 %&gt; 同样，您也可以编写与之等价的 XML 语句： &lt;jsp:expression&gt; 表达式&lt;/jsp:expression&gt; 程序示例： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt; 今天的日期是: &lt;%= (new java.util.Date()).toLocaleString()%&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 运行后得到以下结果： 今天的日期是: 2016-6-25 13:40:07 JSP 注释 JSP 注释主要有两个作用：为代码作注释以及将某段代码注释掉。 JSP 注释的语法格式： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;JSP注释示例&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;%-- 该部分注释在网页中不会被显示--%&gt;&lt;p&gt; 今天的日期是: &lt;%= (new java.util.Date()).toLocaleString()%&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 运行后得到以下结果： 今天的日期是: 2016-6-25 13:41:26 不同情况下使用注释的语法规则： 语法 描述 &lt;%-- 注释 --%&gt; JSP 注释，注释内容不会被发送至浏览器甚至不会被编译 HTML 注释，通过浏览器查看网页源代码时可以看见注释内容 &lt;% 代表静态 &lt;%常量 %&gt; 代表静态 %&gt; 常量 ’ 在属性中使用的单引号 &quot; 在属性中使用的双引号 控制语句 JSP 提供对 Java 语言的全面支持。您可以在 JSP 程序中使用 Java API 甚至建立 Java 代码块，包括判断语句和循环语句等等。 if…else 语句 If…else块，请看下面这个例子： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%! int day = 1; %&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;02.JSP语法 - if...else示例&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;IF...ELSE 实例&lt;/h3&gt;&lt;% if (day == 1 | day == 7) &#123; %&gt;&lt;p&gt;今天是周末&lt;/p&gt;&lt;% &#125; else &#123; %&gt;&lt;p&gt;今天不是周末&lt;/p&gt;&lt;% &#125; %&gt;&lt;/body&gt;&lt;/html&gt; 运行后得到以下结果： IF...ELSE 实例今天不是周末 switch…case 语句 现在来看看 switch…case 块，与 if…else 块有很大的不同，它使用 out.println()，并且整个都装在脚本程序的标签中，就像下面这样： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%! int day = 3; %&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;02.JSP语法 - switch...case示例&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;Sswitch...case示例&lt;/h3&gt;&lt;% switch(day) &#123; case 0: out.println("星期天"); break; case 1: out.println("星期一"); break; case 2: out.println("星期二"); break; case 3: out.println("星期三"); break; case 4: out.println("星期四"); break; case 5: out.println("星期五"); break; default: out.println("星期六"); &#125;%&gt;&lt;/body&gt;&lt;/html&gt; 浏览器访问，运行后得出以下结果： SWITCH...CASE 实例星期三 循环语句 在 JSP 程序中可以使用 Java 的三个基本循环类型：for，while，和 do…while。 让我们来看看 for 循环的例子，以下输出的不同字体大小的&quot;菜鸟教程&quot;： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%! int fontSize; %&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;For 循环实例&lt;/h3&gt;&lt;%for ( fontSize = 1; fontSize &lt;= 3; fontSize++)&#123; %&gt; &lt;font color="green" size="&lt;%= fontSize %&gt;"&gt; 菜鸟教程 &lt;/font&gt;&lt;br /&gt;&lt;%&#125;%&gt;&lt;/body&gt;&lt;/html&gt; 运行后得到以下结果： 将上例改用 while 循环来写： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%! int fontSize; %&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;While 循环实例&lt;/h3&gt;&lt;%while ( fontSize &lt;= 3)&#123; %&gt; &lt;font color="green" size="&lt;%= fontSize %&gt;"&gt; 菜鸟教程 &lt;/font&gt;&lt;br /&gt;&lt;%fontSize++;%&gt;&lt;%&#125;%&gt;&lt;/body&gt;&lt;/html&gt; 浏览器访问，输出结果为（fontSize 初始化为 0，所以多输出了一行）： JSP 运算符 JSP 支持所有 Java 逻辑和算术运算符。 下表罗列出了 JSP 常见运算符，优先级从高到底： 类别 操作符 结合性 后缀 () [] . (点运算符) 左到右 一元 ++ - - ! ~ 右到左 可乘性 * / % 左到右 可加性 + - 左到右 移位 &gt;&gt; &gt;&gt;&gt; &lt;&lt; 左到右 关系 &gt; &gt;= &lt; &lt;= 左到右 相等/不等 == != 左到右 位与 &amp; 左到右 位异或 ^ 左到右 位或 | 左到右 逻辑与 &amp;&amp; 左到右 逻辑或 || 左到右 条件判断 ?: 右到左 赋值 = += -= *= /= %= &gt;&gt;= &lt;&lt;= &amp;= ^= |= 右到左 逗号 , 左到右 JSP 字面量 JSP 语言定义了以下几个字面量： 布尔值(boolean)：true 和 false; 整型(int)：与 Java 中的一样; 浮点型(float)：与 Java 中的一样; 字符串(string)：以单引号或双引号开始和结束; Null：null。 指令 JSP 指令用来设置整个 JSP 页面相关的属性，如网页的编码方式和脚本语言。 JSP 指令以开&lt;%@开始，以%&gt;结束。 JSP 指令语法格式如下： &lt;%@ directive attribute="value" %&gt; 指令可以有很多个属性，它们以键值对的形式存在，并用逗号隔开。 JSP 中的三种指令标签： 指令 描述 &lt;%@ page ... %&gt; 定义网页依赖属性，比如脚本语言、error 页面、缓存需求等等 &lt;%@ include ... %&gt; 包含其他文件 &lt;%@ taglib ... %&gt; 引入标签库的定义，可以是自定义标签 Page 指令 Page 指令为容器提供当前页面的使用说明。一个 JSP 页面可以包含多个page指令。 Page 指令的语法格式： &lt;%@ page attribute="value" %&gt; 等价的 XML 格式： &lt;jsp:directive.page attribute="value" /&gt; 例： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8" %&gt; 属性 下表列出与 Page 指令相关的属性： 属性 描述 buffer 指定 out 对象使用缓冲区的大小 autoFlush 控制 out 对象的 缓存区 contentType 指定当前 JSP 页面的 MIME 类型和字符编码 errorPage 指定当 JSP 页面发生异常时需要转向的错误处理页面 isErrorPage 指定当前页面是否可以作为另一个 JSP 页面的错误处理页面 extends 指定 servlet 从哪一个类继承 import 导入要使用的 Java 类 info 定义 JSP 页面的描述信息 isThreadSafe 指定对 JSP 页面的访问是否为线程安全 language 定义 JSP 页面所用的脚本语言，默认是 Java session 指定 JSP 页面是否使用 session isELIgnored 指定是否执行 EL 表达式 isScriptingEnabled 确定脚本元素能否被使用 Include 指令 JSP 可以通过include指令来包含其他文件。 被包含的文件可以是 JSP 文件、HTML 文件或文本文件。包含的文件就好像是该 JSP 文件的一部分，会被同时编译执行。 Include 指令的语法格式如下： &lt;%@ include file="文件相对 url 地址" %&gt; include 指令中的文件名实际上是一个相对的 URL 地址。 如果您没有给文件关联一个路径，JSP 编译器默认在当前路径下寻找。 等价的 XML 语法： &lt;jsp:directive.include file="文件相对 url 地址" /&gt; Taglib 指令 JSP 允许用户自定义标签，一个自定义标签库就是自定义标签的集合。 taglib指令引入一个自定义标签集合的定义，包括库路径、自定义标签。 taglib指令的语法： &lt;%@ taglib uri="uri" prefix="prefixOfTag" %&gt; uri 属性确定标签库的位置，prefix 属性指定标签库的前缀。 等价的 XML 语法： &lt;jsp:directive.taglib uri="uri" prefix="prefixOfTag" /&gt; JSP 动作元素 JSP 动作元素是一组 JSP 内置的标签，只需要书写很少的标记代码就能使用 JSP 提供的丰富功能。JSP 动作元素是对常用的 JSP 功能的抽象与封装，包括两种，自定义 JSP 动作元素与标准 JSP 动作元素。 与 JSP 指令元素不同的是，JSP 动作元素在请求处理阶段起作用。JSP 动作元素是用 XML 语法写成的。 利用 JSP 动作可以动态地插入文件、重用 JavaBean 组件、把用户重定向到另外的页面、为 Java 插件生成 HTML 代码。 动作元素只有一种语法，它符合 XML 标准： &lt;jsp:action_name attribute="value" /&gt; 动作元素基本上都是预定义的函数，JSP 规范定义了一系列的标准动作，它用 JSP 作为前缀，可用的标准动作元素如下： 语法 描述 jsp:include 在页面被请求的时候引入一个文件。 jsp:useBean 寻找或者实例化一个 JavaBean。 jsp:setProperty 设置 JavaBean 的属性。 jsp:getProperty 输出某个 JavaBean 的属性。 jsp:forward 把请求转到一个新的页面。 jsp:plugin 根据浏览器类型为 Java 插件生成 OBJECT 或 EMBED 标记。 jsp:element 定义动态 XML 元素 jsp:attribute 设置动态定义的 XML 元素属性。 jsp:body 设置动态定义的 XML 元素内容。 jsp:text 在 JSP 页面和文档中使用写入文本的模板 常见的属性 所有的动作要素都有两个属性：id 属性和 scope 属性。 **id 属性：**id 属性是动作元素的唯一标识，可以在 JSP 页面中引用。动作元素创建的 id 值可以通过 PageContext 来调用。 **scope 属性：**该属性用于识别动作元素的生命周期。 id 属性和 scope 属性有直接关系，scope 属性定义了相关联 id 对象的寿命。 scope 属性有四个可能的值： (a) page, (b)request, ©session, 和 (d) application。 &lt;jsp:include&gt; &lt;jsp:include&gt; 用来包含静态和动态的文件。该动作把指定文件插入正在生成的页面。 如果被包含的文件为 JSP 程序，则会先执行 JSP 程序，再将执行结果包含进来。 语法格式如下： &lt;jsp:include page="相对 URL 地址" flush="true" /&gt; 前面已经介绍过 include 指令，它是在 JSP 文件被转换成 Servlet 的时候引入文件，而这里的 jsp:include 动作不同，插入文件的时间是在页面被请求的时候。 以下是 include 动作相关的属性列表。 属性 描述 page 包含在页面中的相对 URL 地址。 flush 布尔属性，定义在包含资源前是否刷新缓存区。 例 以下我们定义了两个文件 date.jsp 和 main.jsp，代码如下所示： date.jsp 文件代码： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;p&gt; 今天的日期是: &lt;%= (new java.util.Date())%&gt;&lt;/p&gt; main.jsp 文件代码： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;include 动作实例&lt;/h2&gt;&lt;jsp:include page="date.jsp" flush="true" /&gt;&lt;/body&gt;&lt;/html&gt; 现在将以上两个文件放在服务器的根目录下，访问 main.jsp 文件。显示结果如下： include 动作实例今天的日期是: 2016-6-25 14:08:17 &lt;jsp:useBean&gt; jsp:useBean 动作用来加载一个将在 JSP 页面中使用的 JavaBean。 这个功能非常有用，因为它使得我们可以发挥 Java 组件复用的优势。 jsp:useBean 动作最简单的语法为： &lt;jsp:useBean id="name" class="package.class" /&gt; 在类载入后，我们既可以通过 jsp:setProperty 和 jsp:getProperty 动作来修改和检索 bean 的属性。 以下是 useBean 动作相关的属性列表。 属性 描述 class 指定 Bean 的完整包名。 type 指定将引用该对象变量的类型。 beanName 通过 java.beans.Beans 的 instantiate() 方法指定 Bean 的名字。 在给出具体实例前，让我们先来看下 jsp:setProperty 和 jsp:getProperty 动作元素： &lt;jsp:setProperty&gt; jsp:setProperty 用来设置已经实例化的 Bean 对象的属性，有两种用法。首先，你可以在 jsp:useBean 元素的外面（后面）使用 jsp:setProperty，如下所示： &lt;jsp:useBean id="myName" ... /&gt;...&lt;jsp:setProperty name="myName" property="someProperty" .../&gt; 此时，不管 jsp:useBean 是找到了一个现有的 Bean，还是新创建了一个 Bean 实例，jsp:setProperty 都会执行。第二种用法是把 jsp:setProperty 放入 jsp:useBean 元素的内部，如下所示： &lt;jsp:useBean id="myName" ... &gt;... &lt;jsp:setProperty name="myName" property="someProperty" .../&gt;&lt;/jsp:useBean&gt; 此时，jsp:setProperty 只有在新建 Bean 实例时才会执行，如果是使用现有实例则不执行 jsp:setProperty。 jsp:setProperty 动作有下面四个属性,如下表： 属性 描述 name name 属性是必需的。它表示要设置属性的是哪个 Bean。 property property 属性是必需的。它表示要设置哪个属性。有一个特殊用法：如果 property 的值是&quot;*&quot;，表示所有名字和 Bean 属性名字匹配的请求参数都将被传递给相应的属性 set 方法。 value value 属性是可选的。该属性用来指定 Bean 属性的值。字符串数据会在目标类中通过标准的 valueOf 方法自动转换成数字、boolean、Boolean、 byte、Byte、char、Character。例如，boolean 和 Boolean 类型的属性值（比如&quot;true&quot;）通过 Boolean.valueOf 转换，int 和 Integer 类型的属性值（比如&quot;42&quot;）通过 Integer.valueOf 转换。 value 和 param 不能同时使用，但可以使用其中任意一个。 param param 是可选的。它指定用哪个请求参数作为 Bean 属性的值。如果当前请求没有参数，则什么事情也不做，系统不会把 null 传递给 Bean 属性的 set 方法。因此，你可以让 Bean 自己提供默认属性值，只有当请求参数明确指定了新值时才修改默认属性值。 &lt;jsp:getProperty&gt; jsp:getProperty 动作提取指定 Bean 属性的值，转换成字符串，然后输出。语法格式如下： &lt;jsp:useBean id="myName" ... /&gt;...&lt;jsp:getProperty name="myName" property="someProperty" .../&gt; 下表是与 getProperty 相关联的属性： 属性 描述 name 要检索的 Bean 属性名称。Bean 必须已定义。 property 表示要提取 Bean 属性的值 实例 以下实例我们使用了 Bean: package com.runoob.main;public class TestBean &#123; private String message = "菜鸟教程"; public String getMessage() &#123; return(message); &#125; public void setMessage(String message) &#123; this.message = message; &#125;&#125; 编译以上实例文件 TestBean.java ： $ javac TestBean.java 编译完成后会在当前目录下生成一个 TestBean.class 文件， 将该文件拷贝至当前 JSP 项目的 WebContent/WEB-INF/classes/com/runoob/main 下( com/runoob/main 包路径，没有需要手动创建)。 下面是一个 Eclipse 中目录结构图： 下面是一个很简单的例子，它的功能是装载一个 Bean，然后设置/读取它的 message 属性。 现在让我们在 main.jsp 文件中调用该 Bean: &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;Jsp 使用 JavaBean 实例&lt;/h2&gt;&lt;jsp:useBean id="test" class="com.runoob.main.TestBean" /&gt;&lt;jsp:setProperty name="test" property="message" value="菜鸟教程..." /&gt;&lt;p&gt;输出信息....&lt;/p&gt;&lt;jsp:getProperty name="test" property="message" /&gt;&lt;/body&gt;&lt;/html&gt; 浏览器访问，执行以上文件，输出如下所示： &lt;jsp:forward&gt; jsp:forward 动作把请求转到另外的页面。jsp:forward 标记只有一个属性 page。语法格式如下所示： &lt;jsp:forward page="相对 URL 地址" /&gt; 以下是 forward 相关联的属性： 属性 描述 page page 属性包含的是一个相对 URL。page 的值既可以直接给出，也可以在请求的时候动态计算，可以是一个 JSP 页面或者一个 Java Servlet. 实例 以下实例我们使用了两个文件，分别是： date.jsp 和 main.jsp。 date.jsp 文件代码如下： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;p&gt; 今天的日期是: &lt;%= (new java.util.Date()).toLocaleString()%&gt;&lt;/p&gt; main.jsp 文件代码： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;forward 动作实例&lt;/h2&gt;&lt;jsp:forward page="date.jsp" /&gt;&lt;/body&gt;&lt;/html&gt; 现在将以上两个文件放在服务器的根目录下，访问 main.jsp 文件。显示结果如下： 今天的日期是: 2016-6-25 14:37:25 &lt;jsp:plugin&gt; jsp:plugin 动作用来根据浏览器的类型，插入通过 Java 插件 运行 Java Applet 所必需的 OBJECT 或 EMBED 元素。 如果需要的插件不存在，它会下载插件，然后执行 Java 组件。 Java 组件可以是一个 applet 或一个 JavaBean。 plugin 动作有多个对应 HTML 元素的属性用于格式化 Java 组件。param 元素可用于向 Applet 或 Bean 传递参数。 以下是使用 plugin 动作元素的典型实例: &lt;jsp:plugin type="applet" codebase="dirname" code="MyApplet.class" width="60" height="80"&gt; &lt;jsp:param name="fontcolor" value="red" /&gt; &lt;jsp:param name="background" value="black" /&gt; &lt;jsp:fallback&gt; Unable to initialize Java Plugin &lt;/jsp:fallback&gt;&lt;/jsp:plugin&gt; 如果你有兴趣可以尝试使用 applet 来测试 jsp:plugin 动作元素，元素是一个新元素，在组件出现故障的错误是发送给用户错误信息。 &lt;jsp:element&gt; 、 &lt;jsp:attribute&gt;、&lt;jsp:body&gt; jsp:element 、 jsp:attribute、 jsp:body动作元素动态定义 XML 元素。动态是非常重要的，这就意味着 XML 元素在编译时是动态生成的而非静态。 以下实例动态定义了 XML 元素： &lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;jsp:element name="xmlElement"&gt;&lt;jsp:attribute name="xmlElementAttr"&gt; 属性值&lt;/jsp:attribute&gt;&lt;jsp:body&gt; XML 元素的主体&lt;/jsp:body&gt;&lt;/jsp:element&gt;&lt;/body&gt;&lt;/html&gt; 浏览器访问以下页面，输出结果如下所示： &lt;jsp:text&gt; jsp:text动作元素允许在 JSP 页面和文档中使用写入文本的模板，语法格式如下： &lt;jsp:text&gt;模板数据&lt;/jsp:text&gt; 以上文本模板不能包含其他元素，只能只能包含文本和 EL 表达式（注：EL 表达式将在后续章节中介绍）。请注意，在 XML 文件中，您不能使用表达式如 ${whatever &gt; 0}，因为&gt;符号是非法的。 你可以使用 ${whatever gt 0}表达式或者嵌入在一个 CDATA 部分的值。 &lt;jsp:text&gt;&lt;![CDATA[&lt;br&gt;]]&gt;&lt;/jsp:text&gt; 如果你需要在 XHTML 中声明 DOCTYPE,必须使用到jsp:text动作元素，实例如下： &lt;jsp:text&gt;&lt;![CDATA[&lt;!DOCTYPE htmlPUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""DTD/xhtml1-strict.dtd"&gt;]]&gt;&lt;/jsp:text&gt;&lt;head&gt;&lt;title&gt;jsp:text action&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;books&gt;&lt;book&gt;&lt;jsp:text&gt; Welcome to JSP Programming&lt;/jsp:text&gt;&lt;/book&gt;&lt;/books&gt;&lt;/body&gt;&lt;/html&gt; 你可以对以上实例尝试使用jsp:text及不使用该动作元素执行结果的区别。 JSP 隐式对象 JSP 隐式对象是 JSP 容器为每个页面提供的 Java 对象，开发者可以直接使用它们而不用显式声明。JSP 隐式对象也被称为预定义变量。 JSP 所支持的九大隐式对象： 对象 描述 request HttpServletRequest类的实例 response HttpServletResponse类的实例 out PrintWriter类的实例，用于把结果输出至网页上 session HttpSession类的实例 application ServletContext类的实例，与应用上下文有关 config ServletConfig类的实例 pageContext PageContext类的实例，提供对 JSP 页面所有对象以及命名空间的访问 page 类似于 Java 类中的 this 关键字 Exception Exception类的对象，代表发生错误的 JSP 页面中对应的异常对象 request 对象 request对象是javax.servlet.http.HttpServletRequest 类的实例。 每当客户端请求一个 JSP 页面时，JSP 引擎就会制造一个新的request对象来代表这个请求。 request对象提供了一系列方法来获取 HTTP 头信息，cookies，HTTP 方法等等。 response 对象 response对象是javax.servlet.http.HttpServletResponse类的实例。 当服务器创建request对象时会同时创建用于响应这个客户端的response对象。 response对象也定义了处理 HTTP 头模块的接口。通过这个对象，开发者们可以添加新的 cookies，时间戳，HTTP 状态码等等。 out 对象 out对象是javax.servlet.jsp.JspWriter类的实例，用来在response对象中写入内容。 最初的JspWriter类对象根据页面是否有缓存来进行不同的实例化操作。可以在page指令中使用buffered='false'属性来轻松关闭缓存。 JspWriter类包含了大部分java.io.PrintWriter类中的方法。不过，JspWriter新增了一些专为处理缓存而设计的方法。还有就是，JspWriter类会抛出IOExceptions异常，而PrintWriter不会。 下表列出了我们将会用来输出boolean，char，int，double，String，object等类型数据的重要方法： 方法 描述 out.print(dataType dt) 输出 Type 类型的值 out.println(dataType dt) 输出 Type 类型的值然后换行 out.flush() 刷新输出流 session 对象 session对象是javax.servlet.http.HttpSession类的实例。和 Java Servlets 中的session对象有一样的行为。 session对象用来跟踪在各个客户端请求间的会话。 application 对象 application对象直接包装了 servlet 的ServletContext类的对象，是javax.servlet.ServletContext类的实例。 这个对象在 JSP 页面的整个生命周期中都代表着这个 JSP 页面。这个对象在 JSP 页面初始化时被创建，随着jspDestroy()方法的调用而被移除。 通过向application中添加属性，则所有组成您 web 应用的 JSP 文件都能访问到这些属性。 config 对象 config对象是javax.servlet.ServletConfig类的实例，直接包装了 servlet 的ServletConfig类的对象。 这个对象允许开发者访问 Servlet 或者 JSP 引擎的初始化参数，比如文件路径等。 以下是 config 对象的使用方法，不是很重要，所以不常用： config.getServletName(); 它返回包含在&lt;servlet-name&gt;元素中的 servlet 名字，注意，&lt;servlet-name&gt;元素在WEB-INF\web.xml文件中定义。 pageContext 对象 pageContext对象是javax.servlet.jsp.PageContext类的实例，用来代表整个 JSP 页面。 这个对象主要用来访问页面信息，同时过滤掉大部分实现细节。 这个对象存储了request对象和response对象的引用。application对象，config对象，session对象，out对象可以通过访问这个对象的属性来导出。 pageContext对象也包含了传给 JSP 页面的指令信息，包括缓存信息，ErrorPage URL,页面 scope 等。 PageContext类定义了一些字段，包括 PAGE_SCOPE，REQUEST_SCOPE，SESSION_SCOPE， APPLICATION_SCOPE。它也提供了 40 余种方法，有一半继承自javax.servlet.jsp.JspContext 类。 其中一个重要的方法就是removeArribute()，它可接受一个或两个参数。比如，pageContext.removeArribute(“attrName”)移除四个 scope 中相关属性，但是下面这种方法只移除特定 scope 中的相关属性： pageContext.removeAttribute("attrName", PAGE_SCOPE); page 对象 这个对象就是页面实例的引用。它可以被看做是整个 JSP 页面的代表。 page对象就是this对象的同义词。 exception 对象 exception对象包装了从先前页面中抛出的异常信息。它通常被用来产生对出错条件的适当响应。 EL 表达式 EL 表达式是用${}括起来的脚本，用来更方便地读取对象。EL 表达式写在 JSP 的 HTML 代码中，而不能写在&lt;%与%&gt;引起的 JSP 脚本中。 JSP 表达式语言（EL）使得访问存储在 JavaBean 中的数据变得非常简单。JSP EL 既可以用来创建算术表达式也可以用来创建逻辑表达式。在 JSP EL 表达式内可以使用整型数，浮点数，字符串，常量 true、false，还有 null。 一个简单的语法 典型的，当您需要在 JSP 标签中指定一个属性值时，只需要简单地使用字符串即可： &lt;jsp:setProperty name="box" property="perimeter" value="100"/&gt; JSP EL 允许您指定一个表达式来表示属性值。一个简单的表达式语法如下： $&#123;expr&#125; 其中，expr 指的是表达式。在 JSP EL 中通用的操作符是&quot;.“和”[]&quot;。这两个操作符允许您通过内嵌的 JSP 对象访问各种各样的 JavaBean 属性。 举例来说，上面的jsp:setProperty标签可以使用表达式语言改写成如下形式： &lt;jsp:setProperty name="box" property="perimeter" value="$&#123;2*box.width+2*box.height&#125;"/&gt; 当 JSP 编译器在属性中见到&quot;${}&quot;格式后，它会产生代码来计算这个表达式，并且产生一个替代品来代替表达式的值。 您也可以在标签的模板文本中使用表达式语言。比如jsp:text标签简单地将其主体中的文本插入到 JSP 输出中： &lt;jsp:text&gt;&lt;h1&gt;Hello JSP!&lt;/h1&gt;&lt;/jsp:text&gt; 现在，在jsp:text标签主体中使用表达式，就像这样： &lt;jsp:text&gt;Box Perimeter is: $&#123;2*box.width + 2*box.height&#125;&lt;/jsp:text&gt; 在 EL 表达式中可以使用圆括号来组织子表达式。比如 ${(1 + 2) _ 3} 等于 9，但是 ${1 + (2 _ 3)} 等于 7。 想要停用对 EL 表达式的评估的话，需要使用 page 指令将 isELIgnored 属性值设为 true： &lt;%@ page isELIgnored ="true|false" %&gt; 这样，EL 表达式就会被忽略。若设为 false，则容器将会计算 EL 表达式。 EL 中的基础操作符 EL 表达式支持大部分 Java 所提供的算术和逻辑操作符： 操作符 描述 . 访问一个 Bean 属性或者一个映射条目 [] 访问一个数组或者链表的元素 ( ) 组织一个子表达式以改变优先级 + 加 - 减或负 * 乘 / or div 除 % or mod 取模 == or eq 测试是否相等 != or ne 测试是否不等 &lt; or lt 测试是否小于 &gt; or gt 测试是否大于 &lt;= or le 测试是否小于等于 &gt;= or ge 测试是否大于等于 &amp;&amp; or and 测试逻辑与 || or or 测试逻辑或 ! or not 测试取反 empty 测试是否空值 JSP EL 中的函数 JSP EL 允许您在表达式中使用函数。这些函数必须被定义在自定义标签库中。函数的使用语法如下： $&#123;ns:func(param1, param2, ...)&#125; ns 指的是命名空间（namespace），func 指的是函数的名称，param1 指的是第一个参数，param2 指的是第二个参数，以此类推。比如，有函数 fn:length，在 JSTL 库中定义，可以像下面这样来获取一个字符串的长度： $&#123;fn:length("Get my length")&#125; 要使用任何标签库中的函数，您需要将这些库安装在服务器中，然后使用 &lt;taglib&gt; 标签在 JSP 文件中包含这些库。 JSP EL 隐含对象 JSP EL 支持下表列出的隐含对象： 隐含对象 描述 pageScope page 作用域 requestScope request 作用域 sessionScope session 作用域 applicationScope application 作用域 param Request 对象的参数，字符串 paramValues Request 对象的参数，字符串集合 header HTTP 信息头，字符串 headerValues HTTP 信息头，字符串集合 initParam 上下文初始化参数 cookie Cookie 值 pageContext 当前页面的 pageContext 您可以在表达式中使用这些对象，就像使用变量一样。接下来会给出几个例子来更好的理解这个概念。 pageContext 对象 pageContext 对象是 JSP 中 pageContext 对象的引用。通过 pageContext 对象，您可以访问 request 对象。比如，访问 request 对象传入的查询字符串，就像这样： $&#123;pageContext.request.queryString&#125; Scope 对象 pageScope，requestScope，sessionScope，applicationScope 变量用来访问存储在各个作用域层次的变量。 举例来说，如果您需要显式访问在 applicationScope 层的 box 变量，可以这样来访问：applicationScope.box。 param 和 paramValues 对象 param 和 paramValues 对象用来访问参数值，通过使用 request.getParameter 方法和 request.getParameterValues 方法。 举例来说，访问一个名为 order 的参数，可以这样使用表达式：${param.order}，或者${param[&quot;order&quot;]}。 接下来的例子表明了如何访问 request 中的 username 参数： &lt;%@ page import="java.io.*,java.util.*" %&gt;&lt;% String title = "Accessing Request Param";%&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;&lt;% out.print(title); %&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;center&gt;&lt;h1&gt;&lt;% out.print(title); %&gt;&lt;/h1&gt;&lt;/center&gt;&lt;div align="center"&gt;&lt;p&gt;$&#123;param["username"]&#125;&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; param 对象返回单一的字符串，而 paramValues 对象则返回一个字符串数组。 header 和 headerValues 对象 header 和 headerValues 对象用来访问信息头，通过使用 request.getHeader 方法和 request.getHeaders 方法。 举例来说，要访问一个名为 user-agent 的信息头，可以这样使用表达式：${header.user-agent}，或者 ${header[&quot;user-agent&quot;]}。 接下来的例子表明了如何访问 user-agent 信息头： &lt;%@ page import="java.io.*,java.util.*" %&gt;&lt;% String title = "User Agent Example";%&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;&lt;% out.print(title); %&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;center&gt;&lt;h1&gt;&lt;% out.print(title); %&gt;&lt;/h1&gt;&lt;/center&gt;&lt;div align="center"&gt;&lt;p&gt;$&#123;header["user-agent"]&#125;&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 运行结果如下： header 对象返回单一值，而 headerValues 则返回一个字符串数组。 JSTL JSP 标准标签库（JSTL）是一个 JSP 标签集合，它封装了 JSP 应用的通用核心功能。 JSTL 支持通用的、结构化的任务，比如迭代，条件判断，XML 文档操作，国际化标签，SQL 标签。 除了这些，它还提供了一个框架来使用集成 JSTL 的自定义标签。 根据 JSTL 标签所提供的功能，可以将其分为 5 个类别。 核心标签 格式化标签 SQL 标签 XML 标签 JSTL 函数 JSTL 库安装 Apache Tomcat 安装 JSTL 库步骤如下： 从 Apache 的标准标签库中下载的二进包(jakarta-taglibs-standard-current.zip)。 官方下载地址：http://archive.apache.org/dist/jakarta/taglibs/standard/binaries/ 本站下载地址：jakarta-taglibs-standard-1.1.2.zip 下载 jakarta-taglibs-standard-1.1.2.zip 包并解压，将 jakarta-taglibs-standard-1.1.2/lib/ 下的两个 jar 文件：standard.jar 和 jstl.jar 文件拷贝到 /WEB-INF/lib/ 下。 将 tld 下的需要引入的 tld 文件复制到 WEB-INF 目录下。 接下来我们在 web.xml 文件中添加以下配置： &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app version="2.4" xmlns="http://java.sun.com/xml/ns/j2ee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd"&gt; &lt;jsp-config&gt; &lt;taglib&gt; &lt;taglib-uri&gt;http://java.sun.com/jsp/jstl/fmt&lt;/taglib-uri&gt; &lt;taglib-location&gt;/WEB-INF/fmt.tld&lt;/taglib-location&gt; &lt;/taglib&gt; &lt;taglib&gt; &lt;taglib-uri&gt;http://java.sun.com/jsp/jstl/fmt-rt&lt;/taglib-uri&gt; &lt;taglib-location&gt;/WEB-INF/fmt-rt.tld&lt;/taglib-location&gt; &lt;/taglib&gt; &lt;taglib&gt; &lt;taglib-uri&gt;http://java.sun.com/jsp/jstl/core&lt;/taglib-uri&gt; &lt;taglib-location&gt;/WEB-INF/c.tld&lt;/taglib-location&gt; &lt;/taglib&gt; &lt;taglib&gt; &lt;taglib-uri&gt;http://java.sun.com/jsp/jstl/core-rt&lt;/taglib-uri&gt; &lt;taglib-location&gt;/WEB-INF/c-rt.tld&lt;/taglib-location&gt; &lt;/taglib&gt; &lt;taglib&gt; &lt;taglib-uri&gt;http://java.sun.com/jsp/jstl/sql&lt;/taglib-uri&gt; &lt;taglib-location&gt;/WEB-INF/sql.tld&lt;/taglib-location&gt; &lt;/taglib&gt; &lt;taglib&gt; &lt;taglib-uri&gt;http://java.sun.com/jsp/jstl/sql-rt&lt;/taglib-uri&gt; &lt;taglib-location&gt;/WEB-INF/sql-rt.tld&lt;/taglib-location&gt; &lt;/taglib&gt; &lt;taglib&gt; &lt;taglib-uri&gt;http://java.sun.com/jsp/jstl/x&lt;/taglib-uri&gt; &lt;taglib-location&gt;/WEB-INF/x.tld&lt;/taglib-location&gt; &lt;/taglib&gt; &lt;taglib&gt; &lt;taglib-uri&gt;http://java.sun.com/jsp/jstl/x-rt&lt;/taglib-uri&gt; &lt;taglib-location&gt;/WEB-INF/x-rt.tld&lt;/taglib-location&gt; &lt;/taglib&gt; &lt;/jsp-config&gt;&lt;/web-app&gt; 使用任何库，你必须在每个 JSP 文件中的头部包含 标签。 核心标签 核心标签是最常用的 JSTL 标签。引用核心标签库的语法如下： &lt;%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core" %&gt; 标签 描述 &lt;c:out&gt; 用于在 JSP 中显示数据，就像&lt;%= … &gt; &lt;c:set&gt; 用于保存数据 &lt;c:remove&gt; 用于删除数据 &lt;c:catch&gt; 用来处理产生错误的异常状况，并且将错误信息储存起来 &lt;c:if&gt; 与我们在一般程序中用的 if 一样 &lt;c:choose&gt; 本身只当做&lt;c:when&gt;和&lt;c:otherwise&gt;的父标签 &lt;c:when&gt; &lt;c:choose&gt;的子标签，用来判断条件是否成立 &lt;c:otherwise&gt; &lt;c:choose&gt;的子标签，接在&lt;c:when&gt;标签后，当&lt;c:when&gt;标签判断为 false 时被执行 &lt;c:import&gt; 检索一个绝对或相对 URL，然后将其内容暴露给页面 &lt;c:forEach&gt; 基础迭代标签，接受多种集合类型 &lt;c:forTokens&gt; 根据指定的分隔符来分隔内容并迭代输出 &lt;c:param&gt; 用来给包含或重定向的页面传递参数 &lt;c:redirect&gt; 重定向至一个新的 URL. &lt;c:url&gt; 使用可选的查询参数来创造一个 URL 格式化标签 JSTL 格式化标签用来格式化并输出文本、日期、时间、数字。引用格式化标签库的语法如下： &lt;%@ taglib prefix="fmt" uri="http://java.sun.com/jsp/jstl/fmt" %&gt; 标签 描述 &lt;fmt:formatNumber&gt; 使用指定的格式或精度格式化数字 &lt;fmt:parseNumber&gt; 解析一个代表着数字，货币或百分比的字符串 &lt;fmt:formatDate&gt; 使用指定的风格或模式格式化日期和时间 &lt;fmt:parseDate&gt; 解析一个代表着日期或时间的字符串 &lt;fmt:bundle&gt; 绑定资源 &lt;fmt:setLocale&gt; 指定地区 &lt;fmt:setBundle&gt; 绑定资源 &lt;fmt:timeZone&gt; 指定时区 &lt;fmt:setTimeZone&gt; 指定时区 &lt;fmt:message&gt; 显示资源配置文件信息 &lt;fmt:requestEncoding&gt; 设置 request 的字符编码 SQL 标签 JSTL SQL 标签库提供了与关系型数据库（Oracle，MySQL，SQL Server 等等）进行交互的标签。引用 SQL 标签库的语法如下： &lt;%@ taglib prefix="sql" uri="http://java.sun.com/jsp/jstl/sql" %&gt; 标签 描述 &lt;sql:setDataSource&gt; 指定数据源 &lt;sql:query&gt; 运行 SQL 查询语句 &lt;sql:update&gt; 运行 SQL 更新语句 &lt;sql:param&gt; 将 SQL 语句中的参数设为指定值 &lt;sql:dateParam&gt; 将 SQL 语句中的日期参数设为指定的 java.util.Date 对象值 &lt;sql:transaction&gt; 在共享数据库连接中提供嵌套的数据库行为元素，将所有语句以一个事务的形式来运行 XML 标签 JSTL XML 标签库提供了创建和操作 XML 文档的标签。引用 XML 标签库的语法如下： &lt;%@ taglib prefix="x" uri="http://java.sun.com/jsp/jstl/xml" %&gt; 在使用 xml 标签前，你必须将 XML 和 XPath 的相关包拷贝至你的&lt;Tomcat 安装目录&gt;\lib 下: XercesImpl.jar 下载地址： http://www.apache.org/dist/xerces/j/ xalan.jar 下载地址： http://xml.apache.org/xalan-j/index.html 标签 描述 &lt;x:out&gt; 与&lt;%= … &gt;,类似，不过只用于 XPath 表达式 &lt;x:parse&gt; 解析 XML 数据 &lt;x:set&gt; 设置 XPath 表达式 &lt;x:if&gt; 判断 XPath 表达式，若为真，则执行本体中的内容，否则跳过本体 &lt;x:forEach&gt; 迭代 XML 文档中的节点 &lt;x:choose&gt; &lt;x:when&gt;和&lt;x:otherwise&gt;的父标签 &lt;x:when&gt; &lt;x:choose&gt;的子标签，用来进行条件判断 &lt;x:otherwise&gt; &lt;x:choose&gt;的子标签，当&lt;x:when&gt;判断为 false 时被执行 &lt;x:transform&gt; 将 XSL 转换应用在 XML 文档中 &lt;x:param&gt; 与&lt;x:transform&gt;共同使用，用于设置 XSL 样式表 JSTL 函数 JSTL 包含一系列标准函数，大部分是通用的字符串处理函数。引用 JSTL 函数库的语法如下： &lt;%@ taglib prefix="fn" uri="http://java.sun.com/jsp/jstl/functions" %&gt; 函数 描述 fn:contains() 测试输入的字符串是否包含指定的子串 fn:containsIgnoreCase() 测试输入的字符串是否包含指定的子串，大小写不敏感 fn:endsWith() 测试输入的字符串是否以指定的后缀结尾 fn:escapeXml() 跳过可以作为 XML 标记的字符 fn:indexOf() 返回指定字符串在输入字符串中出现的位置 fn:join() 将数组中的元素合成一个字符串然后输出 fn:length() 返回字符串长度 fn:replace() 将输入字符串中指定的位置替换为指定的字符串然后返回 fn:split() 将字符串用指定的分隔符分隔然后组成一个子字符串数组并返回 fn:startsWith() 测试输入字符串是否以指定的前缀开始 fn:substring() 返回字符串的子集 fn:substringAfter() 返回字符串在指定子串之后的子集 fn:substringBefore() 返回字符串在指定子串之前的子集 fn:toLowerCase() 将字符串中的字符转为小写 fn:toUpperCase() 将字符串中的字符转为大写 fn:trim() 移除首尾的空白符 Taglib JSP 自定义标签 自定义标签是用户定义的 JSP 语言元素。当 JSP 页面包含一个自定义标签时将被转化为 servlet，标签转化为对被 称为 tag handler 的对象的操作，即当 servlet 执行时 Web container 调用那些操作。 JSP 标签扩展可以让你创建新的标签并且可以直接插入到一个 JSP 页面。 JSP 2.0 规范中引入 Simple Tag Handlers 来编写这些自定义标记。 你可以继承 SimpleTagSupport 类并重写的 doTag()方法来开发一个最简单的自定义标签。 创建&quot;Hello&quot;标签 接下来，我们想创建一个自定义标签叫作ex:Hello，标签格式为： &lt;ex:Hello /&gt; 要创建自定义的 JSP 标签，你首先必须创建处理标签的 Java 类。所以，让我们创建一个 HelloTag 类，如下所示： package com.runoob;import javax.servlet.jsp.tagext.*;import javax.servlet.jsp.*;import java.io.*;public class HelloTag extends SimpleTagSupport &#123; public void doTag() throws JspException, IOException &#123; JspWriter out = getJspContext().getOut(); out.println("Hello Custom Tag!"); &#125;&#125; 以下代码重写了 doTag()方法，方法中使用了 getJspContext()方法来获取当前的 JspContext 对象，并将&quot;Hello Custom Tag!&quot;传递给 JspWriter 对象。 编译以上类，并将其复制到环境变量 CLASSPATH 目录中。最后创建如下标签库：&lt;Tomcat安装目录&gt;webapps\ROOT\WEB-INF\custom.tld。 &lt;taglib&gt; &lt;tlib-version&gt;1.0&lt;/tlib-version&gt; &lt;jsp-version&gt;2.0&lt;/jsp-version&gt; &lt;short-name&gt;Example TLD&lt;/short-name&gt; &lt;tag&gt; &lt;name&gt;Hello&lt;/name&gt; &lt;tag-class&gt;com.runoob.HelloTag&lt;/tag-class&gt; &lt;body-content&gt;empty&lt;/body-content&gt; &lt;/tag&gt;&lt;/taglib&gt; 接下来，我们就可以在 JSP 文件中使用 Hello 标签： &lt;%@ taglib prefix="ex" uri="WEB-INF/custom.tld"%&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;A sample custom tag&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;ex:Hello/&gt; &lt;/body&gt;&lt;/html&gt; 以上程序输出结果为： Hello Custom Tag! 访问标签体 你可以像标准标签库一样在标签中包含消息内容。如我们要在我们自定义的 Hello 中包含内容，格式如下： &lt;ex:Hello&gt; This is message body&lt;/ex:Hello&gt; 我们可以修改标签处理类文件，代码如下： package com.runoob;import javax.servlet.jsp.tagext.*;import javax.servlet.jsp.*;import java.io.*;public class HelloTag extends SimpleTagSupport &#123; StringWriter sw = new StringWriter(); public void doTag() throws JspException, IOException &#123; getJspBody().invoke(sw); getJspContext().getOut().println(sw.toString()); &#125;&#125; 接下来我们需要修改 TLD 文件，如下所示： &lt;taglib&gt; &lt;tlib-version&gt;1.0&lt;/tlib-version&gt; &lt;jsp-version&gt;2.0&lt;/jsp-version&gt; &lt;short-name&gt;Example TLD with Body&lt;/short-name&gt; &lt;tag&gt; &lt;name&gt;Hello&lt;/name&gt; &lt;tag-class&gt;com.runoob.HelloTag&lt;/tag-class&gt; &lt;body-content&gt;scriptless&lt;/body-content&gt; &lt;/tag&gt;&lt;/taglib&gt; 现在我们可以在 JSP 使用修改后的标签，如下所示: &lt;%@ taglib prefix="ex" uri="WEB-INF/custom.tld"%&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;A sample custom tag&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;ex:Hello&gt; This is message body &lt;/ex:Hello&gt; &lt;/body&gt;&lt;/html&gt; 以上程序输出结果如下所示： This is message body 自定义标签属性 你可以在自定义标准中设置各种属性，要接收属性，值自定义标签类必须实现 setter 方法， JavaBean 中的 setter 方法如下所示： package com.runoob;import javax.servlet.jsp.tagext.*;import javax.servlet.jsp.*;import java.io.*;public class HelloTag extends SimpleTagSupport &#123; private String message; public void setMessage(String msg) &#123; this.message = msg; &#125; StringWriter sw = new StringWriter(); public void doTag() throws JspException, IOException &#123; if (message != null) &#123; /* 从属性中使用消息 */ JspWriter out = getJspContext().getOut(); out.println( message ); &#125; else &#123; /* 从内容体中使用消息 */ getJspBody().invoke(sw); getJspContext().getOut().println(sw.toString()); &#125; &#125;&#125; 属性的名称是&quot;message&quot;，所以 setter 方法是的 setMessage()。现在让我们在 TLD 文件中使用的元素添加此属性： &lt;taglib&gt; &lt;tlib-version&gt;1.0&lt;/tlib-version&gt; &lt;jsp-version&gt;2.0&lt;/jsp-version&gt; &lt;short-name&gt;Example TLD with Body&lt;/short-name&gt; &lt;tag&gt; &lt;name&gt;Hello&lt;/name&gt; &lt;tag-class&gt;com.runoob.HelloTag&lt;/tag-class&gt; &lt;body-content&gt;scriptless&lt;/body-content&gt; &lt;attribute&gt; &lt;name&gt;message&lt;/name&gt; &lt;/attribute&gt; &lt;/tag&gt;&lt;/taglib&gt; 现在我们就可以在 JSP 文件中使用 message 属性了，如下所示： &lt;%@ taglib prefix="ex" uri="WEB-INF/custom.tld"%&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;A sample custom tag&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;ex:Hello message="This is custom tag" /&gt; &lt;/body&gt;&lt;/html&gt; 以上实例数据输出结果为： This is custom tag 你还可以包含以下属性： 属性 描述 name 定义属性的名称。每个标签的是属性名称必须是唯一的。 required 指定属性是否是必须的或者可选的,如果设置为 false 为可选。 rtexprvalue 声明在运行表达式时，标签属性是否有效。 type 定义该属性的 Java 类类型 。默认指定为 String description 描述信息 fragment 如果声明了该属性,属性值将被视为一个 JspFragment。 以下是指定相关的属性实例： ..... &lt;attribute&gt; &lt;name&gt;attribute_name&lt;/name&gt; &lt;required&gt;false&lt;/required&gt; &lt;type&gt;java.util.Date&lt;/type&gt; &lt;fragment&gt;false&lt;/fragment&gt; &lt;/attribute&gt;..... 如果你使用了两个属性，修改 TLD 文件，如下所示： ..... &lt;attribute&gt; &lt;name&gt;attribute_name1&lt;/name&gt; &lt;required&gt;false&lt;/required&gt; &lt;type&gt;java.util.Boolean&lt;/type&gt; &lt;fragment&gt;false&lt;/fragment&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;attribute_name2&lt;/name&gt; &lt;required&gt;true&lt;/required&gt; &lt;type&gt;java.util.Date&lt;/type&gt; &lt;/attribute&gt;.....]]></content>
      <categories>
        <category>java</category>
        <category>javaweb</category>
        <category>javaee</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaweb</tag>
        <tag>javaee</tag>
        <tag>jsp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 集成缓存]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fspring%2Fintegration%2Fspring-and-cache%2F</url>
    <content type="text"><![CDATA[前言 Ehcache 是一个成熟的缓存框架，你可以直接使用它来管理你的缓存。 Spring 提供了对缓存功能的抽象：即允许绑定不同的缓存解决方案（如Ehcache），但本身不直接提供缓存功能的实现。它支持注解方式使用缓存，非常方便。 本文先通过Ehcache独立应用的范例来介绍它的基本使用方法，然后再介绍与Spring整合的方法。 概述 Ehcache是什么？ EhCache 是一个纯Java的进程内缓存框架，具有快速、精干等特点。它是Hibernate中的默认缓存框架。 Ehcache已经发布了3.1版本。但是本文的讲解基于2.10.2版本。 为什么不使用最新版呢？因为Spring4还不能直接整合Ehcache 3.x。虽然可以通过JCache间接整合，Ehcache也支持JCache，但是个人觉得不是很方便。 安装 Ehcache 如果你的项目使用maven管理，添加以下依赖到你的pom.xml中。 &lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache&lt;/artifactId&gt; &lt;version&gt;2.10.2&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt;&lt;/dependency&gt; 如果你的项目不使用maven管理，请在 Ehcache官网下载地址 下载jar包。 Spring 如果你的项目使用maven管理，添加以下依赖到你的pom.xml中。 spring-context-support这个jar包中含有Spring对于缓存功能的抽象封装接口。 &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.1.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; Ehcache的使用 HelloWorld范例 接触一种技术最快最直接的途径总是一个Hello World例子，毕竟动手实践印象更深刻，不是吗？ (1) 在classpath下添加ehcache.xml 添加一个名为helloworld的缓存。 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ehcache xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://ehcache.org/ehcache.xsd"&gt; &lt;!-- 磁盘缓存位置 --&gt; &lt;diskStore path="java.io.tmpdir/ehcache"/&gt; &lt;!-- 默认缓存 --&gt; &lt;defaultCache maxEntriesLocalHeap="10000" eternal="false" timeToIdleSeconds="120" timeToLiveSeconds="120" maxEntriesLocalDisk="10000000" diskExpiryThreadIntervalSeconds="120" memoryStoreEvictionPolicy="LRU"/&gt; &lt;!-- helloworld缓存 --&gt; &lt;cache name="helloworld" maxElementsInMemory="1000" eternal="false" timeToIdleSeconds="5" timeToLiveSeconds="5" overflowToDisk="false" memoryStoreEvictionPolicy="LRU"/&gt;&lt;/ehcache&gt; (2) EhcacheDemo.java Ehcache会自动加载classpath根目录下名为ehcache.xml文件。 EhcacheDemo的工作步骤如下： 在EhcacheDemo中，我们引用ehcache.xml声明的名为helloworld的缓存来创建Cache对象； 然后我们用一个键值对来实例化Element对象； 将Element对象添加到Cache； 然后用Cache的get方法获取Element对象。 public class EhcacheDemo &#123; public static void main(String[] args) throws Exception &#123; // Create a cache manager final CacheManager cacheManager = new CacheManager(); // create the cache called "helloworld" final Cache cache = cacheManager.getCache("helloworld"); // create a key to map the data to final String key = "greeting"; // Create a data element final Element putGreeting = new Element(key, "Hello, World!"); // Put the element into the data store cache.put(putGreeting); // Retrieve the data element final Element getGreeting = cache.get(key); // Print the value System.out.println(getGreeting.getObjectValue()); &#125;&#125; 输出 Hello, World! Ehcache基本操作 Element、Cache、CacheManager是Ehcache最重要的API。 Element：缓存的元素，它维护着一个键值对。 Cache：它是Ehcache的核心类，它有多个Element，并被CacheManager管理。它实现了对缓存的逻辑行为。 CacheManager：Cache的容器对象，并管理着Cache的生命周期。 创建CacheManager 下面的代码列举了创建CacheManager的五种方式。 使用静态方法create()会以默认配置来创建单例的CacheManager实例。 newInstance()方法是一个工厂方法，以默认配置创建一个新的CacheManager实例。 此外，newInstance()还有几个重载函数，分别可以通过传入String、URL、InputStream参数来加载配置文件，然后创建CacheManager实例。 // 使用Ehcache默认配置获取单例的CacheManager实例CacheManager.create();String[] cacheNames = CacheManager.getInstance().getCacheNames();// 使用Ehcache默认配置新建一个CacheManager实例CacheManager.newInstance();String[] cacheNames = manager.getCacheNames();// 使用不同的配置文件分别创建一个CacheManager实例CacheManager manager1 = CacheManager.newInstance("src/config/ehcache1.xml");CacheManager manager2 = CacheManager.newInstance("src/config/ehcache2.xml");String[] cacheNamesForManager1 = manager1.getCacheNames();String[] cacheNamesForManager2 = manager2.getCacheNames();// 基于classpath下的配置文件创建CacheManager实例URL url = getClass().getResource("/anotherconfigurationname.xml");CacheManager manager = CacheManager.newInstance(url);// 基于文件流得到配置文件，并创建CacheManager实例InputStream fis = new FileInputStream(new File("src/config/ehcache.xml").getAbsolutePath());try &#123; CacheManager manager = CacheManager.newInstance(fis);&#125; finally &#123; fis.close();&#125; 添加缓存 需要强调一点，Cache对象在用addCache方法添加到CacheManager之前，是无效的。 使用CacheManager的addCache方法可以根据缓存名将ehcache.xml中声明的cache添加到容器中；它也可以直接将Cache对象添加到缓存容器中。 Cache有多个构造函数，提供了不同方式去加载缓存的配置参数。 有时候，你可能需要使用API来动态的添加缓存，下面的例子就提供了这样的范例。 // 除了可以使用xml文件中配置的缓存，你也可以使用API动态增删缓存// 添加缓存manager.addCache(cacheName);// 使用默认配置添加缓存CacheManager singletonManager = CacheManager.create();singletonManager.addCache("testCache");Cache test = singletonManager.getCache("testCache");// 使用自定义配置添加缓存，注意缓存未添加进CacheManager之前并不可用CacheManager singletonManager = CacheManager.create();Cache memoryOnlyCache = new Cache("testCache", 5000, false, false, 5, 2);singletonManager.addCache(memoryOnlyCache);Cache test = singletonManager.getCache("testCache");// 使用特定的配置添加缓存CacheManager manager = CacheManager.create();Cache testCache = new Cache( new CacheConfiguration("testCache", maxEntriesLocalHeap) .memoryStoreEvictionPolicy(MemoryStoreEvictionPolicy.LFU) .eternal(false) .timeToLiveSeconds(60) .timeToIdleSeconds(30) .diskExpiryThreadIntervalSeconds(0) .persistence(new PersistenceConfiguration().strategy(Strategy.LOCALTEMPSWAP))); manager.addCache(testCache); 删除缓存 删除缓存比较简单，你只需要将指定的缓存名传入removeCache方法即可。 CacheManager singletonManager = CacheManager.create();singletonManager.removeCache("sampleCache1"); 实现基本缓存操作 Cache最重要的两个方法就是put和get，分别用来添加Element和获取Element。 Cache还提供了一系列的get、set方法来设置或获取缓存参数，这里不一一列举，更多API操作可参考官方API开发手册。 /** * 测试：使用默认配置或使用指定配置来创建CacheManager * * @author Zhang Peng */public class CacheOperationTest &#123; private final Logger log = LoggerFactory.getLogger(CacheOperationTest.class); /** * 使用Ehcache默认配置(classpath下的ehcache.xml)获取单例的CacheManager实例 */ @Test public void operation() &#123; CacheManager manager = CacheManager.newInstance("src/test/resources/ehcache/ehcache.xml"); // 获得Cache的引用 Cache cache = manager.getCache("userCache"); // 将一个Element添加到Cache cache.put(new Element("key1", "value1")); // 获取Element，Element类支持序列化，所以下面两种方法都可以用 Element element1 = cache.get("key1"); // 获取非序列化的值 log.debug("key:&#123;&#125;, value:&#123;&#125;", element1.getObjectKey(), element1.getObjectValue()); // 获取序列化的值 log.debug("key:&#123;&#125;, value:&#123;&#125;", element1.getKey(), element1.getValue()); // 更新Cache中的Element cache.put(new Element("key1", "value2")); Element element2 = cache.get("key1"); log.debug("key:&#123;&#125;, value:&#123;&#125;", element2.getObjectKey(), element2.getObjectValue()); // 获取Cache的元素数 log.debug("cache size:&#123;&#125;", cache.getSize()); // 获取MemoryStore的元素数 log.debug("MemoryStoreSize:&#123;&#125;", cache.getMemoryStoreSize()); // 获取DiskStore的元素数 log.debug("DiskStoreSize:&#123;&#125;", cache.getDiskStoreSize()); // 移除Element cache.remove("key1"); log.debug("cache size:&#123;&#125;", cache.getSize()); // 关闭当前CacheManager对象 manager.shutdown(); // 关闭CacheManager单例实例 CacheManager.getInstance().shutdown(); &#125;&#125; 缓存配置 Ehcache支持通过xml文件和API两种方式进行配置。 xml方式 Ehcache的CacheManager构造函数或工厂方法被调用时，会默认加载classpath下名为ehcache.xml的配置文件。如果加载失败，会加载Ehcache jar包中的ehcache-failsafe.xml文件，这个文件中含有简单的默认配置。 ehcache.xml配置参数说明： name：缓存名称。 maxElementsInMemory：缓存最大个数。 eternal：缓存中对象是否为永久的，如果是，超时设置将被忽略，对象从不过期。 timeToIdleSeconds：置对象在失效前的允许闲置时间（单位：秒）。仅当eternal=false对象不是永久有效时使用，可选属性，默认值是0，也就是可闲置时间无穷大。 timeToLiveSeconds：缓存数据的生存时间（TTL），也就是一个元素从构建到消亡的最大时间间隔值，这只能在元素不是永久驻留时有效，如果该值是0就意味着元素可以停顿无穷长的时间。 maxEntriesLocalDisk：当内存中对象数量达到maxElementsInMemory时，Ehcache将会对象写到磁盘中。 overflowToDisk：内存不足时，是否启用磁盘缓存。 diskSpoolBufferSizeMB：这个参数设置DiskStore（磁盘缓存）的缓存区大小。默认是30MB。每个Cache都应该有自己的一个缓冲区。 maxElementsOnDisk：硬盘最大缓存个数。 diskPersistent：是否在VM重启时存储硬盘的缓存数据。默认值是false。 diskExpiryThreadIntervalSeconds：磁盘失效线程运行时间间隔，默认是120秒。 memoryStoreEvictionPolicy：当达到maxElementsInMemory限制时，Ehcache将会根据指定的策略去清理内存。默认策略是LRU（最近最少使用）。你可以设置为FIFO（先进先出）或是LFU（较少使用）。 clearOnFlush：内存数量最大时是否清除。 ehcache.xml的一个范例 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ehcache xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://ehcache.org/ehcache.xsd"&gt; &lt;!-- 磁盘缓存位置 --&gt; &lt;diskStore path="java.io.tmpdir/ehcache"/&gt; &lt;!-- 默认缓存 --&gt; &lt;defaultCache maxEntriesLocalHeap="10000" eternal="false" timeToIdleSeconds="120" timeToLiveSeconds="120" maxEntriesLocalDisk="10000000" diskExpiryThreadIntervalSeconds="120" memoryStoreEvictionPolicy="LRU"&gt; &lt;persistence strategy="localTempSwap"/&gt; &lt;/defaultCache&gt; &lt;cache name="userCache" maxElementsInMemory="1000" eternal="false" timeToIdleSeconds="3" timeToLiveSeconds="3" maxEntriesLocalDisk="10000000" overflowToDisk="false" memoryStoreEvictionPolicy="LRU"/&gt;&lt;/ehcache&gt; API方式 xml配置的参数也可以直接通过编程方式来动态的进行配置（dynamicConfig没有设为false）。 Cache cache = manager.getCache("sampleCache"); CacheConfiguration config = cache.getCacheConfiguration(); config.setTimeToIdleSeconds(60); config.setTimeToLiveSeconds(120); config.setmaxEntriesLocalHeap(10000); config.setmaxEntriesLocalDisk(1000000); 也可以通过disableDynamicFeatures()方式关闭动态配置开关。配置以后你将无法再以编程方式配置参数。 Cache cache = manager.getCache("sampleCache"); cache.disableDynamicFeatures(); Spring整合Ehcache Spring3.1开始添加了对缓存的支持。和事务功能的支持方式类似，缓存抽象允许底层使用不同的缓存解决方案来进行整合。 Spring4.1开始支持JSR-107注解。 注：我本人使用的Spring版本为4.1.4.RELEASE，目前Spring版本仅支持Ehcache2.5以上版本，但不支持Ehcache3。 绑定Ehcache org.springframework.cache.ehcache.EhCacheManagerFactoryBean这个类的作用是加载Ehcache配置文件。 org.springframework.cache.ehcache.EhCacheCacheManager这个类的作用是支持net.sf.ehcache.CacheManager。 spring-ehcache.xml的配置 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:cache="http://www.springframework.org/schema/cache" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/cache http://www.springframework.org/schema/cache/spring-cache-3.2.xsd"&gt; &lt;description&gt;ehcache缓存配置管理文件&lt;/description&gt; &lt;bean id="ehcache" class="org.springframework.cache.ehcache.EhCacheManagerFactoryBean"&gt; &lt;property name="configLocation" value="classpath:ehcache/ehcache.xml"/&gt; &lt;/bean&gt; &lt;bean id="cacheManager" class="org.springframework.cache.ehcache.EhCacheCacheManager"&gt; &lt;property name="cacheManager" ref="ehcache"/&gt; &lt;/bean&gt; &lt;!-- 启用缓存注解开关 --&gt; &lt;cache:annotation-driven cache-manager="cacheManager"/&gt;&lt;/beans&gt; 使用Spring的缓存注解 开启注解 Spring为缓存功能提供了注解功能，但是你必须启动注解。 你有两个选择： (1) 在xml中声明 像上一节spring-ehcache.xml中的做法一样，使用&lt;cache:annotation-driven/&gt; &lt;cache:annotation-driven cache-manager="cacheManager"/&gt; (2) 使用标记注解 你也可以通过对一个类进行注解修饰的方式在这个类中使用缓存注解。 范例如下： @Configuration@EnableCachingpublic class AppConfig &#123;&#125; 注解基本使用方法 Spring对缓存的支持类似于对事务的支持。 首先使用注解标记方法，相当于定义了切点，然后使用Aop技术在这个方法的调用前、调用后获取方法的入参和返回值，进而实现了缓存的逻辑。 下面三个注解都是方法级别： @Cacheable 表明所修饰的方法是可以缓存的：当第一次调用这个方法时，它的结果会被缓存下来，在缓存的有效时间内，以后访问这个方法都直接返回缓存结果，不再执行方法中的代码段。 这个注解可以用condition属性来设置条件，如果不满足条件，就不使用缓存能力，直接执行方法。 可以使用key属性来指定key的生成规则。 @CachePut 与@Cacheable不同，@CachePut不仅会缓存方法的结果，还会执行方法的代码段。 它支持的属性和用法都与@Cacheable一致。 @CacheEvict 与@Cacheable功能相反，@CacheEvict表明所修饰的方法是用来删除失效或无用的缓存数据。 下面是@Cacheable、@CacheEvict和@CachePut基本使用方法的一个集中展示： @Servicepublic class UserService &#123; // @Cacheable可以设置多个缓存，形式如：@Cacheable(&#123;"books", "isbns"&#125;) @Cacheable(value=&#123;"users"&#125;, key="#user.id") public User findUser(User user) &#123; return findUserInDB(user.getId()); &#125; @Cacheable(value = "users", condition = "#user.getId() &lt;= 2") public User findUserInLimit(User user) &#123; return findUserInDB(user.getId()); &#125; @CachePut(value = "users", key = "#user.getId()") public void updateUser(User user) &#123; updateUserInDB(user); &#125; @CacheEvict(value = "users") public void removeUser(User user) &#123; removeUserInDB(user.getId()); &#125; @CacheEvict(value = "users", allEntries = true) public void clear() &#123; removeAllInDB(); &#125;&#125; @Caching 如果需要使用同一个缓存注解（@Cacheable、@CacheEvict或@CachePut）多次修饰一个方法，就需要用到@Caching。 @Caching(evict = &#123; @CacheEvict("primary"), @CacheEvict(cacheNames="secondary", key="#p0") &#125;)public Book importBooks(String deposit, Date date) @CacheConfig 与前面的缓存注解不同，这是一个类级别的注解。 如果类的所有操作都是缓存操作，你可以使用@CacheConfig来指定类，省去一些配置。 @CacheConfig("books")public class BookRepositoryImpl implements BookRepository &#123; @Cacheable public Book findBook(ISBN isbn) &#123;...&#125;&#125; 参考 如果想参考我的***完整代码示例***，请点击这里访问我的github。 下面是我在写作时参考的资料或文章。 Ehcache github Ehcache官方文档 Ehcache详细解读 注释驱动的 Spring cache 缓存介绍 Spring官方文档4.3.3.RELEASE 第36章缓存抽象]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>cache</tag>
        <tag>spring</tag>
        <tag>integration</tag>
        <tag>ehcache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring AOP]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fspring%2Fcore%2Faop%2F</url>
    <content type="text"><![CDATA[AOP 概念 什么是 AOP 术语 advice 的类型 关于 AOP Proxy 彻底理解 aspect, join point, point cut, advice @AspectJ 支持 使能 @AspectJ 支持 定义 aspect(切面) 声明 pointcut 声明 advice 概念 其实, 接触了这么久的 AOP, 我感觉, AOP 给人难以理解的一个关键点是它的概念比较多, 而且坑爹的是, 这些概念经过了中文翻译后, 变得面目全非, 相同的一个术语, 在不同的翻译下, 含义总有着各种莫名其妙的差别. 鉴于此, 我在本章的开头, 着重为为大家介绍一个 Spring AOP 的各项术语的基本含义. 为了术语传达的准确性, 我在接下来的叙述中, 能使用英文术语的地方, 尽量使用英文. 什么是 AOP AOP(Aspect-Oriented Programming), 即 面向切面编程, 它与 OOP( Object-Oriented Programming, 面向对象编程) 相辅相成, 提供了与 OOP 不同的抽象软件结构的视角. 在 OOP 中, 我们以类(class)作为我们的基本单元, 而 AOP 中的基本单元是 Aspect(切面) 术语 Aspect(切面) aspect 由 pointcount 和 advice 组成, 它既包含了横切逻辑的定义, 也包括了连接点的定义. Spring AOP 就是负责实施切面的框架, 它将切面所定义的横切逻辑织入到切面所指定的连接点中. AOP 的工作重心在于如何将增强织入目标对象的连接点上, 这里包含两个工作: 如何通过 pointcut 和 advice 定位到特定的 joinpoint 上 如何在 advice 中编写切面代码. 可以简单地认为, 使用 @Aspect 注解的类就是切面. advice(增强) 由 aspect 添加到特定的 join point(即满足 point cut 规则的 join point) 的一段代码. 许多 AOP 框架, 包括 Spring AOP, 会将 advice 模拟为一个拦截器(interceptor), 并且在 join point 上维护多个 advice, 进行层层拦截. 例如 HTTP 鉴权的实现, 我们可以为每个使用 RequestMapping 标注的方法织入 advice, 当 HTTP 请求到来时, 首先进入到 advice 代码中, 在这里我们可以分析这个 HTTP 请求是否有相应的权限, 如果有, 则执行 Controller, 如果没有, 则抛出异常. 这里的 advice 就扮演着鉴权拦截器的角色了. 连接点(join point) a point during the execution of a program, such as the execution of a method or the handling of an exception. In Spring AOP, a join point always represents a method execution. 程序运行中的一些时间点, 例如一个方法的执行, 或者是一个异常的处理. 在 Spring AOP 中, join point 总是方法的执行点, 即只有方法连接点. 切点(point cut) 匹配 join point 的谓词(a predicate that matches join points). Advice 是和特定的 point cut 关联的, 并且在 point cut 相匹配的 join point 中执行. 在 Spring 中, 所有的方法都可以认为是 joinpoint, 但是我们并不希望在所有的方法上都添加 Advice, 而 pointcut 的作用就是提供一组规则(使用 AspectJ pointcut expression language 来描述) 来匹配joinpoint, 给满足规则的 joinpoint 添加 Advice. 关于 join point 和 point cut 的区别 在 Spring AOP 中, 所有的方法执行都是 join point. 而 point cut 是一个描述信息, 它修饰的是 join point, 通过 point cut, 我们就可以确定哪些 join point 可以被织入 Advice. 因此 join point 和 point cut 本质上就是两个不同纬度上的东西. advice 是在 join point 上执行的, 而 point cut 规定了哪些 join point 可以执行哪些 advice introduction 为一个类型添加额外的方法或字段. Spring AOP 允许我们为 目标对象 引入新的接口(和对应的实现). 例如我们可以使用 introduction 来为一个 bean 实现 IsModified 接口, 并以此来简化 caching 的实现. 目标对象(Target) 织入 advice 的目标对象. 目标对象也被称为 advised object. 因为 Spring AOP 使用运行时代理的方式来实现 aspect, 因此 adviced object 总是一个代理对象(proxied object) 注意, adviced object 指的不是原来的类, 而是织入 advice 后所产生的代理类. AOP proxy 一个类被 AOP 织入 advice, 就会产生一个结果类, 它是融合了原类和增强逻辑的代理类. 在 Spring AOP 中, 一个 AOP 代理是一个 JDK 动态代理对象或 CGLIB 代理对象. 织入(Weaving) 将 aspect 和其他对象连接起来, 并创建 adviced object 的过程. 根据不同的实现技术, AOP 织入有三种方式: 编译器织入, 这要求有特殊的 Java 编译器. 类装载期织入, 这需要有特殊的类装载器. 动态代理织入, 在运行期为目标类添加增强(Advice)生成子类的方式. Spring 采用动态代理织入, 而 AspectJ 采用编译器织入和类装载期织入. advice 的类型 before advice, 在 join point 前被执行的 advice. 虽然 before advice 是在 join point 前被执行, 但是它并不能够阻止 join point 的执行, 除非发生了异常(即我们在 before advice 代码中, 不能人为地决定是否继续执行 join point 中的代码) after return advice, 在一个 join point 正常返回后执行的 advice after throwing advice, 当一个 join point 抛出异常后执行的 advice after(final) advice, 无论一个 join point 是正常退出还是发生了异常, 都会被执行的 advice. around advice, 在 join point 前和 joint point 退出后都执行的 advice. 这个是最常用的 advice. 关于 AOP Proxy Spring AOP 默认使用标准的 JDK 动态代理(dynamic proxy)技术来实现 AOP 代理, 通过它, 我们可以为任意的接口实现代理. 如果需要为一个类实现代理, 那么可以使用 CGLIB 代理. 当一个业务逻辑对象没有实现接口时, 那么 Spring AOP 就默认使用 CGLIB 来作为 AOP 代理了. 即如果我们需要为一个方法织入 advice, 但是这个方法不是一个接口所提供的方法, 则此时 Spring AOP 会使用 CGLIB 来实现动态代理. 鉴于此, Spring AOP 建议基于接口编程, 对接口进行 AOP 而不是类. 彻底理解 aspect, join point, point cut, advice 看完了上面的理论部分知识, 我相信还是会有不少朋友感觉到 AOP 的概念还是很模糊, 对 AOP 中的各种概念理解的还不是很透彻. 其实这很正常, 因为 AOP 中的概念是在是太多了, 我当时也是花了老大劲才梳理清楚的. 下面我以一个简单的例子来比喻一下 AOP 中 aspect, jointpoint, pointcut 与 advice 之间的关系. 让我们来假设一下, 从前有一个叫爪哇的小县城, 在一个月黑风高的晚上, 这个县城中发生了命案. 作案的凶手十分狡猾, 现场没有留下什么有价值的线索. 不过万幸的是, 刚从隔壁回来的老王恰好在这时候无意中发现了凶手行凶的过程, 但是由于天色已晚, 加上凶手蒙着面, 老王并没有看清凶手的面目, 只知道凶手是个男性, 身高约七尺五寸. 爪哇县的县令根据老王的描述, 对守门的士兵下命令说: 凡是发现有身高七尺五寸的男性, 都要抓过来审问. 士兵当然不敢违背县令的命令, 只好把进出城的所有符合条件的人都抓了起来. 来让我们看一下上面的一个小故事和 AOP 到底有什么对应关系. 首先我们知道, 在 Spring AOP 中 join point 指代的是所有方法的执行点, 而 point cut 是一个描述信息, 它修饰的是 join point, 通过 point cut, 我们就可以确定哪些 join point 可以被织入 Advice. 对应到我们在上面举的例子, 我们可以做一个简单的类比, join point 就相当于 爪哇的小县城里的百姓, point cut 就相当于 老王所做的指控, 即凶手是个男性, 身高约七尺五寸, 而 advice 则是施加在符合老王所描述的嫌疑人的动作: 抓过来审问. 为什么可以这样类比呢? join point --&gt; 爪哇的小县城里的百姓: 因为根据定义, join point 是所有可能被织入 advice 的候选的点, 在 Spring AOP 中, 则可以认为所有方法执行点都是 join point. 而在我们上面的例子中, 命案发生在小县城中, 按理说在此县城中的所有人都有可能是嫌疑人. point cut --&gt; 男性, 身高约七尺五寸: 我们知道, 所有的方法(joint point) 都可以织入 advice, 但是我们并不希望在所有方法上都织入 advice, 而 pointcut 的作用就是提供一组规则来匹配 joinpoint, 给满足规则的 joinpoint 添加 advice. 同理, 对于县令来说, 他再昏庸, 也知道不能把县城中的所有百姓都抓起来审问, 而是根据凶手是个男性, 身高约七尺五寸, 把符合条件的人抓起来. 在这里凶手是个男性, 身高约七尺五寸 就是一个修饰谓语, 它限定了凶手的范围, 满足此修饰规则的百姓都是嫌疑人, 都需要抓起来审问. advice --&gt; 抓过来审问, advice 是一个动作, 即一段 Java 代码, 这段 Java 代码是作用于 point cut 所限定的那些 join point 上的. 同理, 对比到我们的例子中, 抓过来审问 这个动作就是对作用于那些满足 男性, 身高约七尺五寸 的爪哇的小县城里的百姓. aspect: aspect 是 point cut 与 advice 的组合, 因此在这里我们就可以类比: “根据老王的线索, 凡是发现有身高七尺五寸的男性, 都要抓过来审问” 这一整个动作可以被认为是一个 aspect. 或则我们也可以从语法的角度来简单类比一下. 我们在学英语时, 经常会接触什么 定语, 被动句 之类的概念, 那么可以做一个不严谨的类比, 即 joinpoint 可以认为是一个 宾语, 而 pointcut 则可以类比为修饰 joinpoint 的定语, 那么整个 aspect 就可以描述为: 满足 pointcut 规则的 joinpoint 会被添加相应的 advice 操作. @AspectJ 支持 @AspectJ 是一种使用 Java 注解来实现 AOP 的编码风格。 @AspectJ 风格的 AOP 是 AspectJ Project 在 AspectJ 5 中引入的, 并且 Spring 也支持 @AspectJ 的 AOP 风格. 使能 @AspectJ 支持 @AspectJ 可以以 XML 的方式或以注解的方式来使能, 并且不论以哪种方式使能@ASpectJ, 我们都必须保证 aspectjweaver.jar 在 classpath 中. 使用 Java Configuration 方式使能@AspectJ @Configuration@EnableAspectJAutoProxypublic class AppConfig &#123;&#125; 使用 XML 方式使能@AspectJ &lt;aop:aspectj-autoproxy/&gt; 定义 aspect(切面) 当使用注解 @Aspect 标注一个 Bean 后, 那么 Spring 框架会自动收集这些 Bean, 并添加到 Spring AOP 中, 例如: @Component@Aspectpublic class MyTest &#123;&#125; 注意, 仅仅使用@Aspect 注解, 并不能将一个 Java 对象转换为 Bean, 因此我们还需要使用类似 @Component 之类的注解. 注意, 如果一个 类被@Aspect 标注, 则这个类就不能是其他 aspect 的 **advised object** 了, 因为使用 @Aspect 后, 这个类就会被排除在 auto-proxying 机制之外. 声明 pointcut 一个 pointcut 的声明由两部分组成: 一个方法签名, 包括方法名和相关参数 一个 pointcut 表达式, 用来指定哪些方法执行是我们感兴趣的(即因此可以织入 advice). 在@AspectJ 风格的 AOP 中, 我们使用一个方法来描述 pointcut, 即: @Pointcut("execution(* com.xys.service.UserService.*(..))") // 切点表达式private void dataAccessOperation() &#123;&#125; // 切点前面 这个方法必须无返回值. 这个方法本身就是 pointcut signature, pointcut 表达式使用@Pointcut 注解指定. 上面我们简单地定义了一个 pointcut, 这个 pointcut 所描述的是: 匹配所有在包 com.xys.service.UserService 下的所有方法的执行. 切点标志符(designator) AspectJ5 的切点表达式由标志符(designator)和操作参数组成. 如 “execution(* greetTo(…))” 的切点表达式, **execution** 就是 标志符, 而圆括号里的 *****greetTo(…) 就是操作参数 execution 匹配 join point 的执行, 例如 “execution(* hello(…))” 表示匹配所有目标类中的 hello() 方法. 这个是最基本的 pointcut 标志符. within 匹配特定包下的所有 join point, 例如 within(com.xys.*) 表示 com.xys 包中的所有连接点, 即包中的所有类的所有方法. 而within(com.xys.service.*Service) 表示在 com.xys.service 包中所有以 Service 结尾的类的所有的连接点. this 与 target this 的作用是匹配一个 bean, 这个 bean(Spring AOP proxy) 是一个给定类型的实例(instance of). 而 target 匹配的是一个目标对象(target object, 即需要织入 advice 的原始的类), 此对象是一个给定类型的实例(instance of). bean 匹配 bean 名字为指定值的 bean 下的所有方法, 例如: bean(*Service) // 匹配名字后缀为 Service 的 bean 下的所有方法bean(myService) // 匹配名字为 myService 的 bean 下的所有方法 args 匹配参数满足要求的的方法. 例如: @Pointcut("within(com.xys.demo2.*)")public void pointcut2() &#123;&#125;@Before(value = "pointcut2() &amp;&amp; args(name)")public void doSomething(String name) &#123; logger.info("---page: &#123;&#125;---", name);&#125; @Servicepublic class NormalService &#123; private Logger logger = LoggerFactory.getLogger(getClass()); public void someMethod() &#123; logger.info("---NormalService: someMethod invoked---"); &#125; public String test(String name) &#123; logger.info("---NormalService: test invoked---"); return "服务一切正常"; &#125;&#125; 当 NormalService.test 执行时, 则 advice doSomething 就会执行, test 方法的参数 name 就会传递到 doSomething 中. 常用例子: // 匹配只有一个参数 name 的方法@Before(value = "aspectMethod() &amp;&amp; args(name)")public void doSomething(String name) &#123;&#125;// 匹配第一个参数为 name 的方法@Before(value = "aspectMethod() &amp;&amp; args(name, ..)")public void doSomething(String name) &#123;&#125;// 匹配第二个参数为 name 的方法Before(value = "aspectMethod() &amp;&amp; args(*, name, ..)")public void doSomething(String name) &#123;&#125; @annotation 匹配由指定注解所标注的方法, 例如: @Pointcut("@annotation(com.xys.demo1.AuthChecker)")public void pointcut() &#123;&#125; 则匹配由注解 AuthChecker 所标注的方法. 常见的切点表达式 匹配方法签名 // 匹配指定包中的所有的方法execution(* com.xys.service.*(..))// 匹配当前包中的指定类的所有方法execution(* UserService.*(..))// 匹配指定包中的所有 public 方法execution(public * com.xys.service.*(..))// 匹配指定包中的所有 public 方法, 并且返回值是 int 类型的方法execution(public int com.xys.service.*(..))// 匹配指定包中的所有 public 方法, 并且第一个参数是 String, 返回值是 int 类型的方法execution(public int com.xys.service.*(String name, ..)) 匹配类型签名 // 匹配指定包中的所有的方法, 但不包括子包within(com.xys.service.*)// 匹配指定包中的所有的方法, 包括子包within(com.xys.service..*)// 匹配当前包中的指定类中的方法within(UserService)// 匹配一个接口的所有实现类中的实现的方法within(UserDao+) 匹配 Bean 名字 // 匹配以指定名字结尾的 Bean 中的所有方法bean(*Service) 切点表达式组合 // 匹配以 Service 或 ServiceImpl 结尾的 beanbean(*Service || *ServiceImpl)// 匹配名字以 Service 结尾, 并且在包 com.xys.service 中的 beanbean(*Service) &amp;&amp; within(com.xys.service.*) 声明 advice advice 是和一个 pointcut 表达式关联在一起的, 并且会在匹配的 join point 的方法执行的前/后/周围 运行. pointcut 表达式可以是简单的一个 pointcut 名字的引用, 或者是完整的 pointcut 表达式. 下面我们以几个简单的 advice 为例子, 来看一下一个 advice 是如何声明的. Before advice /** * @author xiongyongshun * @version 1.0 * @created 16/9/9 13:13 */@Component@Aspectpublic class BeforeAspectTest &#123; // 定义一个 Pointcut, 使用 切点表达式函数 来描述对哪些 Join point 使用 advise. @Pointcut("execution(* com.xys.service.UserService.*(..))") public void dataAccessOperation() &#123; &#125;&#125; @Component@Aspectpublic class AdviseDefine &#123; // 定义 advise @Before("com.xys.aspect.PointcutDefine.dataAccessOperation()") public void doBeforeAccessCheck(JoinPoint joinPoint) &#123; System.out.println("*****Before advise, method: " + joinPoint.getSignature().toShortString() + " *****"); &#125;&#125; 这里, @Before 引用了一个 pointcut, 即 “com.xys.aspect.PointcutDefine.dataAccessOperation()” 是一个 pointcut 的名字. 如果我们在 advice 在内置 pointcut, 则可以: @Component@Aspectpublic class AdviseDefine &#123; // 将 pointcut 和 advice 同时定义 @Before("within(com.xys.service..*)") public void doAccessCheck(JoinPoint joinPoint) &#123; System.out.println("*****doAccessCheck, Before advise, method: " + joinPoint.getSignature().toShortString() + " *****"); &#125;&#125; around advice around advice 比较特别, 它可以在一个方法的之前之前和之后添加不同的操作, 并且甚至可以决定何时, 如何, 是否调用匹配到的方法. @Component@Aspectpublic class AdviseDefine &#123; // 定义 advise @Around("com.xys.aspect.PointcutDefine.dataAccessOperation()") public Object doAroundAccessCheck(ProceedingJoinPoint pjp) throws Throwable &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); // 开始 Object retVal = pjp.proceed(); stopWatch.stop(); // 结束 System.out.println("invoke method: " + pjp.getSignature().getName() + ", elapsed time: " + stopWatch.getTotalTimeMillis()); return retVal; &#125;&#125; around advice 和前面的 before advice 差不多, 只是我们把注解 @Before 改为了 @Around 了.]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>aop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 简介]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fspring%2Fweb%2Fspring-mvc-introduction%2F</url>
    <content type="text"><![CDATA[SpringMVC 工作流程描述 Spring MVC的工作流程可以用一幅图来说明： 向服务器发送HTTP请求，请求被前端控制器 DispatcherServlet 捕获。 DispatcherServlet 根据 -servlet.xml 中的配置对请求的URL进行解析，得到请求资源标识符（URI）。然后根据该URI，调用 HandlerMapping 获得该Handler配置的所有相关的对象（包括Handler对象以及Handler对象对应的拦截器），最后以HandlerExecutionChain 对象的形式返回。 DispatcherServlet 根据获得的Handler，选择一个合适的 HandlerAdapter。（附注：如果成功获得HandlerAdapter后，此时将开始执行拦截器的preHandler(…)方法）。 提取Request中的模型数据，填充Handler入参，开始执行Handler（Controller)。 在填充Handler的入参过程中，根据你的配置，Spring将帮你做一些额外的工作： HttpMessageConveter： 将请求消息（如Json、xml等数据）转换成一个对象，将对象转换为指定的响应信息。 数据转换：对请求消息进行数据转换。如String转换成Integer、Double等。 数据根式化：对请求消息进行数据格式化。 如将字符串转换成格式化数字或格式化日期等。 数据验证： 验证数据的有效性（长度、格式等），验证结果存储到BindingResult或Error中。 Handler(Controller)执行完成后，向 DispatcherServlet 返回一个 ModelAndView 对象； 根据返回的ModelAndView，选择一个适合的 ViewResolver（必须是已经注册到Spring容器中的ViewResolver)返回给DispatcherServlet。 ViewResolver 结合Model和View，来渲染视图。 视图负责将渲染结果返回给客户端。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>spring</tag>
        <tag>mvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring IOC]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fspring%2Fcore%2Fioc%2F</url>
    <content type="text"><![CDATA[Spring IOC IoC 概念 IoC 是什么 IoC 能做什么 依赖注入 IoC 和 DI IoC 容器 Bean IoC 容器 核心接口 IoC 容器工作步骤 Bean 概述 依赖 IoC 容器配置 Xml 配置 注解配置 Java 配置 参考资料 IoC 概念 IoC 是什么 IoC，是 Inversion of Control 的缩写，即控制反转。 上层模块不应该依赖于下层模块，它们共同依赖于一个抽象 抽象不能依赖于具体实现，具体实现依赖于抽象 注：又称为依赖倒置原则。这是设计模式六大原则之一。 IoC 不是什么技术，而是一种设计思想。在 Java 开发中，IoC 意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。如何理解 Ioc 呢？理解 Ioc 的关键是要明确“谁控制谁，控制什么，为何是反转（有反转就应该有正转了），哪些方面反转了”，那我们来深入分析一下： **谁控制谁，控制什么：**传统 JavaSE 程序设计，我们直接在对象内部通过 new 进行创建对象，是程序主动去创建依赖对象；而 IoC 是有专门一个容器来创建这些对象，即由 IoC 容器来控制对象的创建；谁控制谁？当然是 IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）。 **为何是反转，哪些方面反转了：**有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。 用图例说明一下，传统程序设计如图 2-1，都是主动去创建相关对象然后再组合起来： 图 2-1 传统应用程序示意图 当有了 IoC/DI 的容器后，在客户端类中不再主动去创建这些对象了，如图 2-2 所示: 图 2-2 有 IoC/DI 容器后程序结构示意图 IoC 能做什么 IoC 不是一种技术，只是一种思想，一个重要的面向对象编程的法则，它能指导我们如何设计出松耦合、更优良的程序。传统应用程序都是由我们在类内部主动创建依赖对象，从而导致类与类之间高耦合，难于测试；有了 IoC 容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。 其实 IoC 对编程带来的最大改变不是从代码上，而是从思想上，发生了“主从换位”的变化。应用程序原本是老大，要获取什么资源都是主动出击，但是在 IoC/DI 思想中，应用程序就变成被动的了，被动的等待 IoC 容器来创建并注入它所需要的资源了。 IoC 很好的体现了面向对象设计法则之一—— 好莱坞法则：“别找我们，我们找你”；即由 IoC 容器帮对象找相应的依赖对象并注入，而不是由对象主动去找。 依赖注入 DI，是 Dependency Injection 的缩写，即依赖注入。 依赖注入是 IoC 的最常见形式。 容器全权负责的组件的装配，它会把符合依赖关系的对象通过 JavaBean 属性或者构造函数传递给需要的对象。 DI 是组件之间依赖关系由容器在运行期决定，形象的说，即由容器动态的将某个依赖关系注入到组件之中。依赖注入的目的并非为软件系统带来更多功能，而是为了提升组件重用的频率，并为系统搭建一个灵活、可扩展的平台。通过依赖注入机制，我们只需要通过简单的配置，而无需任何代码就可指定目标需要的资源，完成自身的业务逻辑，而不需要关心具体的资源来自何处，由谁实现。 理解 DI 的关键是：“谁依赖谁，为什么需要依赖，谁注入谁，注入了什么”，那我们来深入分析一下： **谁依赖于谁：**当然是应用程序依赖于 IoC 容器； **为什么需要依赖：**应用程序需要 IoC 容器来提供对象需要的外部资源； **谁注入谁：**很明显是 IoC 容器注入应用程序某个对象，应用程序依赖的对象； 注入了什么：就是注入某个对象所需要的外部资源（包括对象、资源、常量数据）。 IoC 和 DI 其实它们是同一个概念的不同角度描述，由于控制反转概念比较含糊（可能只是理解为容器控制对象这一个层面，很难让人想到谁来维护对象关系），所以 2004 年大师级人物 Martin Fowler 又给出了一个新的名字：“依赖注入”，相对 IoC 而言，“依赖注入”明确描述了“被注入对象依赖 IoC 容器配置依赖对象”。 注：如果想要更加深入的了解 IoC 和 DI，请参考大师级人物 Martin Fowler 的一篇经典文章 Inversion of Control Containers and the Dependency Injection pattern 。 IoC 容器 IoC 容器就是具有依赖注入功能的容器。IoC 容器负责实例化、定位、配置应用程序中的对象及建立这些对象间的依赖。应用程序无需直接在代码中 new 相关的对象，应用程序由 IoC 容器进行组装。在 Spring 中 BeanFactory 是 IoC 容器的实际代表者。 Spring IoC 容器如何知道哪些是它管理的对象呢？这就需要配置文件，Spring IoC 容器通过读取配置文件中的配置元数据，通过元数据对应用中的各个对象进行实例化及装配。一般使用基于 xml 配置文件进行配置元数据，而且 Spring 与配置文件完全解耦的，可以使用其他任何可能的方式进行配置元数据，比如注解、基于 java 文件的、基于属性文件的配置都可以 那 Spring IoC 容器管理的对象叫什么呢？ Bean JavaBean 是一种 JAVA 语言写成的可重用组件。为写成 JavaBean，类必须是具体的和公共的，并且具有无参数的构造器。JavaBean 对外部通过提供 getter / setter 方法来访问其成员。 由 IoC 容器管理的那些组成你应用程序的对象我们就叫它 Bean。Bean 就是由 Spring 容器初始化、装配及管理的对象，除此之外，bean 就与应用程序中的其他对象没有什么区别了。那 IoC 怎样确定如何实例化 Bean、管理 Bean 之间的依赖关系以及管理 Bean 呢？这就需要配置元数据，在 Spring 中由 BeanDefinition 代表，后边会详细介绍，配置元数据指定如何实例化 Bean、如何组装 Bean 等。 IoC 容器 核心接口 org.springframework.beans 和 org.springframework.context 是 IoC 容器的基础。 在 Spring 中，有两种 IoC 容器：BeanFactory 和 ApplicationContext。 BeanFactory：Spring 实例化、配置和管理对象的最基本接口。 ApplicationContext：BeanFactory 的子接口。它还扩展了其他一些接口，以支持更丰富的功能，如：国际化、访问资源、事件机制、更方便的支持 AOP、在 web 应用中指定应用层上下文等。 实际开发中，更推荐使用 ApplicationContext 作为 IoC 容器，因为它的功能远多于 FactoryBean。 常见 ApplicationContext 实现： ClassPathXmlApplicationContext：ApplicationContext 的实现，从 classpath 获取配置文件； BeanFactory beanFactory = new ClassPathXmlApplicationContext("classpath.xml"); FileSystemXmlApplicationContext：ApplicationContext 的实现，从文件系统获取配置文件。 BeanFactory beanFactory = new FileSystemXmlApplicationContext("fileSystemConfig.xml"); IoC 容器工作步骤 使用 IoC 容器可分为三步骤： 配置元数据：需要配置一些元数据来告诉 Spring，你希望容器如何工作，具体来说，就是如何去初始化、配置、管理 JavaBean 对象。 实例化容器：由 IoC 容器解析配置的元数据。IoC 容器的 Bean Reader 读取并解析配置文件，根据定义生成 BeanDefinition 配置元数据对象，IoC 容器根据 BeanDefinition 进行实例化、配置及组装 Bean。 使用容器：由客户端实例化容器，获取需要的 Bean。 配置元数据 元数据（Metadata） 又称中介数据、中继数据，为描述数据的数据（data about data），主要是描述数据属性（property）的信息。 配置元数据的方式： 基于 xml 配置：Spring 的传统配置方式。在 &lt;beans&gt; 标签中配置元数据内容。 缺点是当 JavaBean 过多时，产生的配置文件足以让你眼花缭乱。 基于注解配置：Spring2.5 引入。可以大大简化你的配置。 基于 Java 配置：可以使用 Java 类来定义 JavaBean 。 为了使用这个新特性，需要用到 @Configuration 、@Bean 、@Import 和 @DependsOn 注解。 Bean 概述 一个 Spring 容器管理一个或多个 bean。 这些 bean 根据你配置的元数据（比如 xml 形式）来创建。 Spring IoC 容器本身，并不能识别你配置的元数据。为此，要将这些配置信息转为 Spring 能识别的格式——BeanDefinition 对象。 命名 Bean 指定 id 和 name 属性不是必须的。 Spring 中，并非一定要指定 id 和 name 属性。实际上，Spring 会自动为其分配一个特殊名。 如果你需要引用声明的 bean，这时你才需要一个标识。官方推荐驼峰命名法来命名。 支持别名 可能存在这样的场景，不同系统中对于同一 bean 的命名方式不一样。 为了适配，Spring 支持 &lt;alias&gt; 为 bean 添加别名的功能。 &lt;alias name="subsystemA-dataSource" alias="subsystemB-dataSource"/&gt;&lt;alias name="subsystemA-dataSource" alias="myApp-dataSource" /&gt; 实例化 Bean 构造器方式 &lt;bean id="exampleBean" class="examples.ExampleBean"/&gt; 静态工厂方法 依赖 依赖注入 依赖注入有两种主要方式： 构造器注入 Setter 注入 构造器注入有可能出现循环注入的错误。如： class A &#123; public A(B b)&#123;&#125;&#125;class B &#123; public B(A a)&#123;&#125;&#125; 依赖和配置细节 使用 depends-on Lazy-initialized Bean 自动装配 方法注入 IoC 容器配置 IoC 容器的配置有三种方式： 基于 xml 配置 基于注解配置 基于 Java 配置 作为 Spring 传统的配置方式，xml 配置方式一般为大家所熟知。 如果厌倦了 xml 配置，Spring 也提供了注解配置方式或 Java 配置方式来简化配置。 本文，将对 Java 配置 IoC 容器做详细的介绍。 Xml 配置 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;import resource="resource1.xml" /&gt; &lt;bean id="bean1" class=""&gt;&lt;/bean&gt; &lt;bean id="bean2" class=""&gt;&lt;/bean&gt; &lt;bean name="bean2" class=""&gt;&lt;/bean&gt; &lt;alias alias="bean3" name="bean2"/&gt; &lt;import resource="resource2.xml" /&gt; &lt;/beans&gt; 标签说明： &lt;beans&gt; 是 Spring 配置文件的根节点。 &lt;bean&gt; 用来定义一个 JavaBean。id 属性是它的标识，在文件中必须唯一；class 属性是它关联的类。 &lt;alias&gt; 用来定义 Bean 的别名。 &lt;import&gt; 用来导入其他配置文件的 Bean 定义。这是为了加载多个配置文件，当然也可以把这些配置文件构造为一个数组（new String[] {“config1.xml”, config2.xml}）传给 ApplicationContext 实现类进行加载多个配置文件，那一个更适合由用户决定；这两种方式都是通过调用 Bean Definition Reader 读取 Bean 定义，内部实现没有任何区别。&lt;import&gt; 标签可以放在 &lt;beans&gt; 下的任何位置，没有顺序关系。 实例化容器 实例化容器的过程： 定位资源（XML 配置文件） 读取配置信息(Resource) 转化为 Spring 可识别的数据形式（BeanDefinition） ApplicationContext context = new ClassPathXmlApplicationContext(new String[] &#123;"services.xml", "daos.xml"&#125;); 组合 xml 配置文件 配置的 Bean 功能各不相同，都放在一个 xml 文件中，不便管理。 Java 设计模式讲究职责单一原则。配置其实也是如此，功能不同的 JavaBean 应该被组织在不同的 xml 文件中。然后使用 import 标签把它们统一导入。 &lt;import resource="classpath:spring/applicationContext.xml"/&gt;&lt;import resource="/WEB-INF/spring/service.xml"/&gt; 使用容器 使用容器的方式就是通过getBean获取 IoC 容器中的 JavaBean。 Spring 也有其他方法去获得 JavaBean，但是 Spring 并不推荐其他方式。 // create and configure beansApplicationContext context =new ClassPathXmlApplicationContext(new String[] &#123;"services.xml", "daos.xml"&#125;);// retrieve configured instancePetStoreService service = context.getBean("petStore", PetStoreService.class);// use configured instanceList&lt;String&gt; userList = service.getUsernameList(); 注解配置 Spring2.5 引入了注解。 于是，一个问题产生了：使用注解方式注入 JavaBean 是不是一定完爆 xml 方式？ 未必。正所谓，仁者见仁智者见智。任何事物都有其优缺点，看你如何取舍。来看看注解的优缺点： 优点：大大减少了配置，并且可以使配置更加精细——类，方法，字段都可以用注解去标记。 缺点：使用注解，不可避免产生了侵入式编程，也产生了一些问题。 你需要将注解加入你的源码并编译它； 注解往往比较分散，不易管控。 注：spring 中，先进行注解注入，然后才是 xml 注入，因此如果注入的目标相同，后者会覆盖前者。 启动注解 Spring 默认是不启用注解的。如果想使用注解，需要先在 xml 中启动注解。 启动方式：在 xml 中加入一个标签，很简单吧。 &lt;context:annotation-config/&gt; 注：&lt;context:annotation-config/&gt; 只会检索定义它的上下文。什么意思呢？就是说，如果你 为 DispatcherServlet 指定了一个WebApplicationContext，那么它只在 controller 中查找@Autowired注解，而不会检查其它的路径。 Spring 注解 @Required @Required 注解只能用于修饰 bean 属性的 setter 方法。受影响的 bean 属性必须在配置时被填充在 xml 配置文件中，否则容器将抛出BeanInitializationException。 public class AnnotationRequired &#123; private String name; private String sex; public String getName() &#123; return name; &#125; /** * @Required 注解用于bean属性的setter方法并且它指示，受影响的bean属性必须在配置时被填充在xml配置文件中， * 否则容器将抛出BeanInitializationException。 */ @Required public void setName(String name) &#123; this.name = name; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125;&#125; @Autowired @Autowired注解可用于修饰属性、setter 方法、构造方法。 注：@Autowired注解也可用于修饰构造方法，但如果类中只有默认构造方法，则没有必要。如果有多个构造器，至少应该修饰一个，来告诉容器哪一个必须使用。 可以使用 JSR330 的注解@Inject来替代@Autowired。 范例 public class AnnotationAutowired &#123; private static final Logger log = LoggerFactory.getLogger(AnnotationRequired.class); @Autowired private Apple fieldA; private Banana fieldB; private Orange fieldC; public Apple getFieldA() &#123; return fieldA; &#125; public void setFieldA(Apple fieldA) &#123; this.fieldA = fieldA; &#125; public Banana getFieldB() &#123; return fieldB; &#125; @Autowired public void setFieldB(Banana fieldB) &#123; this.fieldB = fieldB; &#125; public Orange getFieldC() &#123; return fieldC; &#125; public void setFieldC(Orange fieldC) &#123; this.fieldC = fieldC; &#125; public AnnotationAutowired() &#123;&#125; @Autowired public AnnotationAutowired(Orange fieldC) &#123; this.fieldC = fieldC; &#125; public static void main(String[] args) throws Exception &#123; AbstractApplicationContext ctx = new ClassPathXmlApplicationContext("spring/spring-annotation.xml"); AnnotationAutowired annotationAutowired = (AnnotationAutowired) ctx.getBean("annotationAutowired"); log.debug("fieldA: &#123;&#125;, fieldB:&#123;&#125;, fieldC:&#123;&#125;", annotationAutowired.getFieldA().getName(), annotationAutowired.getFieldB().getName(), annotationAutowired.getFieldC().getName()); ctx.close(); &#125;&#125; xml 中的配置 &lt;!-- 测试@Autowired --&gt;&lt;bean id="apple" class="org.zp.notes.spring.beans.annotation.sample.Apple"/&gt;&lt;bean id="potato" class="org.zp.notes.spring.beans.annotation.sample.Banana"/&gt;&lt;bean id="tomato" class="org.zp.notes.spring.beans.annotation.sample.Orange"/&gt;&lt;bean id="annotationAutowired" class="org.zp.notes.spring.beans.annotation.sample.AnnotationAutowired"/&gt; @Qualifier 在@Autowired注解中，提到了如果发现有多个候选的 bean 都符合修饰类型，Spring 就会抓瞎了。 那么，如何解决这个问题。 可以通过@Qualifier指定 bean 名称来锁定真正需要的那个 bean。 范例 public class AnnotationQualifier &#123; private static final Logger log = LoggerFactory.getLogger(AnnotationQualifier.class); @Autowired @Qualifier("dog") /** 去除这行，会报异常 */ Animal dog; Animal cat; public Animal getDog() &#123; return dog; &#125; public void setDog(Animal dog) &#123; this.dog = dog; &#125; public Animal getCat() &#123; return cat; &#125; @Autowired public void setCat(@Qualifier("cat") Animal cat) &#123; this.cat = cat; &#125; public static void main(String[] args) throws Exception &#123; AbstractApplicationContext ctx = new ClassPathXmlApplicationContext("spring/spring-annotation.xml"); AnnotationQualifier annotationQualifier = (AnnotationQualifier) ctx.getBean("annotationQualifier"); log.debug("Dog name: &#123;&#125;", annotationQualifier.getDog().getName()); log.debug("Cat name: &#123;&#125;", annotationQualifier.getCat().getName()); ctx.close(); &#125;&#125;abstract class Animal &#123; public String getName() &#123; return null; &#125;&#125;class Dog extends Animal &#123; public String getName() &#123; return "狗"; &#125;&#125;class Cat extends Animal &#123; public String getName() &#123; return "猫"; &#125;&#125; xml 中的配置 &lt;!-- 测试@Qualifier --&gt;&lt;bean id="dog" class="org.zp.notes.spring.beans.annotation.sample.Dog"/&gt;&lt;bean id="cat" class="org.zp.notes.spring.beans.annotation.sample.Cat"/&gt;&lt;bean id="annotationQualifier" class="org.zp.notes.spring.beans.annotation.sample.AnnotationQualifier"/&gt; JSR 250 注解 @Resource Spring 支持 JSP250 规定的注解@Resource。这个注解根据指定的名称来注入 bean。 如果没有为@Resource指定名称，它会像@Autowired一样按照类型去寻找匹配。 在 Spring 中，由CommonAnnotationBeanPostProcessor来处理@Resource注解。 范例 public class AnnotationResource &#123; private static final Logger log = LoggerFactory.getLogger(AnnotationResource.class); @Resource(name = "flower") Plant flower; @Resource(name = "tree") Plant tree; public Plant getFlower() &#123; return flower; &#125; public void setFlower(Plant flower) &#123; this.flower = flower; &#125; public Plant getTree() &#123; return tree; &#125; public void setTree(Plant tree) &#123; this.tree = tree; &#125; public static void main(String[] args) throws Exception &#123; AbstractApplicationContext ctx = new ClassPathXmlApplicationContext("spring/spring-annotation.xml"); AnnotationResource annotationResource = (AnnotationResource) ctx.getBean("annotationResource"); log.debug("type: &#123;&#125;, name: &#123;&#125;", annotationResource.getFlower().getClass(), annotationResource.getFlower().getName()); log.debug("type: &#123;&#125;, name: &#123;&#125;", annotationResource.getTree().getClass(), annotationResource.getTree().getName()); ctx.close(); &#125;&#125; xml 的配置 &lt;!-- 测试@Resource --&gt;&lt;bean id="flower" class="org.zp.notes.spring.beans.annotation.sample.Flower"/&gt;&lt;bean id="tree" class="org.zp.notes.spring.beans.annotation.sample.Tree"/&gt;&lt;bean id="annotationResource" class="org.zp.notes.spring.beans.annotation.sample.AnnotationResource"/&gt; @PostConstruct 和 @PreDestroy @PostConstruct 和 @PreDestroy 是 JSR 250 规定的用于生命周期的注解。 从其名号就可以看出，一个是在构造之后调用的方法，一个是销毁之前调用的方法。 public class AnnotationPostConstructAndPreDestroy &#123; private static final Logger log = LoggerFactory.getLogger(AnnotationPostConstructAndPreDestroy.class); @PostConstruct public void init() &#123; log.debug("call @PostConstruct method"); &#125; @PreDestroy public void destroy() &#123; log.debug("call @PreDestroy method"); &#125;&#125; JSR 330 注解 从 Spring3.0 开始，Spring 支持 JSR 330 标准注解（依赖注入）。 注：如果要使用 JSR 330 注解，需要使用外部 jar 包。 若你使用 maven 管理 jar 包，只需要添加依赖到 pom.xml 即可： &lt;dependency&gt; &lt;groupId&gt;javax.inject&lt;/groupId&gt; &lt;artifactId&gt;javax.inject&lt;/artifactId&gt; &lt;version&gt;1&lt;/version&gt;&lt;/dependency&gt; @Inject @Inject和@Autowired一样，可以修饰属性、setter 方法、构造方法。 范例 public class AnnotationInject &#123; private static final Logger log = LoggerFactory.getLogger(AnnotationInject.class); @Inject Apple fieldA; Banana fieldB; Orange fieldC; public Apple getFieldA() &#123; return fieldA; &#125; public void setFieldA(Apple fieldA) &#123; this.fieldA = fieldA; &#125; public Banana getFieldB() &#123; return fieldB; &#125; @Inject public void setFieldB(Banana fieldB) &#123; this.fieldB = fieldB; &#125; public Orange getFieldC() &#123; return fieldC; &#125; public AnnotationInject() &#123;&#125; @Inject public AnnotationInject(Orange fieldC) &#123; this.fieldC = fieldC; &#125; public static void main(String[] args) throws Exception &#123; AbstractApplicationContext ctx = new ClassPathXmlApplicationContext("spring/spring-annotation.xml"); AnnotationInject annotationInject = (AnnotationInject) ctx.getBean("annotationInject"); log.debug("type: &#123;&#125;, name: &#123;&#125;", annotationInject.getFieldA().getClass(), annotationInject.getFieldA().getName()); log.debug("type: &#123;&#125;, name: &#123;&#125;", annotationInject.getFieldB().getClass(), annotationInject.getFieldB().getName()); log.debug("type: &#123;&#125;, name: &#123;&#125;", annotationInject.getFieldC().getClass(), annotationInject.getFieldC().getName()); ctx.close(); &#125;&#125; Java 配置 基于 Java 配置 Spring IoC 容器，实际上是 Spring 允许用户定义一个类，在这个类中去管理 IoC 容器的配置。 为了让 Spring 识别这个定义类为一个 Spring 配置类，需要用到两个注解：@Configuration 和@Bean。 如果你熟悉 Spring 的 xml 配置方式，你可以将@Configuration 等价于标签；将@Bean 等价于标签。 @Bean @Bean 的修饰目标只能是方法或注解。 @Bean 只能定义在@Configuration 或@Component 注解修饰的类中。 声明一个 bean 此外，@Configuration 类允许在同一个类中通过@Bean 定义内部 bean 依赖。 声明一个 bean，只需要在 bean 属性的 set 方法上标注@Bean 即可。 @Configuration public class AnnotationConfiguration { private static final Logger log = LoggerFactory.getLogger(JavaComponentScan.class); @Bean public Job getPolice() { return new Police(); } public static void main(String[] args) { AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(AnnotationConfiguration.class); ctx.scan(&quot;org.zp.notes.spring.beans&quot;); ctx.refresh(); Job job = (Job) ctx.getBean(&quot;police&quot;); log.debug(&quot;job: {}, work: {}&quot;, job.getClass(), job.work()); } } public interface Job { String work(); } @Component(&quot;police&quot;) public class Police implements Job { @Override public String work() { return &quot;抓罪犯&quot;; } } 这等价于配置 &lt;beans&gt; &lt;bean id=&quot;police&quot; class=&quot;org.zp.notes.spring.ioc.sample.job.Police&quot;/&gt; &lt;/beans&gt; @Bean 注解用来表明一个方法实例化、配置合初始化一个被 Spring IoC 容器管理的新对象。 如果你熟悉 Spring 的 xml 配置，你可以将@Bean 视为等价于标签。 @Bean 注解可以用于任何的 Spring @Component bean，然而，通常被用于@Configuration bean。 @Configuration @Configuration 是一个类级别的注解，用来标记被修饰类的对象是一个 BeanDefinition。 @Configuration 类声明 bean 是通过被@Bean 修饰的公共方法。此外，@Configuration 类允许在同一个类中通过@Bean 定义内部 bean 依赖。 @Configuration public class AppConfig { @Bean public MyService myService() { return new MyServiceImpl(); } } 这等价于配置 &lt;beans&gt; &lt;bean id=&quot;myService&quot; class=&quot;com.acme.services.MyServiceImpl&quot;/&gt; &lt;/beans&gt; 用 AnnotationConfigApplicationContext 实例化 IoC 容器。 Java 配置 基于 Java 配置 Spring IoC 容器，实际上是Spring 允许用户定义一个类，在这个类中去管理 IoC 容器的配置。 为了让 Spring 识别这个定义类为一个 Spring 配置类，需要用到两个注解：@Configuration和@Bean。 如果你熟悉 Spring 的 xml 配置方式，你可以将@Configuration等价于&lt;beans&gt;标签；将@Bean等价于&lt;bean&gt;标签。 @Bean @Bean 的修饰目标只能是方法或注解。 @Bean 只能定义在@Configuration或@Component注解修饰的类中。 声明一个 bean 此外，@Configuration 类允许在同一个类中通过@Bean 定义内部 bean 依赖。 声明一个 bean，只需要在 bean 属性的 set 方法上标注@Bean 即可。 @Configurationpublic class AnnotationConfiguration &#123; private static final Logger log = LoggerFactory.getLogger(JavaComponentScan.class); @Bean public Job getPolice() &#123; return new Police(); &#125; public static void main(String[] args) &#123; AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(AnnotationConfiguration.class); ctx.scan("org.zp.notes.spring.beans"); ctx.refresh(); Job job = (Job) ctx.getBean("police"); log.debug("job: &#123;&#125;, work: &#123;&#125;", job.getClass(), job.work()); &#125;&#125;public interface Job &#123; String work();&#125;@Component("police")public class Police implements Job &#123; @Override public String work() &#123; return "抓罪犯"; &#125;&#125; 这等价于配置 &lt;beans&gt; &lt;bean id="police" class="org.zp.notes.spring.ioc.sample.job.Police"/&gt;&lt;/beans&gt; @Bean 注解用来表明一个方法实例化、配置合初始化一个被 Spring IoC 容器管理的新对象。 如果你熟悉 Spring 的 xml 配置，你可以将@Bean 视为等价于&lt;beans&gt;标签。 @Bean 注解可以用于任何的 Spring @Component bean，然而，通常被用于@Configuration bean。 @Configuration @Configuration 是一个类级别的注解，用来标记被修饰类的对象是一个BeanDefinition。 @Configuration 声明 bean 是通过被 @Bean 修饰的公共方法。此外，@Configuration 允许在同一个类中通过 @Bean 定义内部 bean 依赖。 @Configurationpublic class AppConfig &#123; @Bean public MyService myService() &#123; return new MyServiceImpl(); &#125;&#125; 这等价于配置 &lt;beans&gt; &lt;bean id="myService" class="com.acme.services.MyServiceImpl"/&gt;&lt;/beans&gt; 用 AnnotationConfigApplicationContext 实例化 IoC 容器。 参考资料 Spring 官方文档之 The IoC Container]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet 指南]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fjavaee%2Fservlet%2F</url>
    <content type="text"><![CDATA[Servlet 指南 简介 Servlet 是什么？ Servlet 架构 Servlet 任务 Servlet 包 init() 方法 service() 方法 doGet() 方法 doPost() 方法 destroy() 方法 架构图 Servlet 和 HTTP 状态码 HTTP 状态码 设置 HTTP 状态码的方法 HTTP 状态码实例 简介 Servlet 是什么？ Java Servlet 是运行在 Web 服务器或应用服务器上的程序，它是作为来自 Web 浏览器或其他 HTTP 客户端的请求和 HTTP 服务器上的数据库或应用程序之间的中间层。 使用 Servlet，您可以收集来自网页表单的用户输入，呈现来自数据库或者其他源的记录，还可以动态创建网页。 Java Servlet 通常情况下与使用 CGI（Common Gateway Interface，公共网关接口）实现的程序可以达到异曲同工的效果。但是相比于 CGI，Servlet 有以下几点优势： 性能明显更好。 Servlet 在 Web 服务器的地址空间内执行。这样它就没有必要再创建一个单独的进程来处理每个客户端请求。 Servlet 是独立于平台的，因为它们是用 Java 编写的。 服务器上的 Java 安全管理器执行了一系列限制，以保护服务器计算机上的资源。因此，Servlet 是可信的。 Java 类库的全部功能对 Servlet 来说都是可用的。它可以通过 sockets 和 RMI 机制与 applets、数据库或其他软件进行交互。 Servlet 架构 下图显示了 Servlet 在 Web 应用程序中的位置。 Servlet 任务 Servlet 执行以下主要任务： 读取客户端（浏览器）发送的显式的数据。这包括网页上的 HTML 表单，或者也可以是来自 applet 或自定义的 HTTP 客户端程序的表单。 读取客户端（浏览器）发送的隐式的 HTTP 请求数据。这包括 cookies、媒体类型和浏览器能理解的压缩格式等等。 处理数据并生成结果。这个过程可能需要访问数据库，执行 RMI 或 CORBA 调用，调用 Web 服务，或者直接计算得出对应的响应。 发送显式的数据（即文档）到客户端（浏览器）。该文档的格式可以是多种多样的，包括文本文件（HTML 或 XML）、二进制文件（GIF 图像）、Excel 等。 发送隐式的 HTTP 响应到客户端（浏览器）。这包括告诉浏览器或其他客户端被返回的文档类型（例如 HTML），设置 cookies 和缓存参数，以及其他类似的任务。 Servlet 包 Java Servlet 是运行在带有支持 Java Servlet 规范的解释器的 web 服务器上的 Java 类。 Servlet 可以使用 javax.servlet 和 javax.servlet.http 包创建，它是 Java 企业版的标准组成部分，Java 企业版是支持大型开发项目的 Java 类库的扩展版本。 这些类实现 Java Servlet 和 JSP 规范。在写本教程的时候，二者相应的版本分别是 Java Servlet 2.5 和 JSP 2.1。 Java Servlet 就像任何其他的 Java 类一样已经被创建和编译。在您安装 Servlet 包并把它们添加到您的计算机上的 Classpath 类路径中之后，您就可以通过 JDK 的 Java 编译器或任何其他编译器来编译 Servlet。 Servlet 生命周期 Servlet 生命周期可被定义为从创建直到毁灭的整个过程。以下是 Servlet 遵循的过程： Servlet 通过调用 init () 方法进行初始化。 Servlet 调用 service() 方法来处理客户端的请求。 Servlet 通过调用 destroy() 方法终止（结束）。 最后，Servlet 是由 JVM 的垃圾回收器进行垃圾回收的。 现在让我们详细讨论生命周期的方法。 init() 方法 init 方法被设计成只调用一次。它在第一次创建 Servlet 时被调用，在后续每次用户请求时不再调用。因此，它是用于一次性初始化，就像 Applet 的 init 方法一样。 Servlet 创建于用户第一次调用对应于该 Servlet 的 URL 时，但是您也可以指定 Servlet 在服务器第一次启动时被加载。 当用户调用一个 Servlet 时，就会创建一个 Servlet 实例，每一个用户请求都会产生一个新的线程，适当的时候移交给 doGet 或 doPost 方法。init() 方法简单地创建或加载一些数据，这些数据将被用于 Servlet 的整个生命周期。 init 方法的定义如下： public void init() throws ServletException &#123; // 初始化代码...&#125; service() 方法 service() 方法是执行实际任务的主要方法。Servlet 容器（即 Web 服务器）调用 service() 方法来处理来自客户端（浏览器）的请求，并把格式化的响应写回给客户端。 每次服务器接收到一个 Servlet 请求时，服务器会产生一个新的线程并调用服务。service() 方法检查 HTTP 请求类型（GET、POST、PUT、DELETE 等），并在适当的时候调用 doGet、doPost、doPut，doDelete 等方法。 下面是该方法的特征： public void service(ServletRequest request, ServletResponse response) throws ServletException, IOException&#123;&#125; service() 方法由容器调用，service 方法在适当的时候调用 doGet、doPost、doPut、doDelete 等方法。所以，您不用对 service() 方法做任何动作，您只需要根据来自客户端的请求类型来重写 doGet() 或 doPost() 即可。 doGet() 和 doPost() 方法是每次服务请求中最常用的方法。下面是这两种方法的特征。 doGet() 方法 GET 请求来自于一个 URL 的正常请求，或者来自于一个未指定 METHOD 的 HTML 表单，它由 doGet() 方法处理。 public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; // Servlet 代码&#125; doPost() 方法 POST 请求来自于一个特别指定了 METHOD 为 POST 的 HTML 表单，它由 doPost() 方法处理。 public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; // Servlet 代码&#125; destroy() 方法 destroy() 方法只会被调用一次，在 Servlet 生命周期结束时被调用。destroy() 方法可以让您的 Servlet 关闭数据库连接、停止后台线程、把 Cookie 列表或点击计数器写入到磁盘，并执行其他类似的清理活动。 在调用 destroy() 方法之后，servlet 对象被标记为垃圾回收。destroy 方法定义如下所示： public void destroy() &#123; // 终止化代码...&#125; 架构图 下图显示了一个典型的 Servlet 生命周期方案。 第一个到达服务器的 HTTP 请求被委派到 Servlet 容器。 Servlet 容器在调用 service() 方法之前加载 Servlet。 然后 Servlet 容器处理由多个线程产生的多个请求，每个线程执行一个单一的 Servlet 实例的 service() 方法。 Servlet 和 HTTP 状态码 title: JavaEE Servlet HTTP 状态码 date: 2017-11-08 categories: javaee tags: javaee servlet http HTTP 状态码 HTTP 请求和 HTTP 响应消息的格式是类似的，结构如下： 初始状态行 + 回车换行符（回车+换行） 零个或多个标题行+回车换行符 一个空白行，即回车换行符 一个可选的消息主体，比如文件、查询数据或查询输出 例如，服务器的响应头如下所示： HTTP/1.1 200 OKContent-Type: text/htmlHeader2: ......HeaderN: ... (Blank Line)&lt;!doctype ...&gt;&lt;html&gt;&lt;head&gt;...&lt;/head&gt;&lt;body&gt;...&lt;/body&gt;&lt;/html&gt; 状态行包括 HTTP 版本（在本例中为 HTTP/1.1）、一个状态码（在本例中为 200）和一个对应于状态码的短消息（在本例中为 OK）。 以下是可能从 Web 服务器返回的 HTTP 状态码和相关的信息列表： 代码 消息 描述 100 Continue 只有请求的一部分已经被服务器接收，但只要它没有被拒绝，客户端应继续该请求。 101 Switching Protocols 服务器切换协议。 200 OK 请求成功。 201 Created 该请求是完整的，并创建一个新的资源。 202 Accepted 该请求被接受处理，但是该处理是不完整的。 203 Non-authoritative Information 204 No Content 205 Reset Content 206 Partial Content 300 Multiple Choices 链接列表。用户可以选择一个链接，进入到该位置。最多五个地址。 301 Moved Permanently 所请求的页面已经转移到一个新的 URL。 302 Found 所请求的页面已经临时转移到一个新的 URL。 303 See Other 所请求的页面可以在另一个不同的 URL 下被找到。 304 Not Modified 305 Use Proxy 306 Unused 在以前的版本中使用该代码。现在已不再使用它，但代码仍被保留。 307 Temporary Redirect 所请求的页面已经临时转移到一个新的 URL。 400 Bad Request 服务器不理解请求。 401 Unauthorized 所请求的页面需要用户名和密码。 402 Payment Required 您还不能使用该代码。 403 Forbidden 禁止访问所请求的页面。 404 Not Found 服务器无法找到所请求的页面。. 405 Method Not Allowed 在请求中指定的方法是不允许的。 406 Not Acceptable 服务器只生成一个不被客户端接受的响应。 407 Proxy Authentication Required 在请求送达之前，您必须使用代理服务器的验证。 408 Request Timeout 请求需要的时间比服务器能够等待的时间长，超时。 409 Conflict 请求因为冲突无法完成。 410 Gone 所请求的页面不再可用。 411 Length Required “Content-Length” 未定义。服务器无法处理客户端发送的不带 Content-Length 的请求信息。 412 Precondition Failed 请求中给出的先决条件被服务器评估为 false。 413 Request Entity Too Large 服务器不接受该请求，因为请求实体过大。 414 Request-url Too Long 服务器不接受该请求，因为 URL 太长。当您转换一个 “post” 请求为一个带有长的查询信息的 “get” 请求时发生。 415 Unsupported Media Type 服务器不接受该请求，因为媒体类型不被支持。 417 Expectation Failed 500 Internal Server Error 未完成的请求。服务器遇到了一个意外的情况。 501 Not Implemented 未完成的请求。服务器不支持所需的功能。 502 Bad Gateway 未完成的请求。服务器从上游服务器收到无效响应。 503 Service Unavailable 未完成的请求。服务器暂时超载或死机。 504 Gateway Timeout 网关超时。 505 HTTP Version Not Supported 服务器不支持&quot;HTTP 协议&quot;版本。 设置 HTTP 状态码的方法 下面的方法可用于在 Servlet 程序中设置 HTTP 状态码。这些方法通过 HttpServletResponse 对象可用。 序号 方法 &amp; 描述 1 **public void setStatus ( int statusCode )**该方法设置一个任意的状态码。setStatus 方法接受一个 int（状态码）作为参数。如果您的反应包含了一个特殊的状态码和文档，请确保在使用 PrintWriter 实际返回任何内容之前调用 setStatus。 2 **public void sendRedirect(String url)**该方法生成一个 302 响应，连同一个带有新文档 URL 的 Location 头。 3 **public void sendError(int code, String message)**该方法发送一个状态码（通常为 404），连同一个在 HTML 文档内部自动格式化并发送到客户端的短消息。 HTTP 状态码实例 下面的例子把 407 错误代码发送到客户端浏览器，浏览器会显示 “Need authentication!!!” 消息。 // 导入必需的 java 库import java.io.*;import javax.servlet.*;import javax.servlet.http.*;import java.util.*;// 扩展 HttpServlet 类public class showError extends HttpServlet &#123; // 处理 GET 方法请求的方法 public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; // 设置错误代码和原因 response.sendError(407, "Need authentication!!!" ); &#125; // 处理 POST 方法请求的方法 public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request, response); &#125;&#125; 现在，调用上面的 Servlet 将显示以下结果： HTTP Status 407 - Need authentication!!!type Status reportmessage Need authentication!!!description The client must first authenticate itself with the proxy (Need authentication!!!).Apache Tomcat/5.5.29]]></content>
      <categories>
        <category>java</category>
        <category>javaweb</category>
        <category>javaee</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaweb</tag>
        <tag>javaee</tag>
        <tag>servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cookie 和 Session]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fjavaee%2Fcookie-and-sesion%2F</url>
    <content type="text"><![CDATA[Cookie 和 Session Cookie Cookie 是什么？ Cookie 剖析 Cookie 类中的方法 Cookie 的有效期 Cookie 的域名 Cookie 的路径 Cookie 的安全属性 实例 Session Session 是什么？ Session 类中的方法 Session 的有效期 Session 对浏览器的要求 URL 地址重写 Session 中禁用 Cookie 实例 Cookie vs Session 存取方式 隐私安全 有效期 服务器的开销 浏览器的支持 跨域名 Cookie 由于 Http 是一种无状态的协议，服务器单从网络连接上无从知道客户身份。 会话跟踪是 Web 程序中常用的技术，用来跟踪用户的整个会话。常用会话跟踪技术是 Cookie 与 Session。 Cookie 是什么？ Cookie 实际上是存储在客户端上的文本信息，并保留了各种跟踪的信息。 Cookie 工作步骤： 客户端请求服务器，如果服务器需要记录该用户的状态，就是用 response 向客户端浏览器颁发一个 Cookie。 客户端浏览器会把 Cookie 保存下来。 当浏览器再请求该网站时，浏览器把该请求的网址连同 Cookie 一同提交给服务器。服务器检查该 Cookie，以此来辨认用户状态。 注：Cookie 功能需要浏览器的支持，如果浏览器不支持 Cookie 或者 Cookie 禁用了，Cookie 功能就会失效。 Java 中把 Cookie 封装成了javax.servlet.http.Cookie类。 Cookie 剖析 Cookies 通常设置在 HTTP 头信息中（虽然 JavaScript 也可以直接在浏览器上设置一个 Cookie）。 设置 Cookie 的 Servlet 会发送如下的头信息： HTTP/1.1 200 OKDate: Fri, 04 Feb 2000 21:03:38 GMTServer: Apache/1.3.9 (UNIX) PHP/4.0b3Set-Cookie: name=xyz; expires=Friday, 04-Feb-07 22:03:38 GMT; path=/; domain=w3cschool.ccConnection: closeContent-Type: text/html 正如您所看到的，Set-Cookie 头包含了一个名称值对、一个 GMT 日期、一个路径和一个域。名称和值会被 URL 编码。expires 字段是一个指令，告诉浏览器在给定的时间和日期之后&quot;忘记&quot;该 Cookie。 如果浏览器被配置为存储 Cookies，它将会保留此信息直到到期日期。如果用户的浏览器指向任何匹配该 Cookie 的路径和域的页面，它会重新发送 Cookie 到服务器。浏览器的头信息可能如下所示： GET / HTTP/1.0Connection: Keep-AliveUser-Agent: Mozilla/4.6 (X11; I; Linux 2.2.6-15apmac ppc)Host: zink.demon.co.uk:1126Accept: image/gif, */*Accept-Encoding: gzipAccept-Language: enAccept-Charset: iso-8859-1,*,utf-8Cookie: name=xyz Cookie 类中的方法 方法 功能 public void setDomain(String pattern) 该方法设置 cookie 适用的域。 public String getDomain() 该方法获取 cookie 适用的域。 public void setMaxAge(int expiry) 该方法设置 cookie 过期的时间（以秒为单位）。如果不这样设置，cookie 只会在当前 session 会话中持续有效。 public int getMaxAge() 该方法返回 cookie 的最大生存周期（以秒为单位），默认情况下，-1 表示 cookie 将持续下去，直到浏览器关闭。 public String getName() 该方法返回 cookie 的名称。名称在创建后不能改变。 public void setValue(String newValue) 该方法设置与 cookie 关联的值。 public String getValue() 该方法获取与 cookie 关联的值。 public void setPath(String uri) 该方法设置 cookie 适用的路径。如果您不指定路径，与当前页面相同目录下的（包括子目录下的）所有 URL 都会返回 cookie。 public String getPath() 该方法获取 cookie 适用的路径。 public void setSecure(boolean flag) 该方法设置布尔值，向浏览器指示，只会在 HTTPS 和 SSL 等安全协议中传输此类 Cookie。 public void setComment(String purpose) 该方法规定了描述 cookie 目的的注释。该注释在浏览器向用户呈现 cookie 时非常有用。 public String getComment() 该方法返回了描述 cookie 目的的注释，如果 cookie 没有注释则返回 null。 Cookie 的有效期 Cookie的maxAge决定着 Cookie 的有效期，单位为秒。 如果 maxAge 为 0，则表示删除该 Cookie； 如果为负数，表示该 Cookie 仅在本浏览器中以及本窗口打开的子窗口内有效，关闭窗口后该 Cookie 即失效。 Cookie 中提供getMaxAge()和setMaxAge(int expiry)方法来读写maxAge属性。 Cookie 的域名 Cookie 是不可以跨域名的。域名 www.google.com 颁发的 Cookie 不会被提交到域名 www.baidu.com 去。这是由 Cookie 的隐私安全机制决定的。隐私安全机制能够禁止网站非法获取其他网站的 Cookie。 正常情况下，同一个一级域名的两个二级域名之间也不能互相使用 Cookie。如果想让某域名下的子域名也可以使用该 Cookie，需要设置 Cookie 的 domain 参数。 Java 中使用setDomain(Stringdomain)和getDomain()方法来设置、获取 domain。 Cookie 的路径 Path 属性决定允许访问 Cookie 的路径。 Java 中使用setPath(Stringuri)和getPath()方法来设置、获取 path。 Cookie 的安全属性 HTTP 协议不仅是无状态的，而且是不安全的。 使用 HTTP 协议的数据不经过任何加密就直接在网络上传播，有被截获的可能。如果不希望 Cookie 在 HTTP 等非安全协议中传输，可以设置 Cookie 的 secure 属性为 true。浏览器只会在 HTTPS 和 SSL 等安全协议中传输此类 Cookie。 Java 中使用setSecure(booleanflag)和getSecure ()方法来设置、获取 Secure。 实例 添加 Cookie 通过 Servlet 添加 Cookies 包括三个步骤： 创建一个 Cookie 对象：您可以调用带有 cookie 名称和 cookie 值的 Cookie 构造函数，cookie 名称和 cookie 值都是字符串。 设置最大生存周期：您可以使用 setMaxAge 方法来指定 cookie 能够保持有效的时间（以秒为单位）。 发送 Cookie 到 HTTP 响应头：您可以使用 response.addCookie 来添加 HTTP 响应头中的 Cookies。 AddCookies.java import java.io.IOException;import java.io.PrintWriter;import java.net.URLEncoder;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;@WebServlet("/servlet/AddCookies")public class AddCookies extends HttpServlet &#123; private static final long serialVersionUID = 1L; /** * @see HttpServlet#HttpServlet() */ public AddCookies() &#123; super(); &#125; /** * @see HttpServlet#doGet(HttpServletRequest request, HttpServletResponse response) */ public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; // 为名字和姓氏创建 Cookie Cookie name = new Cookie("name", URLEncoder.encode(request.getParameter("name"), "UTF-8")); // 中文转码 Cookie url = new Cookie("url", request.getParameter("url")); // 为两个 Cookie 设置过期日期为 24 小时后 name.setMaxAge(60 * 60 * 24); url.setMaxAge(60 * 60 * 24); // 在响应头中添加两个 Cookie response.addCookie(name); response.addCookie(url); // 设置响应内容类型 response.setContentType("text/html;charset=UTF-8"); PrintWriter out = response.getWriter(); String title = "设置 Cookie 实例"; String docType = "&lt;!DOCTYPE html&gt;\n"; out.println(docType + "&lt;html&gt;\n" + "&lt;head&gt;&lt;title&gt;" + title + "&lt;/title&gt;&lt;/head&gt;\n" + "&lt;body bgcolor=\"#f0f0f0\"&gt;\n" + "&lt;h1 align=\"center\"&gt;" + title + "&lt;/h1&gt;\n" + "&lt;ul&gt;\n" + " &lt;li&gt;&lt;b&gt;站点名：&lt;/b&gt;：" + request.getParameter("name") + "\n&lt;/li&gt;" + " &lt;li&gt;&lt;b&gt;站点 URL：&lt;/b&gt;：" + request.getParameter("url") + "\n&lt;/li&gt;" + "&lt;/ul&gt;\n" + "&lt;/body&gt;&lt;/html&gt;"); &#125; /** * @see HttpServlet#doPost(HttpServletRequest request, HttpServletResponse response) */ protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request, response); &#125;&#125; addCookies.jsp &lt;%@ page language="java" pageEncoding="UTF-8" %&gt;&lt;!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;添加Cookie&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form action=/servlet/AddCookies method="GET"&gt; 站点名 ：&lt;input type="text" name="name"&gt; &lt;br/&gt; 站点 URL：&lt;input type="text" name="url"/&gt;&lt;br&gt; &lt;input type="submit" value="提交"/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 显示 Cookie 要读取 Cookies，您需要通过调用 HttpServletRequest 的 getCookies() 方法创建一个 javax.servlet.http.Cookie 对象的数组。然后循环遍历数组，并使用 getName() 和 getValue() 方法来访问每个 cookie 和关联的值。 ReadCookies.java import java.io.IOException;import java.io.PrintWriter;import java.net.URLDecoder;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;@WebServlet("/servlet/ReadCookies")public class ReadCookies extends HttpServlet &#123; private static final long serialVersionUID = 1L; /** * @see HttpServlet#HttpServlet() */ public ReadCookies() &#123; super(); &#125; /** * @see HttpServlet#doGet(HttpServletRequest request, HttpServletResponse response) */ public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; Cookie cookie = null; Cookie[] cookies = null; // 获取与该域相关的 Cookie 的数组 cookies = request.getCookies(); // 设置响应内容类型 response.setContentType("text/html;charset=UTF-8"); PrintWriter out = response.getWriter(); String title = "Delete Cookie Example"; String docType = "&lt;!DOCTYPE html&gt;\n"; out.println(docType + "&lt;html&gt;\n" + "&lt;head&gt;&lt;title&gt;" + title + "&lt;/title&gt;&lt;/head&gt;\n" + "&lt;body bgcolor=\"#f0f0f0\"&gt;\n"); if (cookies != null) &#123; out.println("&lt;h2&gt;Cookie 名称和值&lt;/h2&gt;"); for (int i = 0; i &lt; cookies.length; i++) &#123; cookie = cookies[i]; if ((cookie.getName()).compareTo("name") == 0) &#123; cookie.setMaxAge(0); response.addCookie(cookie); out.print("已删除的 cookie：" + cookie.getName() + "&lt;br/&gt;"); &#125; out.print("名称：" + cookie.getName() + "，"); out.print("值：" + URLDecoder.decode(cookie.getValue(), "utf-8") + " &lt;br/&gt;"); &#125; &#125; else &#123; out.println("&lt;h2 class=\"tutheader\"&gt;No Cookie founds&lt;/h2&gt;"); &#125; out.println("&lt;/body&gt;"); out.println("&lt;/html&gt;"); &#125; /** * @see HttpServlet#doPost(HttpServletRequest request, HttpServletResponse response) */ protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request, response); &#125;&#125; 删除 Cookie Java 中并没有提供直接删除 Cookie 的方法，如果想要删除一个 Cookie，直接将这个 Cookie 的有效期设为 0 就可以了。步骤如下： 读取一个现有的 cookie，并把它存储在 Cookie 对象中。 使用 setMaxAge() 方法设置 cookie 的年龄为零，来删除现有的 cookie。 把这个 cookie 添加到响应头。 DeleteCookies.java import java.io.IOException;import java.io.PrintWriter;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;@WebServlet("/servlet/DeleteCookies")public class DeleteCookies extends HttpServlet &#123; private static final long serialVersionUID = 1L; /** * @see HttpServlet#HttpServlet() */ public DeleteCookies() &#123; super(); &#125; /** * @see HttpServlet#doGet(HttpServletRequest request, HttpServletResponse response) */ public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; Cookie cookie = null; Cookie[] cookies = null; // 获取与该域相关的 Cookie 的数组 cookies = request.getCookies(); // 设置响应内容类型 response.setContentType("text/html;charset=UTF-8"); PrintWriter out = response.getWriter(); String title = "删除 Cookie 实例"; String docType = "&lt;!DOCTYPE html&gt;\n"; out.println(docType + "&lt;html&gt;\n" + "&lt;head&gt;&lt;title&gt;" + title + "&lt;/title&gt;&lt;/head&gt;\n" + "&lt;body bgcolor=\"#f0f0f0\"&gt;\n"); if (cookies != null) &#123; out.println("&lt;h2&gt;Cookie 名称和值&lt;/h2&gt;"); for (int i = 0; i &lt; cookies.length; i++) &#123; cookie = cookies[i]; if ((cookie.getName()).compareTo("url") == 0) &#123; cookie.setMaxAge(0); response.addCookie(cookie); out.print("已删除的 cookie：" + cookie.getName() + "&lt;br/&gt;"); &#125; out.print("名称：" + cookie.getName() + "，"); out.print("值：" + cookie.getValue() + " &lt;br/&gt;"); &#125; &#125; else &#123; out.println("&lt;h2 class=\"tutheader\"&gt;No Cookie founds&lt;/h2&gt;"); &#125; out.println("&lt;/body&gt;"); out.println("&lt;/html&gt;"); &#125; /** * @see HttpServlet#doPost(HttpServletRequest request, HttpServletResponse response) */ protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; doGet(request, response); &#125;&#125; Session Session 是什么？ 不同于 Cookie 保存在客户端浏览器中，Session 保存在服务器上。 如果说 Cookie 机制是通过检查客户身上的“通行证”来确定客户身份的话，那么 Session 机制就是通过检查服务器上的“客户明细表”来确认客户身份。 Session 对应的类为 javax.servlet.http.HttpSession 类。Session 对象是在客户第一次请求服务器时创建的。 Session 类中的方法 javax.servlet.http.HttpSession 类中的方法： 方法 功能 public Object getAttribute(String name) 该方法返回在该 session 会话中具有指定名称的对象，如果没有指定名称的对象，则返回 null。 public Enumeration getAttributeNames() 该方法返回 String 对象的枚举，String 对象包含所有绑定到该 session 会话的对象的名称。 public long getCreationTime() 该方法返回该 session 会话被创建的时间，自格林尼治标准时间 1970 年 1 月 1 日午夜算起，以毫秒为单位。 public String getId() 该方法返回一个包含分配给该 session 会话的唯一标识符的字符串。 public long getLastAccessedTime() 该方法返回客户端最后一次发送与该 session 会话相关的请求的时间自格林尼治标准时间 1970 年 1 月 1 日午夜算起，以毫秒为单位。 public int getMaxInactiveInterval() 该方法返回 Servlet 容器在客户端访问时保持 session 会话打开的最大时间间隔，以秒为单位。 public void invalidate() 该方法指示该 session 会话无效，并解除绑定到它上面的任何对象。 public boolean isNew() 如果客户端还不知道该 session 会话，或者如果客户选择不参入该 session 会话，则该方法返回 true。 public void removeAttribute(String name) 该方法将从该 session 会话移除指定名称的对象。 public void setAttribute(String name, Object value) 该方法使用指定的名称绑定一个对象到该 session 会话。 public void setMaxInactiveInterval(int interval) 该方法在 Servlet 容器指示该 session 会话无效之前，指定客户端请求之间的时间，以秒为单位。 Session 的有效期 由于会有越来越多的用户访问服务器，因此 Session 也会越来越多。为防止内存溢出，服务器会把长时间没有活跃的 Session 从内存中删除。 Session 的超时时间为maxInactiveInterval属性，可以通过getMaxInactiveInterval()、setMaxInactiveInterval(longinterval)来读写这个属性。 Tomcat 中 Session 的默认超时时间为 20 分钟。可以修改 web.xml 改变 Session 的默认超时时间。 例： &lt;session-config&gt; &lt;session-timeout&gt;60&lt;/session-timeout&gt;&lt;/session-config&gt; Session 对浏览器的要求 HTTP 协议是无状态的，Session 不能依据 HTTP 连接来判断是否为同一客户。因此服务器向客户端浏览器发送一个名为 JESSIONID 的 Cookie，他的值为该 Session 的 id（也就是 HttpSession.getId()的返回值）。Session 依据该 Cookie 来识别是否为同一用户。 该 Cookie 为服务器自动生成的，它的maxAge属性一般为-1，表示仅当前浏览器内有效，并且各浏览器窗口间不共享，关闭浏览器就会失效。 URL 地址重写 URL 地址重写的原理是将该用户 Session 的 id 信息重写到 URL 地址中。服务器能够解析重写后的 URL 获取 Session 的 id。这样即使客户端不支持 Cookie，也可以使用 Session 来记录用户状态。 HttpServletResponse类提供了encodeURL(Stringurl)实现 URL 地址重写。 Session 中禁用 Cookie 在META-INF/context.xml中编辑如下： &lt;Context path="/SessionNotes" cookies="true"&gt;&lt;/Context&gt; 部署后，TOMCAT 便不会自动生成名 JESSIONID 的 Cookie，Session 也不会以 Cookie 为识别标志，而仅仅以重写后的 URL 地址为识别标志了。 实例 Session 跟踪 SessionTrackServlet.java import java.io.IOException;import java.io.PrintWriter;import java.text.SimpleDateFormat;import java.util.Date;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpSession;@WebServlet("/servlet/SessionTrackServlet")public class SessionTrackServlet extends HttpServlet &#123; private static final long serialVersionUID = 1L; public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; // 如果不存在 session 会话，则创建一个 session 对象 HttpSession session = request.getSession(true); // 获取 session 创建时间 Date createTime = new Date(session.getCreationTime()); // 获取该网页的最后一次访问时间 Date lastAccessTime = new Date(session.getLastAccessedTime()); // 设置日期输出的格式 SimpleDateFormat df = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); String title = "Servlet Session 实例"; Integer visitCount = new Integer(0); String visitCountKey = new String("visitCount"); String userIDKey = new String("userID"); String userID = new String("admin"); // 检查网页上是否有新的访问者 if (session.isNew()) &#123; session.setAttribute(userIDKey, userID); &#125; else &#123; visitCount = (Integer) session.getAttribute(visitCountKey); visitCount = visitCount + 1; userID = (String) session.getAttribute(userIDKey); &#125; session.setAttribute(visitCountKey, visitCount); // 设置响应内容类型 response.setContentType("text/html;charset=UTF-8"); PrintWriter out = response.getWriter(); String docType = "&lt;!DOCTYPE html&gt;\n"; out.println(docType + "&lt;html&gt;\n" + "&lt;head&gt;&lt;title&gt;" + title + "&lt;/title&gt;&lt;/head&gt;\n" + "&lt;body bgcolor=\"#f0f0f0\"&gt;\n" + "&lt;h1 align=\"center\"&gt;" + title + "&lt;/h1&gt;\n" + "&lt;h2 align=\"center\"&gt;Session 信息&lt;/h2&gt;\n" + "&lt;table border=\"1\" align=\"center\"&gt;\n" + "&lt;tr bgcolor=\"#949494\"&gt;\n" + " &lt;th&gt;Session 信息&lt;/th&gt;&lt;th&gt;值&lt;/th&gt;&lt;/tr&gt;\n" + "&lt;tr&gt;\n" + " &lt;td&gt;id&lt;/td&gt;\n" + " &lt;td&gt;" + session.getId() + "&lt;/td&gt;&lt;/tr&gt;\n" + "&lt;tr&gt;\n" + " &lt;td&gt;创建时间&lt;/td&gt;\n" + " &lt;td&gt;" + df.format(createTime) + " &lt;/td&gt;&lt;/tr&gt;\n" + "&lt;tr&gt;\n" + " &lt;td&gt;最后访问时间&lt;/td&gt;\n" + " &lt;td&gt;" + df.format(lastAccessTime) + " &lt;/td&gt;&lt;/tr&gt;\n" + "&lt;tr&gt;\n" + " &lt;td&gt;用户 ID&lt;/td&gt;\n" + " &lt;td&gt;" + userID + " &lt;/td&gt;&lt;/tr&gt;\n" + "&lt;tr&gt;\n" + " &lt;td&gt;访问统计：&lt;/td&gt;\n" + " &lt;td&gt;" + visitCount + "&lt;/td&gt;&lt;/tr&gt;\n" + "&lt;/table&gt;\n" + "&lt;/body&gt;&lt;/html&gt;"); &#125;&#125; web.xml &lt;servlet&gt; &lt;servlet-name&gt;SessionTrackServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;SessionTrackServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;SessionTrackServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlet/SessionTrackServlet&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 删除 Session 会话数据 当您完成了一个用户的 session 会话数据，您有以下几种选择： **移除一个特定的属性：**您可以调用 removeAttribute(String name) 方法来删除与特定的键相关联的值。 **删除整个 session 会话：**您可以调用 invalidate() 方法来丢弃整个 session 会话。 **设置 session 会话过期时间：**您可以调用 setMaxInactiveInterval(int interval) 方法来单独设置 session 会话超时。 **注销用户：**如果使用的是支持 servlet 2.4 的服务器，您可以调用 logout 来注销 Web 服务器的客户端，并把属于所有用户的所有 session 会话设置为无效。 **web.xml 配置：**如果您使用的是 Tomcat，除了上述方法，您还可以在 web.xml 文件中配置 session 会话超时，如下所示： &lt;session-config&gt; &lt;session-timeout&gt;15&lt;/session-timeout&gt;&lt;/session-config&gt; 上面实例中的超时时间是以分钟为单位，将覆盖 Tomcat 中默认的 30 分钟超时时间。 在一个 Servlet 中的 getMaxInactiveInterval() 方法会返回 session 会话的超时时间，以秒为单位。所以，如果在 web.xml 中配置 session 会话超时时间为 15 分钟，那么getMaxInactiveInterval() 会返回 900。 Cookie vs Session 存取方式 Cookie 只能保存ASCII字符串，如果需要存取 Unicode 字符或二进制数据，需要进行UTF-8、GBK或BASE64等方式的编码。 Session 可以存取任何类型的数据，甚至是任何 Java 类。可以将 Session 看成是一个 Java 容器类。 隐私安全 Cookie 存于客户端浏览器，一些客户端的程序可能会窥探、复制或修改 Cookie 内容。 Session 存于服务器，对客户端是透明的，不存在敏感信息泄露的危险。 有效期 使用 Cookie 可以保证长时间登录有效，只要设置 Cookie 的maxAge属性为一个很大的数字。 而 Session 虽然理论上也可以通过设置很大的数值来保持长时间登录有效，但是，由于 Session 依赖于名为JESSIONID的 Cookie，而 Cookie JESSIONID的maxAge默认为-1，只要关闭了浏览器该 Session 就会失效，因此，Session 不能实现信息永久有效的效果。使用 URL 地址重写也不能实现。 服务器的开销 由于 Session 是保存在服务器的，每个用户都会产生一个 Session，如果并发访问的用户非常多，会产生很多的 Session，消耗大量的内存。 而 Cookie 由于保存在客户端浏览器上，所以不占用服务器资源。 浏览器的支持 Cookie 需要浏览器支持才能使用。 如果浏览器不支持 Cookie，需要使用 Session 以及 URL 地址重写。 需要注意的事所有的用到 Session 程序的 URL 都要使用response.encodeURL(StringURL) 或response.encodeRediretURL(String URL)进行 URL 地址重写，否则导致 Session 会话跟踪失效。 跨域名 Cookie 支持跨域名。 Session 不支持跨域名。]]></content>
      <categories>
        <category>java</category>
        <category>javaweb</category>
        <category>javaee</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaweb</tag>
        <tag>javaee</tag>
        <tag>cookie</tag>
        <tag>session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 概述]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fspring%2Fintroduction%2Fspring-overview%2F</url>
    <content type="text"><![CDATA[Spring 是最受欢迎的企业级 Java 应用程序开发框架。 Spring 框架是一个开源的 Java 平台。 当谈论到大小和透明度时， Spring 是轻量级的。 Spring 框架的基础版本是在 2 MB 左右的。 Spring 框架的核心特性可以用于开发任何 Java 应用程序，但是在 Java EE 平台上构建 web 应用程序是需要扩展的。 Spring 框架的目标是使 J2EE 开发变得更容易使用，通过启用基于 POJO 编程模型来促进良好的编程实践。 为什么使用 Spring 下面列出的是使用 Spring 框架主要的好处： Spring 可以使开发人员使用 POJOs 开发企业级的应用程序。只使用 POJOs 的好处是你不需要一个 EJB 容器产品，比如一个应用程序服务器，但是你可以选择使用一个健壮的 servlet 容器，比如 Tomcat 或者一些商业产品。 Spring 在一个单元模式中是有组织的。即使包和类的数量非常大，你只需要选择你需要的部分，而忽略剩余的那部分。 Spring 不会让你白费力气做重复工作，它真正的利用了一些现有的技术，像几个 ORM 框架、日志框架、JEE、Quartz 和 JDK 计时器，其他视图技术。 测试一个用 Spring 编写的应用程序很容易，因为 environment-dependent 代码被放进了这个框架中。此外，通过使用 JavaBean-style POJOs，它在使用依赖注入注入测试数据时变得更容易。 Spring 的 web 框架是一个设计良好的 web MVC 框架，它为 web 框架，比如 Structs 或者其他工程上的或者很少受欢迎的 web 框架，提供了一个很好的供替代的选择。 为将特定技术的异常（例如，由 JDBC、Hibernate，或者 JDO 抛出的异常）翻译成一致的， Spring 提供了一个方便的 API，而这些都是未经检验的异常。 轻量级的 IOC 容器往往是轻量级的，例如，特别是当与 EJB 容器相比的时候。这有利于在内存和 CPU 资源有限的计算机上开发和部署应用程序。 Spring 提供了一个一致的事务管理界面，该界面可以缩小成一个本地事务（例如，使用一个单一的数据库）和扩展成一个全局事务（例如，使用 JTA）。 核心思想 Spring最核心的两个技术思想是：IoC 和 Aop IoC IoC 即 Inversion of Control ，意为控制反转。 Spring 最认同的技术是控制反转的**依赖注入（DI）**模式。控制反转（IoC）是一个通用的概念，它可以用许多不同的方式去表达，依赖注入仅仅是控制反转的一个具体的例子。 当编写一个复杂的 Java 应用程序时，应用程序类应该尽可能的独立于其他的 Java 类来增加这些类可重用可能性，当进行单元测试时，可以使它们独立于其他类进行测试。依赖注入（或者有时被称为配线）有助于将这些类粘合在一起，并且在同一时间让它们保持独立。 到底什么是依赖注入？让我们将这两个词分开来看一看。这里将依赖关系部分转化为两个类之间的关联。例如，类 A 依赖于类 B。现在，让我们看一看第二部分，注入。所有这一切都意味着类 B 将通过 IoC 被注入到类 A 中。 依赖注入可以以向构造函数传递参数的方式发生，或者通过使用 setter 方法 post-construction。由于依赖注入是 Spring 框架的核心部分，所以我将在一个单独的章节中利用很好的例子去解释这一概念。 Aop Spring 框架的一个关键组件是面向方面的程序设计（AOP）框架。一个程序中跨越多个点的功能被称为横切关注点，这些横切关注点在概念上独立于应用程序的业务逻辑。有各种各样常见的很好的关于方面的例子，比如日志记录、声明性事务、安全性，和缓存等等。 在 OOP 中模块化的关键单元是类，而在 AOP 中模块化的关键单元是方面。AOP 帮助你将横切关注点从它们所影响的对象中分离出来，然而依赖注入帮助你将你的应用程序对象从彼此中分离出来。 Spring 框架的 AOP 模块提供了面向方面的程序设计实现，允许你定义拦截器方法和切入点，可以实现将应该被分开的代码干净的分开功能。我将在一个独立的章节中讨论更多关于 Spring AOP 的概念。 Spring体系结构 Spring当前框架有20个jar包，大致可以分为6大模块: Core Container AOP and Instrumentation Messaging Data Access/Integration Web Test Spring框架提供了非常丰富的功能，因此整个架构也很庞大。 在我们实际的应用开发中，并不一定要使用所有的功能，而是可以根据需要选择合适的Spring模块。 Core Container IoC容器是Spring框架的核心。spring容器使用依赖注入管理构成应用的组件，它会创建相互协作的组件之间的关联。毫无疑问，这些对象更简单干净，更容易理解，也更容易重用和测试。 Spring自带了几种容器的实现，可归纳为两种类型： BeanFactory 由org.springframework.beans.factory.BeanFactory接口定义。 它是最简单的容器，提供基本的DI支持。 ApplicationContext 由org.springframework.context.ApplicationContext接口定义。 它是基于BeanFactory 之上构建，并提供面向应用的服务，例如从属性文件解析文本信息的能力，以及发布应用事件给感兴趣的事件监听者的能力。 注：Bean工厂对于大多数应用来说往往太低级了，所以应用上下文使用更广泛。推荐在开发中使用应用上下文容器。 Spring自带了多种应用上下文，最可能遇到的有以下几种： ClassPathXmlApplicationContext：从类路径下的XML配置文件中加载上下文定义，把应用上下文定义文件当做类资源。 FileSystemXmlApplicationContext：读取文件系统下的XML配置文件并加载上下文定义。 XmlWebApplicationContext：读取Web应用下的XML配置文件并装载上下文定义。 范例 ApplicationContext context = new FileSystemXmlApplicationContext("D:\Temp\build.xml");ApplicationContext context2 = new ClassPathXmlApplicationContext("build.xml"); 可以看到，加载 FileSystemXmlApplicationContext 和 ClassPathXmlApplicationContext 十分相似。 差异在于：前者在指定文件系统路径下查找build.xml文件；而后在所有类路径（包含JAR文件）下查找build.xml文件。 通过引用应用上下文，可以很方便的调用 getBean() 方法从 Spring 容器中获取 Bean。 相关jar包 spring-core, spring-beans, 提供框架的基础部分，包括IoC和依赖注入特性。 spring-context, 在spring-core, spring-beans基础上构建。它提供一种框架式的访问对象的方法。它也支持类似Java EE特性，例如：EJB，JMX和基本remoting。ApplicationContext接口是它的聚焦点。 springcontext-support, 集成第三方库到Spring application context。 spring-expression，提供一种强有力的表达语言在运行时来查询和操纵一个对象图。 AOP and Instrumentation 相关jar包 spring-aop，提供了对面向切面编程的丰富支持。 spring-aspects，提供了对AspectJ的集成。 spring-instrument，提供了对类instrumentation的支持和类加载器。 spring-instrument-tomcat，包含了Spring对Tomcat的instrumentation代理。 Messaging 相关jar包 spring-messaging，包含spring的消息处理功能，如Message，MessageChannel，MessageHandler。 Data Access / Integaration Data Access/Integration层包含了JDBC / ORM / OXM / JMS和Transaction模块。 相关jar包 spring-jdbc，提供了一个JDBC抽象层。 spring-tx，支持编程和声明式事务管理类。 spring-orm，提供了流行的对象关系型映射API集，如JPA，JDO，Hibernate。 spring-oxm，提供了一个抽象层以支持对象/XML 映射的实现，如JAXB，Castor，XMLBeans，JiBX 和 XStream. spring-jms，包含了生产和消费消息的功能。 Web 相关jar包 spring-web，提供了基本的面向web的功能，如多文件上传、使用Servlet监听器的Ioc容器的初始化。一个面向web的应用层上下文。 spring-webmvc，包括MVC和REST web服务实现。 spring-webmvc-portlet，提供在Protlet环境的MVC实现和spring-webmvc功能的镜像。 Test 相关jar包 spring-test，以Junit和TestNG来支持spring组件的单元测试和集成测试。 术语 应用程序：是能完成我们所需要功能的成品，比如购物网站、OA系统。 框架：是能完成一定功能的半成品，比如我们可以使用框架进行购物网站开发；框架做一部分功能，我们自己做一部分功能，这样应用程序就创建出来了。而且框架规定了你在开发应用程序时的整体架构，提供了一些基础功能，还规定了类和对象的如何创建、如何协作等，从而简化我们开发，让我们专注于业务逻辑开发。 非侵入式设计：从框架角度可以这样理解，无需继承框架提供的类，这种设计就可以看作是非侵入式设计，如果继承了这些框架类，就是侵入设计，如果以后想更换框架之前写过的代码几乎无法重用，如果非侵入式设计则之前写过的代码仍然可以继续使用。 轻量级及重量级：轻量级是相对于重量级而言的，轻量级一般就是非入侵性的、所依赖的东西非常少、资源占用非常少、部署简单等等，其实就是比较容易使用，而重量级正好相反。 POJO：POJO（Plain Old Java Objects）简单的Java对象，它可以包含业务逻辑或持久化逻辑，但不担当任何特殊角色且不继承或不实现任何其它Java框架的类或接口。 容器：在日常生活中容器就是一种盛放东西的器具，从程序设计角度看就是装对象的的对象，因为存在放入、拿出等操作，所以容器还要管理对象的生命周期。 **控制反转：**即Inversion of Control，缩写为IoC，控制反转还有一个名字叫做依赖注入（Dependency Injection），就是由容器控制程序之间的关系，而非传统实现中，由程序代码直接操控。 JavaBean：一般指容器管理对象，在Spring中指Spring IoC容器管理对象。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>introduction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 和 WebSocket]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fspring%2Fweb%2Fspring-websocket%2F</url>
    <content type="text"><![CDATA[概述 WebSocket 是什么？ WebSocket 是一种网络通信协议。RFC6455 定义了它的通信标准。 WebSocket 是 HTML5 开始提供的一种在单个 TCP 连接上进行全双工通讯的协议。 为什么需要 WebSocket ？ 了解计算机网络协议的人，应该都知道：HTTP 协议是一种无状态的、无连接的、单向的应用层协议。它采用了请求/响应模型。通信请求只能由客户端发起，服务端对请求做出应答处理。 这种通信模型有一个弊端：HTTP 协议无法实现服务器主动向客户端发起消息。 这种单向请求的特点，注定了如果服务器有连续的状态变化，客户端要获知就非常麻烦。大多数 Web 应用程序将通过频繁的异步JavaScript和XML（AJAX）请求实现长轮询。轮询的效率低，非常浪费资源（因为必须不停连接，或者 HTTP 连接始终打开）。 因此，工程师们一直在思考，有没有更好的方法。WebSocket 就是这样发明的。WebSocket 连接允许客户端和服务器之间进行全双工通信，以便任一方都可以通过建立的连接将数据推送到另一端。WebSocket 只需要建立一次连接，就可以一直保持连接状态。这相比于轮询方式的不停建立连接显然效率要大大提高。 WebSocket 如何工作？ Web浏览器和服务器都必须实现 WebSockets 协议来建立和维护连接。由于 WebSockets 连接长期存在，与典型的HTTP连接不同，对服务器有重要的影响。 基于多线程或多进程的服务器无法适用于 WebSockets，因为它旨在打开连接，尽可能快地处理请求，然后关闭连接。任何实际的 WebSockets 服务器端实现都需要一个异步服务器。 WebSocket 客户端 在客户端，没有必要为 WebSockets 使用 JavaScript 库。实现 WebSockets 的 Web 浏览器将通过 WebSockets 对象公开所有必需的客户端功能（主要指支持 Html5 的浏览器）。 客户端 API 以下 API 用于创建 WebSocket 对象。 var Socket = new WebSocket(url, [protocol] ); 以上代码中的第一个参数 url, 指定连接的 URL。第二个参数 protocol 是可选的，指定了可接受的子协议。 WebSocket 属性 以下是 WebSocket 对象的属性。假定我们使用了以上代码创建了 Socket 对象： 属性 描述 Socket.readyState 只读属性 readyState 表示连接状态，可以是以下值：0 - 表示连接尚未建立。1 - 表示连接已建立，可以进行通信。2 - 表示连接正在进行关闭。3 - 表示连接已经关闭或者连接不能打开。 Socket.bufferedAmount 只读属性 bufferedAmount 已被 send() 放入正在队列中等待传输，但是还没有发出的 UTF-8 文本字节数。 WebSocket 事件 以下是 WebSocket 对象的相关事件。假定我们使用了以上代码创建了 Socket 对象： 事件 事件处理程序 描述 open Socket.onopen 连接建立时触发 message Socket.onmessage 客户端接收服务端数据时触发 error Socket.onerror 通信发生错误时触发 close Socket.onclose 连接关闭时触发 WebSocket 方法 以下是 WebSocket 对象的相关方法。假定我们使用了以上代码创建了 Socket 对象： 方法 描述 Socket.send() 使用连接发送数据 Socket.close() 关闭连接 示例 // 初始化一个 WebSocket 对象var ws = new WebSocket("ws://localhost:9998/echo");// 建立 web socket 连接成功触发事件ws.onopen = function () &#123; // 使用 send() 方法发送数据 ws.send("发送数据"); alert("数据发送中...");&#125;;// 接收服务端数据时触发事件ws.onmessage = function (evt) &#123; var received_msg = evt.data; alert("数据已接收...");&#125;;// 断开 web socket 连接成功触发事件ws.onclose = function () &#123; alert("连接已关闭...");&#125;; WebSocket 服务端 WebSocket 在服务端的实现非常丰富。Node.js、Java、C++、Python 等多种语言都有自己的解决方案。 以下，介绍我在学习 WebSocket 过程中接触过的 WebSocket 服务端解决方案。 Node.js 常用的 Node 实现有以下三种。 µWebSockets Socket.IO WebSocket-Node Java Java 的 web 一般都依托于 servlet 容器。 我使用过的 servlet 容器有：Tomcat、Jetty、Resin。其中Tomcat7、Jetty7及以上版本均开始支持 WebSocket（推荐较新的版本，因为随着版本的更迭，对 WebSocket 的支持可能有变更）。 此外，Spring 框架对 WebSocket 也提供了支持。 虽然，以上应用对于 WebSocket 都有各自的实现。但是，它们都遵循RFC6455 的通信标准，并且 Java API 统一遵循 JSR 356 - JavaTM API for WebSocket 规范。所以，在实际编码中，API 差异不大。 Spring Spring 对于 WebSocket 的支持基于下面的 jar 包： &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-websocket&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt; 在 Spring 实现 WebSocket 服务器大概分为以下几步： 创建 WebSocket 处理器 扩展 TextWebSocketHandler 或 BinaryWebSocketHandler ，你可以覆写指定的方法。Spring 在收到 WebSocket 事件时，会自动调用事件对应的方法。 import org.springframework.web.socket.WebSocketHandler;import org.springframework.web.socket.WebSocketSession;import org.springframework.web.socket.TextMessage;public class MyHandler extends TextWebSocketHandler &#123; @Override public void handleTextMessage(WebSocketSession session, TextMessage message) &#123; // ... &#125;&#125; WebSocketHandler 源码如下，这意味着你的处理器大概可以处理哪些 WebSocket 事件： public interface WebSocketHandler &#123; /** * 建立连接后触发的回调 */ void afterConnectionEstablished(WebSocketSession session) throws Exception; /** * 收到消息时触发的回调 */ void handleMessage(WebSocketSession session, WebSocketMessage&lt;?&gt; message) throws Exception; /** * 传输消息出错时触发的回调 */ void handleTransportError(WebSocketSession session, Throwable exception) throws Exception; /** * 断开连接后触发的回调 */ void afterConnectionClosed(WebSocketSession session, CloseStatus closeStatus) throws Exception; /** * 是否处理分片消息 */ boolean supportsPartialMessages();&#125; 配置 WebSocket 配置有两种方式：注解和 xml 。其作用就是将 WebSocket 处理器添加到注册中心。 实现 WebSocketConfigurer import org.springframework.web.socket.config.annotation.EnableWebSocket;import org.springframework.web.socket.config.annotation.WebSocketConfigurer;import org.springframework.web.socket.config.annotation.WebSocketHandlerRegistry;@Configuration@EnableWebSocketpublic class WebSocketConfig implements WebSocketConfigurer &#123; @Override public void registerWebSocketHandlers(WebSocketHandlerRegistry registry) &#123; registry.addHandler(myHandler(), "/myHandler"); &#125; @Bean public WebSocketHandler myHandler() &#123; return new MyHandler(); &#125;&#125; xml 方式 &lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:websocket="http://www.springframework.org/schema/websocket" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/websocket http://www.springframework.org/schema/websocket/spring-websocket.xsd"&gt; &lt;websocket:handlers&gt; &lt;websocket:mapping path="/myHandler" handler="myHandler"/&gt; &lt;/websocket:handlers&gt; &lt;bean id="myHandler" class="org.springframework.samples.MyHandler"/&gt;&lt;/beans&gt; 更多配置细节可以参考：Spring WebSocket 文档 javax.websocket 如果不想使用 Spring 框架的 WebSocket API，你也可以选择基本的 javax.websocket。 首先，需要引入 API jar 包。 &lt;!-- To write basic javax.websocket against --&gt;&lt;dependency&gt; &lt;groupId&gt;javax.websocket&lt;/groupId&gt; &lt;artifactId&gt;javax.websocket-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 如果使用嵌入式 jetty，你还需要引入它的实现包： &lt;!-- To run javax.websocket in embedded server --&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty.websocket&lt;/groupId&gt; &lt;artifactId&gt;javax-websocket-server-impl&lt;/artifactId&gt; &lt;version&gt;$&#123;jetty-version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- To run javax.websocket client --&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty.websocket&lt;/groupId&gt; &lt;artifactId&gt;javax-websocket-client-impl&lt;/artifactId&gt; &lt;version&gt;$&#123;jetty-version&#125;&lt;/version&gt;&lt;/dependency&gt; @ServerEndpoint 这个注解用来标记一个类是 WebSocket 的处理器。 然后，你可以在这个类中使用下面的注解来表明所修饰的方法是触发事件的回调 // 收到消息触发事件@OnMessagepublic void onMessage(String message, Session session) throws IOException, InterruptedException &#123; ...&#125;// 打开连接触发事件@OnOpenpublic void onOpen(Session session, EndpointConfig config, @PathParam("id") String id) &#123; ...&#125;// 关闭连接触发事件@OnClosepublic void onClose(Session session, CloseReason closeReason) &#123; ...&#125;// 传输消息错误触发事件@OnErrorpublic void onError(Throwable error) &#123; ...&#125; ServerEndpointConfig.Configurator 编写完处理器，你需要扩展 ServerEndpointConfig.Configurator 类完成配置： public class WebSocketServerConfigurator extends ServerEndpointConfig.Configurator &#123; @Override public void modifyHandshake(ServerEndpointConfig sec, HandshakeRequest request, HandshakeResponse response) &#123; HttpSession httpSession = (HttpSession) request.getHttpSession(); sec.getUserProperties().put(HttpSession.class.getName(), httpSession); &#125;&#125; 然后就没有然后了，就是这么简单。 WebSocket 代理 如果把 WebSocket 的通信看成是电话连接，Nginx 的角色则像是电话接线员，负责将发起电话连接的电话转接到指定的客服。 Nginx 从 1.3 版开始正式支持 WebSocket 代理。如果你的 web 应用使用了代理服务器 Nginx，那么你还需要为 Nginx 做一些配置，使得它开启 WebSocket 代理功能。 以下为参考配置： server &#123; # this section is specific to the WebSockets proxying location /socket.io &#123; proxy_pass http://app_server_wsgiapp/socket.io; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection "upgrade"; proxy_read_timeout 600; &#125;&#125; 更多配置细节可以参考：Nginx 官方的 websocket 文档 FAQ HTTP 和 WebSocket 有什么关系？ Websocket 其实是一个新协议，跟 HTTP 协议基本没有关系，只是为了兼容现有浏览器的握手规范而已，也就是说它是 HTTP 协议上的一种补充。 Html 和 HTTP 有什么关系？ Html 是超文本标记语言，是一种用于创建网页的标准标记语言。它是一种技术标准。Html5 是它的最新版本。 Http 是一种网络通信协议。其本身和 Html 没有直接关系。 完整示例 如果需要完整示例代码，可以参考我的 Github 代码： Spring 对 WebSocket 支持的示例 嵌入式 Jetty 服务器的 WebSocket 示例 spring-websocket 和 jetty 9.3 版本似乎存在兼容性问题，Tomcat则木有问题。 我尝试了好几次，没有找到解决方案，只好使用 Jetty 官方的嵌入式示例在 Jetty 中使用 WebSocket 。 资料 知乎高票答案——WebSocket是什么原理 by Ovear 对 WebSocket 原理的阐述简单易懂。 WebSocket 教程 by ruanyf 阮一峰大神的科普一如既往的浅显易懂。 WebSockets by fullstackpython Nginx 官方的 websocket 文档 Spring WebSocket 文档 Tomcat7 WebSocket 文档 Jetty WebSocket 文档]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>spring</tag>
        <tag>websocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 集成 Dubbo]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fspring%2Fintegration%2Fspring-and-dubbo%2F</url>
    <content type="text"><![CDATA[ZooKeeper ZooKeeper 可以作为 Dubbo 的注册中心。 Dubbo 未对 Zookeeper 服务器端做任何侵入修改，只需安装原生的 Zookeeper 服务器即可，所有注册中心逻辑适配都在调用 Zookeeper 客户端时完成。 安装 在 ZooKeeper 发布中心 选择需要的版本，下载后解压到本地。 配置 vi conf/zoo.cfg 如果不需要集群，zoo.cfg 的内容如下 2： tickTime=2000initLimit=10syncLimit=5dataDir=/home/dubbo/zookeeper-3.3.3/dataclientPort=2181 如果需要集群，zoo.cfg 的内容如下 3： tickTime=2000initLimit=10syncLimit=5dataDir=/home/dubbo/zookeeper-3.3.3/dataclientPort=2181server.1=10.20.153.10:2555:3555server.2=10.20.153.11:2555:3555 并在 data 目录 4 下放置 myid 文件： mkdir datavi myid myid 指明自己的 id，对应上面 zoo.cfg 中 server. 后的数字，第一台的内容为 1，第二台的内容为 2，内容如下： 1 启动 Linux 下执行 bin/zkServer.sh ；Windows bin/zkServer.cmd 启动 ZooKeeper 。 命令行 telnet 127.0.0.1 2181dump 或者: echo dump | nc 127.0.0.1 2181 用法: dubbo.registry.address=zookeeper://10.20.153.10:2181?backup=10.20.153.11:2181 或者: &lt;dubbo:registry protocol="zookeeper" address="10.20.153.10:2181,10.20.153.11:2181" /&gt; Zookeeper是 Apache Hadoop 的子项目，强度相对较好，建议生产环境使用该注册中心 其中 data 目录需改成你真实输出目录 其中 data 目录和 server 地址需改成你真实部署机器的信息 上面 zoo.cfg 中的 dataDir http://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html Dubbo Dubbo 采用全 Spring 配置方式，透明化接入应用，对应用没有任何 API 侵入，只需用 Spring 加载 Dubbo 的配置即可，Dubbo 基于 Spring 的 Schema 扩展进行加载。 如果不想使用 Spring 配置，可以通过 API 的方式 进行调用。 服务提供者 完整安装步骤，请参见：示例提供者安装 定义服务接口 DemoService.java 1： package com.alibaba.dubbo.demo;public interface DemoService &#123; String sayHello(String name);&#125; 在服务提供方实现接口 DemoServiceImpl.java 2： package com.alibaba.dubbo.demo.provider;import com.alibaba.dubbo.demo.DemoService;public class DemoServiceImpl implements DemoService &#123; public String sayHello(String name) &#123; return "Hello " + name; &#125;&#125; 用 Spring 配置声明暴露服务 provider.xml： &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;!-- 提供方应用信息，用于计算依赖关系 --&gt; &lt;dubbo:application name="hello-world-app" /&gt; &lt;!-- 使用multicast广播注册中心暴露服务地址 --&gt; &lt;dubbo:registry address="multicast://224.5.6.7:1234" /&gt; &lt;!-- 用dubbo协议在20880端口暴露服务 --&gt; &lt;dubbo:protocol name="dubbo" port="20880" /&gt; &lt;!-- 声明需要暴露的服务接口 --&gt; &lt;dubbo:service interface="com.alibaba.dubbo.demo.DemoService" ref="demoService" /&gt; &lt;!-- 和本地bean一样实现服务 --&gt; &lt;bean id="demoService" class="com.alibaba.dubbo.demo.provider.DemoServiceImpl" /&gt;&lt;/beans&gt; 如果注册中心使用 ZooKeeper，可以将 dubbo:registry 改为 zookeeper://127.0.0.1:2181 加载 Spring 配置 Provider.java： import org.springframework.context.support.ClassPathXmlApplicationContext;public class Provider &#123; public static void main(String[] args) throws Exception &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[] &#123;"http://10.20.160.198/wiki/display/dubbo/provider.xml"&#125;); context.start(); System.in.read(); // 按任意键退出 &#125;&#125; 服务消费者 完整安装步骤，请参见：示例消费者安装 通过 Spring 配置引用远程服务 consumer.xml： &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;!-- 消费方应用名，用于计算依赖关系，不是匹配条件，不要与提供方一样 --&gt; &lt;dubbo:application name="consumer-of-helloworld-app" /&gt; &lt;!-- 使用multicast广播注册中心暴露发现服务地址 --&gt; &lt;dubbo:registry address="multicast://224.5.6.7:1234" /&gt; &lt;!-- 生成远程服务代理，可以和本地bean一样使用demoService --&gt; &lt;dubbo:reference id="demoService" interface="com.alibaba.dubbo.demo.DemoService" /&gt;&lt;/beans&gt; 如果注册中心使用 ZooKeeper，可以将 dubbo:registry 改为 zookeeper://127.0.0.1:2181 加载Spring配置，并调用远程服务 Consumer.java 3： import org.springframework.context.support.ClassPathXmlApplicationContext;import com.alibaba.dubbo.demo.DemoService;public class Consumer &#123; public static void main(String[] args) throws Exception &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[] &#123;"http://10.20.160.198/wiki/display/dubbo/consumer.xml"&#125;); context.start(); DemoService demoService = (DemoService)context.getBean("demoService"); // 获取远程服务代理 String hello = demoService.sayHello("world"); // 执行远程方法 System.out.println( hello ); // 显示调用结果 &#125;&#125; 该接口需单独打包，在服务提供方和消费方共享 对服务消费方隐藏实现 也可以使用 IoC 注入 FAQ 建议使用 dubbo-2.3.3 以上版本的 zookeeper 注册中心客户端。 资料 Dubbo Github | 用户手册 | 开发手册 | 管理员手册 ZooKeeper 官网 | 官方文档]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>integration</tag>
        <tag>rpc</tag>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jetty 使用小结]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Ftools%2Fjetty%2F</url>
    <content type="text"><![CDATA[Jetty 使用小结 概述 jetty 的嵌入式启动 API 方式 Maven 插件方式 参考 概述 jetty 是什么？ jetty 是轻量级的 web 服务器和 servlet 引擎。 它的最大特点是：可以很方便的作为嵌入式服务器。 它是 eclipse 的一个开源项目。不用怀疑，就是你常用的那个 eclipse。 它是使用 Java 开发的，所以天然对 Java 支持良好。 官方网址 github 源码地址 什么是嵌入式服务器？ 以 jetty 来说明，就是只要引入 jetty 的 jar 包，可以通过直接调用其 API 的方式来启动 web 服务。 用过 Tomcat、Resin 等服务器的朋友想必不会陌生那一套安装、配置、部署的流程吧，还是挺繁琐的。使用 jetty，就不需要这些过程了。 jetty 非常适用于项目的开发、测试，因为非常快捷。如果想用于生产环境，则需要谨慎考虑，它不一定能像成熟的 Tomcat、Resin 等服务器一样支持企业级 Java EE 的需要。 jetty 的嵌入式启动 我觉得嵌入式启动方式的一个好处在于：可以直接运行项目，无需每次部署都得再配置服务器。 jetty 的嵌入式启动使用有两种方式： API 方式 maven 插件方式 API 方式 添加 maven 依赖 &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-webapp&lt;/artifactId&gt; &lt;version&gt;9.3.2.v20150730&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-annotations&lt;/artifactId&gt; &lt;version&gt;9.3.2.v20150730&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;apache-jsp&lt;/artifactId&gt; &lt;version&gt;9.3.2.v20150730&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;apache-jstl&lt;/artifactId&gt; &lt;version&gt;9.3.2.v20150730&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 官方的启动代码 public class SplitFileServer&#123; public static void main( String[] args ) throws Exception &#123; // 创建Server对象，并绑定端口 Server server = new Server(); ServerConnector connector = new ServerConnector(server); connector.setPort(8090); server.setConnectors(new Connector[] &#123; connector &#125;); // 创建上下文句柄，绑定上下文路径。这样启动后的url就会是:http://host:port/context ResourceHandler rh0 = new ResourceHandler(); ContextHandler context0 = new ContextHandler(); context0.setContextPath("/"); // 绑定测试资源目录（在本例的配置目录dir0的路径是src/test/resources/dir0） File dir0 = MavenTestingUtils.getTestResourceDir("dir0"); context0.setBaseResource(Resource.newResource(dir0)); context0.setHandler(rh0); // 和上面的例子一样 ResourceHandler rh1 = new ResourceHandler(); ContextHandler context1 = new ContextHandler(); context1.setContextPath("/"); File dir1 = MavenTestingUtils.getTestResourceDir("dir1"); context1.setBaseResource(Resource.newResource(dir1)); context1.setHandler(rh1); // 绑定两个资源句柄 ContextHandlerCollection contexts = new ContextHandlerCollection(); contexts.setHandlers(new Handler[] &#123; context0, context1 &#125;); server.setHandler(contexts); // 启动 server.start(); // 打印dump时的信息 System.out.println(server.dump()); // join当前线程 server.join(); &#125;&#125; 直接运行 Main 方法，就可以启动 web 服务。 注：以上代码在 eclipse 中运行没有问题，如果想在 Intellij 中运行还需要为它指定配置文件。 如果想了解在 Eclipse 和 Intellij 都能运行的通用方法可以参考我的 github 代码示例。 我的实现也是参考 springside 的方式。 代码行数有点多，不在这里贴代码了。 完整参考代码 Maven 插件方式 如果你熟悉 maven，那么实在太简单了 注： Maven 版本必须在 3.3 及以上版本。 (1) 添加 maven 插件 &lt;plugin&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-maven-plugin&lt;/artifactId&gt; &lt;version&gt;9.3.12.v20160915&lt;/version&gt;&lt;/plugin&gt; (2) 执行 maven 命令： mvn jetty:run 讲真，就是这么简单。jetty 默认会为你创建一个 web 服务，地址为 127.0.0.1:8080。 当然，你也可以在插件中配置你的 webapp 环境 &lt;plugin&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-maven-plugin&lt;/artifactId&gt; &lt;version&gt;9.3.12.v20160915&lt;/version&gt; &lt;configuration&gt; &lt;webAppSourceDirectory&gt;$&#123;project.basedir&#125;/src/staticfiles&lt;/webAppSourceDirectory&gt; &lt;!-- 配置webapp --&gt; &lt;webApp&gt; &lt;contextPath&gt;/&lt;/contextPath&gt; &lt;descriptor&gt;$&#123;project.basedir&#125;/src/over/here/web.xml&lt;/descriptor&gt; &lt;jettyEnvXml&gt;$&#123;project.basedir&#125;/src/over/here/jetty-env.xml&lt;/jettyEnvXml&gt; &lt;/webApp&gt; &lt;!-- 配置classes --&gt; &lt;classesDirectory&gt;$&#123;project.basedir&#125;/somewhere/else&lt;/classesDirectory&gt; &lt;scanClassesPattern&gt; &lt;excludes&gt; &lt;exclude&gt;**/Foo.class&lt;/exclude&gt; &lt;/excludes&gt; &lt;/scanClassesPattern&gt; &lt;scanTargets&gt; &lt;scanTarget&gt;src/mydir&lt;/scanTarget&gt; &lt;scanTarget&gt;src/myfile.txt&lt;/scanTarget&gt; &lt;/scanTargets&gt; &lt;!-- 扫描target目录下的资源文件 --&gt; &lt;scanTargetPatterns&gt; &lt;scanTargetPattern&gt; &lt;directory&gt;src/other-resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/myspecial.xml&lt;/exclude&gt; &lt;exclude&gt;**/myspecial.properties&lt;/exclude&gt; &lt;/excludes&gt; &lt;/scanTargetPattern&gt; &lt;/scanTargetPatterns&gt; &lt;/configuration&gt;&lt;/plugin&gt; 官方给的 jetty-env.xml 范例 &lt;?xml version="1.0"?&gt;&lt;!DOCTYPE Configure PUBLIC "-//Mort Bay Consulting//DTD Configure//EN" "http://jetty.mortbay.org/configure.dtd"&gt;&lt;Configure class="org.eclipse.jetty.webapp.WebAppContext"&gt; &lt;!-- Add an EnvEntry only valid for this webapp --&gt; &lt;New id="gargle" class="org.eclipse.jetty.plus.jndi.EnvEntry"&gt; &lt;Arg&gt;gargle&lt;/Arg&gt; &lt;Arg type="java.lang.Double"&gt;100&lt;/Arg&gt; &lt;Arg type="boolean"&gt;true&lt;/Arg&gt; &lt;/New&gt; &lt;!-- Add an override for a global EnvEntry --&gt; &lt;New id="wiggle" class="org.eclipse.jetty.plus.jndi.EnvEntry"&gt; &lt;Arg&gt;wiggle&lt;/Arg&gt; &lt;Arg type="java.lang.Double"&gt;55.0&lt;/Arg&gt; &lt;Arg type="boolean"&gt;true&lt;/Arg&gt; &lt;/New&gt; &lt;!-- an XADataSource --&gt; &lt;New id="mydatasource99" class="org.eclipse.jetty.plus.jndi.Resource"&gt; &lt;Arg&gt;jdbc/mydatasource99&lt;/Arg&gt; &lt;Arg&gt; &lt;New class="com.atomikos.jdbc.SimpleDataSourceBean"&gt; &lt;Set name="xaDataSourceClassName"&gt;org.apache.derby.jdbc.EmbeddedXADataSource&lt;/Set&gt; &lt;Set name="xaDataSourceProperties"&gt;databaseName=testdb99;createDatabase=create&lt;/Set&gt; &lt;Set name="UniqueResourceName"&gt;mydatasource99&lt;/Set&gt; &lt;/New&gt; &lt;/Arg&gt; &lt;/New&gt;&lt;/Configure&gt; 参考 jetty wiki jetty 官方文档]]></content>
      <categories>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet 指南]]></title>
    <url>%2Fblog%2F2017%2F11%2F08%2Fjava%2Fjavaweb%2Fjavaee%2Flistener%2F</url>
    <content type="text"><![CDATA[监听器 监听器是一个专门用于对其他对象身上发生的事件或状态改变进行监听和相应处理的对象，当被监视的对象发生情况时，立即采取相应的行动。监听器其实就是一个实现特定接口的普通 java 程序，这个程序专门用于监听另一个 java 对象的方法调用或属性改变，当被监听对象发生上述事件后，监听器某个方法立即被执行。 简介 监听器的分类 监听对象的创建和销毁 监听对象的属性变化 监听 Session 内的对象 简介 JavaWeb 中的监听器是 Servlet 规范中定义的一种特殊类，它用于监听 web 应用程序中的ServletContext, HttpSession和 ServletRequest等域对象的创建与销毁事件，以及监听这些域对象中的属性发生修改的事件。 监听器的分类 在 Servlet 规范中定义了多种类型的监听器，它们用于监听的事件源分别为ServletContext，HttpSession和ServletRequest这三个域对象 Servlet 规范针对这三个对象上的操作，又把多种类型的监听器划分为三种类型： 监听域对象自身的创建和销毁的事件监听器。 监听域对象中的属性的增加和删除的事件监听器。 监听绑定到 HttpSession 域中的某个对象的状态的事件监听器。 监听对象的创建和销毁 HttpSessionListener HttpSessionListener 接口用于监听HttpSession对象的创建和销毁 创建一个Session时，激发sessionCreated (HttpSessionEvent se) 方法 销毁一个Session时，激发sessionDestroyed (HttpSessionEvent se) 方法。 ServletContextListener ServletContextListener接口用于监听ServletContext对象的创建和销毁事件。 实现了ServletContextListener接口的类都可以对ServletContext对象的创建和销毁进行监听。 当ServletContext对象被创建时，激发contextInitialized (ServletContextEvent sce)方法。 当ServletContext对象被销毁时，激发contextDestroyed(ServletContextEvent sce)方法。 ServletContext域对象创建和销毁时机： 创建：服务器启动针对每一个 Web 应用创建ServletContext 销毁：服务器关闭前先关闭代表每一个 web 应用的ServletContext ServletRequestListener ServletRequestListener接口用于监听ServletRequest 对象的创建和销毁 Request对象被创建时，监听器的requestInitialized(ServletRequestEvent sre)方法将会被调用 Request对象被销毁时，监听器的requestDestroyed(ServletRequestEvent sre)方法将会被调用 ServletRequest域对象创建和销毁时机： 创建：用户每一次访问都会创建 request 对象 销毁：当前访问结束，request 对象就会销毁 监听对象的属性变化 域对象中属性的变更的事件监听器就是用来监听 ServletContext, HttpSession, HttpServletRequest 这三个对象中的属性变更信息事件的监听器。 这三个监听器接口分别是 ServletContextAttributeListener, HttpSessionAttributeListener 和 ServletRequestAttributeListener，这三个接口中都定义了三个方法来处理被监听对象中的属性的增加，删除和替换的事件，同一个事件在这三个接口中对应的方法名称完全相同，只是接受的参数类型不同。 attributeAdded 方法 当向被监听对象中增加一个属性时，web 容器就调用事件监听器的attributeAdded方法进行响应，这个方法接收一个事件类型的参数，监听器可以通过这个参数来获得正在增加属性的域对象和被保存到域中的属性对象 各个域属性监听器中的完整语法定义为： public void attributeAdded(ServletContextAttributeEvent scae)public void attributeReplaced(HttpSessionBindingEvent hsbe)public void attributeRmoved(ServletRequestAttributeEvent srae) attributeRemoved 方法 当删除被监听对象中的一个属性时，web 容器调用事件监听器的attributeRemoved方法进行响应 各个域属性监听器中的完整语法定义为： public void attributeRemoved(ServletContextAttributeEvent scae)public void attributeRemoved (HttpSessionBindingEvent hsbe)public void attributeRemoved (ServletRequestAttributeEvent srae) attributeReplaced 方法 当监听器的域对象中的某个属性被替换时，web 容器调用事件监听器的attributeReplaced方法进行响应 各个域属性监听器中的完整语法定义为： public void attributeReplaced(ServletContextAttributeEvent scae)public void attributeReplaced (HttpSessionBindingEvent hsbe)public void attributeReplaced (ServletRequestAttributeEvent srae) 监听 Session 内的对象 保存在 Session 域中的对象可以有多种状态： 绑定(session.setAttribute(“bean”,Object))到 Session 中； 从 Session 域中解除(session.removeAttribute(“bean”))绑定； 随 Session 对象持久化到一个存储设备中； 随 Session 对象从一个存储设备中恢复 Servlet 规范中定义了两个特殊的监听器接口HttpSessionBindingListener和HttpSessionActivationListener来帮助 JavaBean 对象了解自己在 Session 域中的这些状态。 实现这两个接口的类不需要 web.xml 文件中进行注册。 HttpSessionBindingListener 实现了HttpSessionBindingListener接口的 JavaBean 对象可以感知自己被绑定到 Session 中和 Session 中删除的事件。 当对象被绑定到HttpSession对象中时，web 服务器调用该对象的valueBound(HttpSessionBindingEvent event)方法。 当对象从HttpSession对象中解除绑定时，web 服务器调用该对象的valueUnbound(HttpSessionBindingEvent event)方法。 HttpSessionActivationListener 实现了HttpSessionActivationListener接口的 JavaBean 对象可以感知自己被活化(反序列化)和钝化(序列化)的事件。 当绑定到 HttpSession 对象中的 javabean 对象将要随 HttpSession 对象被钝化(序列化)之前，web 服务器调用该 javabean 对象的sessionWillPassivate(HttpSessionEvent event) 方法。这样 javabean 对象就可以知道自己将要和 HttpSession 对象一起被序列化(钝化)到硬盘中. 当绑定到 HttpSession 对象中的 javabean 对象将要随 HttpSession 对象被活化(反序列化)之后，web 服务器调用该 javabean 对象的sessionDidActive(HttpSessionEvent event)方法。这样 javabean 对象就可以知道自己将要和 HttpSession 对象一起被反序列化(活化)回到内存中]]></content>
      <categories>
        <category>java</category>
        <category>javaweb</category>
        <category>javaee</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaweb</tag>
        <tag>javaee</tag>
        <tag>listener</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Gitbook 打造你的电子书]]></title>
    <url>%2Fblog%2F2017%2F09%2F04%2Ftools%2Fgitbook%2F</url>
    <content type="text"><![CDATA[使用 Gitbook 打造你的电子书 本文详细讲解了 Gitbook 生成电子书的完整过程，内容包括：安装、命令、配置、文档结构、生成电子书、部署。 限于篇幅，本文不介绍任何 Gitbook 定制化页面的内容。 想看看 Gitbook 在线电子书效果，请猛戳这里：gitbook-notes 📓 本文已归档到：「blog」 概述 GitBook 安装 GitBook 命令 Gitbook 目录结构 Gitbook 配置 生成电子书 Gitbook 部署 参考资料 概述 GitBook 是使用 GitHub / Git 和 Markdown（或 AsciiDoc）构建漂亮书籍的命令行工具（和 Node.js 库）。 GitBook 可以将您的内容作为网站（可定制和可扩展）或电子书（PDF，ePub 或 Mobi）输出。 GitBook.com 是使用 GitBook 格式创建和托管图书的在线平台。它提供托管，协作功能和易于使用的编辑器。 GitBook 安装 本地安装 环境要求 安装 GitBook 是很简单的。您的系统只需要满足这两个要求： NodeJS（推荐使用 v4.0.0 及以上版本） Windows，Linux，Unix 或 Mac OS X 通过 NPM 安装 安装 GitBook 的最好办法是通过 NPM。在终端提示符下，只需运行以下命令即可安装 GitBook： $ npm install gitbook-cli -g gitbook-cli 是 GitBook 的一个命令行工具。它将自动安装所需版本的 GitBook 来构建一本书。 执行下面的命令，查看 GitBook 版本，以验证安装成功。 $ gitbook -V 安装历史版本 gitbook-cli 可以轻松下载并安装其他版本的 GitBook 来测试您的书籍： $ gitbook fetch beta 使用 gitbook ls-remote 会列举可以下载的版本。 创建一本书 初始化 GitBook 可以设置一个样板书： $ gitbook init 如果您希望将书籍创建到一个新目录中，可以通过运行 gitbook init ./directory 这样做。 构建 使用下面的命令，会在项目的目录下生成一个 _book 目录，里面的内容为静态站点的资源文件： $ gitbook build Debugging 您可以使用选项 --log=debug 和 --debug 来获取更好的错误消息（使用堆栈跟踪）。例如： $ gitbook build ./ --log=debug --debug 启动服务 使用下列命令会运行一个 web 服务, 通过 http://localhost:4000/ 可以预览书籍 $ gitbook serve GitBook 命令 这里主要介绍一下 GitBook 的命令行工具 gitbook-cli 的一些命令, 首先说明两点: gitbook-cli 和 gitbook 是两个软件 gitbook-cli 会将下载的 gitbook 的不同版本放到 \~/.gitbook中, 可以通过设置GITBOOK_DIR环境变量来指定另外的文件夹 列出 gitbook 所有的命令 gitbook help 输出 gitbook-cli 的帮助信息 gitbook --help 生成静态网页 gitbook build 生成静态网页并运行服务器 gitbook serve 生成时指定 gitbook 的版本, 本地没有会先下载 gitbook build --gitbook=2.0.1 列出本地所有的 gitbook 版本 gitbook ls 列出远程可用的 gitbook 版本 gitbook ls-remote 安装对应的 gitbook 版本 gitbook fetch 标签/版本号 更新到 gitbook 的最新版本 gitbook update 卸载对应的 gitbook 版本 gitbook uninstall 2.0.1 指定 log 的级别 gitbook build --log=debug 输出错误信息 gitbook builid --debug Gitbook 目录结构 GitBook 项目结构 GitBook 使用简单的目录结构。在 SUMMARY （即 SUMMARY.md 文件）中列出的所有 Markdown / Asciidoc 文件将被转换为 HTML。多语言书籍结构略有不同。 一个基本的 GitBook 电子书结构通常如下： .├── book.json├── README.md├── SUMMARY.md├── chapter-1/| ├── README.md| └── something.md└── chapter-2/ ├── README.md └── something.md GitBook 特殊文件的功能： 文件 描述 book.json 配置数据 (optional) README.md 电子书的前言或简介 (required) SUMMARY.md 电子书目录 (optional) GLOSSARY.md 词汇/注释术语列表 (optional) 静态文件和图片 静态文件是在 SUMMARY.md 中未列出的文件。除非被忽略，否则所有静态文件都将复制到输出路径。 忽略文件和文件夹 GitBook 将读取 .gitignore，.bookignore 和 .ignore 文件，以获取要过滤的文件和文件夹。这些文件中的格式遵循 .gitignore 的规则： # This is a comment# Ignore the file test.mdtest.md# Ignore everything in the directory "bin"bin/* 项目与子目录集成 对于软件项目，您可以使用子目录（如 docs/ ）来存储项目文档的图书。您可以配置根选项来指示 GitBook 可以找到该图书文件的文件夹： .├── book.json└── docs/ ├── README.md └── SUMMARY.md 在 book.json 中配置以下内容： &#123; "root": "./docs"&#125; Summary GitBook 使用 SUMMARY.md 文件来定义本书的章节和子章节的结构。 SUMMARY.md 文件用于生成本书的目录。 SUMMARY.md 的格式是一个链接列表。链接的标题将作为章节的标题，链接的目标是该章节文件的路径。 向父章节添加嵌套列表将创建子章节。 简单示例： # Summary* [Part I](part1/README.md) * [Writing is nice](part1/writing.md) * [GitBook is nice](part1/gitbook.md)* [Part II](part2/README.md) * [We love feedback](part2/feedback_please.md) * [Better tools for authors](part2/better_tools.md) 每章都有一个专用页面（part#/README.md），并分为子章节。 锚点 目录中的章节可以使用锚点指向文件的特定部分。 # Summary### Part I* [Part I](part1/README.md) * [Writing is nice](part1/README.md#writing) * [GitBook is nice](part1/README.md#gitbook)* [Part II](part2/README.md) * [We love feedback](part2/README.md#feedback) * [Better tools for authors](part2/README.md#tools) 部分 目录可以分为以标题或水平线 ---- 分隔的部分： # Summary### Part I* [Writing is nice](part1/writing.md)* [GitBook is nice](part1/gitbook.md)### Part II* [We love feedback](part2/feedback_please.md)* [Better tools for authors](part2/better_tools.md)----* [Last part without title](part3/title.md) Parts 只是章节组，没有专用页面，但根据主题，它将在导航中显示。 页面 Markdown 语法 默认情况下，GitBook 的大多数文件都使用 Markdown 语法。 GitBook 推荐使用这种语法。所使用的语法类似于 GitHub Flavored Markdown syntax 。 此外，你还可以选择 AsciiDoc 语法。 页面内容示例： # Title of the chapterThis is a great introduction.## Section 1Markdown will dictates _most_ of your **book's structure**## Section 2... 页面前言 页面可以包含一个可选的前言。它可以用于定义页面的描述。前面的事情必须是文件中的第一件事，必须采取在三虚线之间设置的有效 YAML 的形式。这是一个基本的例子： ---description: This is a short description of my page---# The content of my page... Glossary 允许您指定要显示为注释的术语及其各自的定义。根据这些术语，GitBook 将自动构建索引并突出显示这些术语。 GLOSSARY.md 的格式是 h2 标题的列表，以及描述段落： ## TermDefinition for this term## Another termWith it's definition, this can contain bold textand all other kinds of inline markup ... Gitbook 配置 GitBook 允许您使用灵活的配置自定义您的电子书。 这些选项在 book.json 文件中指定。对于不熟悉 JSON 语法的作者，您可以使用 JSONlint 等工具验证语法。 常规设置 变量 描述 root 包含所有图书文件的根文件夹的路径，除了 book.json structure 指定自述文件，摘要，词汇表等的路径，参考 Structure paragraph. title 您的书名，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 description 您的书籍的描述，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 author 作者名。在 GitBook.com 上，这个字段是预填的。 isbn 国际标准书号 ISBN language 本书的语言类型 —— ISO code 。默认值是 en direction 文本阅读顺序。可以是 rtl （从右向左）或 ltr （从左向右），默认值依赖于 language 的值。 gitbook 应该使用的 GitBook 版本。使用 SemVer 规范，并接受类似于 “&gt; = 3.0.0” 的条件。 author 作者姓名，在 GitBook.com 上，这个字段是预先填写的。 例： "author" : "victor zhang" description 电子书的描述，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预先填写的。 例： "description" : "Gitbook 教程" direction 文本的方向。可以是 rtl 或 ltr，默认值取决于语言的值。 例： "direction" : "ltr" gitbook 应该使用的 GitBook 版本。使用 SemVer 规范，接受类似于 &gt;=3.0.0 的条件。 例： "gitbook" : "3.0.0","gitbook" : "&gt;=3.0.0" language Gitbook 使用的语言, 版本 2.6.4 中可选的语言如下： en, ar, bn, cs, de, en, es, fa, fi, fr, he, it, ja, ko, no, pl, pt, ro, ru, sv, uk, vi, zh-hans, zh-tw 例： "language" : "zh-hans", links 在左侧导航栏添加链接信息 例： "links" : &#123; "sidebar" : &#123; "Home" : "https://github.com/dunwu/gitbook-notes" &#125;&#125; root 包含所有图书文件的根文件夹的路径， book.json 文件除外。 例： "root" : "./docs", structure 指定 Readme、Summary、Glossary 和 Languages 对应的文件名。 styles 自定义页面样式， 默认情况下各 generator 对应的 css 文件 例： "styles": &#123; "website": "styles/website.css", "ebook": "styles/ebook.css", "pdf": "styles/pdf.css", "mobi": "styles/mobi.css", "epub": "styles/epub.css"&#125; 例如要使 h1、h2 标签有下边框， 可以在 website.css 中设置 h1,h2 &#123; border-bottom: 1px solid #efeaea;&#125; title 电子书的书名，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预先填写的。 例： "title" : "gitbook-notes", plugins 插件及其配置在 book.json 中指定。有关详细信息。 自 3.0.0 版本开始，GitBook 可以使用主题。有关详细信息，请参阅 the theming section 。 变量 描述 plugins 要加载的插件列表 pluginsConfig 插件的配置 添加插件 "plugins": [ "splitter"] 添加新插件之后需要运行 gitbook install 来安装新的插件 去除自带插件 Gitbook 默认带有 5 个插件： highlight search sharing font-settings livereload "plugins": [ "-search"] structure 除了 root 属性之外，您可以指定 Readme，Summary，Glossary 和 Languages 的名称（而不是使用默认名称，如 README.md）。这些文件必须在项目的根目录下（或 root 的根目录，如果你在 book.json 中配置了 root 属性）。不接受的路径，如：dir / MY_README.md。 变量 描述 structure.readme Readme 文件名（默认值是 README.md ） structure.summary Summary 文件名（默认值是 SUMMARY.md ） structure.glossary Glossary 文件名（默认值是 GLOSSARY.md ） structure.languages Languages 文件名（默认值是 LANGS.md ） pdf 可以使用 book.json 中的一组选项来定制 PDF 输出： Variable Description pdf.pageNumbers 将页码添加到每个页面的底部（默认为 true） pdf.fontSize 基本字体大小（默认是 12） pdf.fontFamily 基本字体样式（默认是 Arial） pdf.paperSize 页面尺寸，选项有： 'a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'legal', 'letter' （默认值是 a4） pdf.margin.top 上边界（默认值是 56） pdf.margin.bottom 下边界（默认值是 56） pdf.margin.right 右边界（默认值是 62） pdf.margin.left 左边界（默认值是 62） 生成电子书 GitBook 可以生成一个网站，但也可以输出内容作为电子书（ePub，Mobi，PDF）。 # Generate a PDF file$ gitbook pdf ./ ./mybook.pdf# Generate an ePub file$ gitbook epub ./ ./mybook.epub# Generate a Mobi file$ gitbook mobi ./ ./mybook.mobi 安装 ebook-convert ebook-convert 可以用来生成电子书（epub，mobi，pdf）。 GNU/Linux 安装 Calibre application. $ sudo aptitude install calibre 在一些 GNU / Linux 发行版中，节点被安装为 nodejs，您需要手动创建一个符号链接： $sudo ln -s /usr/bin/nodejs /usr/bin/node OS X 下载 Calibre application。将 calibre.app 移动到应用程序文件夹后，创建一个符号链接到 ebook-convert 工具： $ sudo ln -s ~/Applications/calibre.app/Contents/MacOS/ebook-convert /usr/bin 您可以使用 $PATH 中的任何目录替换 /usr/bin 。 封面 封面用于所有电子书格式。您可以自己提供一个，也可以使用 autocover plugin 生成一个。 要提供封面，请将 cover.jpg 文件放在书本的根目录下。添加一个 cover_small.jpg 将指定一个较小版本的封面。封面应为 JPEG 文件。 好的封面应该遵守以下准则： cover.jpg 的尺寸为 1800x2360 像素，cover_small.jpg 为 200x262 没有边界 清晰可见的书名 任何重要的文字应该在小版本中可见 Gitbook 部署 托管到 gitbook.com GitBook.com 是使用 GitBook 格式创建和托管图书的在线平台。它提供托管，协作功能和易于使用的编辑器。 创建新书 如下图所示，根据个人需求，选择一个模板创建你的电子书。 设置书的基本信息 clone 到本地 Gitbook.com 会为每本书创建一个 git 仓库。 如下图所示，拷贝 git 地址，然后 git clone 到本地。 发布 在本地按照 Gitbook 规范编辑电子书，然后 git push 到 Gitbook 的远程仓库。 默认访问地址是：https://用户名.gitbooks.io/项目名/content/ 例如：我的用户名为 dunwu，一个电子书项目名为 test，则访问路径是： https://dunwu.gitbooks.io/test/content/ 当然，如果你有自己的域名，也可以设置 Domains 选项，来指定访问路径为你的域。 托管到 Github 如果你不希望使用 Gitbook 的仓库，而是想直接使用 Github 的仓库，也是可以的。 首先，你需要绑定你的 Github 账号。最简单的方式当然就是登录 Gitbook.com 时使用 Github 账号登录方式了。否则，你也可以在 Account Settings 中的 Github 设置选项中去进行绑定。 绑定了 Github 账号后，你可以在新建电子书时，选择从一个指定的 Github 仓库导入电子书项目。参考下图： 只要你指定的 Github 仓库中的文档内容符合 Gitbook 规范，Gitbook 就会自动根据你的每次更新去构建生成电子书网站。 默认访问地址是： https://Github用户名.gitbooks.io/Github 仓库/content/ 例如：我的用户名为 dunwu，Github 仓库名为 gitbook-notes，则访问路径是： https://dunwu.gitbooks.io/gitbook-notes/content/ 托管到 Github Pages 也许你以前也了解 Github 的一个功能： GitHub Pages 。它允许用户在 GitHub 仓库托管你的个人、组织或项目的静态页面（自动识别 html、css、javascript）。 建立 xxx.github.io 仓库 要使用这个特性，首先，你必须建立一个严格遵循以下命名要求的仓库：Github账号名.github.io举例，我的 Github 账号为 dunwu，则这个仓库应该叫 dunwu.github.io。通常，这个仓库被用来作为个人或组织的博客。 建立 gh-pages 分支 完成第 1 步后，在任意一个 Github 仓库中建立一个名为 gh-pages 的分支。只要 gh-pages 中的内容符合一个静态站点要求，就可以在如下地址中进行访问：https://Github用户名.gitbooks.io/Github 仓库 。例如：我的一个 Github 仓库名为 react-notes，则访问路径是：https://dunwu.github.io/react-notes 自动化发布到 gh-pages 如果每次都手动 git push 到远程 gh-pages 分支，略有点麻烦。 怎么实现自动化发布呢？ 有两种方法： 使用 gh-pages 插件 如果你了解 Nodejs，那么最简单的发布方式就是使用 gh-pages 插件。 先在本地安装插件 $ npm i -D gh-pages 然后，在 package.json 文件中添加脚本命令： 如下：-d 命令参数后面是要发布的静态站点内容的目录 "scripts": &#123; "deploy": "gh-pages -d build"&#125;, 脚本 写一个执行 git 命令的脚本就搞定了。 下面的脚本无论是在 bat 或 sh 脚本中都可以执行。 cd buildgit initgit checkout -b gh-pagesgit add .git commit -am "Update"git push git@github.com:dunwu/gitbook-notes gh-pages --force" 参考资料 官方资源 Gitbook Github Gitbook 官网 Gitbook Toolchain 文档 Gitbook 帮助中心 教程资源 gitbook-use by zhangjikai 工具 Gitbook 编辑器]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>gitbook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 注解]]></title>
    <url>%2Fblog%2F2017%2F08%2F22%2Fjava%2Fjavacore%2Fbasics%2FJava%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[深入理解 Java 注解 本文内容基于 JDK8。注解是 JDK5 引入的，后续 JDK 版本扩展了一些内容，本文中没有明确指明版本的注解都是 JDK5 就已经支持的注解。 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「javacore」 简介 注解的形式 什么是注解 注解的作用 注解的代价 注解的应用范围 内置注解 @Override @Deprecated @SuppressWarnnings @SafeVarargs @FunctionalInterface 元注解 @Retention @Documented @Target @Inherited @Repeatable 自定义注解 1. 注解的定义 2. 注解属性 3. 注解处理器 4. 使用注解 小结 参考资料 简介 注解的形式 Java 中，注解是以 @ 字符开始的修饰符。如下： @Overridevoid mySuperMethod() &#123; ... &#125; 注解可以包含命名或未命名的属性，并且这些属性有值。 @Author( name = "Benjamin Franklin", date = "3/27/2003")class MyClass() &#123; ... &#125; 如果只有一个名为 value 的属性，那么名称可以省略，如： @SuppressWarnings("unchecked")void myMethod() &#123; ... &#125; 如果注解没有属性，则称为标记注解。如：@Override。 什么是注解 从本质上来说，注解是一种标签，其实质上可以视为一种特殊的注释，如果没有解析它的代码，它并不比普通注释强。 解析一个注解往往有两种形式： 编译期直接的扫描 - 编译器的扫描指的是编译器在对 java 代码编译字节码的过程中会检测到某个类或者方法被一些注解修饰，这时它就会对于这些注解进行某些处理。这种情况只适用于 JDK 内置的注解类。 运行期的反射 - 如果要自定义注解，Java 编译器无法识别并处理这个注解，它只能根据该注解的作用范围来选择是否编译进字节码文件。如果要处理注解，必须利用反射技术，识别该注解以及它所携带的信息，然后做相应的处理。 注解的作用 注解有许多用途： 编译器信息 - 编译器可以使用注解来检测错误或抑制警告。 编译时和部署时的处理 - 程序可以处理注解信息以生成代码，XML 文件等。 运行时处理 - 可以在运行时检查某些注解并处理。 作为 Java 程序员，多多少少都曾经历过被各种配置文件（xml、properties）支配的恐惧。过多的配置文件会使得项目难以维护。个人认为，使用注解以减少配置文件或代码，是注解最大的用处。 注解的代价 凡事有得必有失，注解技术同样如此。使用注解也有一定的代价： 显然，它是一种侵入式编程，那么，自然就存在着增加程序耦合度的问题。 自定义注解的处理需要在运行时，通过反射技术来获取属性。如果注解所修饰的元素是类的非 public 成员，也可以通过反射获取。这就违背了面向对象的封装性。 注解所产生的问题，相对而言，更难以 debug 或定位。 但是，正所谓瑕不掩瑜，注解所付出的代价，相较于它提供的功能而言，还是可以接受的。 注解的应用范围 注解可以应用于类、字段、方法和其他程序元素的声明。 JDK8 开始，注解的应用范围进一步扩大，以下是新的应用范围： 类实例初始化表达式： new @Interned MyObject(); 类型转换： myString = (@NonNull String) str; 实现接口的声明： class UnmodifiableList&lt;T&gt; implements @Readonly List&lt;@Readonly T&gt; &#123;&#125; 抛出异常声明： void monitorTemperature() throws @Critical TemperatureException &#123;&#125; 内置注解 JDK 中内置了以下注解： @Override @Deprecated @SuppressWarnnings @SafeVarargs（JDK7 引入） @FunctionalInterface（JDK8 引入） @Override @Override 用于表明被修饰方法覆写了父类的方法。 如果试图使用 @Override 标记一个实际上并没有覆写父类的方法时，java 编译器会告警。 @Override 示例： public class OverrideAnnotationDemo &#123; static class Person &#123; public String getName() &#123; return "getName"; &#125; &#125; static class Man extends Person &#123; @Override public String getName() &#123; return "override getName"; &#125; /** * 放开下面的注释，编译时会告警 */ /* @Override public String getName2() &#123; return "override getName2"; &#125; */ &#125; public static void main(String[] args) &#123; Person per = new Man(); System.out.println(per.getName()); &#125;&#125; @Deprecated @Deprecated 用于标明被修饰的类或类成员、类方法已经废弃、过时，不建议使用。 @Deprecated 有一定的延续性：如果我们在代码中通过继承或者覆盖的方式使用了过时的类或类成员，即使子类或子方法没有标记为 @Deprecated，但编译器仍然会告警。 注意： @Deprecated 这个注解类型和 javadoc 中的 @deprecated 这个 tag 是有区别的：前者是 java 编译器识别的；而后者是被 javadoc 工具所识别用来生成文档（包含程序成员为什么已经过时、它应当如何被禁止或者替代的描述）。 @Deprecated 示例： public class DeprecatedAnnotationDemo &#123; static class DeprecatedField &#123; @Deprecated public static final String DEPRECATED_FIELD = "DeprecatedField"; &#125; static class DeprecatedMethod &#123; @Deprecated public String print() &#123; return "DeprecatedMethod"; &#125; &#125; @Deprecated static class DeprecatedClass &#123; public String print() &#123; return "DeprecatedClass"; &#125; &#125; public static void main(String[] args) &#123; System.out.println(DeprecatedField.DEPRECATED_FIELD); DeprecatedMethod dm = new DeprecatedMethod(); System.out.println(dm.print()); DeprecatedClass dc = new DeprecatedClass(); System.out.println(dc.print()); &#125;&#125;//Output://DeprecatedField//DeprecatedMethod//DeprecatedClass @SuppressWarnnings @SuppressWarnings 用于关闭对类、方法、成员编译时产生的特定警告。 @SuppressWarning 不是一个标记注解。它有一个类型为 String[] 的数组成员，这个数组中存储的是要关闭的告警类型。对于 javac 编译器来讲，对 -Xlint 选项有效的警告名也同样对 @SuppressWarings 有效，同时编译器会忽略掉无法识别的警告名。 @SuppressWarning 示例： @SuppressWarnings(&#123;"rawtypes", "unchecked"&#125;)public class SuppressWarningsAnnotationDemo &#123; static class SuppressDemo&lt;T&gt; &#123; private T value; public T getValue() &#123; return this.value; &#125; public void setValue(T var) &#123; this.value = var; &#125; &#125; @SuppressWarnings(&#123;"deprecation"&#125;) public static void main(String[] args) &#123; SuppressDemo d = new SuppressDemo(); d.setValue("南京"); System.out.println("地名：" + d.getValue()); &#125;&#125; @SuppressWarnings 注解的常见参数值的简单说明： deprecation - 使用了不赞成使用的类或方法时的警告； unchecked - 执行了未检查的转换时的警告，例如当使用集合时没有用泛型 (Generics) 来指定集合保存的类型; fallthrough - 当 Switch 程序块直接通往下一种情况而没有 Break 时的警告; path - 在类路径、源文件路径等中有不存在的路径时的警告; serial - 当在可序列化的类上缺少 serialVersionUID 定义时的警告; finally - 任何 finally 子句不能正常完成时的警告; all - 所有的警告。 @SuppressWarnings(&#123;"uncheck", "deprecation"&#125;)public class InternalAnnotationDemo &#123; /** * @SuppressWarnings 标记消除当前类的告警信息 */ @SuppressWarnings(&#123;"deprecation"&#125;) static class A &#123; public void method1() &#123; System.out.println("call method1"); &#125; /** * @Deprecated 标记当前方法为废弃方法，不建议使用 */ @Deprecated public void method2() &#123; System.out.println("call method2"); &#125; &#125; /** * @Deprecated 标记当前类为废弃类，不建议使用 */ @Deprecated static class B extends A &#123; /** * @Override 标记显示指明当前方法覆写了父类或接口的方法 */ @Override public void method1() &#123; &#125; &#125; public static void main(String[] args) &#123; A obj = new B(); obj.method1(); obj.method2(); &#125;&#125; @SafeVarargs @SafeVarargs 在 JDK7 中引入。 @SafeVarargs 的作用是：告诉编译器，在可变长参数中的泛型是类型安全的。可变长参数是使用数组存储的，而数组和泛型不能很好的混合使用。 简单的说，数组元素的数据类型在编译和运行时都是确定的，而泛型的数据类型只有在运行时才能确定下来。因此，当把一个泛型存储到数组中时，编译器在编译阶段无法确认数据类型是否匹配，因此会给出警告信息；即如果泛型的真实数据类型无法和参数数组的类型匹配，会导致 ClassCastException 异常。 @SafeVarargs 注解使用范围： @SafeVarargs 注解可以用于构造方法。 @SafeVarargs 注解可以用于 static 或 final 方法。 @SafeVarargs 示例： public class SafeVarargsAnnotationDemo &#123; /** * 此方法实际上并不安全，不使用此注解，编译时会告警 */ @SafeVarargs static void wrongMethod(List&lt;String&gt;... stringLists) &#123; Object[] array = stringLists; List&lt;Integer&gt; tmpList = Arrays.asList(42); array[0] = tmpList; // 语法错误，但是编译不告警 String s = stringLists[0].get(0); // 运行时报 ClassCastException &#125; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add("A"); list.add("B"); List&lt;String&gt; list2 = new ArrayList&lt;&gt;(); list.add("1"); list.add("2"); wrongMethod(list, list2); &#125;&#125; 以上代码，如果不使用 @SafeVarargs ，编译时会告警 [WARNING] /D:/Codes/ZP/Java/javacore/codes/basics/src/main/java/io/github/dunwu/javacore/annotation/SafeVarargsAnnotationDemo.java: 某些输入文件使用了未经检查或不安全的操作。[WARNING] /D:/Codes/ZP/Java/javacore/codes/basics/src/main/java/io/github/dunwu/javacore/annotation/SafeVarargsAnnotationDemo.java: 有关详细信息, 请使用 -Xlint:unchecked 重新编译。 @FunctionalInterface @FunctionalInterface 在 JDK8 引入。 @FunctionalInterface 用于指示被修饰的接口是函数式接口。 需要注意的是，如果一个接口符合&quot;函数式接口&quot;定义，不加 @FunctionalInterface 也没关系；但如果编写的不是函数式接口，却使用 @FunctionInterface，那么编译器会报错。 什么是函数式接口？ 函数式接口(Functional Interface)就是一个有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。函数式接口可以被隐式转换为 lambda 表达式。 函数式接口的特点： 接口有且只能有个一个抽象方法（抽象方法只有方法定义，没有方法体）。 不能在接口中覆写 Object 类中的 public 方法（写了编译器也会报错）。 允许有 default 实现方法。 示例： public class FunctionalInterfaceAnnotationDemo &#123; @FunctionalInterface public interface Func1&lt;T&gt; &#123; void printMessage(T message); &#125; /** * @FunctionalInterface 修饰的接口中定义两个抽象方法，编译时会报错 * @param &lt;T&gt; */ /*@FunctionalInterface public interface Func2&lt;T&gt; &#123; void printMessage(T message); void printMessage2(T message); &#125;*/ public static void main(String[] args) &#123; Func1 func1 = message -&gt; System.out.println(message); func1.printMessage("Hello"); func1.printMessage(100); &#125;&#125; 元注解 JDK 中虽然内置了几个注解，但这远远不能满足开发过程中遇到的千变万化的需求。所以我们需要自定义注解，而这就需要用到元注解。 元注解的作用就是用于定义其它的注解。 Java 中提供了以下元注解类型： @Retention @Target @Documented @Inherited（JDK8 引入） @Repeatable（JDK8 引入） 这些类型和它们所支持的类在 java.lang.annotation 包中可以找到。下面我们看一下每个元注解的作用和相应分参数的使用说明。 @Retention @Retention 指明了注解的保留级别。 @Retention 源码： @Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Retention &#123; RetentionPolicy value();&#125; RetentionPolicy 是一个枚举类型，它定义了被 @Retention 修饰的注解所支持的保留级别： RetentionPolicy.SOURCE - 标记的注解仅在源文件中有效，编译器会忽略。 RetentionPolicy.CLASS - 标记的注解在 class 文件中有效，JVM 会忽略。 RetentionPolicy.RUNTIME - 标记的注解在运行时有效。 @Retention 示例： @Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)public @interface Column &#123; public String name() default "fieldName"; public String setFuncName() default "setField"; public String getFuncName() default "getField"; public boolean defaultDBValue() default false;&#125; @Documented @Documented 表示无论何时使用指定的注解，都应使用 Javadoc（默认情况下，注释不包含在 Javadoc 中）。更多内容可以参考：Javadoc tools page。 @Documented 示例： @Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Column &#123; public String name() default "fieldName"; public String setFuncName() default "setField"; public String getFuncName() default "getField"; public boolean defaultDBValue() default false;&#125; @Target @Target 指定注解可以修饰的元素类型。 @Target 源码： @Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.ANNOTATION_TYPE)public @interface Target &#123; ElementType[] value();&#125; ElementType 是一个枚举类型，它定义了被 @Target 修饰的注解可以应用的范围： ElementType.ANNOTATION_TYPE - 标记的注解可以应用于注解类型。 ElementType.CONSTRUCTOR - 标记的注解可以应用于构造函数。 ElementType.FIELD - 标记的注解可以应用于字段或属性。 ElementType.LOCAL_VARIABLE - 标记的注解可以应用于局部变量。 ElementType.METHOD - 标记的注解可以应用于方法。 ElementType.PACKAGE - 标记的注解可以应用于包声明。 ElementType.PARAMETER - 标记的注解可以应用于方法的参数。 ElementType.TYPE - 标记的注解可以应用于类的任何元素。 @Target 示例： @Target(ElementType.TYPE)public @interface Table &#123; /** * 数据表名称注解，默认值为类名称 * @return */ public String tableName() default "className";&#125;@Target(ElementType.FIELD)public @interface NoDBColumn &#123;&#125; @Inherited @Inherited 表示注解类型可以被继承（默认情况下不是这样）。 表示自动继承注解类型。 如果注解类型声明中存在 @Inherited 元注解，则注解所修饰类的所有子类都将会继承此注解。 注意：@Inherited 注解类型是被标注过的类的子类所继承。类并不从它所实现的接口继承注解，方法并不从它所覆写的方法继承注解。 此外，当 @Inherited 类型标注的注解的 @Retention 是 RetentionPolicy.RUNTIME，则反射 API 增强了这种继承性。如果我们使用 java.lang.reflect 去查询一个 @Inherited 类型的注解时，反射代码检查将展开工作：检查类和其父类，直到发现指定的注解类型被发现，或者到达类继承结构的顶层。 @Inheritedpublic @interface Greeting &#123; public enum FontColor&#123; BULE,RED,GREEN&#125;; String name(); FontColor fontColor() default FontColor.GREEN;&#125; @Repeatable @Repeatable 表示注解可以重复使用。 以 Spring @Scheduled 为例： @Target(&#123;ElementType.METHOD, ElementType.ANNOTATION_TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Schedules &#123; Scheduled[] value();&#125;@Target(&#123;ElementType.METHOD, ElementType.ANNOTATION_TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Repeatable(Schedules.class)public @interface Scheduled &#123; // ...&#125; 应用示例： public class TaskRunner &#123; @Scheduled("0 0/15 * * * ?") @Scheduled("0 0 12 * ?") public void task1() &#123;&#125;&#125; 自定义注解 使用 @interface 自定义注解时，自动继承了 java.lang.annotation.Annotation 接口，由编译程序自动完成其他细节。在定义注解时，不能继承其他的注解或接口。@interface 用来声明一个注解，其中的每一个方法实际上是声明了一个配置参数。方法的名称就是参数的名称，返回值类型就是参数的类型（返回值类型只能是基本类型、Class、String、enum）。可以通过 default 来声明参数的默认值。 这里，我会通过实现一个名为 RegexValid 的正则校验注解工具来展示自定义注解的全步骤。 1. 注解的定义 注解的语法格式如下： public @interface 注解名 &#123;定义体&#125; 我们来定义一个注解： @Documented@Target(&#123;ElementType.FIELD, ElementType.PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface RegexValid &#123;&#125; 说明： 通过上一节对于元注解 @Target、@Retention、@Documented 的说明，这里就很容易理解了。 上面的代码中定义了一个名为 @RegexValid 的注解。 @Documented 表示 @RegexValid 应该使用 javadoc。 @Target({ElementType.FIELD, ElementType.PARAMETER}) 表示 @RegexValid 可以在类成员或方法参数上修饰。 @Retention(RetentionPolicy.RUNTIME) 表示 @RegexValid 在运行时有效。 此时，我们已经定义了一个没有任何属性的注解，如果到此为止，它仅仅是一个标记注解。作为正则工具，没有属性可什么也做不了。接下来，我们将为它添加注解属性。 2. 注解属性 注解属性的语法形式如下： [访问级别修饰符] [数据类型] 名称() default 默认值; 例如，我们要定义在注解中定义一个名为 value 的字符串属性，其默认值为空字符串，访问级别为默认级别，那么应该定义如下： String value() default ""; 注意：在注解中，我们定义属性时，属性名后面需要加 ()。 定义注解属性有以下要点： 注解属性只能使用 public 或默认访问级别（即不指定访问级别修饰符）修饰。 注解属性的数据类型有限制要求。支持的数据类型如下： 所有基本数据类型（byte、char、short、int、long、float、double、boolean） String 类型 Class 类 enum 类型 Annotation 类型 以上所有类型的数组 注解属性必须有确定的值，建议指定默认值。注解属性只能通过指定默认值或使用注解时指定属性值，相较之下，指定默认值的方式更为可靠。注解属性如果是引用类型，不可以为 null。这个约束使得注解处理器很难判断注解属性是默认值，或是使用注解时所指定的属性值。为此，我们设置默认值时，一般会定义一些特殊的值，例如空字符串或者负数。 如果注解中只有一个属性值，最好将其命名为 value。因为，指定属性名为 value，在使用注解时，指定 value 的值可以不指定属性名称。 // 这两种方式效果相同@RegexValid("^((\\+)?86\\s*)?((13[0-9])|(15([0-3]|[5-9]))|(18[0,2,5-9]))\\d&#123;8&#125;$")@RegexValid(value = "^((\\+)?86\\s*)?((13[0-9])|(15([0-3]|[5-9]))|(18[0,2,5-9]))\\d&#123;8&#125;$") 示例： 了解了注解属性的定义要点，让我们来为 @RegexValid 注解定义几个属性。 @Documented@Target(&#123;ElementType.FIELD, ElementType.PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface RegexValid &#123; enum Policy &#123; // @formatter:off EMPTY(null), DATE("^(?:(?!0000)[0-9]&#123;4&#125;([-/.]?)(?:(?:0?[1-9]|1[0-2])\\1(?:0?[1-9]|1[0-9]|2[0-8])|(?:0?[13-9]|1[0-2])\\1" + "(?:29|30)|(?:0?[13578]|1[02])\\1(?:31))|(?:[0-9]&#123;2&#125;(?:0[48]|[2468][048]|[13579][26])|" + "(?:0[48]|[2468][048]|[13579][26])00)([-/.]?)0?2\\2(?:29))$"), MAIL("^[A-Za-z0-9](([_\\.\\-]?[a-zA-Z0-9]+)*)@([A-Za-z0-9]+)(([\\.\\-]?[a-zA-Z0-9]+)*)\\.([A-Za-z]&#123;2,&#125;)$"); // @formatter:on private String policy; Policy(String policy) &#123; this.policy = policy; &#125; public String getPolicy() &#123; return policy; &#125; &#125; String value() default ""; Policy policy() default Policy.EMPTY;&#125; 说明： 在上面的示例代码中，我们定义了两个注解属性：String 类型的 value 属性和 Policy 枚举类型的 policy 属性。Policy 枚举中定义了几个默认的正则表达式，这是为了直接使用这几个常用表达式去正则校验。考虑到，我们可能需要自己传入一些自定义正则表达式去校验其他场景，所以定义了 value 属性，允许使用者传入正则表达式。 至此，@RegexValid 的声明已经结束。但是，程序仍不知道如何处理 @RegexValid 这个注解。我们还需要定义注解处理器。 3. 注解处理器 如果没有用来读取注解的方法和工作，那么注解也就不会比注释更有用处了。使用注解的过程中，很重要的一部分就是创建于使用注解处理器。JDK5 扩展了反射机制的 API，以帮助程序员快速的构造自定义注解处理器。 java.lang.annotation.Annotation 是一个接口，程序可以通过反射来获取指定程序元素的注解对象，然后通过注解对象来获取注解里面的元数据。 Annotation 接口源码如下： public interface Annotation &#123; boolean equals(Object obj); int hashCode(); String toString(); Class&lt;? extends Annotation&gt; annotationType();&#125; 除此之外，Java 中支持注解处理器接口 java.lang.reflect.AnnotatedElement ，该接口代表程序中可以接受注解的程序元素，该接口主要有如下几个实现类： Class - 类定义 Constructor - 构造器定义 Field - 累的成员变量定义 Method - 类的方法定义 Package - 类的包定义 java.lang.reflect 包下主要包含一些实现反射功能的工具类。实际上，java.lang.reflect 包所有提供的反射 API 扩充了读取运行时注解信息的能力。当一个注解类型被定义为运行时的注解后，该注解才能是运行时可见，当 class 文件被装载时被保存在 class 文件中的注解才会被虚拟机读取。 AnnotatedElement 接口是所有程序元素（Class、Method 和 Constructor）的父接口，所以程序通过反射获取了某个类的AnnotatedElement 对象之后，程序就可以调用该对象的如下四个个方法来访问注解信息： getAnnotation - 返回该程序元素上存在的、指定类型的注解，如果该类型注解不存在，则返回 null。 getAnnotations - 返回该程序元素上存在的所有注解。 isAnnotationPresent - 判断该程序元素上是否包含指定类型的注解，存在则返回 true，否则返回 false。 getDeclaredAnnotations - 返回直接存在于此元素上的所有注释。与此接口中的其他方法不同，该方法将忽略继承的注释。（如果没有注释直接存在于此元素上，则返回长度为零的一个数组。）该方法的调用者可以随意修改返回的数组；这不会对其他调用者返回的数组产生任何影响。 了解了以上内容，让我们来实现 @RegexValid 的注解处理器： import java.lang.reflect.Field;import java.util.regex.Matcher;import java.util.regex.Pattern;public class RegexValidUtil &#123; public static boolean check(Object obj) throws Exception &#123; boolean result = true; StringBuilder sb = new StringBuilder(); Field[] fields = obj.getClass().getDeclaredFields(); for (Field field : fields) &#123; // 判断成员是否被 @RegexValid 注解所修饰 if (field.isAnnotationPresent(RegexValid.class)) &#123; RegexValid valid = field.getAnnotation(RegexValid.class); // 如果 value 为空字符串，说明没有注入自定义正则表达式，改用 policy 属性 String value = valid.value(); if ("".equals(value)) &#123; RegexValid.Policy policy = valid.policy(); value = policy.getPolicy(); &#125; // 通过设置 setAccessible(true) 来访问私有成员 field.setAccessible(true); Object fieldObj = null; try &#123; fieldObj = field.get(obj); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; if (fieldObj == null) &#123; sb.append("\n") .append(String.format("%s 类中的 %s 字段不能为空！", obj.getClass().getName(), field.getName())); result = false; &#125; else &#123; if (fieldObj instanceof String) &#123; String text = (String) fieldObj; Pattern p = Pattern.compile(value); Matcher m = p.matcher(text); result = m.matches(); if (!result) &#123; sb.append("\n").append(String.format("%s 不是合法的 %s ！", text, field.getName())); &#125; &#125; else &#123; sb.append("\n").append( String.format("%s 类中的 %s 字段不是字符串类型，不能使用此注解校验！", obj.getClass().getName(), field.getName())); result = false; &#125; &#125; &#125; &#125; if (sb.length() &gt; 0) &#123; throw new Exception(sb.toString()); &#125; return result; &#125;&#125; 说明： 以上示例中的注解处理器，执行步骤如下： 通过 getDeclaredFields 反射方法获取传入对象的所有成员。 遍历成员，使用 isAnnotationPresent 判断成员是否被指定注解所修饰，如果不是，直接跳过。 如果成员被注解所修饰，通过 RegexValid valid = field.getAnnotation(RegexValid.class); 这样的形式获取，注解实例化对象，然后，就可以使用 valid.value() 或 valid.policy() 这样的形式获取注解中设定的属性值。 根据属性值，进行逻辑处理。 4. 使用注解 完成了以上工作，我们就可以使用自定义注解了，示例如下： public class RegexValidDemo &#123; static class User &#123; private String name; @RegexValid(policy = RegexValid.Policy.DATE) private String date; @RegexValid(policy = RegexValid.Policy.MAIL) private String mail; @RegexValid("^((\\+)?86\\s*)?((13[0-9])|(15([0-3]|[5-9]))|(18[0,2,5-9]))\\d&#123;8&#125;$") private String phone; public User(String name, String date, String mail, String phone) &#123; this.name = name; this.date = date; this.mail = mail; this.phone = phone; &#125; @Override public String toString() &#123; return "User&#123;" + "name='" + name + '\'' + ", date='" + date + '\'' + ", mail='" + mail + '\'' + ", phone='" + phone + '\'' + '&#125;'; &#125; &#125; static void printDate(@RegexValid(policy = RegexValid.Policy.DATE) String date)&#123; System.out.println(date); &#125; public static void main(String[] args) throws Exception &#123; User user = new User("Tom", "1990-01-31", "xxx@163.com", "18612341234"); User user2 = new User("Jack", "2019-02-29", "sadhgs", "183xxxxxxxx"); if (RegexValidUtil.check(user)) &#123; System.out.println(user + "正则校验通过"); &#125; if (RegexValidUtil.check(user2)) &#123; System.out.println(user2 + "正则校验通过"); &#125; &#125;&#125; 小结 参考资料 Java 编程思想 JAVA 核心技术（卷 1） Effective java Oracle 官方文档之注解篇 深入理解 Java：注解（Annotation）自定义注解入门 https://blog.csdn.net/briblue/article/details/73824058]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
        <tag>annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ActiveMQ]]></title>
    <url>%2Fblog%2F2017%2F08%2F18%2Fjava%2Fjavaweb%2Fdistributed%2Fmq%2FActiveMQ%2F</url>
    <content type="text"><![CDATA[ActiveMQ 1. JMS 基本概念 1.1. 消息模型 1.2. JMS 编程模型 2. 安装 3. 项目中的应用 4. 资源 1. JMS 基本概念 JMS 即 Java 消息服务（Java Message Service）API，是一个 Java 平台中关于面向消息中间件的 API。它用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java 消息服务是一个与具体平台无关的 API，绝大多数 MOM 提供商都对 JMS 提供支持。 1.1. 消息模型 JMS 有两种消息模型： Point-to-Point(P2P) Publish/Subscribe(Pub/Sub) P2P 的特点 在点对点的消息系统中，消息分发给一个单独的使用者。点对点消息往往与队列 javax.jms.Queue 相关联。 每个消息只有一个消费者（Consumer）(即一旦被消费，消息就不再在消息队列中)。 发送者和接收者之间在时间上没有依赖性，也就是说当发送者发送了消息之后，不管接收者有没有正在运行，它不会影响到消息被发送到队列。 接收者在成功接收消息之后需向队列应答成功。 如果你希望发送的每个消息都应该被成功处理的话，那么你需要 P2P 模式。 Pub/Sub 的特点 发布/订阅消息系统支持一个事件驱动模型，消息生产者和消费者都参与消息的传递。生产者发布事件，而使用者订阅感兴趣的事件，并使用事件。该类型消息一般与特定的主题 javax.jms.Topic 关联。 每个消息可以有多个消费者。 发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息，而且为了消费消息，订阅者必须保持运行的状态。 为了缓和这样严格的时间相关性，JMS 允许订阅者创建一个可持久化的订阅。这样，即使订阅者没有被激活（运行），它也能接收到发布者的消息。 如果你希望发送的消息可以不被做任何处理、或者被一个消息者处理、或者可以被多个消费者处理的话，那么可以采用 Pub/Sub 模型。 1.2. JMS 编程模型 ConnectionFactory 创建 Connection 对象的工厂，针对两种不同的 jms 消息模型，分别有 QueueConnectionFactory 和TopicConnectionFactory 两种。可以通过 JNDI 来查找 ConnectionFactory 对象。 Connection Connection 表示在客户端和 JMS 系统之间建立的链接（对 TCP/IP socket 的包装）。Connection 可以产生一个或多个Session。跟 ConnectionFactory 一样，Connection 也有两种类型：QueueConnection 和 TopicConnection。 Destination Destination 是一个包装了消息目标标识符的被管对象。消息目标是指消息发布和接收的地点，或者是队列 Queue ，或者是主题 Topic 。JMS 管理员创建这些对象，然后用户通过 JNDI 发现它们。和连接工厂一样，管理员可以创建两种类型的目标，点对点模型的 Queue，以及发布者/订阅者模型的 Topic。 Session Session 表示一个单线程的上下文，用于发送和接收消息。由于会话是单线程的，所以消息是连续的，就是说消息是按照发送的顺序一个一个接收的。会话的好处是它支持事务。如果用户选择了事务支持，会话上下文将保存一组消息，直到事务被提交才发送这些消息。在提交事务之前，用户可以使用回滚操作取消这些消息。一个会话允许用户创建消息，生产者来发送消息，消费者来接收消息。同样，Session 也分 QueueSession 和 TopicSession。 MessageConsumer MessageConsumer 由 Session 创建，并用于将消息发送到 Destination。消费者可以同步地（阻塞模式），或（非阻塞）接收 Queue 和 Topic 类型的消息。同样，消息生产者分两种类型：QueueSender 和TopicPublisher。 MessageProducer MessageProducer 由 Session 创建，用于接收被发送到 Destination 的消息。MessageProducer 有两种类型：QueueReceiver 和 TopicSubscriber。可分别通过 session 的 createReceiver(Queue) 或 createSubscriber(Topic) 来创建。当然，也可以 session 的 creatDurableSubscriber 方法来创建持久化的订阅者。 Message 是在消费者和生产者之间传送的对象，也就是说从一个应用程序传送到另一个应用程序。一个消息有三个主要部分： 消息头（必须）：包含用于识别和为消息寻找路由的操作设置。 一组消息属性（可选）：包含额外的属性，支持其他提供者和用户的兼容。可以创建定制的字段和过滤器（消息选择器）。 一个消息体（可选）：允许用户创建五种类型的消息（文本消息，映射消息，字节消息，流消息和对象消息）。 消息接口非常灵活，并提供了许多方式来定制消息的内容。 Common Point-to-Point Publish-Subscribe ConnectionFactory QueueConnectionFactory TopicConnectionFactory Connection QueueConnection TopicConnection Destination Queue Topic Session QueueSession TopicSession MessageProducer QueueSender TopicPublisher MessageSender QueueReceiver, QueueBrowser TopicSubscriber 2. 安装 安装条件 JDK1.7 及以上版本 本地配置了 JAVA_HOME 环境变量。 下载 支持 Windows/Unix/Linux/Cygwin ActiveMQ 官方下载地址 Windows 下运行 解压压缩包到本地； 打开控制台，进入安装目录的 bin 目录下； cd [activemq_install_dir] 执行 activemq start 来启动 ActiveMQ bin\activemq start 测试安装是否成功 ActiveMQ 默认监听端口为 61616 netstat -an|find “61616” 访问 http://127.0.0.1:8161/admin/ 输入用户名、密码 Login: admin Passwort: admin 点击导航栏的 Queues 菜单 添加一个队列（queue） 3. 项目中的应用 引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt; &lt;version&gt;5.14.1&lt;/version&gt;&lt;/dependency&gt; Sender.java public class Sender &#123; private static final int SEND_NUMBER = 4; public static void main(String[] args) &#123; // ConnectionFactory ：连接工厂，JMS 用它创建连接 ConnectionFactory connectionFactory; // Connection ：JMS 客户端到JMS Provider 的连接 Connection connection = null; // Session： 一个发送或接收消息的线程 Session session; // Destination ：消息的目的地;消息发送给谁. Destination destination; // MessageProducer：消息发送者 MessageProducer producer; // TextMessage message; // 构造ConnectionFactory实例对象，此处采用ActiveMq的实现jar connectionFactory = new ActiveMQConnectionFactory( ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, "tcp://localhost:61616"); try &#123; // 构造从工厂得到连接对象 connection = connectionFactory.createConnection(); // 启动 connection.start(); // 获取操作连接 session = connection.createSession(Boolean.TRUE, Session.AUTO_ACKNOWLEDGE); // 获取session注意参数值xingbo.xu-queue是一个服务器的queue，须在在ActiveMq的console配置 destination = session.createQueue("FirstQueue"); // 得到消息生成者【发送者】 producer = session.createProducer(destination); // 设置不持久化，此处学习，实际根据项目决定 producer.setDeliveryMode(DeliveryMode.NON_PERSISTENT); // 构造消息，此处写死，项目就是参数，或者方法获取 sendMessage(session, producer); session.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (null != connection) connection.close(); &#125; catch (Throwable ignore) &#123; &#125; &#125; &#125; public static void sendMessage(Session session, MessageProducer producer) throws Exception &#123; for (int i = 1; i &lt;= SEND_NUMBER; i++) &#123; TextMessage message = session .createTextMessage("ActiveMq 发送的消息" + i); // 发送消息到目的地方 System.out.println("发送消息：" + "ActiveMq 发送的消息" + i); producer.send(message); &#125; &#125;&#125; Receiver.java public class Receiver &#123; public static void main(String[] args) &#123; // ConnectionFactory ：连接工厂，JMS 用它创建连接 ConnectionFactory connectionFactory; // Connection ：JMS 客户端到JMS Provider 的连接 Connection connection = null; // Session： 一个发送或接收消息的线程 Session session; // Destination ：消息的目的地;消息发送给谁. Destination destination; // 消费者，消息接收者 MessageConsumer consumer; connectionFactory = new ActiveMQConnectionFactory( ActiveMQConnection.DEFAULT_USER, ActiveMQConnection.DEFAULT_PASSWORD, "tcp://localhost:61616"); try &#123; // 构造从工厂得到连接对象 connection = connectionFactory.createConnection(); // 启动 connection.start(); // 获取操作连接 session = connection.createSession(Boolean.FALSE, Session.AUTO_ACKNOWLEDGE); // 获取session注意参数值xingbo.xu-queue是一个服务器的queue，须在在ActiveMq的console配置 destination = session.createQueue("FirstQueue"); consumer = session.createConsumer(destination); while (true) &#123; //设置接收者接收消息的时间，为了便于测试，这里谁定为100s TextMessage message = (TextMessage) consumer.receive(100000); if (null != message) &#123; System.out.println("收到消息" + message.getText()); &#125; else &#123; break; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (null != connection) connection.close(); &#125; catch (Throwable ignore) &#123; &#125; &#125; &#125;&#125; 运行 先运行 Receiver.java 进行消息监听，再运行 Send.java 发送消息。 输出 Send 的输出内容 发送消息：Activemq 发送消息0发送消息：Activemq 发送消息1发送消息：Activemq 发送消息2发送消息：Activemq 发送消息3 Receiver 的输出内容 收到消息ActiveMQ 发送消息0收到消息ActiveMQ 发送消息1收到消息ActiveMQ 发送消息2收到消息ActiveMQ 发送消息3 4. 资源 ActiveMQ 官网 oracle 官方的 jms 介绍]]></content>
      <categories>
        <category>javaweb</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaweb</tag>
        <tag>分布式</tag>
        <tag>mq</tag>
        <tag>jms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jsoup 使用小结]]></title>
    <url>%2Fblog%2F2017%2F08%2F18%2Fjava%2Fjavalib%2Fjsoup%2F</url>
    <content type="text"><![CDATA[jsoup 使用小结 概述 jsoup 是一款 Java 的 HTML 解析器，可直接解析某个 URL 地址、HTML 文本内容。它提供了一套非常省力的 API，可通过 DOM，CSS 以及类似于 JQuery 的操作方法来取出和操作数据。 jsoup 工作的流程主要如下： 从一个 URL，文件或字符串中解析 HTML，并加载为一个 Document 对象。 使用 DOM 或 CSS 选择器来取出数据； 可操作 HTML 元素、属性、文本。 jsoup 是基于 MIT 协议发布的，可放心使用于商业项目。 加载 从HTML字符串加载一个文档 使用静态 Jsoup.parse(String html) 方法或 Jsoup.parse(String html, String baseUri) 示例代码： String html = "&lt;html&gt;&lt;head&gt;&lt;title&gt;First parse&lt;/title&gt;&lt;/head&gt;" + "&lt;body&gt;&lt;p&gt;Parsed HTML into a doc.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;";Document doc = Jsoup.parse(html); 说明 parse(String html, String baseUri) 这方法能够将输入的HTML解析为一个新的文档 (Document），参数 baseUri 是用来将相对 URL 转成绝对URL，并指定从哪个网站获取文档。如这个方法不适用，你可以使用 parse(String html) 方法来解析成HTML字符串如上面的示例。 只要解析的不是空字符串，就能返回一个结构合理的文档，其中包含(至少) 一个head和一个body元素。 一旦拥有了一个Document，你就可以使用Document中适当的方法或它父类 Element和Node中的方法来取得相关数据。 解析一个body片断 问题 假如你有一个HTML片断 (比如. 一个 div 包含一对 p 标签; 一个不完整的HTML文档) 想对它进行解析。这个HTML片断可以是用户提交的一条评论或在一个CMS页面中编辑body部分。 办法 使用Jsoup.parseBodyFragment(String html)方法. String html = "&lt;div&gt;&lt;p&gt;Lorem ipsum.&lt;/p&gt;";Document doc = Jsoup.parseBodyFragment(html);Element body = doc.body(); 说明 parseBodyFragment 方法创建一个空壳的文档，并插入解析过的HTML到body元素中。假如你使用正常的 Jsoup.parse(String html) 方法，通常你也可以得到相同的结果，但是明确将用户输入作为 body片段处理，以确保用户所提供的任何糟糕的HTML都将被解析成body元素。 Document.body() 方法能够取得文档body元素的所有子元素，与 doc.getElementsByTag(&quot;body&quot;)相同。 保证安全Stay safe 假如你可以让用户输入HTML内容，那么要小心避免跨站脚本攻击。利用基于 Whitelist 的清除器和 clean(String bodyHtml, Whitelist whitelist)方法来清除用户输入的恶意内容。 从URL加载一个文档 使用 Jsoup.connect(String url)方法 Document doc = Jsoup.connect("http://example.com/").get(); 说明 connect(String url) 方法创建一个新的 Connection, 和 get() 取得和解析一个HTML文件。如果从该URL获取HTML时发生错误，便会抛出 IOException，应适当处理。 Connection 接口还提供一个方法链来解决特殊请求，具体如下： Document doc = Jsoup.connect("http://example.com") .data("query", "Java") .userAgent("Mozilla") .cookie("auth", "token") .timeout(3000) .post(); 从一个文件加载一个文档 可以使用静态 Jsoup.parse(File in, String charsetName, String baseUri) 方法 File input = new File("/tmp/input.html");Document doc = Jsoup.parse(input, "UTF-8", "http://example.com/"); 说明 parse(File in, String charsetName, String baseUri) 这个方法用来加载和解析一个HTML文件。如在加载文件的时候发生错误，将抛出IOException，应作适当处理。 baseUri 参数用于解决文件中URLs是相对路径的问题。如果不需要可以传入一个空的字符串。 另外还有一个方法parse(File in, String charsetName) ，它使用文件的路径做为 baseUri。 这个方法适用于如果被解析文件位于网站的本地文件系统，且相关链接也指向该文件系统。 解析 使用DOM方法来遍历一个文档 问题 你有一个HTML文档要从中提取数据，并了解这个HTML文档的结构。 方法 将HTML解析成一个Document之后，就可以使用类似于DOM的方法进行操作。示例代码： File input = new File("/tmp/input.html");Document doc = Jsoup.parse(input, "UTF-8", "http://example.com/");Element content = doc.getElementById("content");Elements links = content.getElementsByTag("a");for (Element link : links) &#123; String linkHref = link.attr("href"); String linkText = link.text();&#125; 说明 Elements 这个对象提供了一系列类似于 DOM 的方法来查找元素，抽取并处理其中的数据。 具体如下： ####查找元素 getElementById(String id) getElementsByTag(String tag) getElementsByClass(String className) getElementsByAttribute(String key) (and related methods) Element siblings: siblingElements(), firstElementSibling(), lastElementSibling();nextElementSibling(), previousElementSibling() Graph: parent(), children(), child(int index) ####元素数据 attr(String key)获取属性attr(String key, String value)设置属性 attributes()获取所有属性 id(), className() and classNames() text()获取文本内容text(String value) 设置文本内容 html()获取元素内HTMLhtml(String value)设置元素内的HTML内容 outerHtml()获取元素外HTML内容 data()获取数据内容（例如：script和style标签) tag() and tagName() ####操作HTML和文本 append(String html), prepend(String html) appendText(String text), prependText(String text) appendElement(String tagName), prependElement(String tagName) html(String value) 使用选择器语法来查找元素 问题 你想使用类似于CSS或jQuery的语法来查找和操作元素。 方法 可以使用Element.select(String selector) 和 Elements.select(String selector) 方法实现： File input = new File("/tmp/input.html");Document doc = Jsoup.parse(input, "UTF-8", "http://example.com/");Elements links = doc.select("a[href]"); //带有href属性的a元素Elements pngs = doc.select("img[src$=.png]"); //扩展名为.png的图片Element masthead = doc.select("div.masthead").first(); //class等于masthead的div标签Elements resultLinks = doc.select("h3.r &gt; a"); //在h3元素之后的a元素 说明 jsoup elements对象支持类似于CSS (或jquery)的选择器语法，来实现非常强大和灵活的查找功能。. 这个select 方法在Document, Element,或Elements对象中都可以使用。且是上下文相关的，因此可实现指定元素的过滤，或者链式选择访问。 Select方法将返回一个Elements集合，并提供一组方法来抽取和处理结果。 ####Selector选择器概述 tagname: 通过标签查找元素，比如：a ns|tag: 通过标签在命名空间查找元素，比如：可以用 fb|name 语法来查找 `` 元素 #id: 通过ID查找元素，比如：#logo .class: 通过class名称查找元素，比如：.masthead [attribute]: 利用属性查找元素，比如：[href] [^attr]: 利用属性名前缀来查找元素，比如：可以用[^data-] 来查找带有HTML5 Dataset属性的元素 [attr=value]: 利用属性值来查找元素，比如：[width=500] [attr^=value], [attr$=value], [attr*=value]: 利用匹配属性值开头、结尾或包含属性值来查找元素，比如：[href*=/path/] [attr\~=regex]: 利用属性值匹配正则表达式来查找元素，比如： img[src\~=(?i)\.(png|jpe?g)] *: 这个符号将匹配所有元素 ####Selector选择器组合使用 el##id: 元素+ID，比如： div##logo el.class: 元素+class，比如： div.masthead el[attr]: 元素+class，比如： a[href] 任意组合，比如：a[href].highlight ancestor child: 查找某个元素下子元素，比如：可以用.body p 查找在&quot;body&quot;元素下的所有p元素 parent &gt; child: 查找某个父元素下的直接子元素，比如：可以用div.content &gt; p 查找 p 元素，也可以用body &gt; * 查找body标签下所有直接子元素 siblingA + siblingB: 查找在A元素之前第一个同级元素B，比如：div.head + div siblingA \~ siblingX: 查找A元素之前的同级X元素，比如：h1 \~ p el, el, el:多个选择器组合，查找匹配任一选择器的唯一元素，例如：div.masthead, div.logo ####伪选择器selectors :lt(n): 查找哪些元素的同级索引值（它的位置在DOM树中是相对于它的父节点）小于n，比如：td:lt(3) 表示小于三列的元素 :gt(n):查找哪些元素的同级索引值大于n``，比如： div p:gt(2)表示哪些div中有包含2个以上的p元素 :eq(n): 查找哪些元素的同级索引值与n相等，比如：form input:eq(1)表示包含一个input标签的Form元素 :has(seletor): 查找匹配选择器包含元素的元素，比如：div:has(p)表示哪些div包含了p元素 :not(selector): 查找与选择器不匹配的元素，比如： div:not(.logo) 表示不包含 class=logo 元素的所有 div 列表 :contains(text): 查找包含给定文本的元素，搜索不区分大不写，比如： p:contains(jsoup) :containsOwn(text): 查找直接包含给定文本的元素 :matches(regex): 查找哪些元素的文本匹配指定的正则表达式，比如：div:matches((?i)login) :matchesOwn(regex): 查找自身包含文本匹配指定正则表达式的元素 注意：上述伪选择器索引是从0开始的，也就是说第一个元素索引值为0，第二个元素index为1等 可以查看Selector API参考来了解更详细的内容 从元素抽取属性，文本和HTML 问题 在解析获得一个Document实例对象，并查找到一些元素之后，你希望取得在这些元素中的数据。 方法 要取得一个属性的值，可以使用Node.attr(String key) 方法 对于一个元素中的文本，可以使用Element.text()方法 对于要取得元素或属性中的HTML内容，可以使用Element.html(), 或 Node.outerHtml()方法 示例： String html = "&lt;p&gt;An &lt;a href='http://example.com/'&gt;&lt;b&gt;example&lt;/b&gt;&lt;/a&gt; link.&lt;/p&gt;";Document doc = Jsoup.parse(html);//解析HTML字符串返回一个Document实现Element link = doc.select("a").first();//查找第一个a元素String text = doc.body().text(); // "An example link"//取得字符串中的文本String linkHref = link.attr("href"); // "http://example.com/"//取得链接地址String linkText = link.text(); // "example""//取得链接地址中的文本String linkOuterH = link.outerHtml(); // "&lt;a href="http://example.com"&gt;&lt;b&gt;example&lt;/b&gt;&lt;/a&gt;"String linkInnerH = link.html(); // "&lt;b&gt;example&lt;/b&gt;"//取得链接内的html内容 说明 上述方法是元素数据访问的核心办法。此外还其它一些方法可以使用： Element.id() Element.tagName() Element.className() and Element.hasClass(String className) 这些访问器方法都有相应的setter方法来更改数据 参见 Element和Elements集合类的参考文档 URLs处理 使用CSS选择器语法来查找元素 处理URLs 问题 你有一个包含相对URLs路径的HTML文档，需要将这些相对路径转换成绝对路径的URLs。 方法 在你解析文档时确保有指定base URI，然后 使用 abs: 属性前缀来取得包含base URI的绝对路径。代码如下： Document doc = Jsoup.connect("http://www.open-open.com").get();Element link = doc.select("a").first();String relHref = link.attr("href"); // == "/"String absHref = link.attr("abs:href"); // "http://www.open-open.com/" 说明 在HTML元素中，URLs 经常写成相对于文档位置的相对路径： &lt;a href=&quot;/download&quot;&gt;...&lt;/a&gt;. 当你使用 Node.attr(String key) 方法来取得a元素的href属性时，它将直接返回在HTML源码中指定定的值。 假如你需要取得一个绝对路径，需要在属性名前加 abs: 前缀。这样就可以返回包含根路径的URL地址attr(&quot;abs:href&quot;) 因此，在解析HTML文档时，定义base URI非常重要。 如果你不想使用abs: 前缀，还有一个方法能够实现同样的功能 Node.absUrl(String key)。 数据修改 设置属性的值 问题 在你解析一个 Document 之后可能想修改其中的某些属性值，然后再保存到磁盘或都输出到前台页面。 方法 可以使用属性设置方法 Element.attr(String key, String value), 和 Elements.attr(String key, String value). 假如你需要修改一个元素的 class 属性，可以使用 Element.addClass(String className) 和Element.removeClass(String className) 方法。 Elements 提供了批量操作元素属性和class的方法，比如：要为div中的每一个a元素都添加一个rel=&quot;nofollow&quot; 可以使用如下方法： doc.select("div.comments a").attr("rel", "nofollow"); 说明 与Element中的其它方法一样，attr 方法也是返回当 Element (或在使用选择器是返回 Elements集合)。这样能够很方便使用方法连用的书写方式。比如： &gt; doc.select("div.masthead").attr("title", "jsoup").addClass("round-box");&gt; 设置一个元素的HTML内容 问题 你需要一个元素中的HTML内容 方法 可以使用Element中的HTML设置方法具体如下： Element div = doc.select("div").first(); // &lt;div&gt;&lt;/div&gt;div.html("&lt;p&gt;lorem ipsum&lt;/p&gt;"); // &lt;div&gt;&lt;p&gt;lorem ipsum&lt;/p&gt;&lt;/div&gt;div.prepend("&lt;p&gt;First&lt;/p&gt;");//在div前添加html内容div.append("&lt;p&gt;Last&lt;/p&gt;");//在div之后添加html内容// 添完后的结果: &lt;div&gt;&lt;p&gt;First&lt;/p&gt;&lt;p&gt;lorem ipsum&lt;/p&gt;&lt;p&gt;Last&lt;/p&gt;&lt;/div&gt;Element span = doc.select("span").first(); // &lt;span&gt;One&lt;/span&gt;span.wrap("&lt;li&gt;&lt;a href='http://example.com/'&gt;&lt;/a&gt;&lt;/li&gt;");// 添完后的结果: &lt;li&gt;&lt;a href="http://example.com"&gt;&lt;span&gt;One&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; 说明 Element.html(String html) 这个方法将先清除元素中的HTML内容，然后用传入的HTML代替。 Element.prepend(String first) 和 Element.append(String last) 方法用于在分别在元素内部HTML的前面和后面添加HTML内容 Element.wrap(String around) 对元素包裹一个外部HTML内容。 参见 可以查看API参考文档中 Element.prependElement(String tag)和Element.appendElement(String tag) 方法来创建新的元素并作为文档的子元素插入其中。 设置元素的文本内容 问题 你需要修改一个HTML文档中的文本内容 方法 可以使用Element的设置方法：: Element div = doc.select("div").first(); // &lt;div&gt;&lt;/div&gt;div.text("five &gt; four"); // &lt;div&gt;five &amp;gt; four&lt;/div&gt;div.prepend("First ");div.append(" Last");// now: &lt;div&gt;First five &amp;gt; four Last&lt;/div&gt; 说明 文本设置方法与 HTML setter 方法一样： Element.text(String text) 将清除一个元素中的内部HTML内容，然后提供的文本进行代替 Element.prepend(String first) 和 Element.append(String last) 将分别在元素的内部html前后添加文本节点。 对于传入的文本如果含有像 &lt;, &gt; 等这样的字符，将以文本处理，而非HTML。 HTML清理 消除不受信任的HTML (来防止XSS攻击) 问题 在做网站的时候，经常会提供用户评论的功能。有些不怀好意的用户，会搞一些脚本到评论内容中，而这些脚本可能会破坏整个页面的行为，更严重的是获取一些机要信息，此时需要清理该HTML，以避免跨站脚本cross-site scripting攻击（XSS）。 方法 使用jsoup HTML Cleaner 方法进行清除，但需要指定一个可配置的 Whitelist。 String unsafe = "&lt;p&gt;&lt;a href='http://example.com/' onclick='stealCookies()'&gt;Link&lt;/a&gt;&lt;/p&gt;";String safe = Jsoup.clean(unsafe, Whitelist.basic());// now: &lt;p&gt;&lt;a href="http://example.com/" rel="nofollow"&gt;Link&lt;/a&gt;&lt;/p&gt; 说明 XSS 又叫 CSS (Cross Site Script) ，跨站脚本攻击。它指的是恶意攻击者往 Web 页面里插入恶意 html 代码，当用户浏览该页之时，嵌入其中 Web 里面的 html 代码会被执行，从而达到恶意攻击用户的特殊目的。XSS 属于被动式的攻击，因为其被动且不好利用，所以许多人常忽略其危害性。所以我们经常只让用户输入纯文本的内容，但这样用户体验就比较差了。 一个更好的解决方法就是使用一个富文本编辑器 WYSIWYG 如 CKEditor 和 TinyMCE。这些可以输出 HTML 并能够让用户可视化编辑。虽然他们可以在客户端进行校验，但是这样还不够安全，需要在服务器端进行校验并清除有害的HTML代码，这样才能确保输入到你网站的HTML是安全的。否则，攻击者能够绕过客户端的 Javascript 验证，并注入不安全的 HMTL 直接进入您的网站。 jsoup 的 whitelist 清理器能够在服务器端对用户输入的 HTML 进行过滤，只输出一些安全的标签和属性。 jsoup 提供了一系列的 Whitelist 基本配置，能够满足大多数要求；但如有必要，也可以进行修改，不过要小心。 这个 cleaner 非常好用不仅可以避免 XSS 攻击，还可以限制用户可以输入的标签范围。 参见 参阅XSS cheat sheet ，有一个例子可以了解为什么不能使用正则表达式，而采用安全的whitelist parser-based清理器才是正确的选择。 参阅Cleaner ，了解如何返回一个 Document 对象，而不是字符串 参阅Whitelist，了解如何创建一个自定义的whitelist nofollow 链接属性了解 参考 jsoup github托管代码 jsoup Cookbook jsoup Cookbook(中文版) 不错的jsoup学习笔记]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>dom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thumbnailator 使用小结]]></title>
    <url>%2Fblog%2F2017%2F01%2F17%2Fjava%2Fjavalib%2Fthumbnailator%2F</url>
    <content type="text"><![CDATA[Thumbnailator 使用小结 概述 Thumbnailator 是一个开源的 Java 项目，它提供了非常简单的 API 来对图片进行缩放、旋转以及加水印的处理。 有多简单呢？简单到一行代码就可以完成图片处理。形式如下： Thumbnails.of(new File("path/to/directory").listFiles()) .size(640, 480) .outputFormat("jpg") .toFiles(Rename.PREFIX_DOT_THUMBNAIL); 当然，Thumbnailator 还有一些使用细节，下面我会一一道来。 核心 API Thumbnails Thumbnails 是使用 Thumbnailator 创建缩略图的主入口。 它提供了一组初始化 Thumbnails.Builder 的接口。 先看下这组接口的声明： // 可变长度参数列表public static Builder&lt;File&gt; of(String... files) &#123;...&#125;public static Builder&lt;File&gt; of(File... files) &#123;...&#125;public static Builder&lt;URL&gt; of(URL... urls) &#123;...&#125;public static Builder&lt;? extends InputStream&gt; of(InputStream... inputStreams) &#123;...&#125;public static Builder&lt;BufferedImage&gt; of(BufferedImage... images) &#123;...&#125;// 迭代器（所有实现 Iterable 接口的 Java 对象都可以，当然也包括 List、Set）public static Builder&lt;File&gt; fromFilenames(Iterable&lt;String&gt; files) &#123;...&#125;public static Builder&lt;File&gt; fromFiles(Iterable&lt;File&gt; files) &#123;...&#125;public static Builder&lt;URL&gt; fromURLs(Iterable&lt;URL&gt; urls) &#123;...&#125;public static Builder&lt;InputStream&gt; fromInputStreams(Iterable&lt;? extends InputStream&gt; inputStreams) &#123;...&#125;public static Builder&lt;BufferedImage&gt; fromImages(Iterable&lt;BufferedImage&gt; images) &#123;...&#125; 很显然，Thumbnails 允许通过传入文件名、文件、网络图的URL、图片流、图片缓存多种方式来初始化构造器。 因此，你可以根据实际需求来灵活的选择图片的输入方式。 需要注意一点：如果输入是多个对象（无论你是直接输入容器对象或使用可变参数方式传入多个对象），则输出也必须选用输出多个对象的方式，否则会报异常。 Thumbnails.Builder Thumbnails.Builder 是 Thumbnails 的内部静态类。它用于设置生成缩略图任务的相关参数。 注：Thumbnails.Builder 的构造函数是私有函数。所以，它只允许通过 Thumbnails 的实例化函数来进行初始化。 设置参数的函数 Thumbnails.Builder 提供了一组函数链形式的接口来设置缩放图参数。 以设置大小函数为例： public Builder&lt;T&gt; size(int width, int height)&#123; updateStatus(Properties.SIZE, Status.ALREADY_SET); updateStatus(Properties.SCALE, Status.CANNOT_SET); validateDimensions(width, height); this.width = width; this.height = height; return this;&#125; 通过返回this指针，使得设置参数函数可以以链式调用的方式来使用，形式如下： Thumbnails.of(new File("original.jpg")) .size(160, 160) .rotate(90) .watermark(Positions.BOTTOM_RIGHT, ImageIO.read(new File("watermark.png")), 0.5f) .outputQuality(0.8) .toFile(new File("image-with-watermark.jpg")); 好处，不言自明：那就是大大简化了代码。 输出函数 Thumbnails.Builder 提供了一组重载函数来输出生成的缩放图。 函数声明如下： // 返回图片缓存public List&lt;BufferedImage&gt; asBufferedImages() throws IOException &#123;...&#125;public BufferedImage asBufferedImage() throws IOException &#123;...&#125;// 返回文件列表public List&lt;File&gt; asFiles(Iterable&lt;File&gt; iterable) throws IOException &#123;...&#125;public List&lt;File&gt; asFiles(Rename rename) throws IOException &#123;...&#125;public List&lt;File&gt; asFiles(File destinationDir, Rename rename) throws IOException &#123;...&#125;// 创建文件public void toFile(File outFile) throws IOException &#123;...&#125;public void toFile(String outFilepath) throws IOException &#123;...&#125;public void toFiles(Iterable&lt;File&gt; iterable) throws IOException &#123;...&#125;public void toFiles(Rename rename) throws IOException &#123;...&#125;public void toFiles(File destinationDir, Rename rename) throws IOException &#123;...&#125;// 创建输出流public void toOutputStream(OutputStream os) throws IOException &#123;...&#125;public void toOutputStreams(Iterable&lt;? extends OutputStream&gt; iterable) throws IOException &#123;...&#125; 工作流 Thumbnailator 的工作步骤十分简单，可分为三步： 输入：Thumbnails 根据输入初始化构造器—— Thumbnails.Builder 。 设置：Thumbnails.Builder 设置缩放图片的参数。 输出：Thumbnails.Builder 输出图片文件或图片流。 更多详情可以参考： Thumbnailator 官网javadoc 实战 前文介绍了 Thumbnailator 的核心 API，接下来我们就可以通过实战来看看 Thumbnailator 究竟可以做些什么。 Thumbnailator 生成什么样的图片，是根据设置参数来决定的。 安装 maven项目中引入依赖： &lt;dependency&gt; &lt;groupId&gt;net.coobird&lt;/groupId&gt; &lt;artifactId&gt;thumbnailator&lt;/artifactId&gt; &lt;version&gt;[0.4, 0.5)&lt;/version&gt;&lt;/dependency&gt; 图片缩放 Thumbnails.Builder 的 size 函数可以设置新图片精确的宽度和高度，也可以用 scale 函数设置缩放比例。 Thumbnails.of("oldFile.png") .size(16, 16) .toFile("newFile_16_16.png");Thumbnails.of("oldFile.png") .scale(2.0) .toFile("newFile_scale_2.0.png");Thumbnails.of("oldFile.png") .scale(1.0, 0.5) .toFile("newFile_scale_1.0_0.5.png"); oldFile.png newFile_scale_1.0_0.5.png 图片旋转 Thumbnails.Builder 的 size 函数可以设置新图片的旋转角度。 Thumbnails.of("oldFile.png") .scale(0.8) .rotate(90) .toFile("newFile_rotate_90.png");Thumbnails.of("oldFile.png") .scale(0.8) .rotate(180) .toFile("newFile_rotate_180.png"); newFile_rotate_90.png 加水印 Thumbnails.Builder 的 watermark 函数可以为图片添加水印图片。第一个参数是水印的位置；第二个参数是水印图片的缓存数据；第三个参数是透明度。 BufferedImage watermarkImage = ImageIO.read(new File("wartermarkFile.png"));Thumbnails.of("oldFile.png") .scale(0.8) .watermark(Positions.BOTTOM_LEFT, watermarkImage, 0.5f) .toFile("newFile_watermark.png"); wartermarkFile.png newFile_watermark.png 批量处理图片 下面以批量给图片加水印来展示一下如何处理多个图片文件。 BufferedImage watermarkImage = ImageIO.read(new File("wartermarkFile.png"));File destinationDir = new File("D:\\watermark\\");Thumbnails.of("oldFile.png", "oldFile2.png") .scale(0.8) .watermark(Positions.BOTTOM_LEFT, watermarkImage, 0.5f) .toFiles(destinationDir, Rename.PREFIX_DOT_THUMBNAIL); 需要参考完整测试例代码请 点击这里 参考 Thumbnailator 官方示例文档]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZXing 使用小结]]></title>
    <url>%2Fblog%2F2017%2F01%2F17%2Fjava%2Fjavalib%2Fzxing%2F</url>
    <content type="text"><![CDATA[ZXing 使用小结 概述 ZXing 是一个开源 Java 类库用于解析多种格式的 1D/2D 条形码。目标是能够对QR编码、Data Matrix、UPC的1D条形码进行解码。 其提供了多种平台下的客户端包括：J2ME、J2SE和Android。 官网：ZXing github仓库 实战 本例演示如何在一个非 android 的 Java 项目中使用 ZXing 来生成、解析二维码图片。 安装 maven项目只需引入依赖： &lt;dependency&gt; &lt;groupId&gt;com.google.zxing&lt;/groupId&gt; &lt;artifactId&gt;core&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.zxing&lt;/groupId&gt; &lt;artifactId&gt;javase&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; 如果非maven项目，就去官网下载发布版本：下载地址 生成二维码图片 ZXing 生成二维码图片有以下步骤： com.google.zxing.MultiFormatWriter 根据内容以及图像编码参数生成图像2D矩阵。 ​ com.google.zxing.client.j2se.MatrixToImageWriter 根据图像矩阵生成图片文件或图片缓存 BufferedImage 。 public void encode(String content, String filepath) throws WriterException, IOException &#123; int width = 100; int height = 100; Map&lt;EncodeHintType, Object&gt; encodeHints = new HashMap&lt;EncodeHintType, Object&gt;(); encodeHints.put(EncodeHintType.CHARACTER_SET, "UTF-8"); BitMatrix bitMatrix = new MultiFormatWriter().encode(content, BarcodeFormat.QR_CODE, width, height, encodeHints); Path path = FileSystems.getDefault().getPath(filepath); MatrixToImageWriter.writeToPath(bitMatrix, "png", path);&#125; 解析二维码图片 ZXing 解析二维码图片有以下步骤： 使用 javax.imageio.ImageIO 读取图片文件，并存为一个 java.awt.image.BufferedImage 对象。 将 java.awt.image.BufferedImage 转换为 ZXing 能识别的 com.google.zxing.BinaryBitmap 对象。 com.google.zxing.MultiFormatReader 根据图像解码参数来解析 com.google.zxing.BinaryBitmap 。 public String decode(String filepath) throws IOException, NotFoundException &#123; BufferedImage bufferedImage = ImageIO.read(new FileInputStream(filepath)); LuminanceSource source = new BufferedImageLuminanceSource(bufferedImage); Binarizer binarizer = new HybridBinarizer(source); BinaryBitmap bitmap = new BinaryBitmap(binarizer); HashMap&lt;DecodeHintType, Object&gt; decodeHints = new HashMap&lt;DecodeHintType, Object&gt;(); decodeHints.put(DecodeHintType.CHARACTER_SET, "UTF-8"); Result result = new MultiFormatReader().decode(bitmap, decodeHints); return result.getText();&#125; 完整参考示例：测试例代码 以下是一个生成的二维码图片示例： 参考 ZXing github仓库]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic 技术栈之 Filebeat]]></title>
    <url>%2Fblog%2F2017%2F01%2F03%2Fos%2Flinux%2Fops%2Fservice%2Felastic%2Felastic-beats%2F</url>
    <content type="text"><![CDATA[Elastic 技术栈之 Filebeat 简介 Beats 是安装在服务器上的数据中转代理。 Beats 可以将数据直接传输到 Elasticsearch 或传输到 Logstash 。 Beats 有多种类型，可以根据实际应用需要选择合适的类型。 常用的类型有： **Packetbeat：**网络数据包分析器，提供有关您的应用程序服务器之间交换的事务的信息。 **Filebeat：**从您的服务器发送日志文件。 **Metricbeat：**是一个服务器监视代理程序，它定期从服务器上运行的操作系统和服务收集指标。 **Winlogbeat：**提供Windows事件日志。 参考 更多 Beats 类型可以参考：community-beats 说明 由于本人工作中只应用了 FileBeat，所以后面内容仅介绍 FileBeat 。 FileBeat 的作用 相比 Logstash，FileBeat 更加轻量化。 在任何环境下，应用程序都有停机的可能性。 Filebeat 读取并转发日志行，如果中断，则会记住所有事件恢复联机状态时所在位置。 Filebeat带有内部模块（auditd，Apache，Nginx，System和MySQL），可通过一个指定命令来简化通用日志格式的收集，解析和可视化。 FileBeat 不会让你的管道超负荷。FileBeat 如果是向 Logstash 传输数据，当 Logstash 忙于处理数据，会通知 FileBeat 放慢读取速度。一旦拥塞得到解决，FileBeat 将恢复到原来的速度并继续传播。 安装 Unix / Linux 系统建议使用下面方式安装，因为比较通用。 wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.1.1-linux-x86_64.tar.gztar -zxf filebeat-6.1.1-linux-x86_64.tar.gz 参考 更多内容可以参考：filebeat-installation 配置 配置文件 首先，需要知道的是：filebeat.yml 是 filebeat 的配置文件。配置文件的路径会因为你安装方式的不同而变化。 Beat 所有系列产品的配置文件都基于 YAML 格式，FileBeat 当然也不例外。 filebeat.yml 部分配置示例： filebeat: prospectors: - type: log paths: - /var/log/*.log multiline: pattern: '^[' match: after 参考 更多 filebeat 配置内容可以参考：配置 filebeat 更多 filebeat.yml 文件格式内容可以参考：filebeat.yml 文件格式 重要配置项 filebeat.prospectors （文件监视器）用于指定需要关注的文件。 示例 filebeat.prospectors:- type: log enabled: true paths: - /var/log/*.log output.elasticsearch 如果你希望使用 filebeat 直接向 elasticsearch 输出数据，需要配置 output.elasticsearch 。 示例 output.elasticsearch: hosts: ["192.168.1.42:9200"] output.logstash 如果你希望使用 filebeat 向 logstash输出数据，然后由 logstash 再向elasticsearch 输出数据，需要配置 output.logstash。 注意 相比于向 elasticsearch 输出数据，个人更推荐向 logstash 输出数据。 因为 logstash 和 filebeat 一起工作时，如果 logstash 忙于处理数据，会通知 FileBeat 放慢读取速度。一旦拥塞得到解决，FileBeat 将恢复到原来的速度并继续传播。这样，可以减少管道超负荷的情况。 示例 output.logstash: hosts: ["127.0.0.1:5044"] 此外，还需要在 logstash 的配置文件（如 logstash.conf）中指定 beats input 插件： input &#123; beats &#123; port =&gt; 5044 # 此端口需要与 filebeat.yml 中的端口相同 &#125;&#125;# The filter part of this file is commented out to indicate that it is# optional.# filter &#123;## &#125;output &#123; elasticsearch &#123; hosts =&gt; "localhost:9200" manage_template =&gt; false index =&gt; "%&#123;[@metadata][beat]&#125;-%&#123;[@metadata][version]&#125;-%&#123;+YYYY.MM.dd&#125;" document_type =&gt; "%&#123;[@metadata][type]&#125;" &#125;&#125; setup.kibana 如果打算使用 Filebeat 提供的 Kibana 仪表板，需要配置 setup.kibana 。 示例 setup.kibana: host: "localhost:5601" setup.template.settings 在 Elasticsearch 中，索引模板用于定义设置和映射，以确定如何分析字段。 在 Filebeat 中，setup.template.settings 用于配置索引模板。 Filebeat 推荐的索引模板文件由 Filebeat 软件包安装。如果您接受 filebeat.yml 配置文件中的默认配置，Filebeat在成功连接到 Elasticsearch 后自动加载模板。 您可以通过在 Filebeat 配置文件中配置模板加载选项来禁用自动模板加载，或加载自己的模板。您还可以设置选项来更改索引和索引模板的名称。 参考 更多内容可以参考：filebeat-template 说明 如无必要，使用 Filebeat 配置文件中的默认索引模板即可。 setup.dashboards Filebeat 附带了示例 Kibana 仪表板。在使用仪表板之前，您需要创建索引模式 filebeat- *，并将仪表板加载到Kibana 中。为此，您可以运行 setup 命令或在 filebeat.yml 配置文件中配置仪表板加载。 为了在 Kibana 中加载 Filebeat 的仪表盘，需要在 filebeat.yml 配置中启动开关： setup.dashboards.enabled: true 参考 更多内容可以参考：configuration-dashboards 命令 filebeat 提供了一系列命令来完成各种功能。 执行命令方式： ./filebeat COMMAND 参考 更多内容可以参考：command-line-options 说明 个人认为命令行没有必要一一掌握，因为绝大部分功能都可以通过配置来完成。且通过命令行指定功能这种方式要求每次输入同样参数，不利于固化启动方式。 最重要的当然是启动命令 run 了。 示例 指定配置文件启动 &gt; ./filebeat run -e -c filebeat.yml -d "publish"&gt; ./filebeat -e -c filebeat.yml -d "publish" # run 可以省略&gt; 模块 Filebeat 提供了一套预构建的模块，让您可以快速实施和部署日志监视解决方案，并附带示例仪表板和数据可视化。这些模块支持常见的日志格式，例如Nginx，Apache2和MySQL 等。 运行模块的步骤 配置 elasticsearch 和 kibana output.elasticsearch: hosts: ["myEShost:9200"] username: "elastic" password: "elastic"setup.kibana: host: "mykibanahost:5601" username: "elastic" password: "elastic username 和 password 是可选的，如果不需要认证则不填。 初始化环境 执行下面命令，filebeat 会加载推荐索引模板。 ./filebeat setup -e 指定模块 执行下面命令，指定希望加载的模块。 ./filebeat -e --modules system,nginx,mysql 参考 更多内容可以参考： 配置 filebeat 模块 | filebeat 支持模块 原理 Filebeat 有两个主要组件： harvester：负责读取一个文件的内容。它会逐行读取文件内容，并将内容发送到输出目的地。 prospector：负责管理 harvester 并找到所有需要读取的文件源。比如类型是日志，prospector 就会遍历制定路径下的所有匹配要求的文件。 filebeat.prospectors:- type: log paths: - /var/log/*.log - /var/path2/*.log Filebeat保持每个文件的状态，并经常刷新注册表文件中的磁盘状态。状态用于记住 harvester 正在读取的最后偏移量，并确保发送所有日志行。 Filebeat 将每个事件的传递状态存储在注册表文件中。所以它能保证事件至少传递一次到配置的输出，没有数据丢失。 资料 Beats 官方文档]]></content>
      <categories>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>log</tag>
        <tag>elastic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 教程之 pom.xml 详解]]></title>
    <url>%2Fblog%2F2016%2F11%2F10%2Fjava%2Fjavatool%2Fbuild%2Fmaven%2Fmaven-pom%2F</url>
    <content type="text"><![CDATA[Maven 教程之 pom.xml 详解 📓 本文已归档到：「blog」 简介 什么是 pom？ pom 配置一览 基本配置 maven 坐标 依赖配置 dependencies parent dependencyManagement modules properties 构建配置 build reporting 项目信息 环境配置 issueManagement ciManagement mailingLists scm prerequisites repositories pluginRepositories distributionManagement profiles 参考资料 简介 什么是 pom？ POM 是 Project Object Model 的缩写，即项目对象模型。 pom.xml 就是 maven 的配置文件，用以描述项目的各种信息。 pom 配置一览 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- The Basics --&gt; &lt;groupId&gt;...&lt;/groupId&gt; &lt;artifactId&gt;...&lt;/artifactId&gt; &lt;version&gt;...&lt;/version&gt; &lt;packaging&gt;...&lt;/packaging&gt; &lt;dependencies&gt;...&lt;/dependencies&gt; &lt;parent&gt;...&lt;/parent&gt; &lt;dependencyManagement&gt;...&lt;/dependencyManagement&gt; &lt;modules&gt;...&lt;/modules&gt; &lt;properties&gt;...&lt;/properties&gt; &lt;!-- Build Settings --&gt; &lt;build&gt;...&lt;/build&gt; &lt;reporting&gt;...&lt;/reporting&gt; &lt;!-- More Project Information --&gt; &lt;name&gt;...&lt;/name&gt; &lt;description&gt;...&lt;/description&gt; &lt;url&gt;...&lt;/url&gt; &lt;inceptionYear&gt;...&lt;/inceptionYear&gt; &lt;licenses&gt;...&lt;/licenses&gt; &lt;organization&gt;...&lt;/organization&gt; &lt;developers&gt;...&lt;/developers&gt; &lt;contributors&gt;...&lt;/contributors&gt; &lt;!-- Environment Settings --&gt; &lt;issueManagement&gt;...&lt;/issueManagement&gt; &lt;ciManagement&gt;...&lt;/ciManagement&gt; &lt;mailingLists&gt;...&lt;/mailingLists&gt; &lt;scm&gt;...&lt;/scm&gt; &lt;prerequisites&gt;...&lt;/prerequisites&gt; &lt;repositories&gt;...&lt;/repositories&gt; &lt;pluginRepositories&gt;...&lt;/pluginRepositories&gt; &lt;distributionManagement&gt;...&lt;/distributionManagement&gt; &lt;profiles&gt;...&lt;/profiles&gt;&lt;/project&gt; 基本配置 project - project 是 pom.xml 中描述符的根。 modelVersion - modelVersion 指定 pom.xml 符合哪个版本的描述符。maven 2 和 3 只能为 4.0.0。 一般 jar 包被识别为： groupId:artifactId:version 的形式。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;my-project&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt;&lt;/project&gt; maven 坐标 在 maven 中，根据 groupId、artifactId、version 组合成 groupId:artifactId:version 来唯一识别一个 jar 包。 groupId - 团体、组织的标识符。团体标识的约定是，它以创建这个项目的组织名称的逆向域名(reverse domain name)开头。一般对应着 java 的包结构。 artifactId - 单独项目的唯一标识符。比如我们的 tomcat、commons 等。不要在 artifactId 中包含点号(.)。 version - 一个项目的特定版本。 maven 有自己的版本规范，一般是如下定义 major version、minor version、incremental version-qualifier ，比如 1.2.3-beta-01。要说明的是，maven 自己判断版本的算法是 major、minor、incremental 部分用数字比较，qualifier 部分用字符串比较，所以要小心 alpha-2 和 alpha-15 的比较关系，最好用 alpha-02 的格式。 maven 在版本管理时候可以使用几个特殊的字符串 SNAPSHOT、LATEST、RELEASE。比如 1.0-SNAPSHOT。各个部分的含义和处理逻辑如下说明： SNAPSHOT - 这个版本一般用于开发过程中，表示不稳定的版本。 LATEST - 指某个特定构件的最新发布，这个发布可能是一个发布版，也可能是一个 snapshot 版，具体看哪个时间最后。 RELEASE ：指最后一个发布版。 packaging - 项目的类型，描述了项目打包后的输出，默认是 jar。常见的输出类型为：pom, jar, maven-plugin, ejb, war, ear, rar, par。 依赖配置 dependencies &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.maven&lt;/groupId&gt; &lt;artifactId&gt;maven-embedder&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.maven&lt;/groupId&gt; &lt;artifactId&gt;maven-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; ... &lt;/dependencies&gt; ...&lt;/project&gt; groupId, artifactId, version - 和基本配置中的 groupId、artifactId、version 意义相同。 type - 对应 packaging 的类型，如果不使用 type 标签，maven 默认为 jar。 scope - 此元素指的是任务的类路径（编译和运行时，测试等）以及如何限制依赖关系的传递性。有 5 种可用的限定范围： compile - 如果没有指定 scope 标签，maven 默认为这个范围。编译依赖关系在所有 classpath 中都可用。此外，这些依赖关系被传播到依赖项目。 provided - 与 compile 类似，但是表示您希望 jdk 或容器在运行时提供它。它只适用于编译和测试 classpath，不可传递。 runtime - 此范围表示编译不需要依赖关系，而是用于执行。它是在运行时和测试 classpath，但不是编译 classpath。 test - 此范围表示正常使用应用程序不需要依赖关系，仅适用于测试编译和执行阶段。它不是传递的。 system - 此范围与 provided 类似，除了您必须提供明确包含它的 jar。该 artifact 始终可用，并且不是在仓库中查找。 systemPath - 仅当依赖范围是系统时才使用。否则，如果设置此元素，构建将失败。该路径必须是绝对路径，因此建议使用 propertie 来指定特定的路径，如$ {java.home} / lib。由于假定先前安装了系统范围依赖关系，maven 将不会检查项目的仓库，而是检查库文件是否存在。如果没有，maven 将会失败，并建议您手动下载安装。 optional - optional 让其他项目知道，当您使用此项目时，您不需要这种依赖性才能正常工作。 exclusions - 包含一个或多个排除元素，每个排除元素都包含一个表示要排除的依赖关系的 groupId 和 artifactId。与可选项不同，可能或可能不会安装和使用，排除主动从依赖关系树中删除自己。 parent maven 支持继承功能。子 POM 可以使用 parent 指定父 POM ，然后继承其配置。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;my-parent&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;relativePath&gt;../my-parent&lt;/relativePath&gt; &lt;/parent&gt; &lt;artifactId&gt;my-project&lt;/artifactId&gt;&lt;/project&gt; relativePath - 注意 relativePath 元素。在搜索本地和远程存储库之前，它不是必需的，但可以用作 maven 的指示符，以首先搜索给定该项目父级的路径。 dependencyManagement dependencyManagement 是表示依赖 jar 包的声明。即你在项目中的 dependencyManagement 下声明了依赖，maven 不会加载该依赖，dependencyManagement 声明可以被子 POM 继承。 dependencyManagement 的一个使用案例是当有父子项目的时候，父项目中可以利用 dependencyManagement 声明子项目中需要用到的依赖 jar 包，之后，当某个或者某几个子项目需要加载该依赖的时候，就可以在子项目中 dependencies 节点只配置 groupId 和 artifactId 就可以完成依赖的引用。 dependencyManagement 主要是为了统一管理依赖包的版本，确保所有子项目使用的版本一致，类似的还有plugins和pluginManagement。 modules 子模块列表。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;my-parent&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;modules&gt; &lt;module&gt;my-project&lt;/module&gt; &lt;module&gt;another-project&lt;/module&gt; &lt;module&gt;third-project/pom-example.xml&lt;/module&gt; &lt;/modules&gt;&lt;/project&gt; properties 属性列表。定义的属性可以在 pom.xml 文件中任意处使用。使用方式为 ${propertie} 。 &lt;project&gt; ... &lt;properties&gt; &lt;maven.compiler.source&gt;1.7&lt;maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;/properties&gt; ...&lt;/project&gt; 构建配置 build build 可以分为 “project build” 和 “profile build”。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;!-- "Project Build" contains more elements than just the BaseBuild set --&gt; &lt;build&gt;...&lt;/build&gt; &lt;profiles&gt; &lt;profile&gt; &lt;!-- "Profile Build" contains a subset of "Project Build"s elements --&gt; &lt;build&gt;...&lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; 基本构建配置： &lt;build&gt; &lt;defaultGoal&gt;install&lt;/defaultGoal&gt; &lt;directory&gt;$&#123;basedir&#125;/target&lt;/directory&gt; &lt;finalName&gt;$&#123;artifactId&#125;-$&#123;version&#125;&lt;/finalName&gt; &lt;filters&gt; &lt;filter&gt;filters/filter1.properties&lt;/filter&gt; &lt;/filters&gt; ...&lt;/build&gt; defaultGoal : 默认执行目标或阶段。如果给出了一个目标，它应该被定义为它在命令行中（如 jar：jar）。如果定义了一个阶段（如安装），也是如此。 directory ：构建时的输出路径。默认为：${basedir}/target 。 finalName ：这是项目的最终构建名称（不包括文件扩展名，例如：my-project-1.0.jar） filter ：定义 * .properties 文件，其中包含适用于接受其设置的资源的属性列表（如下所述）。换句话说，过滤器文件中定义的“name = value”对在代码中替换$ {name}字符串。 resources 资源的配置。资源文件通常不是代码，不需要编译，而是在项目需要捆绑使用的内容。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;build&gt; ... &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;META-INF/plexus&lt;/targetPath&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;directory&gt;$&#123;basedir&#125;/src/main/plexus&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;configuration.xml&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.properties&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;testResources&gt; ... &lt;/testResources&gt; ... &lt;/build&gt;&lt;/project&gt; resources: 资源元素的列表，每个资源元素描述与此项目关联的文件和何处包含文件。 targetPath: 指定从构建中放置资源集的目录结构。目标路径默认为基本目录。将要包装在 jar 中的资源的通常指定的目标路径是 META-INF。 filtering: 值为 true 或 false。表示是否要为此资源启用过滤。请注意，该过滤器 * .properties 文件不必定义为进行过滤 - 资源还可以使用默认情况下在 POM 中定义的属性（例如$ {project.version}），并将其传递到命令行中“-D”标志（例如，“-Dname = value”）或由 properties 元素显式定义。过滤文件覆盖上面。 directory: 值定义了资源的路径。构建的默认目录是${basedir}/src/main/resources。 includes: 一组文件匹配模式，指定目录中要包括的文件，使用*作为通配符。 excludes: 与 includes 类似，指定目录中要排除的文件，使用*作为通配符。注意：如果 include 和 exclude 发生冲突，maven 会以 exclude 作为有效项。 testResources: testResources 与 resources 功能类似，区别仅在于：testResources 指定的资源仅用于 test 阶段，并且其默认资源目录为：${basedir}/src/test/resources 。 plugins &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;build&gt; ... &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;extensions&gt;false&lt;/extensions&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;configuration&gt; &lt;classifier&gt;test&lt;/classifier&gt; &lt;/configuration&gt; &lt;dependencies&gt;...&lt;/dependencies&gt; &lt;executions&gt;...&lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; groupId, artifactId, version ：和基本配置中的 groupId、artifactId、version 意义相同。 extensions ：值为 true 或 false。是否加载此插件的扩展名。默认为 false。 inherited ：值为 true 或 false。这个插件配置是否应该适用于继承自这个插件的 POM。默认值为 true。 configuration - 这是针对个人插件的配置，这里不扩散讲解。 dependencies ：这里的 dependencies 是插件本身所需要的依赖。 executions ：需要记住的是，插件可能有多个目标。每个目标可能有一个单独的配置，甚至可能将插件的目标完全绑定到不同的阶段。执行配置插件的目标的执行。 id: 执行目标的标识。 goals: 像所有多元化的 POM 元素一样，它包含单个元素的列表。在这种情况下，这个执行块指定的插件目标列表。 phase: 这是执行目标列表的阶段。这是一个非常强大的选项，允许将任何目标绑定到构建生命周期中的任何阶段，从而改变 maven 的默认行为。 inherited: 像上面的继承元素一样，设置这个 false 会阻止 maven 将这个执行传递给它的子代。此元素仅对父 POM 有意义。 configuration: 与上述相同，但将配置限制在此特定目标列表中，而不是插件下的所有目标。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt; &lt;version&gt;1.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;echodir&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;inherited&gt;false&lt;/inherited&gt; &lt;configuration&gt; &lt;tasks&gt; &lt;echo&gt;Build Dir: $&#123;project.build.directory&#125;&lt;/echo&gt; &lt;/tasks&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; pluginManagement 与 dependencyManagement 很相似，在当前 POM 中仅声明插件，而不是实际引入插件。子 POM 中只配置 groupId 和 artifactId 就可以完成插件的引用，且子 POM 有权重写 pluginManagement 定义。 它的目的在于统一所有子 POM 的插件版本。 directories &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;build&gt; &lt;sourceDirectory&gt;$&#123;basedir&#125;/src/main/java&lt;/sourceDirectory&gt; &lt;scriptSourceDirectory&gt;$&#123;basedir&#125;/src/main/scripts&lt;/scriptSourceDirectory&gt; &lt;testSourceDirectory&gt;$&#123;basedir&#125;/src/test/java&lt;/testSourceDirectory&gt; &lt;outputDirectory&gt;$&#123;basedir&#125;/target/classes&lt;/outputDirectory&gt; &lt;testOutputDirectory&gt;$&#123;basedir&#125;/target/test-classes&lt;/testOutputDirectory&gt; ... &lt;/build&gt;&lt;/project&gt; 目录元素集合存在于 build 元素中，它为整个 POM 设置了各种目录结构。由于它们在配置文件构建中不存在，所以这些不能由配置文件更改。 如果上述目录元素的值设置为绝对路径（扩展属性时），则使用该目录。否则，它是相对于基础构建目录：${basedir}。 extensions 扩展是在此构建中使用的 artifacts 的列表。它们将被包含在运行构建的 classpath 中。它们可以启用对构建过程的扩展（例如为 Wagon 传输机制添加一个 ftp 提供程序），并使活动的插件能够对构建生命周期进行更改。简而言之，扩展是在构建期间激活的 artifacts。扩展不需要实际执行任何操作，也不包含 Mojo。因此，扩展对于指定普通插件接口的多个实现中的一个是非常好的。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;build&gt; ... &lt;extensions&gt; &lt;extension&gt; &lt;groupId&gt;org.apache.maven.wagon&lt;/groupId&gt; &lt;artifactId&gt;wagon-ftp&lt;/artifactId&gt; &lt;version&gt;1.0-alpha-3&lt;/version&gt; &lt;/extension&gt; &lt;/extensions&gt; ... &lt;/build&gt;&lt;/project&gt; reporting 报告包含特定针对 site 生成阶段的元素。某些 maven 插件可以生成 reporting 元素下配置的报告，例如：生成 javadoc 报告。reporting 与 build 元素配置插件的能力相似。明显的区别在于：在执行块中插件目标的控制不是细粒度的，报表通过配置 reportSet 元素来精细控制。而微妙的区别在于 reporting 元素下的 configuration 元素可以用作 build 下的 configuration ，尽管相反的情况并非如此（ build 下的 configuration 不影响 reporting 元素下的 configuration ）。 另一个区别就是 plugin 下的 outputDirectory 元素。在报告的情况下，默认输出目录为 ${basedir}/target/site。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;reporting&gt; &lt;plugins&gt; &lt;plugin&gt; ... &lt;reportSets&gt; &lt;reportSet&gt; &lt;id&gt;sunlink&lt;/id&gt; &lt;reports&gt; &lt;report&gt;javadoc&lt;/report&gt; &lt;/reports&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;configuration&gt; &lt;links&gt; &lt;link&gt;http://java.sun.com/j2se/1.5.0/docs/api/&lt;/link&gt; &lt;/links&gt; &lt;/configuration&gt; &lt;/reportSet&gt; &lt;/reportSets&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt; ...&lt;/project&gt; 项目信息 项目信息相关的这部分标签都不是必要的，也就是说完全可以不填写。 它的作用仅限于描述项目的详细信息。 下面的示例是项目信息相关标签的清单： &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;!-- 项目信息 begin --&gt; &lt;!--项目名--&gt; &lt;name&gt;maven-notes&lt;/name&gt; &lt;!--项目描述--&gt; &lt;description&gt;maven 学习笔记&lt;/description&gt; &lt;!--项目url--&gt; &lt;url&gt;https://github.com/dunwu/maven-notes&lt;/url&gt; &lt;!--项目开发年份--&gt; &lt;inceptionYear&gt;2017&lt;/inceptionYear&gt; &lt;!--开源协议--&gt; &lt;licenses&gt; &lt;license&gt; &lt;name&gt;Apache License, Version 2.0&lt;/name&gt; &lt;url&gt;https://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;comments&gt;A business-friendly OSS license&lt;/comments&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;!--组织信息(如公司、开源组织等)--&gt; &lt;organization&gt; &lt;name&gt;...&lt;/name&gt; &lt;url&gt;...&lt;/url&gt; &lt;/organization&gt; &lt;!--开发者列表--&gt; &lt;developers&gt; &lt;developer&gt; &lt;id&gt;victor&lt;/id&gt; &lt;name&gt;Zhang Peng&lt;/name&gt; &lt;email&gt;forbreak at 163.com&lt;/email&gt; &lt;url&gt;https://github.com/dunwu&lt;/url&gt; &lt;organization&gt;...&lt;/organization&gt; &lt;organizationUrl&gt;...&lt;/organizationUrl&gt; &lt;roles&gt; &lt;role&gt;architect&lt;/role&gt; &lt;role&gt;developer&lt;/role&gt; &lt;/roles&gt; &lt;timezone&gt;+8&lt;/timezone&gt; &lt;properties&gt;...&lt;/properties&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;!--代码贡献者列表--&gt; &lt;contributors&gt; &lt;contributor&gt; &lt;!--标签内容和&lt;developer&gt;相同--&gt; &lt;/contributor&gt; &lt;/contributors&gt; &lt;!-- 项目信息 end --&gt; ...&lt;/project&gt; 这部分标签都非常简单，基本都能做到顾名思义，且都属于可有可无的标签，所以这里仅简单介绍一下： name - 项目完整名称 description - 项目描述 url - 一般为项目仓库的 host inceptionYear - 开发年份 licenses - 开源协议 organization - 项目所属组织信息 developers - 项目开发者列表 contributors - 项目贡献者列表，&lt;contributor&gt; 的子标签和 &lt;developer&gt; 的完全相同。 环境配置 issueManagement 这定义了所使用的缺陷跟踪系统（Bugzilla，TestTrack，ClearQuest 等）。虽然没有什么可以阻止插件使用这些信息的东西，但它主要用于生成项目文档。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;issueManagement&gt; &lt;system&gt;Bugzilla&lt;/system&gt; &lt;url&gt;http://127.0.0.1/bugzilla/&lt;/url&gt; &lt;/issueManagement&gt; ...&lt;/project&gt; ciManagement CI 构建系统配置，主要是指定通知机制以及被通知的邮箱。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;ciManagement&gt; &lt;system&gt;continuum&lt;/system&gt; &lt;url&gt;http://127.0.0.1:8080/continuum&lt;/url&gt; &lt;notifiers&gt; &lt;notifier&gt; &lt;type&gt;mail&lt;/type&gt; &lt;sendOnError&gt;true&lt;/sendOnError&gt; &lt;sendOnFailure&gt;true&lt;/sendOnFailure&gt; &lt;sendOnSuccess&gt;false&lt;/sendOnSuccess&gt; &lt;sendOnWarning&gt;false&lt;/sendOnWarning&gt; &lt;configuration&gt;&lt;address&gt;continuum@127.0.0.1&lt;/address&gt;&lt;/configuration&gt; &lt;/notifier&gt; &lt;/notifiers&gt; &lt;/ciManagement&gt; ...&lt;/project&gt; mailingLists 邮件列表 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;mailingLists&gt; &lt;mailingList&gt; &lt;name&gt;User List&lt;/name&gt; &lt;subscribe&gt;user-subscribe@127.0.0.1&lt;/subscribe&gt; &lt;unsubscribe&gt;user-unsubscribe@127.0.0.1&lt;/unsubscribe&gt; &lt;post&gt;user@127.0.0.1&lt;/post&gt; &lt;archive&gt;http://127.0.0.1/user/&lt;/archive&gt; &lt;otherArchives&gt; &lt;otherArchive&gt;http://base.google.com/base/1/127.0.0.1&lt;/otherArchive&gt; &lt;/otherArchives&gt; &lt;/mailingList&gt; &lt;/mailingLists&gt; ...&lt;/project&gt; scm SCM（软件配置管理，也称为源代码/控制管理或简洁的版本控制）。常见的 scm 有 svn 和 git 。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;scm&gt; &lt;connection&gt;scm:svn:http://127.0.0.1/svn/my-project&lt;/connection&gt; &lt;developerConnection&gt;scm:svn:https://127.0.0.1/svn/my-project&lt;/developerConnection&gt; &lt;tag&gt;HEAD&lt;/tag&gt; &lt;url&gt;http://127.0.0.1/websvn/my-project&lt;/url&gt; &lt;/scm&gt; ...&lt;/project&gt; prerequisites POM 执行的预设条件。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;prerequisites&gt; &lt;maven&gt;2.0.6&lt;/maven&gt; &lt;/prerequisites&gt; ...&lt;/project&gt; repositories repositories 是遵循 Maven 存储库目录布局的 artifacts 集合。默认的 Maven 中央存储库位于https://repo.maven.apache.org/maven2/上。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;repositories&gt; &lt;repository&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;never&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;id&gt;codehausSnapshots&lt;/id&gt; &lt;name&gt;Codehaus Snapshots&lt;/name&gt; &lt;url&gt;http://snapshots.maven.codehaus.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; ... &lt;/pluginRepositories&gt; ...&lt;/project&gt; pluginRepositories 与 repositories 差不多。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;distributionManagement&gt; ... &lt;downloadUrl&gt;http://mojo.codehaus.org/my-project&lt;/downloadUrl&gt; &lt;status&gt;deployed&lt;/status&gt; &lt;/distributionManagement&gt; ...&lt;/project&gt; distributionManagement 它管理在整个构建过程中生成的 artifact 和支持文件的分布。从最后的元素开始： &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;distributionManagement&gt; ... &lt;downloadUrl&gt;http://mojo.codehaus.org/my-project&lt;/downloadUrl&gt; &lt;status&gt;deployed&lt;/status&gt; &lt;/distributionManagement&gt; ...&lt;/project&gt; repository - 与 repositories 相似 site - 站点信息 relocation - 项目迁移位置 profiles activation 是一个 profile 的关键。配置文件的功能来自于在某些情况下仅修改基本 POM 的功能。这些情况通过 activation 元素指定。 &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; ... &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;jdk&gt;1.5&lt;/jdk&gt; &lt;os&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;property&gt; &lt;name&gt;sparrow-type&lt;/name&gt; &lt;value&gt;African&lt;/value&gt; &lt;/property&gt; &lt;file&gt; &lt;exists&gt;$&#123;basedir&#125;/file2.properties&lt;/exists&gt; &lt;missing&gt;$&#123;basedir&#125;/file1.properties&lt;/missing&gt; &lt;/file&gt; &lt;/activation&gt; ... &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; 参考资料 maven 官方文档之 pom]]></content>
      <categories>
        <category>java</category>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>build</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 教程之 settings.xml 详解]]></title>
    <url>%2Fblog%2F2016%2F11%2F10%2Fjava%2Fjavatool%2Fbuild%2Fmaven%2Fmaven-settings%2F</url>
    <content type="text"><![CDATA[Maven 教程之 settings.xml 详解 📓 本文已归档到：「blog」 简介 settings.xml 有什么用？ settings.xml 文件位置 配置优先级 settings.xml 元素详解 顶级元素概览 LocalRepository InteractiveMode UsePluginRegistry Offline PluginGroups Servers Mirrors Proxies Profiles ActiveProfiles 参考资料 简介 settings.xml 有什么用？ 从 settings.xml 的文件名就可以看出，它是用来设置 maven 参数的配置文件。settings.xml 中包含类似本地仓储位置、修改远程仓储服务器、认证信息等配置。 settings.xml 是 maven 的全局配置文件。 pom.xml 文件是本地项目配置文件。 settings.xml 文件位置 settings.xml 文件一般存在于两个位置： 全局配置 - ${maven.home}/conf/settings.xml 用户配置 - ${user.home}/.m2/settings.xml 注意：用户配置优先于全局配置。${user.home} 和和所有其他系统属性只能在 3.0+版本上使用。请注意 windows 和 Linux 使用变量的区别。 配置优先级 重要：局部配置优先于全局配置。 配置优先级从高到低：pom.xml &gt; user settings &gt; global settings 如果这些文件同时存在，在应用配置时，会合并它们的内容，如果有重复的配置，优先级高的配置会覆盖优先级低的。 settings.xml 元素详解 顶级元素概览 下面列举了settings.xml中的顶级元素 &lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;localRepository/&gt; &lt;interactiveMode/&gt; &lt;usePluginRegistry/&gt; &lt;offline/&gt; &lt;pluginGroups/&gt; &lt;servers/&gt; &lt;mirrors/&gt; &lt;proxies/&gt; &lt;profiles/&gt; &lt;activeProfiles/&gt;&lt;/settings&gt; LocalRepository 作用：该值表示构建系统本地仓库的路径。 其默认值：~/.m2/repository。 &lt;localRepository&gt;$&#123;user.home&#125;/.m2/repository&lt;/localRepository&gt; InteractiveMode 作用：表示 maven 是否需要和用户交互以获得输入。 如果 maven 需要和用户交互以获得输入，则设置成 true，反之则应为 false。默认为 true。 &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; UsePluginRegistry 作用：maven 是否需要使用 plugin-registry.xml 文件来管理插件版本。 如果需要让 maven 使用文件~/.m2/plugin-registry.xml 来管理插件版本，则设为 true。默认为 false。 &lt;usePluginRegistry&gt;false&lt;/usePluginRegistry&gt; Offline 作用：表示 maven 是否需要在离线模式下运行。 如果构建系统需要在离线模式下运行，则为 true，默认为 false。 当由于网络设置原因或者安全因素，构建服务器不能连接远程仓库的时候，该配置就十分有用。 &lt;offline&gt;false&lt;/offline&gt; PluginGroups 作用：当插件的组织 id（groupId）没有显式提供时，供搜寻插件组织 Id（groupId）的列表。 该元素包含一个 pluginGroup 元素列表，每个子元素包含了一个组织 Id（groupId）。 当我们使用某个插件，并且没有在命令行为其提供组织 Id（groupId）的时候，Maven 就会使用该列表。默认情况下该列表包含了 org.apache.maven.plugins 和 org.codehaus.mojo。 &lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; ... &lt;pluginGroups&gt; &lt;!--plugin的组织Id（groupId） --&gt; &lt;pluginGroup&gt;org.codehaus.mojo&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; ...&lt;/settings&gt; Servers 作用：一般，仓库的下载和部署是在 pom.xml 文件中的 repositories 和 distributionManagement 元素中定义的。然而，一般类似用户名、密码（有些仓库访问是需要安全认证的）等信息不应该在 pom.xml 文件中配置，这些信息可以配置在 settings.xml 中。 &lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; ... &lt;!--配置服务端的一些设置。一些设置如安全证书不应该和pom.xml一起分发。这种类型的信息应该存在于构建服务器上的settings.xml文件中。 --&gt; &lt;servers&gt; &lt;!--服务器元素包含配置服务器时需要的信息 --&gt; &lt;server&gt; &lt;!--这是server的id（注意不是用户登陆的id），该id与distributionManagement中repository元素的id相匹配。 --&gt; &lt;id&gt;server001&lt;/id&gt; &lt;!--鉴权用户名。鉴权用户名和鉴权密码表示服务器认证所需要的登录名和密码。 --&gt; &lt;username&gt;my_login&lt;/username&gt; &lt;!--鉴权密码 。鉴权用户名和鉴权密码表示服务器认证所需要的登录名和密码。密码加密功能已被添加到2.1.0 +。详情请访问密码加密页面 --&gt; &lt;password&gt;my_password&lt;/password&gt; &lt;!--鉴权时使用的私钥位置。和前两个元素类似，私钥位置和私钥密码指定了一个私钥的路径（默认是$&#123;user.home&#125;/.ssh/id_dsa）以及如果需要的话，一个密语。将来passphrase和password元素可能会被提取到外部，但目前它们必须在settings.xml文件以纯文本的形式声明。 --&gt; &lt;privateKey&gt;$&#123;usr.home&#125;/.ssh/id_dsa&lt;/privateKey&gt; &lt;!--鉴权时使用的私钥密码。 --&gt; &lt;passphrase&gt;some_passphrase&lt;/passphrase&gt; &lt;!--文件被创建时的权限。如果在部署的时候会创建一个仓库文件或者目录，这时候就可以使用权限（permission）。这两个元素合法的值是一个三位数字，其对应了unix文件系统的权限，如664，或者775。 --&gt; &lt;filePermissions&gt;664&lt;/filePermissions&gt; &lt;!--目录被创建时的权限。 --&gt; &lt;directoryPermissions&gt;775&lt;/directoryPermissions&gt; &lt;/server&gt; &lt;/servers&gt; ...&lt;/settings&gt; Mirrors 作用：为仓库列表配置的下载镜像列表。 &lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; ... &lt;mirrors&gt; &lt;!-- 给定仓库的下载镜像。 --&gt; &lt;mirror&gt; &lt;!-- 该镜像的唯一标识符。id用来区分不同的mirror元素。 --&gt; &lt;id&gt;planetmirror.com&lt;/id&gt; &lt;!-- 镜像名称 --&gt; &lt;name&gt;PlanetMirror Australia&lt;/name&gt; &lt;!-- 该镜像的URL。构建系统会优先考虑使用该URL，而非使用默认的服务器URL。 --&gt; &lt;url&gt;http://downloads.planetmirror.com/pub/maven2&lt;/url&gt; &lt;!-- 被镜像的服务器的id。例如，如果我们要设置了一个Maven中央仓库（http://repo.maven.apache.org/maven2/）的镜像，就需要将该元素设置成central。这必须和中央仓库的id central完全一致。 --&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; ...&lt;/settings&gt; Proxies 作用：用来配置不同的代理。 &lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; ... &lt;proxies&gt; &lt;!--代理元素包含配置代理时需要的信息 --&gt; &lt;proxy&gt; &lt;!--代理的唯一定义符，用来区分不同的代理元素。 --&gt; &lt;id&gt;myproxy&lt;/id&gt; &lt;!--该代理是否是激活的那个。true则激活代理。当我们声明了一组代理，而某个时候只需要激活一个代理的时候，该元素就可以派上用处。 --&gt; &lt;active&gt;true&lt;/active&gt; &lt;!--代理的协议。 协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;!--代理的主机名。协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;host&gt;proxy.somewhere.com&lt;/host&gt; &lt;!--代理的端口。协议://主机名:端口，分隔成离散的元素以方便配置。 --&gt; &lt;port&gt;8080&lt;/port&gt; &lt;!--代理的用户名，用户名和密码表示代理服务器认证的登录名和密码。 --&gt; &lt;username&gt;proxyuser&lt;/username&gt; &lt;!--代理的密码，用户名和密码表示代理服务器认证的登录名和密码。 --&gt; &lt;password&gt;somepassword&lt;/password&gt; &lt;!--不该被代理的主机名列表。该列表的分隔符由代理服务器指定；例子中使用了竖线分隔符，使用逗号分隔也很常见。 --&gt; &lt;nonProxyHosts&gt;*.google.com|ibiblio.org&lt;/nonProxyHosts&gt; &lt;/proxy&gt; &lt;/proxies&gt; ...&lt;/settings&gt; Profiles 作用：根据环境参数来调整构建配置的列表。 settings.xml 中的 profile 元素是 pom.xml 中 profile 元素的裁剪版本。 它包含了id、activation、repositories、pluginRepositories 和 properties 元素。这里的 profile 元素只包含这五个子元素是因为这里只关心构建系统这个整体（这正是 settings.xml 文件的角色定位），而非单独的项目对象模型设置。如果一个 settings.xml 中的 profile 被激活，它的值会覆盖任何其它定义在 pom.xml 中带有相同 id 的 profile。 &lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; ... &lt;profiles&gt; &lt;profile&gt; &lt;!-- profile的唯一标识 --&gt; &lt;id&gt;test&lt;/id&gt; &lt;!-- 自动触发profile的条件逻辑 --&gt; &lt;activation /&gt; &lt;!-- 扩展属性列表 --&gt; &lt;properties /&gt; &lt;!-- 远程仓库列表 --&gt; &lt;repositories /&gt; &lt;!-- 插件仓库列表 --&gt; &lt;pluginRepositories /&gt; &lt;/profile&gt; &lt;/profiles&gt; ...&lt;/settings&gt; Activation 作用：自动触发 profile 的条件逻辑。 如 pom.xml 中的 profile 一样，profile 的作用在于它能够在某些特定的环境中自动使用某些特定的值；这些环境通过 activation 元素指定。 activation 元素并不是激活 profile 的唯一方式。settings.xml 文件中的 activeProfile 元素可以包含 profile 的 id。profile 也可以通过在命令行，使用 -P 标记和逗号分隔的列表来显式的激活（如，-P test）。 &lt;activation&gt; &lt;!--profile默认是否激活的标识 --&gt; &lt;activeByDefault&gt;false&lt;/activeByDefault&gt; &lt;!--当匹配的jdk被检测到，profile被激活。例如，1.4激活JDK1.4，1.4.0_2，而!1.4激活所有版本不是以1.4开头的JDK。 --&gt; &lt;jdk&gt;1.5&lt;/jdk&gt; &lt;!--当匹配的操作系统属性被检测到，profile被激活。os元素可以定义一些操作系统相关的属性。 --&gt; &lt;os&gt; &lt;!--激活profile的操作系统的名字 --&gt; &lt;name&gt;Windows XP&lt;/name&gt; &lt;!--激活profile的操作系统所属家族(如 'windows') --&gt; &lt;family&gt;Windows&lt;/family&gt; &lt;!--激活profile的操作系统体系结构 --&gt; &lt;arch&gt;x86&lt;/arch&gt; &lt;!--激活profile的操作系统版本 --&gt; &lt;version&gt;5.1.2600&lt;/version&gt; &lt;/os&gt; &lt;!--如果Maven检测到某一个属性（其值可以在POM中通过$&#123;name&#125;引用），其拥有对应的name = 值，Profile就会被激活。如果值字段是空的，那么存在属性名称字段就会激活profile，否则按区分大小写方式匹配属性值字段 --&gt; &lt;property&gt; &lt;!--激活profile的属性的名称 --&gt; &lt;name&gt;mavenVersion&lt;/name&gt; &lt;!--激活profile的属性的值 --&gt; &lt;value&gt;2.0.3&lt;/value&gt; &lt;/property&gt; &lt;!--提供一个文件名，通过检测该文件的存在或不存在来激活profile。missing检查文件是否存在，如果不存在则激活profile。另一方面，exists则会检查文件是否存在，如果存在则激活profile。 --&gt; &lt;file&gt; &lt;!--如果指定的文件存在，则激活profile。 --&gt; &lt;exists&gt;$&#123;basedir&#125;/file2.properties&lt;/exists&gt; &lt;!--如果指定的文件不存在，则激活profile。 --&gt; &lt;missing&gt;$&#123;basedir&#125;/file1.properties&lt;/missing&gt; &lt;/file&gt;&lt;/activation&gt; 注：在 maven 工程的 pom.xml 所在目录下执行 mvn help:active-profiles 命令可以查看中央仓储的 profile 是否在工程中生效。 properties 作用：对应profile的扩展属性列表。 maven 属性和 ant 中的属性一样，可以用来存放一些值。这些值可以在 pom.xml 中的任何地方使用标记${X}来使用，这里 X 是指属性的名称。属性有五种不同的形式，并且都能在 settings.xml 文件中访问。 &lt;!-- 1. env.X: 在一个变量前加上"env."的前缀，会返回一个shell环境变量。例如,"env.PATH"指代了$path环境变量（在Windows上是%PATH%）。 2. project.x：指代了POM中对应的元素值。例如: &lt;project&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;/project&gt;通过$&#123;project.version&#125;获得version的值。 3. settings.x: 指代了settings.xml中对应元素的值。例如：&lt;settings&gt;&lt;offline&gt;false&lt;/offline&gt;&lt;/settings&gt;通过 $&#123;settings.offline&#125;获得offline的值。 4. Java System Properties: 所有可通过java.lang.System.getProperties()访问的属性都能在POM中使用该形式访问，例如 $&#123;java.home&#125;。 5. x: 在&lt;properties/&gt;元素中，或者外部文件中设置，以$&#123;someVar&#125;的形式使用。 --&gt;&lt;properties&gt; &lt;user.install&gt;$&#123;user.home&#125;/our-project&lt;/user.install&gt;&lt;/properties&gt; 注：如果该 profile 被激活，则可以在pom.xml中使用${user.install}。 Repositories 作用：远程仓库列表，它是 maven 用来填充构建系统本地仓库所使用的一组远程仓库。 &lt;repositories&gt; &lt;!--包含需要连接到远程仓库的信息 --&gt; &lt;repository&gt; &lt;!--远程仓库唯一标识 --&gt; &lt;id&gt;codehausSnapshots&lt;/id&gt; &lt;!--远程仓库名称 --&gt; &lt;name&gt;Codehaus Snapshots&lt;/name&gt; &lt;!--如何处理远程仓库里发布版本的下载 --&gt; &lt;releases&gt; &lt;!--true或者false表示该仓库是否为下载某种类型构件（发布版，快照版）开启。 --&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;!--该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。这里的选项是：always（一直），daily（默认，每日），interval：X（这里X是以分钟为单位的时间间隔），或者never（从不）。 --&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;!--当Maven验证构件校验文件失败时该怎么做-ignore（忽略），fail（失败），或者warn（警告）。 --&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;!--如何处理远程仓库里快照版本的下载。有了releases和snapshots这两组配置，POM就可以在每个单独的仓库中，为每种类型的构件采取不同的策略。例如，可能有人会决定只为开发目的开启对快照版本下载的支持。参见repositories/repository/releases元素 --&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;!--远程仓库URL，按protocol://hostname/path形式 --&gt; &lt;url&gt;http://snapshots.maven.codehaus.org/maven2&lt;/url&gt; &lt;!--用于定位和排序构件的仓库布局类型-可以是default（默认）或者legacy（遗留）。Maven 2为其仓库提供了一个默认的布局；然而，Maven 1.x有一种不同的布局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留）。 --&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt;&lt;/repositories&gt; pluginRepositories 作用：发现插件的远程仓库列表。 和 repository 类似，只是 repository 是管理 jar 包依赖的仓库，pluginRepositories 则是管理插件的仓库。 maven 插件是一种特殊类型的构件。由于这个原因，插件仓库独立于其它仓库。pluginRepositories 元素的结构和 repositories 元素的结构类似。每个 pluginRepository 元素指定一个 Maven 可以用来寻找新插件的远程地址。 &lt;pluginRepositories&gt; &lt;!-- 包含需要连接到远程插件仓库的信息.参见profiles/profile/repositories/repository元素的说明 --&gt; &lt;pluginRepository&gt; &lt;releases&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled /&gt; &lt;updatePolicy /&gt; &lt;checksumPolicy /&gt; &lt;/snapshots&gt; &lt;id /&gt; &lt;name /&gt; &lt;url /&gt; &lt;layout /&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt; ActiveProfiles 作用：手动激活 profiles 的列表，按照profile被应用的顺序定义activeProfile。 该元素包含了一组 activeProfile 元素，每个 activeProfile 都含有一个 profile id。任何在 activeProfile 中定义的 profile id，不论环境设置如何，其对应的 profile 都会被激活。如果没有匹配的 profile，则什么都不会发生。 例如，env-test 是一个 activeProfile，则在 pom.xml（或者 profile.xml）中对应 id 的 profile 会被激活。如果运行过程中找不到这样一个 profile，Maven 则会像往常一样运行。 &lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; ... &lt;activeProfiles&gt; &lt;!-- 要激活的profile id --&gt; &lt;activeProfile&gt;env-test&lt;/activeProfile&gt; &lt;/activeProfiles&gt; ...&lt;/settings&gt; 至此，maven settings.xml 中的标签都讲解完毕，希望对大家有所帮助。 参考资料 maven 官方文档之 settings]]></content>
      <categories>
        <category>java</category>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>build</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 教程之部署私服或中央仓库]]></title>
    <url>%2Fblog%2F2016%2F11%2F10%2Fjava%2Fjavatool%2Fbuild%2Fmaven%2Fmaven-deploy%2F</url>
    <content type="text"><![CDATA[Maven 教程之发布 jar 到私服或中央仓库 📓 本文已归档到：「blog」 发布 jar 包到中央仓库 在 Sonatype 创建 Issue 使用 GPG 生成公私钥对 Maven 配置 部署和发布 部署 maven 私服 下载安装 Nexus 启动停止 Nexus 使用 Nexus 参考资料 发布 jar 包到中央仓库 为了避免重复造轮子，相信每个 Java 程序员都想打造自己的脚手架或工具包（自己定制的往往才是最适合自己的）。那么如何将自己的脚手架发布到中央仓库呢？下面我们将一步步来实现。 在 Sonatype 创建 Issue （1）注册 Sonatype 账号 发布 Java 包到 Maven 中央仓库，首先需要在 Sonatype 网站创建一个工单(Issues)。 第一次使用这个网站的时候需要注册自己的帐号（这个帐号和密码需要记住，后面会用到）。 （2）创建 Issue 注册账号成功后，根据你 Java 包的功能分别写上Summary、Description、Group Id、SCM url以及Project URL等必要信息，可以参见我之前创建的 Issue：OSSRH-36187。 创建完之后需要等待 Sonatype 的工作人员审核处理，审核时间还是很快的，我的审核差不多等待了两小时。当 Issue 的 Status 变为RESOLVED后，就可以进行下一步操作了。 说明：如果你的 Group Id 填写的是自己的网站（我的就是这种情况），Sonatype 的工作人员会询问你那个 Group Id 是不是你的域名，你只需要在上面回答是就行，然后就会通过审核。 使用 GPG 生成公私钥对 （1）安装 Gpg4win Windows 系统，可以下载 Gpg4win 软件来生成密钥对。 Gpg4win 下载地址 安装后，执行命令 gpg --version 检查是否安装成功。 C:\Program Files (x86)\GnuPG\bin&gt;gpg --versiongpg (GnuPG) 2.2.10libgcrypt 1.8.3Copyright (C) 2018 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;https://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the exdunwu permitted by law.Home: C:/Users/Administrator/AppData/Roaming/gnupgSupported algorithms:Pubkey: RSA, ELG, DSA, ECDH, ECDSA, EDDSACipher: IDEA, 3DES, CAST5, BLOWFISH, AES, AES192, AES256, TWOFISH, CAMELLIA128, CAMELLIA192, CAMELLIA256Hash: SHA1, RIPEMD160, SHA256, SHA384, SHA512, SHA224Compression: Uncompressed, ZIP, ZLIB, BZIP2 （2）生成密钥对 执行命令 gpg --gen-key C:\Program Files (x86)\GnuPG\bin&gt;gpg --gen-keygpg (GnuPG) 2.2.10; Copyright (C) 2018 Free Software Foundation, Inc.This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the exdunwu permitted by law.Note: Use "gpg --full-generate-key" for a full featured key generation dialog.GnuPG needs to construct a user ID to identify your key.Real name: Zhang PengEmail address: forbreak@163.comYou selected this USER-ID: "Zhang Peng &lt;forbreak@163.com&gt;"Change (N)ame, (E)mail, or (O)kay/(Q)uit? O 说明：按照提示，依次输入用户名、邮箱。 （3）查看公钥 C:\Program Files (x86)\GnuPG\bin&gt;gpg --list-keysgpg: checking the trustdbgpg: marginals needed: 3 completes needed: 1 trust model: pgpgpg: depth: 0 valid: 2 signed: 0 trust: 0-, 0q, 0n, 0m, 0f, 2ugpg: next trustdb check due at 2020-11-05C:/Users/Administrator/AppData/Roaming/gnupg/pubring.kbx--------------------------------------------------------pub rsa2048 2018-11-06 [SC] [expires: 2020-11-06] E4CE537A3803D49C35332221A306519BAFF57F60uid [ultimate] forbreak &lt;forbreak@163.com&gt;sub rsa2048 2018-11-06 [E] [expires: 2020-11-06] 说明：其中，E4CE537A3803D49C35332221A306519BAFF57F60 就是公钥 （4）将公钥发布到 PGP 密钥服务器 执行 gpg --keyserver hkp://pool.sks-keyservers.net --send-keys 发布公钥： C:\Program Files (x86)\GnuPG\bin&gt;gpg --keyserver hkp://pool.sks-keyservers.net --send-keys E4CE537A3803D49C35332221A306519BAFF57F60gpg: sending key A306519BAFF57F60 to hkp://pool.sks-keyservers.net 注意：有可能出现 gpg: keyserver receive failed: No dat 错误，等大约 30 分钟后再执行就不会报错了。 （5）查看公钥是否发布成功 执行 gpg --keyserver hkp://pool.sks-keyservers.net --recv-keys 查看公钥是否发布成功。 C:\Program Files (x86)\GnuPG\bin&gt;gpg --keyserver hkp://pool.sks-keyservers.net --recv-keys E4CE537A3803D49C35332221A306519BAFF57F60gpg: key A306519BAFF57F60: "forbreak &lt;forbreak@163.com&gt;" not changedgpg: Total number processed: 1gpg: unchanged: 1 Maven 配置 完成了前两个章节的准备工作，就可以将 jar 包上传到中央仓库了。当然了，我们还要对 maven 做一些配置。 settings.xml 配置 一份完整的 settings.xml 配置示例如下： &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;pluginGroups&gt; &lt;pluginGroup&gt;org.sonatype.plugins&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; &lt;!-- 用户名、密码就是 Sonatype 账号、密码 --&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;sonatype-snapshots&lt;/id&gt; &lt;username&gt;xxxxxx&lt;/username&gt; &lt;password&gt;xxxxxx&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;sonatype-staging&lt;/id&gt; &lt;username&gt;xxxxxx&lt;/username&gt; &lt;password&gt;xxxxxx&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;!-- 使用 aliyun maven 仓库加速下载 --&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/condunwu/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;!-- gpg 的密码，注意，这里不是指公钥 --&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;sonatype&lt;/id&gt; &lt;properties&gt; &lt;gpg.executable&gt;C:/Program Files (x86)/GnuPG/bin/gpg.exe&lt;/gpg.executable&gt; &lt;gpg.passphrase&gt;xxxxxx&lt;/gpg.passphrase&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;sonatype&lt;/activeProfile&gt; &lt;/activeProfiles&gt;&lt;/settings&gt; pom.xml 配置 （1）添加 licenses、scm、developers 配置： &lt;licenses&gt; &lt;license&gt; &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt; &lt;url&gt;http://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;/license&gt;&lt;/licenses&gt;&lt;developers&gt; &lt;developer&gt; &lt;name&gt;xxxxxx&lt;/name&gt; &lt;email&gt;forbreak@163.com&lt;/email&gt; &lt;url&gt;https://github.com/dunwu&lt;/url&gt; &lt;/developer&gt;&lt;/developers&gt;&lt;scm&gt; &lt;url&gt;https://github.com/dunwu/dunwu&lt;/url&gt; &lt;connection&gt;git@github.com:dunwu/dunwu.git&lt;/connection&gt; &lt;developerConnection&gt;https://github.com/dunwu&lt;/developerConnection&gt;&lt;/scm&gt; （2）添加 distributionManagement 配置 &lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;sonatype-snapshots&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;repository&gt; &lt;id&gt;sonatype-staging&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/service/local/staging/deploy/maven2&lt;/url&gt; &lt;/repository&gt;&lt;/distributionManagement&gt; 说明：&lt;snapshotRepository&gt; 指定的是 snapshot 仓库地址；&lt;repository&gt; 指定的是 staging （正式版）仓库地址。需要留意的是，这里的 id 需要和 settings.xml 中的 &lt;server&gt; 的 id 保持一致。 （3）添加 profiles 配置 &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;sonatype&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.sonatype.plugins&lt;/groupId&gt; &lt;artifactId&gt;nexus-staging-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.6.7&lt;/version&gt; &lt;extensions&gt;true&lt;/extensions&gt; &lt;configuration&gt; &lt;serverId&gt;sonatype-snapshots&lt;/serverId&gt; &lt;nexusUrl&gt;https://oss.sonatype.org/&lt;/nexusUrl&gt; &lt;autoReleaseAfterClose&gt;true&lt;/autoReleaseAfterClose&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;configuration&gt; &lt;failOnError&gt;false&lt;/failOnError&gt; &lt;quiet&gt;true&lt;/quiet&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-javadocs&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;sign-artifacts&lt;/id&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;sign&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt;&lt;/profiles&gt; 部署和发布 按照上面的步骤配置完后，一切都已经 OK。 此时，使用 mvn clean deploy -P sonatype 命令就可以发布 jar 包到中央仓库了： 说明：-P 参数后面的 sonatype 需要和 pom.xml 中 &lt;profile&gt; 的 id 保持一致，才能激活 profile。 部署 maven 私服 工作中，Java 程序员开发的商用 Java 项目，一般不想发布到中央仓库，使得人人尽知。这时，我们就需要搭建私服，将 maven 服务器部署在公司内部网络，从而避免 jar 包流传出去。怎么做呢，让我们来一步步学习吧。 下载安装 Nexus 进入官方下载地址，选择合适版本下载。 本人希望将 Nexus 部署在 Linux 机器，所以选用的是 Unix 版本。 这里，如果想通过命令方式直接下载（比如用脚本安装），可以在官方历史发布版本页面中找到合适版本，然后执行以下命令： wget -O /opt/maven/nexus-unix.tar.gz http://download.sonatype.com/nexus/3/nexus-3.13.0-01-unix.tar.gztar -zxf nexus-unix.tar.gz 解压后，有两个目录： nexus-3.13.0-01 - 包含了 Nexus 运行所需要的文件。是 Nexus 运行必须的。 sonatype-work - 包含了 Nexus 生成的配置文件、日志文件、仓库文件等。当我们需要备份 Nexus 的时候默认备份此目录即可。 启动停止 Nexus 进入 nexus-3.13.0-01/bin 目录，有一个可执行脚本 nexus。 执行 ./nexus，可以查看允许执行的参数，如下所示，含义可谓一目了然： $ ./nexusUsage: ./nexus &#123;start|stop|run|run-redirect|status|restart|force-reload&#125; 启动 nexus - ./nexus start 停止 nexus - 启动成功后，在浏览器中访问 http://&lt;ip&gt;:8081，欢迎页面如下图所示： 点击右上角 Sign in 登录，默认用户名/密码为：admin/admin123。 有必要提一下的是，在 Nexus 的 Repositories 管理页面，展示了可用的 maven 仓库，如下图所示： 说明： maven-central - maven 中央库（如果没有配置 mirror，默认就从这里下载 jar 包），从 https://repo1.maven.org/maven2/ 获取资源 maven-releases - 存储私有仓库的发行版 jar 包 maven-snapshots - 存储私有仓库的快照版（调试版本） jar 包 maven-public - 私有仓库的公共空间，把上面三个仓库组合在一起对外提供服务，在本地 maven 基础配置 settings.xml 中使用。 使用 Nexus 如果要使用 Nexus，还必须在 settings.xml 和 pom.xml 中配置认证信息。 配置 settings.xml 一份完整的 settings.xml： &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;pluginGroups&gt; &lt;pluginGroup&gt;org.sonatype.plugins&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; &lt;!-- Maven 私服账号信息 --&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;!-- jar 包下载地址 --&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;public&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;url&gt;http://10.255.255.224:8081/repository/maven-public/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;zp&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;url&gt;http://central&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;central&lt;/id&gt; &lt;url&gt;http://central&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;activeProfile&gt;zp&lt;/activeProfile&gt; &lt;/activeProfiles&gt;&lt;/settings&gt; 配置 pom.xml 在 pom.xml 中添加如下配置： &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;name&gt;Releases&lt;/name&gt; &lt;url&gt;http://10.255.255.224:8081/repository/maven-releases&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;name&gt;Snapshot&lt;/name&gt; &lt;url&gt;http://10.255.255.224:8081/repository/maven-snapshots&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 注意： &lt;repository&gt; 和 &lt;snapshotRepository&gt; 的 id 必须和 settings.xml 配置文件中的 &lt;server&gt; 标签中的 id 匹配。 &lt;url&gt; 标签的地址需要和 maven 私服的地址匹配。 执行 maven 构建 如果要使用 settings.xml 中的私服配置，必须通过指定 -P zp 来激活 profile。 示例： ## 编译并打包 maven 项目$ mvn clean package -Dmaven.skip.test=true -P zp## 编译并上传 maven 交付件（jar 包）$ mvn clean deploy -Dmaven.skip.test=true -P zp 参考资料 https://www.jianshu.com/p/8c3d7fb09bce http://www.ruanyifeng.com/blog/2013/07/gpg.html https://www.cnblogs.com/hoobey/p/6102382.html https://blog.csdn.net/wzygis/article/details/49276779 https://blog.csdn.net/clj198606061111/article/details/52200928]]></content>
      <categories>
        <category>java</category>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>build</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 正则]]></title>
    <url>%2Fblog%2F2016%2F11%2F03%2Fjava%2Fjavacore%2Fadvanced%2FJava%E6%AD%A3%E5%88%99%2F</url>
    <content type="text"><![CDATA[Java 正则 📓 本文已归档到：「blog」 导读 招式篇 Pattern 类 Matcher 类 心法篇 元字符 分组构造 贪婪与懒惰 附录 正则应用 最实用的正则 特定字符 特定数字 参考 导读 正则表达式是什么？有什么用？ 正则表达式(Regular Expression)是一种文本规则，可以用来校验、查找、替换与规则匹配的文本。 又爱又恨的正则 正则表达式是一个强大的文本匹配工具，但是它的规则实在很繁琐，而且理解起来也颇为蛋疼，容易让人望而生畏。 如何学习正则 刚接触正则时，我看了一堆正则的语义说明，但是仍然不明所以。后来，我多接触一些正则的应用实例，渐渐有了感觉，再结合语义说明，终有领悟。我觉得正则表达式和武侠修练武功差不多，应该先练招式，再练心法。如果一开始就直接看正则的规则，保证你会懵逼。当你熟悉基本招式（正则基本使用案例）后，也该修炼修炼心法（正则语法）了。真正的高手不能只靠死记硬背那么几招把式。就像张三丰教张无忌太极拳一样，领悟心法，融会贯通，少侠你就可以无招胜有招，成为传说中的绝世高手。 以上闲话可归纳为一句：学习正则应该从实例去理解规则。 招式篇 JDK 中的java.util.regex包提供了对正则表达式的支持。 java.util.regex有三个核心类： Pattern 类：Pattern是一个正则表达式的编译表示。 Matcher 类：Matcher是对输入字符串进行解释和匹配操作的引擎。 PatternSyntaxException：PatternSyntaxException是一个非强制异常类，它表示一个正则表达式模式中的语法错误。 **注：**需要格外注意一点，在 Java 中使用反斜杠&quot;\“时必须写成 &quot;\\&quot;。所以本文的代码出现形如String regex = &quot;\\$\\{.*?\\}&quot; 其实就是”\$\{.*?\}&quot;，不要以为是画风不对哦。 Pattern 类 Pattern类没有公共构造方法。要创建一个Pattern对象，你必须首先调用其静态方法compile，加载正则规则字符串，然后返回一个 Pattern 对象。 与Pattern类一样，Matcher类也没有公共构造方法。你需要调用Pattern对象的matcher方法来获得一个Matcher对象。 案例：Pattern 和 Matcher 的初始化 Pattern p = Pattern.compile(regex);Matcher m = p.matcher(content); Matcher 类 Matcher类可以说是java.util.regex核心类中的必杀技！ Matcher类有三板斧（三类功能）： 校验 查找 替换 下面我们来领略一下这三块的功能。 校验文本是否与正则规则匹配 为了检查文本是否与正则规则匹配，Matcher 提供了以下几个返回值为boolean的方法。 序号 方法及说明 1 **public boolean lookingAt() ** 尝试将从区域开头开始的输入序列与该模式匹配。 2 **public boolean find() **尝试查找与该模式匹配的输入序列的下一个子序列。 3 **public boolean find(int start）**重置此匹配器，然后尝试查找匹配该模式、从指定索引开始的输入序列的下一个子序列。 4 **public boolean matches() **尝试将整个区域与模式匹配。 如果你傻傻分不清上面的查找方法有什么区别，那么下面一个例子就可以让你秒懂。 案例：lookingAt vs find vs matches public static void main(String[] args) &#123; checkLookingAt("hello", "helloworld"); checkLookingAt("world", "helloworld"); checkFind("hello", "helloworld"); checkFind("world", "helloworld"); checkMatches("hello", "helloworld"); checkMatches("world", "helloworld"); checkMatches("helloworld", "helloworld");&#125;private static void checkLookingAt(String regex, String content) &#123; Pattern p = Pattern.compile(regex); Matcher m = p.matcher(content); if (m.lookingAt()) &#123; System.out.println(content + "\tlookingAt： " + regex); &#125; else &#123; System.out.println(content + "\tnot lookingAt： " + regex); &#125;&#125;private static void checkFind(String regex, String content) &#123; Pattern p = Pattern.compile(regex); Matcher m = p.matcher(content); if (m.find()) &#123; System.out.println(content + "\tfind： " + regex); &#125; else &#123; System.out.println(content + "\tnot find： " + regex); &#125;&#125;private static void checkMatches(String regex, String content) &#123; Pattern p = Pattern.compile(regex); Matcher m = p.matcher(content); if (m.matches()) &#123; System.out.println(content + "\tmatches： " + regex); &#125; else &#123; System.out.println(content + "\tnot matches： " + regex); &#125;&#125; 输出 helloworld lookingAt： hellohelloworld not lookingAt： worldhelloworld find： hellohelloworld find： worldhelloworld not matches： hellohelloworld not matches： worldhelloworld matches： helloworld 说明 regex = “world” 表示的正则规则是以 world 开头的字符串，regex = “hello” 和 regex = “helloworld” 也是同理。 lookingAt方法从头部开始，检查 content 字符串是否有子字符串于正则规则匹配。 find方法检查 content 字符串是否有子字符串于正则规则匹配，不管字符串所在位置。 matches方法检查 content 字符串整体是否与正则规则匹配。 查找匹配正则规则的文本位置 为了查找文本匹配正则规则的位置，Matcher提供了以下方法： 序号 方法及说明 1 **public int start() **返回以前匹配的初始索引。 2 public int start(int group) 返回在以前的匹配操作期间，由给定组所捕获的子序列的初始索引 3 **public int end()**返回最后匹配字符之后的偏移量。 4 **public int end(int group)**返回在以前的匹配操作期间，由给定组所捕获子序列的最后字符之后的偏移量。 5 **public String group()**返回前一个符合匹配条件的子序列。 6 **public String group(int group)**返回指定的符合匹配条件的子序列。 案例：使用 start()、end()、group() 查找所有匹配正则条件的子序列 public static void main(String[] args) &#123; final String regex = "world"; final String content = "helloworld helloworld"; Pattern p = Pattern.compile(regex); Matcher m = p.matcher(content); System.out.println("content: " + content); int i = 0; while (m.find()) &#123; i++; System.out.println("[" + i + "th] found"); System.out.print("start: " + m.start() + ", "); System.out.print("end: " + m.end() + ", "); System.out.print("group: " + m.group() + "\n"); &#125;&#125; 输出 content: helloworld helloworld[1th] foundstart: 5, end: 10, group: world[2th] foundstart: 16, end: 21, group: world 说明 例子很直白，不言自明了吧。 替换匹配正则规则的文本 替换方法是替换输入字符串里文本的方法： 序号 方法及说明 1 **public Matcher appendReplacement(StringBuffer sb, String replacement)**实现非终端添加和替换步骤。 2 **public StringBuffer appendTail(StringBuffer sb)**实现终端添加和替换步骤。 3 **public String replaceAll(String replacement) ** 替换模式与给定替换字符串相匹配的输入序列的每个子序列。 4 public String replaceFirst(String replacement) 替换模式与给定替换字符串匹配的输入序列的第一个子序列。 5 **public static String quoteReplacement(String s)**返回指定字符串的字面替换字符串。这个方法返回一个字符串，就像传递给 Matcher 类的 appendReplacement 方法一个字面字符串一样工作。 案例：replaceFirst vs replaceAll public static void main(String[] args) &#123; String regex = "can"; String replace = "can not"; String content = "I can because I think I can."; Pattern p = Pattern.compile(regex); Matcher m = p.matcher(content); System.out.println("content: " + content); System.out.println("replaceFirst: " + m.replaceFirst(replace)); System.out.println("replaceAll: " + m.replaceAll(replace));&#125; 输出 content: I can because I think I can.replaceFirst: I can not because I think I can.replaceAll: I can not because I think I can not. 说明 replaceFirst：替换第一个匹配正则规则的子序列。 replaceAll：替换所有匹配正则规则的子序列。 案例：appendReplacement、appendTail 和 replaceAll public static void main(String[] args) &#123; String regex = "can"; String replace = "can not"; String content = "I can because I think I can."; StringBuffer sb = new StringBuffer(); StringBuffer sb2 = new StringBuffer(); System.out.println("content: " + content); Pattern p = Pattern.compile(regex); Matcher m = p.matcher(content); while (m.find()) &#123; m.appendReplacement(sb, replace); &#125; System.out.println("appendReplacement: " + sb); m.appendTail(sb); System.out.println("appendTail: " + sb);&#125; 输出 content: I can because I think I can.appendReplacement: I can not because I think I can notappendTail: I can not because I think I can not. 说明 从输出结果可以看出，appendReplacement和appendTail方法组合起来用，功能和replaceAll是一样的。 如果你查看replaceAll的源码，会发现其内部就是使用appendReplacement和appendTail方法组合来实现的。 案例：quoteReplacement 和 replaceAll，解决特殊字符替换问题 public static void main(String[] args) &#123; String regex = "\\$\\&#123;.*?\\&#125;"; String replace = "$&#123;product&#125;"; String content = "product is $&#123;productName&#125;."; Pattern p = Pattern.compile(regex); Matcher m = p.matcher(content); String replaceAll = m.replaceAll(replace); System.out.println("content: " + content); System.out.println("replaceAll: " + replaceAll);&#125; 输出 Exception in thread "main" java.lang.IllegalArgumentException: No group with name &#123;product&#125; at java.util.regex.Matcher.appendReplacement(Matcher.java:849) at java.util.regex.Matcher.replaceAll(Matcher.java:955) at org.zp.notes.javase.regex.RegexDemo.wrongMethod(RegexDemo.java:42) at org.zp.notes.javase.regex.RegexDemo.main(RegexDemo.java:18) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147) 说明 String regex = &quot;\\$\\{.*?\\}&quot;;表示匹配类似${name}这样的字符串。由于$、{ 、}都是特殊字符，需要用反义字符\来修饰才能被当做一个字符串字符来处理。 上面的例子是想将 ${productName} 替换为 ${product} ，然而replaceAll方法却将传入的字符串中的$当做特殊字符来处理了。结果产生异常。 如何解决这个问题? JDK1.5 引入了quoteReplacement方法。它可以用来转换特殊字符。其实源码非常简单，就是判断字符串中如果有\或$，就为它加一个转义字符\ 我们对上面的代码略作调整： m.replaceAll(replace)改为m.replaceAll(Matcher.quoteReplacement(replace))，新代码如下： public static void main(String[] args) &#123; String regex = "\\$\\&#123;.*?\\&#125;"; String replace = "$&#123;product&#125;"; String content = "product is $&#123;productName&#125;."; Pattern p = Pattern.compile(regex); Matcher m = p.matcher(content); String replaceAll = m.replaceAll(Matcher.quoteReplacement(replace)); System.out.println("content: " + content); System.out.println("replaceAll: " + replaceAll);&#125; 输出 content: product is $&#123;productName&#125;.replaceAll: product is $&#123;product&#125;. 说明 字符串中如果有\或$，不能被正常解析的问题解决。 心法篇 为了理解下面章节的内容，你需要先了解一些基本概念。 正则表达式 正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。 元字符 元字符(metacharacters)就是正则表达式中具有特殊意义的专用字符。 普通字符 普通字符包括没有显式指定为元字符的所有可打印和不可打印字符。这包括所有大写和小写字母、所有数字、所有标点符号和一些其他符号。 元字符 基本元字符 正则表达式的元字符难以记忆，很大程度上是因为有很多为了简化表达而出现的等价字符。 而实际上最基本的元字符，并没有那么多。对于大部分的场景，基本元字符都可以搞定。 让我们从一个个实例出发，由浅入深的去体会正则的奥妙。 多选 - | 例 匹配一个确定的字符串 checkMatches("abc", "abc"); 如果要匹配一个确定的字符串，非常简单，如例 1 所示。 如果你不确定要匹配的字符串，希望有多个选择，怎么办？ 答案是：使用元字符| ，它的含义是或。 例 匹配多个可选的字符串 // 测试正则表达式字符：|Assert.assertTrue(checkMatches("yes|no", "yes"));Assert.assertTrue(checkMatches("yes|no", "no"));Assert.assertFalse(checkMatches("yes|no", "right")); 输出 yes matches： yes|nono matches： yes|noright not matches： yes|no 分组 - () 如果你希望表达式由多个子表达式组成，你可以使用 ()。 例 匹配组合字符串 Assert.assertTrue(checkMatches("(play|end)(ing|ed)", "ended"));Assert.assertTrue(checkMatches("(play|end)(ing|ed)", "ending"));Assert.assertTrue(checkMatches("(play|end)(ing|ed)", "playing"));Assert.assertTrue(checkMatches("(play|end)(ing|ed)", "played")); 输出 ended matches： (play|end)(ing|ed)ending matches： (play|end)(ing|ed)playing matches： (play|end)(ing|ed)played matches： (play|end)(ing|ed) 指定单字符有效范围 - [] 前面展示了如何匹配字符串，但是很多时候你需要精确的匹配一个字符，这时可以使用[] 。 例 字符在指定范围 // 测试正则表达式字符：[]Assert.assertTrue(checkMatches("[abc]", "b")); // 字符只能是a、b、cAssert.assertTrue(checkMatches("[a-z]", "m")); // 字符只能是a - zAssert.assertTrue(checkMatches("[A-Z]", "O")); // 字符只能是A - ZAssert.assertTrue(checkMatches("[a-zA-Z]", "K")); // 字符只能是a - z和A - ZAssert.assertTrue(checkMatches("[a-zA-Z]", "k"));Assert.assertTrue(checkMatches("[0-9]", "5")); // 字符只能是0 - 9 输出 b matches： [abc]m matches： [a-z]O matches： [A-Z]K matches： [a-zA-Z]k matches： [a-zA-Z]5 matches： [0-9] 指定单字符无效范围 - [^] 例 字符不能在指定范围 如果需要匹配一个字符的逆操作，即字符不能在指定范围，可以使用[^]。 // 测试正则表达式字符：[^]Assert.assertFalse(checkMatches("[^abc]", "b")); // 字符不能是a、b、cAssert.assertFalse(checkMatches("[^a-z]", "m")); // 字符不能是a - zAssert.assertFalse(checkMatches("[^A-Z]", "O")); // 字符不能是A - ZAssert.assertFalse(checkMatches("[^a-zA-Z]", "K")); // 字符不能是a - z和A - ZAssert.assertFalse(checkMatches("[^a-zA-Z]", "k"));Assert.assertFalse(checkMatches("[^0-9]", "5")); // 字符不能是0 - 9 输出 b not matches： [^abc]m not matches： [^a-z]O not matches： [^A-Z]K not matches： [^a-zA-Z]k not matches： [^a-zA-Z]5 not matches： [^0-9] 限制字符数量 - {} 如果想要控制字符出现的次数，可以使用{}。 字符 描述 {n} n 是一个非负整数。匹配确定的 n 次。 {n,} n 是一个非负整数。至少匹配 n 次。 {n,m} m 和 n 均为非负整数，其中 n &lt;= m。最少匹配 n 次且最多匹配 m 次。 例 限制字符出现次数 // &#123;n&#125;: n 是一个非负整数。匹配确定的 n 次。checkMatches("ap&#123;1&#125;", "a");checkMatches("ap&#123;1&#125;", "ap");checkMatches("ap&#123;1&#125;", "app");checkMatches("ap&#123;1&#125;", "apppppppppp");// &#123;n,&#125;: n 是一个非负整数。至少匹配 n 次。checkMatches("ap&#123;1,&#125;", "a");checkMatches("ap&#123;1,&#125;", "ap");checkMatches("ap&#123;1,&#125;", "app");checkMatches("ap&#123;1,&#125;", "apppppppppp");// &#123;n,m&#125;: m 和 n 均为非负整数，其中 n &lt;= m。最少匹配 n 次且最多匹配 m 次。checkMatches("ap&#123;2,5&#125;", "a");checkMatches("ap&#123;2,5&#125;", "ap");checkMatches("ap&#123;2,5&#125;", "app");checkMatches("ap&#123;2,5&#125;", "apppppppppp"); 输出 a not matches： ap&#123;1&#125;ap matches： ap&#123;1&#125;app not matches： ap&#123;1&#125;apppppppppp not matches： ap&#123;1&#125;a not matches： ap&#123;1,&#125;ap matches： ap&#123;1,&#125;app matches： ap&#123;1,&#125;apppppppppp matches： ap&#123;1,&#125;a not matches： ap&#123;2,5&#125;ap not matches： ap&#123;2,5&#125;app matches： ap&#123;2,5&#125;apppppppppp not matches： ap&#123;2,5&#125; 转义字符 - / 如果想要查找元字符本身，你需要使用转义符，使得正则引擎将其视作一个普通字符，而不是一个元字符去处理。 * 的转义字符：\*+ 的转义字符：\+? 的转义字符：\?^ 的转义字符：\^$ 的转义字符：\$. 的转义字符：\. 如果是转义符\本身，你也需要使用\\ 。 指定表达式字符串的开始和结尾 - ^、$ 如果希望匹配的字符串必须以特定字符串开头，可以使用^ 。 注：请特别留意，这里的^ 一定要和 [^] 中的 “^” 区分。 例 限制字符串头部 Assert.assertTrue(checkMatches("^app[a-z]&#123;0,&#125;", "apple")); // 字符串必须以app开头Assert.assertFalse(checkMatches("^app[a-z]&#123;0,&#125;", "aplause")); 输出 apple matches： ^app[a-z]&#123;0,&#125;aplause not matches： ^app[a-z]&#123;0,&#125; 如果希望匹配的字符串必须以特定字符串开头，可以使用$ 。 例 限制字符串尾部 Assert.assertTrue(checkMatches("[a-z]&#123;0,&#125;ing$", "playing")); // 字符串必须以ing结尾Assert.assertFalse(checkMatches("[a-z]&#123;0,&#125;ing$", "long")); 输出 playing matches： [a-z]&#123;0,&#125;ing$long not matches： [a-z]&#123;0,&#125;ing$ 等价字符 等价字符，顾名思义，就是对于基本元字符表达的一种简化（等价字符的功能都可以通过基本元字符来实现）。 在没有掌握基本元字符之前，可以先不用理会，因为很容易把人绕晕。 等价字符的好处在于简化了基本元字符的写法。 表示某一类型字符的等价字符 下表中的等价字符都表示某一类型的字符。 字符 描述 . 匹配除“\n”之外的任何单个字符。 \d 匹配一个数字字符。等价于[0-9]。 \D 匹配一个非数字字符。等价于[^0-9]。 \w 匹配包括下划线的任何单词字符。类似但不等价于“[A-Za-z0-9_]”，这里的单词字符指的是 Unicode 字符集。 \W 匹配任何非单词字符。 \s 匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]。 \S 匹配任何可见字符。等价于[ \f\n\r\t\v]。 案例 基本等价字符的用法 // 匹配除“\n”之外的任何单个字符Assert.assertTrue(checkMatches(".&#123;1,&#125;", "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_"));Assert.assertTrue(checkMatches(".&#123;1,&#125;", "~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\"));Assert.assertFalse(checkMatches(".", "\n"));Assert.assertFalse(checkMatches("[^\n]", "\n"));// 匹配一个数字字符。等价于[0-9]Assert.assertTrue(checkMatches("\\d&#123;1,&#125;", "0123456789"));// 匹配一个非数字字符。等价于[^0-9]Assert.assertFalse(checkMatches("\\D&#123;1,&#125;", "0123456789"));// 匹配包括下划线的任何单词字符。类似但不等价于“[A-Za-z0-9_]”，这里的单词字符指的是Unicode字符集Assert.assertTrue(checkMatches("\\w&#123;1,&#125;", "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_"));Assert.assertFalse(checkMatches("\\w&#123;1,&#125;", "~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\"));// 匹配任何非单词字符Assert.assertFalse(checkMatches("\\W&#123;1,&#125;", "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_"));Assert.assertTrue(checkMatches("\\W&#123;1,&#125;", "~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\"));// 匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]Assert.assertTrue(checkMatches("\\s&#123;1,&#125;", " \f\r\n\t"));// 匹配任何可见字符。等价于[^ \f\n\r\t\v]Assert.assertFalse(checkMatches("\\S&#123;1,&#125;", " \f\r\n\t")); 输出 ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_ matches： .&#123;1,&#125;~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\ matches： .&#123;1,&#125;\n not matches： .\n not matches： [^\n]0123456789 matches： \\d&#123;1,&#125;0123456789 not matches： \\D&#123;1,&#125;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_ matches： \\w&#123;1,&#125;~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\ not matches： \\w&#123;1,&#125;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_ not matches： \\W&#123;1,&#125;~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\ matches： \\W&#123;1,&#125; \f\r\n\t matches： \\s&#123;1,&#125; \f\r\n\t not matches： \\S&#123;1,&#125; 限制字符数量的等价字符 在基本元字符章节中，已经介绍了限制字符数量的基本元字符 - {} 。 此外，还有 *、+、? 这个三个为了简化写法而出现的等价字符，我们来认识一下。 字符 描述 * 匹配前面的子表达式零次或多次。等价于{0,}。 + 匹配前面的子表达式一次或多次。等价于{1,}。 ? 匹配前面的子表达式零次或一次。等价于 {0,1}。 案例 限制字符数量的等价字符 // *: 匹配前面的子表达式零次或多次。* 等价于&#123;0,&#125;。checkMatches("ap*", "a");checkMatches("ap*", "ap");checkMatches("ap*", "app");checkMatches("ap*", "apppppppppp");// +: 匹配前面的子表达式一次或多次。+ 等价于 &#123;1,&#125;。checkMatches("ap+", "a");checkMatches("ap+", "ap");checkMatches("ap+", "app");checkMatches("ap+", "apppppppppp");// ?: 匹配前面的子表达式零次或一次。? 等价于 &#123;0,1&#125;。checkMatches("ap?", "a");checkMatches("ap?", "ap");checkMatches("ap?", "app");checkMatches("ap?", "apppppppppp"); 输出 a matches： ap*ap matches： ap*app matches： ap*apppppppppp matches： ap*a not matches： ap+ap matches： ap+app matches： ap+apppppppppp matches： ap+a matches： ap?ap matches： ap?app not matches： ap?apppppppppp not matches： ap? 元字符优先级顺序 正则表达式从左到右进行计算，并遵循优先级顺序，这与算术表达式非常类似。 下表从最高到最低说明了各种正则表达式运算符的优先级顺序： 运算符 说明 \ 转义符 (), (?😃, (?=), [] 括号和中括号 *, +, ?, {n}, {n,}, {n,m} 限定符 ^, $, *任何元字符、任何字符* 定位点和序列 | 替换 字符具有高于替换运算符的优先级，使得“m|food”匹配“m”或“food”。若要匹配“mood”或“food”，请使用括号创建子表达式，从而产生“(m|f)ood”。 分组构造 在基本元字符章节，提到了 () 字符可以用来对表达式分组。实际上分组还有更多复杂的用法。 所谓分组构造，是用来描述正则表达式的子表达式，用于捕获字符串中的子字符串。 捕获与非捕获 下表为分组构造中的捕获和非捕获分类。 表达式 描述 捕获或非捕获 (exp) 匹配的子表达式 捕获 (?&lt;name&gt;exp) 命名的反向引用 捕获 (?:exp) 非捕获组 非捕获 (?=exp) 零宽度正预测先行断言 非捕获 (?!exp) 零宽度负预测先行断言 非捕获 (?&lt;=exp) 零宽度正回顾后发断言 非捕获 (?&lt;!exp) 零宽度负回顾后发断言 非捕获 注：Java 正则引擎不支持平衡组。 反向引用 带编号的反向引用 带编号的反向引用使用以下语法：\number 其中number 是正则表达式中捕获组的序号位置。 例如，\4 匹配第四个捕获组的内容。 如果正则表达式模式中未定义number，则将发生分析错误 例 匹配重复的单词和紧随每个重复的单词的单词(不命名子表达式) // (\w+)\s\1\W(\w+) 匹配重复的单词和紧随每个重复的单词的单词Assert.assertTrue(findAll("(\\w+)\\s\\1\\W(\\w+)", "He said that that was the the correct answer.") &gt; 0); 输出 regex = (\w+)\s\1\W(\w+), content: He said that that was the the correct answer.[1th] start: 8, end: 21, group: that that was[2th] start: 22, end: 37, group: the the correct 说明 (\w+): 匹配一个或多个单词字符。 \s: 与空白字符匹配。 \1: 匹配第一个组，即(\w+)。 \W: 匹配包括空格和标点符号的一个非单词字符。 这样可以防止正则表达式模式匹配从第一个捕获组的单词开头的单词。 命名的反向引用 命名后向引用通过使用下面的语法进行定义：\k&lt;name &gt; 例 匹配重复的单词和紧随每个重复的单词的单词(命名子表达式) // (?&lt;duplicateWord&gt;\w+)\s\k&lt;duplicateWord&gt;\W(?&lt;nextWord&gt;\w+) 匹配重复的单词和紧随每个重复的单词的单词Assert.assertTrue(findAll("(?&lt;duplicateWord&gt;\\w+)\\s\\k&lt;duplicateWord&gt;\\W(?&lt;nextWord&gt;\\w+)", "He said that that was the the correct answer.") &gt; 0); 输出 regex = (?&lt;duplicateWord&gt;\w+)\s\k&lt;duplicateWord&gt;\W(?&lt;nextWord&gt;\w+), content: He said that that was the the correct answer.[1th] start: 8, end: 21, group: that that was[2th] start: 22, end: 37, group: the the correct 说明 (?\w+): 匹配一个或多个单词字符。 命名此捕获组 duplicateWord。 \s: 与空白字符匹配。 \k: 匹配名为 duplicateWord 的捕获的组。 \W: 匹配包括空格和标点符号的一个非单词字符。 这样可以防止正则表达式模式匹配从第一个捕获组的单词开头的单词。 (?\w+): 匹配一个或多个单词字符。 命名此捕获组 nextWord。 非捕获组 (?:exp) 表示当一个限定符应用到一个组，但组捕获的子字符串并非所需时，通常会使用非捕获组构造。 例 匹配以.结束的语句。 // 匹配由句号终止的语句。Assert.assertTrue(findAll("(?:\\b(?:\\w+)\\W*)+\\.", "This is a short sentence. Never end") &gt; 0); 输出 regex = (?:\b(?:\w+)\W*)+\., content: This is a short sentence. Never end[1th] start: 0, end: 25, group: This is a short sentence. 零宽断言 用于查找在某些内容(但并不包括这些内容)之前或之后的东西，也就是说它们像\b,^,$那样用于指定一个位置，这个位置应该满足一定的条件(即断言)，因此它们也被称为零宽断言。 表达式 描述 (?=exp) 匹配 exp 前面的位置 (?&lt;=exp) 匹配 exp 后面的位置 (?!exp) 匹配后面跟的不是 exp 的位置 (?&lt;!exp) 匹配前面不是 exp 的位置 匹配 exp 前面的位置 (?=exp) 表示输入字符串必须匹配子表达式中的正则表达式模式，尽管匹配的子字符串未包含在匹配结果中。 // \b\w+(?=\sis\b) 表示要捕获is之前的单词Assert.assertTrue(findAll("\\b\\w+(?=\\sis\\b)", "The dog is a Malamute.") &gt; 0);Assert.assertFalse(findAll("\\b\\w+(?=\\sis\\b)", "The island has beautiful birds.") &gt; 0);Assert.assertFalse(findAll("\\b\\w+(?=\\sis\\b)", "The pitch missed home plate.") &gt; 0);Assert.assertTrue(findAll("\\b\\w+(?=\\sis\\b)", "Sunday is a weekend day.") &gt; 0); 输出 regex = \b\w+(?=\sis\b), content: The dog is a Malamute.[1th] start: 4, end: 7, group: dogregex = \b\w+(?=\sis\b), content: The island has beautiful birds.not foundregex = \b\w+(?=\sis\b), content: The pitch missed home plate.not foundregex = \b\w+(?=\sis\b), content: Sunday is a weekend day.[1th] start: 0, end: 6, group: Sunday 说明 \b: 在单词边界处开始匹配。 \w+: 匹配一个或多个单词字符。 (?=\sis\b): 确定单词字符是否后接空白字符和字符串“is”，其在单词边界处结束。 如果如此，则匹配成功。 匹配 exp 后面的位置 (?&lt;=exp) 表示子表达式不得在输入字符串当前位置左侧出现，尽管子表达式未包含在匹配结果中。零宽度正回顾后发断言不会回溯。 // (?&lt;=\b20)\d&#123;2&#125;\b 表示要捕获以20开头的数字的后面部分Assert.assertTrue(findAll("(?&lt;=\\b20)\\d&#123;2&#125;\\b", "2010 1999 1861 2140 2009") &gt; 0); 输出 regex = (?&lt;=\b20)\d&#123;2&#125;\b, content: 2010 1999 1861 2140 2009[1th] start: 2, end: 4, group: 10[2th] start: 22, end: 24, group: 09 说明 \d{2}: 匹配两个十进制数字。 {?&lt;=\b20): 如果两个十进制数字的字边界以小数位数“20”开头，则继续匹配。 \b: 在单词边界处结束匹配。 匹配后面跟的不是 exp 的位置 (?!exp) 表示输入字符串不得匹配子表达式中的正则表达式模式，尽管匹配的子字符串未包含在匹配结果中。 例 捕获未以“un”开头的单词 // \b(?!un)\w+\b 表示要捕获未以“un”开头的单词Assert.assertTrue(findAll("\\b(?!un)\\w+\\b", "unite one unethical ethics use untie ultimate") &gt; 0); 输出 regex = \b(?!un)\w+\b, content: unite one unethical ethics use untie ultimate[1th] start: 6, end: 9, group: one[2th] start: 20, end: 26, group: ethics[3th] start: 27, end: 30, group: use[4th] start: 37, end: 45, group: ultimate 说明 \b: 在单词边界处开始匹配。 (?!un): 确定接下来的两个的字符是否为“un”。 如果没有，则可能匹配。 \w+: 匹配一个或多个单词字符。 \b: 在单词边界处结束匹配。 匹配前面不是 exp 的位置 (?&lt;!exp) 表示子表达式不得在输入字符串当前位置的左侧出现。 但是，任何不匹配子表达式 的子字符串不包含在匹配结果中。 例 捕获任意工作日 // (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b 表示要捕获任意工作日（即周一到周五）Assert.assertTrue(findAll("(?&lt;!(Saturday|Sunday) )\\b\\w+ \\d&#123;1,2&#125;, \\d&#123;4&#125;\\b", "Monday February 1, 2010") &gt; 0);Assert.assertTrue(findAll("(?&lt;!(Saturday|Sunday) )\\b\\w+ \\d&#123;1,2&#125;, \\d&#123;4&#125;\\b", "Wednesday February 3, 2010") &gt; 0);Assert.assertFalse(findAll("(?&lt;!(Saturday|Sunday) )\\b\\w+ \\d&#123;1,2&#125;, \\d&#123;4&#125;\\b", "Saturday February 6, 2010") &gt; 0);Assert.assertFalse(findAll("(?&lt;!(Saturday|Sunday) )\\b\\w+ \\d&#123;1,2&#125;, \\d&#123;4&#125;\\b", "Sunday February 7, 2010") &gt; 0);Assert.assertTrue(findAll("(?&lt;!(Saturday|Sunday) )\\b\\w+ \\d&#123;1,2&#125;, \\d&#123;4&#125;\\b", "Monday, February 8, 2010") &gt; 0); 输出 regex = (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b, content: Monday February 1, 2010[1th] start: 7, end: 23, group: February 1, 2010regex = (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b, content: Wednesday February 3, 2010[1th] start: 10, end: 26, group: February 3, 2010regex = (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b, content: Saturday February 6, 2010not foundregex = (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b, content: Sunday February 7, 2010not foundregex = (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b, content: Monday, February 8, 2010[1th] start: 8, end: 24, group: February 8, 2010 贪婪与懒惰 当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能多的字符。以这个表达式为例：a.*b，它将会匹配最长的以 a 开始，以 b 结束的字符串。如果用它来搜索 aabab 的话，它会匹配整个字符串 aabab。这被称为贪婪匹配。 有时，我们更需要懒惰匹配，也就是匹配尽可能少的字符。前面给出的限定符都可以被转化为懒惰匹配模式，只要在它后面加上一个问号?。这样.*?就意味着匹配任意数量的重复，但是在能使整个匹配成功的前提下使用最少的重复。 表达式 描述 *? 重复任意次，但尽可能少重复 +? 重复 1 次或更多次，但尽可能少重复 ?? 重复 0 次或 1 次，但尽可能少重复 {n,m}? 重复 n 到 m 次，但尽可能少重复 {n,}? 重复 n 次以上，但尽可能少重复 例 Java 正则中贪婪与懒惰的示例 // 贪婪匹配Assert.assertTrue(findAll("a\\w*b", "abaabaaabaaaab") &gt; 0);// 懒惰匹配Assert.assertTrue(findAll("a\\w*?b", "abaabaaabaaaab") &gt; 0);Assert.assertTrue(findAll("a\\w+?b", "abaabaaabaaaab") &gt; 0);Assert.assertTrue(findAll("a\\w??b", "abaabaaabaaaab") &gt; 0);Assert.assertTrue(findAll("a\\w&#123;0,4&#125;?b", "abaabaaabaaaab") &gt; 0);Assert.assertTrue(findAll("a\\w&#123;3,&#125;?b", "abaabaaabaaaab") &gt; 0); 输出 regex = a\w*b, content: abaabaaabaaaab[1th] start: 0, end: 14, group: abaabaaabaaaabregex = a\w*?b, content: abaabaaabaaaab[1th] start: 0, end: 2, group: ab[2th] start: 2, end: 5, group: aab[3th] start: 5, end: 9, group: aaab[4th] start: 9, end: 14, group: aaaabregex = a\w+?b, content: abaabaaabaaaab[1th] start: 0, end: 5, group: abaab[2th] start: 5, end: 9, group: aaab[3th] start: 9, end: 14, group: aaaabregex = a\w??b, content: abaabaaabaaaab[1th] start: 0, end: 2, group: ab[2th] start: 2, end: 5, group: aab[3th] start: 6, end: 9, group: aab[4th] start: 11, end: 14, group: aabregex = a\w&#123;0,4&#125;?b, content: abaabaaabaaaab[1th] start: 0, end: 2, group: ab[2th] start: 2, end: 5, group: aab[3th] start: 5, end: 9, group: aaab[4th] start: 9, end: 14, group: aaaabregex = a\w&#123;3,&#125;?b, content: abaabaaabaaaab[1th] start: 0, end: 5, group: abaab[2th] start: 5, end: 14, group: aaabaaaab 说明 本例中代码展示的是使用不同贪婪或懒惰策略去查找字符串&quot;abaabaaabaaaab&quot; 中匹配以&quot;a&quot;开头，以&quot;b&quot;结尾的所有子字符串。 请从输出结果中，细细体味使用不同的贪婪或懒惰策略，对于匹配子字符串有什么影响。 附录 匹配正则字符串的方法 由于正则表达式中很多元字符本身就是转义字符，在 Java 字符串的规则中不会被显示出来。 为此，可以使用一个工具类org.apache.commons.lang3.StringEscapeUtils来做特殊处理，使得转义字符可以打印。这个工具类提供的都是静态方法，从方法命名大致也可以猜出用法，这里不多做说明。 如果你了解 maven，可以直接引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;$&#123;commons-lang3.version&#125;&lt;/version&gt;&lt;/dependency&gt; 本文为了展示正则匹配规则用到的方法 private boolean checkMatches(String regex, String content) &#123; Pattern p = Pattern.compile(regex); Matcher m = p.matcher(content); boolean flag = m.matches(); if (m.matches()) &#123; System.out.println(StringEscapeUtils.escapeJava(content) + "\tmatches： " + StringEscapeUtils.escapeJava(regex)); &#125; else &#123; System.out.println(StringEscapeUtils.escapeJava(content) + "\tnot matches： " + StringEscapeUtils.escapeJava(regex)); &#125; return flag;&#125;public int findAll(String regex, String content) &#123; Pattern p = Pattern.compile(regex); Matcher m = p.matcher(content); System.out.println("regex = " + regex + ", content: " + content); int count = 0; while (m.find()) &#123; count++; System.out.println("[" + count + "th] " + "start: " + m.start() + ", end: " + m.end() + ", group: " + m.group()); &#125; if (0 == count) &#123; System.out.println("not found"); &#125; return count;&#125; 速查元字符字典 为了方便快查正则的元字符含义，在本节根据元字符的功能集中罗列正则的各种元字符。 限定符 字符 描述 * 匹配前面的子表达式零次或多次。例如，zo* 能匹配 “z” 以及 “zoo”。* 等价于{0,}。 + 匹配前面的子表达式一次或多次。例如，‘zo+’ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,}。 ? 匹配前面的子表达式零次或一次。例如，“do(es)?” 可以匹配 “do” 或 “does” 中的&quot;do&quot; 。? 等价于 {0,1}。 {n} n 是一个非负整数。匹配确定的 n 次。例如，‘o{2}’ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。 {n,} n 是一个非负整数。至少匹配 n 次。例如，‘o{2,}’ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。‘o{1,}’ 等价于 ‘o+’。‘o{0,}’ 则等价于 ‘o*’。 {n,m} m 和 n 均为非负整数，其中 n &lt;= m。最少匹配 n 次且最多匹配 m 次。例如，“o{1,3}” 将匹配 “fooooood” 中的前三个 o。‘o{0,1}’ 等价于 ‘o?’。请注意在逗号和两个数之间不能有空格。 定位符 字符 描述 ^ 匹配输入字符串开始的位置。如果设置了 RegExp 对象的 Multiline 属性，^ 还会与 \n 或 \r 之后的位置匹配。 $ 匹配输入字符串结尾的位置。如果设置了 RegExp 对象的 Multiline 属性，$ 还会与 \n 或 \r 之前的位置匹配。 \b 匹配一个字边界，即字与空格间的位置。 \B 非字边界匹配。 非打印字符 字符 描述 \cx 匹配由 x 指明的控制字符。例如， \cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。 \f 匹配一个换页符。等价于 \x0c 和 \cL。 \n 匹配一个换行符。等价于 \x0a 和 \cJ。 \r 匹配一个回车符。等价于 \x0d 和 \cM。 \s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。 \S 匹配任何非空白字符。等价于 [ \f\n\r\t\v]。 \t 匹配一个制表符。等价于 \x09 和 \cI。 \v 匹配一个垂直制表符。等价于 \x0b 和 \cK。 分组 表达式 描述 (exp) 匹配的子表达式。()中的内容就是子表达式。 (?&lt;name&gt;exp) 命名的子表达式（反向引用）。 (?:exp) 非捕获组，表示当一个限定符应用到一个组，但组捕获的子字符串并非所需时，通常会使用非捕获组构造。 (?=exp) 匹配 exp 前面的位置。 (?&lt;=exp) 匹配 exp 后面的位置。 (?!exp) 匹配后面跟的不是 exp 的位置。 (?&lt;!exp) 匹配前面不是 exp 的位置。 特殊符号 字符 描述 \ 将下一个字符标记为或特殊字符、或原义字符、或向后引用、或八进制转义符。例如， ‘n’ 匹配字符 ‘n’。’\n’ 匹配换行符。序列 ‘\’ 匹配 “”，而 ‘(’ 则匹配 “(”。 \| 指明两项之间的一个选择。 [] 匹配方括号范围内的任意一个字符。形式如：[xyz]、[xyz]、[a-z]、[a-z]、[x,y,z] 正则应用 虽然本系列洋洋洒洒的大谈特谈正则表达式。但是我还是要在这里建议，如果一个正则表达式没有经过充分测试，还是要谨慎使用。 正则是把双刃剑，它可以为你节省大量的代码行。但是由于它不易阅读，维护起来可是头疼的哦（你需要一个字符一个字符的去理解）。 最实用的正则 校验中文 **描述：**校验字符串中只能有中文字符（不包括中文标点符号）。中文字符的 Unicode 编码范围是\u4e00 到 \u9fa5。 如有兴趣，可以参考百度百科-Unicode 。 ^[\u4e00-\u9fa5]+$ 匹配： 春眠不觉晓 **不匹配：**春眠不觉晓， 校验身份证号码 **描述：**身份证为 15 位或 18 位。15 位是第一代身份证。从 1999 年 10 月 1 日起，全国实行公民身份证号码制度，居民身份证编号由原 15 位升至 18 位。 15 位身份证 **描述：**由 15 位数字组成。排列顺序从左至右依次为：六位数字地区码；六位数字出生日期；三位顺序号，其中 15 位男为单数，女为双数。 18 位身份证 **描述：**由十七位数字本体码和一位数字校验码组成。排列顺序从左至右依次为：六位数字地区码；八位数字出生日期；三位数字顺序码和一位数字校验码（也可能是 X）。 身份证号含义详情请见：百度百科-居民身份证号码 地区码（6 位） (1[1-5]|2[1-3]|3[1-7]|4[1-3]|5[0-4]|6[1-5])\d&#123;4&#125; 出生日期（8 位） 注：下面的是 18 位身份证的有效出生日期，如果是 15 位身份证，只要将第一个\d{4}改为\d{2}即可。 ((\d&#123;4&#125;((0[13578]|1[02])(0[1-9]|[12]\d|3[01])|(0[13456789]|1[012])(0[1-9]|[12]\d|30)|02(0[1-9]|1\d|2[0-8])))|([02468][048]|[13579][26])0229) 15 位有效身份证 ^((1[1-5]|2[1-3]|3[1-7]|4[1-3]|5[0-4]|6[1-5])\d&#123;4&#125;)((\d&#123;2&#125;((0[13578]|1[02])(0[1-9]|[12]\d|3[01])|(0[13456789]|1[012])(0[1-9]|[12]\d|30)|02(0[1-9]|1\d|2[0-8])))|([02468][048]|[13579][26])0229)(\d&#123;3&#125;)$ **匹配：**110001700101031 **不匹配：**110001701501031 18 位有效身份证 ^((1[1-5]|2[1-3]|3[1-7]|4[1-3]|5[0-4]|6[1-5])\d&#123;4&#125;)((\d&#123;4&#125;((0[13578]|1[02])(0[1-9]|[12]\d|3[01])|(0[13456789]|1[012])(0[1-9]|[12]\d|30)|02(0[1-9]|1\d|2[0-8])))|([02468][048]|[13579][26])0229)(\d&#123;3&#125;(\d|X))$ **匹配：**110001199001010310 | 11000019900101015X **不匹配：**990000199001010310 | 110001199013010310 校验有效用户名、密码 **描述：**长度为 6-18 个字符，允许输入字母、数字、下划线，首字符必须为字母。 ^[a-zA-Z]\w&#123;5,17&#125;$ **匹配：**he_llo@worl.d.com | hel.l-o@wor-ld.museum | h1ello@123.com **不匹配：**hello@worl_d.com | he&amp;llo@world.co1 | .hello@wor#.co.uk 校验邮箱 **描述：**不允许使用 IP 作为域名，如 : hello@154.145.68.12 @符号前的邮箱用户和.符号前的域名(domain)必须满足以下条件： 字符只能是英文字母、数字、下划线_、.、- ； 首字符必须为字母或数字； _、.、- 不能连续出现。 域名的根域只能为字母，且至少为两个字符。 ^[A-Za-z0-9](([_\.\-]?[a-zA-Z0-9]+)*)@([A-Za-z0-9]+)(([\.\-]?[a-zA-Z0-9]+)*)\.([A-Za-z]&#123;2,&#125;)$ **匹配：**he_llo@worl.d.com | hel.l-o@wor-ld.museum | h1ello@123.com **不匹配：**hello@worl_d.com | he&amp;llo@world.co1 | .hello@wor#.co.uk 校验 URL **描述：**校验 URL。支持 http、https、ftp、ftps。 ^(ht|f)(tp|tps)\://[a-zA-Z0-9\-\.]+\.([a-zA-Z]&#123;2,3&#125;)?(/\S*)?$ **匹配：**http://google.com/help/me | http://www.google.com/help/me/ | https://www.google.com/help.asp | ftp://www.google.com | ftps://google.org **不匹配：**http://un/www.google.com/index.asp 校验时间 **描述：**校验时间。时、分、秒必须是有效数字，如果数值不是两位数，十位需要补零。 ^([0-1][0-9]|[2][0-3]):([0-5][0-9])$ **匹配：**00:00:00 | 23:59:59 | 17:06:30 **不匹配：**17:6:30 | 24:16:30 校验日期 **描述：**校验日期。日期满足以下条件： 格式 yyyy-MM-dd 或 yyyy-M-d 连字符可以没有或是“-”、“/”、“.”之一 闰年的二月可以有 29 日；而平年不可以。 一、三、五、七、八、十、十二月为 31 日。四、六、九、十一月为 30 日。 ^(?:(?!0000)[0-9]&#123;4&#125;([-/.]?)(?:(?:0?[1-9]|1[0-2])\1(?:0?[1-9]|1[0-9]|2[0-8])|(?:0?[13-9]|1[0-2])\1(?:29|30)|(?:0?[13578]|1[02])\1(?:31))|(?:[0-9]&#123;2&#125;(?:0[48]|[2468][048]|[13579][26])|(?:0[48]|[2468][048]|[13579][26])00)([-/.]?)0?2\2(?:29))$ **匹配：**2016/1/1 | 2016/01/01 | 20160101 | 2016-01-01 | 2016.01.01 | 2000-02-29 **不匹配：**2001-02-29 | 2016/12/32 | 2016/6/31 | 2016/13/1 | 2016/0/1 校验中国手机号码 **描述：**中国手机号码正确格式：11 位数字。 移动有 16 个号段：134、135、136、137、138、139、147、150、151、152、157、158、159、182、187、188。其中 147、157、188 是 3G 号段，其他都是 2G 号段。联通有 7 种号段：130、131、132、155、156、185、186。其中 186 是 3G（WCDMA）号段，其余为 2G 号段。电信有 4 个号段：133、153、180、189。其中 189 是 3G 号段（CDMA2000），133 号段主要用作无线网卡号。总结：13 开头手机号 0-9；15 开头手机号 0-3、5-9；18 开头手机号 0、2、5-9。 此外，中国在国际上的区号为 86，所以手机号开头有+86、86 也是合法的。 以上信息来源于 百度百科-手机号 ^((\+)?86\s*)?((13[0-9])|(15([0-3]|[5-9]))|(18[0,2,5-9]))\d&#123;8&#125;$ 匹配：+86 18012345678 | 86 18012345678 | 15812345678 **不匹配：**15412345678 | 12912345678 | 180123456789 校验中国固话号码 **描述：**固话号码，必须加区号（以 0 开头）。 3 位有效区号：010、020~029，固话位数为 8 位。 4 位有效区号：03xx 开头到 09xx，固话位数为 7。 如果想了解更详细的信息，请参考 百度百科-电话区号 。 ^(010|02[0-9])(\s|-)\d&#123;8&#125;|(0[3-9]\d&#123;2&#125;)(\s|-)\d&#123;7&#125;$ **匹配：**010-12345678 | 010 12345678 | 0512-1234567 | 0512 1234567 **不匹配：**1234567 | 12345678 校验 IPv4 地址 **描述：**IP 地址是一个 32 位的二进制数，通常被分割为 4 个“8 位二进制数”（也就是 4 个字节）。IP 地址通常用“点分十进制”表示成（a.b.c.d）的形式，其中，a,b,c,d 都是 0~255 之间的十进制整数。 ^([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])$ **匹配：**0.0.0.0 | 255.255.255.255 | 127.0.0.1 **不匹配：**10.10.10 | 10.10.10.256 校验 IPv6 地址 **描述：**IPv6 的 128 位地址通常写成 8 组，每组为四个十六进制数的形式。 IPv6 地址可以表示为以下形式： IPv6 地址 零压缩 IPv6 地址(section 2.2 of rfc5952) 带有本地链接区域索引的 IPv6 地址 (section 11 of rfc4007) 嵌入 IPv4 的 IPv6 地址(section 2 of rfc6052 映射 IPv4 的 IPv6 地址 (section 2.1 of rfc2765) 翻译 IPv4 的 IPv6 地址 (section 2.1 of rfc2765) 显然，IPv6 地址的表示方式很复杂。你也可以参考 百度百科-IPv6 Stack overflow 上的 IPv6 正则表达高票答案 (([0-9a-fA-F]&#123;1,4&#125;:)&#123;7,7&#125;[0-9a-fA-F]&#123;1,4&#125;|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,7&#125;:|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,6&#125;:[0-9a-fA-F]&#123;1,4&#125;|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,5&#125;(:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,2&#125;|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,4&#125;(:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,3&#125;|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,3&#125;(:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,4&#125;|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,2&#125;(:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,5&#125;|[0-9a-fA-F]&#123;1,4&#125;:((:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,6&#125;)|:((:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,7&#125;|:)|fe80:(:[0-9a-fA-F]&#123;0,4&#125;)&#123;0,4&#125;%[0-9a-zA-Z]&#123;1,&#125;|::(ffff(:0&#123;1,4&#125;)&#123;0,1&#125;:)&#123;0,1&#125;((25[0-5]|(2[0-4]|1&#123;0,1&#125;[0-9])&#123;0,1&#125;[0-9])\.)&#123;3,3&#125;(25[0-5]|(2[0-4]|1&#123;0,1&#125;[0-9])&#123;0,1&#125;[0-9])|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,4&#125;:((25[0-5]|(2[0-4]|1&#123;0,1&#125;[0-9])&#123;0,1&#125;[0-9])\.)&#123;3,3&#125;(25[0-5]|(2[0-4]|1&#123;0,1&#125;[0-9])&#123;0,1&#125;[0-9])) **匹配：**1:2:3:4:5:6:7:8 | 1:: | 1::8 | 1::6:7:8 | 1::5:6:7:8 | 1::4:5:6:7:8 | 1::3:4:5:6:7:8 | ::2:3:4:5:6:7:8 | 1:2:3:4:5:6:7:: | 1:2:3:4:5:6::8 | 1:2:3:4:5::8 | 1:2:3:4::8 | 1:2:3::8 | 1:2::8 | 1::8 | ::8 | fe80::7:8%1 | ::255.255.255.255 | 2001:db8:3:4::192.0.2.33 | 64:ff9b::192.0.2.33 **不匹配：**1.2.3.4.5.6.7.8 | 1::2::3 特定字符 匹配长度为 3 的字符串：^.{3}$。 匹配由 26 个英文字母组成的字符串：^[A-Za-z]+$。 匹配由 26 个大写英文字母组成的字符串：^[A-Z]+$。 匹配由 26 个小写英文字母组成的字符串：^[a-z]+$。 匹配由数字和 26 个英文字母组成的字符串：^[A-Za-z0-9]+$。 匹配由数字、26 个英文字母或者下划线组成的字符串：^\w+$。 特定数字 匹配正整数：^[1-9]\d*$ 匹配负整数：^-[1-9]\d*$ 匹配整数：^(-?[1-9]\d*)|0$ 匹配正浮点数：^[1-9]\d*\.\d+|0\.\d+$ 匹配负浮点数：^-([1-9]\d*\.\d*|0\.\d*[1-9]\d*)$ 匹配浮点数：^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$ 参考 正则表达式 30 分钟入门教程 msdn 正则表达式教程 正则应用之——日期正则表达式 http://www.regexlib.com/]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>advanced</tag>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[细说 Java 主流日志工具库]]></title>
    <url>%2Fblog%2F2016%2F10%2F14%2Fjava%2Fjavalib%2Fjava-log%2F</url>
    <content type="text"><![CDATA[细说 Java 主流日志工具库 在项目开发中，为了跟踪代码的运行情况，常常要使用日志来记录信息。 在 Java 世界，有很多的日志工具库来实现日志功能，避免了我们重复造轮子。 我们先来逐一了解一下主流日志工具。 📓 本文已归档到：「blog」 日志框架 java.util.logging (JUL) Log4j Logback Log4j2 Log4j vs Logback vs Log4j2 日志门面 common-logging slf4j common-logging vs slf4j 总结 实施日志解决方案 引入 jar 包 使用 API log4j2 配置 logback 配置 &lt;configuration&gt; &lt;appender&gt; &lt;logger&gt; &lt;root&gt; 完整的 logback.xml 参考示例 log4j 配置 完整的 log4j.xml 参考示例 参考 日志框架 java.util.logging (JUL) JDK1.4 开始，通过 java.util.logging 提供日志功能。 它能满足基本的日志需要，但是功能没有 Log4j 强大，而且使用范围也没有 Log4j 广泛。 Log4j Log4j 是 apache 的一个开源项目，创始人 Ceki Gulcu。 Log4j 应该说是 Java 领域资格最老，应用最广的日志工具。从诞生之日到现在一直广受业界欢迎。 Log4j 是高度可配置的，并可通过在运行时的外部文件配置。它根据记录的优先级别，并提供机制，以指示记录信息到许多的目的地，诸如：数据库，文件，控制台，UNIX 系统日志等。 Log4j 中有三个主要组成部分： loggers - 负责捕获记录信息。 appenders - 负责发布日志信息，以不同的首选目的地。 layouts - 负责格式化不同风格的日志信息。 官网地址 Logback Logback 是由 log4j 创始人 Ceki Gulcu 设计的又一个开源日记组件，目标是替代 log4j。 logback 当前分成三个模块：logback-core、logback-classic 和 logback-access。 logback-core - 是其它两个模块的基础模块。 logback-classic - 是 log4j 的一个 改良版本。此外 logback-classic 完整实现 SLF4J API 使你可以很方便地更换成其它日记系统如 log4j 或 JDK14 Logging。 logback-access - 访问模块与 Servlet 容器集成提供通过 Http 来访问日记的功能。 官网地址 Log4j2 官网地址 按照官方的说法，Log4j2 是 Log4j 和 Logback 的替代。 Log4j2 架构： Log4j vs Logback vs Log4j2 按照官方的说法，Log4j2 大大优于 Log4j 和 Logback。 那么，Log4j2 相比于先问世的 Log4j 和 Logback，它具有哪些优势呢？ Log4j2 旨在用作审计日志记录框架。 Log4j 1.x 和 Logback 都会在重新配置时丢失事件。 Log4j 2 不会。在 Logback 中，Appender 中的异常永远不会对应用程序可见。在 Log4j 中，可以将 Appender 配置为允许异常渗透到应用程序。 Log4j2 在多线程场景中，异步 Loggers 的吞吐量比 Log4j 1.x 和 Logback 高 10 倍，延迟低几个数量级。 Log4j2 对于独立应用程序是无垃圾的，对于稳定状态日志记录期间的 Web 应用程序来说是低垃圾。这减少了垃圾收集器的压力，并且可以提供更好的响应时间性能。 Log4j2 使用插件系统，通过添加新的 Appender、Filter、Layout、Lookup 和 Pattern Converter，可以非常轻松地扩展框架，而无需对 Log4j 进行任何更改。 由于插件系统配置更简单。配置中的条目不需要指定类名。 支持自定义日志等级。 支持 lambda 表达式。 支持消息对象。 Log4j 和 Logback 的 Layout 返回的是字符串，而 Log4j2 返回的是二进制数组，这使得它能被各种 Appender 使用。 Syslog Appender 支持 TCP 和 UDP 并且支持 BSD 系统日志。 Log4j2 利用 Java5 并发特性，尽量小粒度的使用锁，减少锁的开销。 日志门面 何谓日志门面？ 日志门面是对不同日志框架提供的一个 API 封装，可以在部署的时候不修改任何配置即可接入一种日志实现方案。 common-logging common-logging 是 apache 的一个开源项目。也称Jakarta Commons Logging，缩写 JCL。 common-logging 的功能是提供日志功能的 API 接口，本身并不提供日志的具体实现（当然，common-logging 内部有一个 Simple logger 的简单实现，但是功能很弱，直接忽略），而是在运行时动态的绑定日志实现组件来工作（如 log4j、java.util.loggin）。 官网地址 slf4j 全称为 Simple Logging Facade for Java，即 java 简单日志门面。 什么，作者又是 Ceki Gulcu！这位大神写了 Log4j、Logback 和 slf4j，专注日志组件开发五百年，一直只能超越自己。 类似于 Common-Logging，slf4j 是对不同日志框架提供的一个 API 封装，可以在部署的时候不修改任何配置即可接入一种日志实现方案。但是，slf4j 在编译时静态绑定真正的 Log 库。使用 SLF4J 时，如果你需要使用某一种日志实现，那么你必须选择正确的 SLF4J 的 jar 包的集合（各种桥接包）。 官网地址 common-logging vs slf4j slf4j 库类似于 Apache Common-Logging。但是，他在编译时静态绑定真正的日志库。这点似乎很麻烦，其实也不过是导入桥接 jar 包而已。 slf4j 一大亮点是提供了更方便的日志记录方式： 不需要使用logger.isDebugEnabled()来解决日志因为字符拼接产生的性能问题。slf4j 的方式是使用{}作为字符串替换符，形式如下： logger.debug("id: &#123;&#125;, name: &#123;&#125; ", id, name); 总结 综上所述，使用 slf4j + Logback 可谓是目前最理想的日志解决方案了。 接下来，就是如何在项目中实施了。 实施日志解决方案 使用日志解决方案基本可分为三步： 引入 jar 包 配置 使用 API 常见的各种日志解决方案的第 2 步和第 3 步基本一样，实施上的差别主要在第 1 步，也就是使用不同的库。 引入 jar 包 这里首选推荐使用 slf4j + logback 的组合。 如果你习惯了 common-logging，可以选择 common-logging+log4j。 强烈建议不要直接使用日志实现组件(logback、log4j、java.util.logging)，理由前面也说过，就是无法灵活替换日志库。 还有一种情况：你的老项目使用了 common-logging，或是直接使用日志实现组件。如果修改老的代码，工作量太大，需要兼容处理。在下文，都将看到各种应对方法。 注：据我所知，当前仍没有方法可以将 slf4j 桥接到 common-logging。如果我孤陋寡闻了，请不吝赐教。 slf4j 直接绑定日志组件 slf4j + logback 添加依赖到 pom.xml 中即可。 logback-classic-1.0.13.jar 会自动将 slf4j-api-1.7.21.jar 和 logback-core-1.0.13.jar 也添加到你的项目中。 &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.0.13&lt;/version&gt;&lt;/dependency&gt; slf4j + log4j 添加依赖到 pom.xml 中即可。 slf4j-log4j12-1.7.21.jar 会自动将 slf4j-api-1.7.21.jar 和 log4j-1.2.17.jar 也添加到你的项目中。 &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt; slf4j + java.util.logging 添加依赖到 pom.xml 中即可。 slf4j-jdk14-1.7.21.jar 会自动将 slf4j-api-1.7.21.jar 也添加到你的项目中。 &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-jdk14&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt; slf4j 兼容非 slf4j 日志组件 在介绍解决方案前，先提一个概念——桥接 什么是桥接呢 假如你正在开发应用程序所调用的组件当中已经使用了 common-logging，这时你需要 jcl-over-slf4j.jar 把日志信息输出重定向到 slf4j-api，slf4j-api 再去调用 slf4j 实际依赖的日志组件。这个过程称为桥接。下图是官方的 slf4j 桥接策略图： 从图中应该可以看出，无论你的老项目中使用的是 common-logging 或是直接使用 log4j、java.util.logging，都可以使用对应的桥接 jar 包来解决兼容问题。 slf4j 兼容 common-logging &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt; slf4j 兼容 log4j &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt; slf4j 兼容 java.util.logging &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jul-to-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt; spring 集成 slf4j 做 java web 开发，基本离不开 spring 框架。很遗憾，spring 使用的日志解决方案是 common-logging + log4j。 所以，你需要一个桥接 jar 包：logback-ext-spring。 &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.logback-extensions&lt;/groupId&gt; &lt;artifactId&gt;logback-ext-spring&lt;/artifactId&gt; &lt;version&gt;0.1.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.12&lt;/version&gt;&lt;/dependency&gt; common-logging 绑定日志组件 common-logging + log4j 添加依赖到 pom.xml 中即可。 &lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; 使用 API slf4j 用法 使用 slf4j 的 API 很简单。使用LoggerFactory初始化一个Logger实例，然后调用 Logger 对应的打印等级函数就行了。 import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class App &#123; private static final Logger log = LoggerFactory.getLogger(App.class); public static void main(String[] args) &#123; String msg = "print log, current level: &#123;&#125;"; log.trace(msg, "trace"); log.debug(msg, "debug"); log.info(msg, "info"); log.warn(msg, "warn"); log.error(msg, "error"); &#125;&#125; common-logging 用法 common-logging 用法和 slf4j 几乎一样，但是支持的打印等级多了一个更高级别的：fatal。 此外，common-logging 不支持{}替换参数，你只能选择拼接字符串这种方式了。 import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;public class JclTest &#123; private static final Log log = LogFactory.getLog(JclTest.class); public static void main(String[] args) &#123; String msg = "print log, current level: "; log.trace(msg + "trace"); log.debug(msg + "debug"); log.info(msg + "info"); log.warn(msg + "warn"); log.error(msg + "error"); log.fatal(msg + "fatal"); &#125;&#125; log4j2 配置 log4j2 基本配置形式如下： &lt;?xml version="1.0" encoding="UTF-8"?&gt;;&lt;Configuration&gt; &lt;Properties&gt; &lt;Property name="name1"&gt;value&lt;/property&gt; &lt;Property name="name2" value="value2"/&gt; &lt;/Properties&gt; &lt;Filter type="type" ... /&gt; &lt;Appenders&gt; &lt;Appender type="type" name="name"&gt; &lt;Filter type="type" ... /&gt; &lt;/Appender&gt; ... &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Logger name="name1"&gt; &lt;Filter type="type" ... /&gt; &lt;/Logger&gt; ... &lt;Root level="level"&gt; &lt;AppenderRef ref="name"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 配置示例： &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Configuration status="debug" strict="true" name="XMLConfigTest" packages="org.apache.logging.log4j.test"&gt; &lt;Properties&gt; &lt;Property name="filename"&gt;target/test.log&lt;/Property&gt; &lt;/Properties&gt; &lt;Filter type="ThresholdFilter" level="trace"/&gt; &lt;Appenders&gt; &lt;Appender type="Console" name="STDOUT"&gt; &lt;Layout type="PatternLayout" pattern="%m MDC%X%n"/&gt; &lt;Filters&gt; &lt;Filter type="MarkerFilter" marker="FLOW" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;Filter type="MarkerFilter" marker="EXCEPTION" onMatch="DENY" onMismatch="ACCEPT"/&gt; &lt;/Filters&gt; &lt;/Appender&gt; &lt;Appender type="Console" name="FLOW"&gt; &lt;Layout type="PatternLayout" pattern="%C&#123;1&#125;.%M %m %ex%n"/&gt;&lt;!-- class and line number --&gt; &lt;Filters&gt; &lt;Filter type="MarkerFilter" marker="FLOW" onMatch="ACCEPT" onMismatch="NEUTRAL"/&gt; &lt;Filter type="MarkerFilter" marker="EXCEPTION" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;/Filters&gt; &lt;/Appender&gt; &lt;Appender type="File" name="File" fileName="$&#123;filename&#125;"&gt; &lt;Layout type="PatternLayout"&gt; &lt;Pattern&gt;%d %p %C&#123;1.&#125; [%t] %m%n&lt;/Pattern&gt; &lt;/Layout&gt; &lt;/Appender&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Logger name="org.apache.logging.log4j.test1" level="debug" additivity="false"&gt; &lt;Filter type="ThreadContextMapFilter"&gt; &lt;KeyValuePair key="test" value="123"/&gt; &lt;/Filter&gt; &lt;AppenderRef ref="STDOUT"/&gt; &lt;/Logger&gt; &lt;Logger name="org.apache.logging.log4j.test2" level="debug" additivity="false"&gt; &lt;AppenderRef ref="File"/&gt; &lt;/Logger&gt; &lt;Root level="trace"&gt; &lt;AppenderRef ref="STDOUT"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; logback 配置 &lt;configuration&gt; 作用：&lt;configuration&gt; 是 logback 配置文件的根元素。 要点 它有 &lt;appender&gt;、&lt;logger&gt;、&lt;root&gt; 三个子元素。 &lt;appender&gt; 作用：将记录日志的任务委托给名为 appender 的组件。 要点 可以配置零个或多个。 它有 &lt;file&gt;、&lt;filter&gt;、&lt;layout&gt;、&lt;encoder&gt; 四个子元素。 属性 name：设置 appender 名称。 class：设置具体的实例化类。 &lt;file&gt; 作用：设置日志文件路径。 &lt;filter&gt; 作用：设置过滤器。 要点 可以配置零个或多个。 &lt;layout&gt; 作用：设置 appender。 要点 可以配置零个或一个。 属性 class：设置具体的实例化类。 &lt;encoder&gt; 作用：设置编码。 要点 可以配置零个或多个。 属性 class：设置具体的实例化类。 &lt;logger&gt; 作用：设置 logger。 要点 可以配置零个或多个。 属性 name level：设置日志级别。不区分大小写。可选值：TRACE、DEBUG、INFO、WARN、ERROR、ALL、OFF。 additivity：可选值：true 或 false。 &lt;appender-ref&gt; 作用：appender 引用。 要点 可以配置零个或多个。 &lt;root&gt; 作用：设置根 logger。 要点 只能配置一个。 除了 level，不支持任何属性。level 属性和 &lt;logger&gt; 中的相同。 有一个子元素 &lt;appender-ref&gt;，与 &lt;logger&gt; 中的相同。 完整的 logback.xml 参考示例 在下面的配置文件中，我为自己的项目代码（根目录：org.zp.notes.spring）设置了五种等级： TRACE、DEBUG、INFO、WARN、ERROR，优先级依次从低到高。 因为关注 spring 框架本身的一些信息，我增加了专门打印 spring WARN 及以上等级的日志。 &lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!-- logback中一共有5种有效级别，分别是TRACE、DEBUG、INFO、WARN、ERROR，优先级依次从低到高 --&gt;&lt;configuration scan="true" scanPeriod="60 seconds" debug="false"&gt; &lt;property name="DIR_NAME" value="spring-helloworld"/&gt; &lt;!-- 将记录日志打印到控制台 --&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] [%-5p] %c&#123;36&#125;.%M - %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- RollingFileAppender begin --&gt; &lt;appender name="ALL" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 根据时间来制定滚动策略 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;user.dir&#125;/logs/$&#123;DIR_NAME&#125;/all.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 根据文件大小来制定滚动策略 --&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;30MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] [%-5p] %c&#123;36&#125;.%M - %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 根据时间来制定滚动策略 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;user.dir&#125;/logs/$&#123;DIR_NAME&#125;/error.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 根据文件大小来制定滚动策略 --&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] [%-5p] %c&#123;36&#125;.%M - %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="WARN" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 根据时间来制定滚动策略 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;user.dir&#125;/logs/$&#123;DIR_NAME&#125;/warn.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 根据文件大小来制定滚动策略 --&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;WARN&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] [%-5p] %c&#123;36&#125;.%M - %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="INFO" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 根据时间来制定滚动策略 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;user.dir&#125;/logs/$&#123;DIR_NAME&#125;/info.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 根据文件大小来制定滚动策略 --&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] [%-5p] %c&#123;36&#125;.%M - %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="DEBUG" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 根据时间来制定滚动策略 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;user.dir&#125;/logs/$&#123;DIR_NAME&#125;/debug.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 根据文件大小来制定滚动策略 --&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;DEBUG&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] [%-5p] %c&#123;36&#125;.%M - %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="TRACE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 根据时间来制定滚动策略 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;user.dir&#125;/logs/$&#123;DIR_NAME&#125;/trace.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 根据文件大小来制定滚动策略 --&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;TRACE&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] [%-5p] %c&#123;36&#125;.%M - %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="SPRING" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 根据时间来制定滚动策略 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;user.dir&#125;/logs/$&#123;DIR_NAME&#125;/springframework.%d&#123;yyyy-MM-dd&#125;.log &lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 根据文件大小来制定滚动策略 --&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] [%-5p] %c&#123;36&#125;.%M - %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- RollingFileAppender end --&gt; &lt;!-- logger begin --&gt; &lt;!-- 本项目的日志记录，分级打印 --&gt; &lt;logger name="org.zp.notes.spring" level="TRACE" additivity="false"&gt; &lt;appender-ref ref="STDOUT"/&gt; &lt;appender-ref ref="ERROR"/&gt; &lt;appender-ref ref="WARN"/&gt; &lt;appender-ref ref="INFO"/&gt; &lt;appender-ref ref="DEBUG"/&gt; &lt;appender-ref ref="TRACE"/&gt; &lt;/logger&gt; &lt;!-- SPRING框架日志 --&gt; &lt;logger name="org.springframework" level="WARN" additivity="false"&gt; &lt;appender-ref ref="SPRING"/&gt; &lt;/logger&gt; &lt;root level="TRACE"&gt; &lt;appender-ref ref="ALL"/&gt; &lt;/root&gt; &lt;!-- logger end --&gt;&lt;/configuration&gt; log4j 配置 完整的 log4j.xml 参考示例 log4j 的配置文件一般有 xml 格式或 properties 格式。这里为了和 logback.xml 做个对比，就不介绍 properties 了，其实也没太大差别。 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE log4j:configuration SYSTEM "log4j.dtd"&gt;&lt;log4j:configuration xmlns:log4j='http://jakarta.apache.org/log4j/'&gt; &lt;appender name="STDOUT" class="org.apache.log4j.ConsoleAppender"&gt; &lt;layout class="org.apache.log4j.PatternLayout"&gt; &lt;param name="ConversionPattern" value="%d&#123;yyyy-MM-dd HH:mm:ss,SSS\&#125; [%-5p] [%t] %c&#123;36\&#125;.%M - %m%n"/&gt; &lt;/layout&gt; &lt;!--过滤器设置输出的级别--&gt; &lt;filter class="org.apache.log4j.varia.LevelRangeFilter"&gt; &lt;param name="levelMin" value="debug"/&gt; &lt;param name="levelMax" value="fatal"/&gt; &lt;param name="AcceptOnMatch" value="true"/&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;appender name="ALL" class="org.apache.log4j.DailyRollingFileAppender"&gt; &lt;param name="File" value="$&#123;user.dir&#125;/logs/spring-common/jcl/all"/&gt; &lt;param name="Append" value="true"/&gt; &lt;!-- 每天重新生成日志文件 --&gt; &lt;param name="DatePattern" value="'-'yyyy-MM-dd'.log'"/&gt; &lt;!-- 每小时重新生成日志文件 --&gt; &lt;!--&lt;param name="DatePattern" value="'-'yyyy-MM-dd-HH'.log'"/&gt;--&gt; &lt;layout class="org.apache.log4j.PatternLayout"&gt; &lt;param name="ConversionPattern" value="%d&#123;yyyy-MM-dd HH:mm:ss,SSS\&#125; [%-5p] [%t] %c&#123;36\&#125;.%M - %m%n"/&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!-- 指定logger的设置，additivity指示是否遵循缺省的继承机制--&gt; &lt;logger name="org.zp.notes.spring" additivity="false"&gt; &lt;level value="error"/&gt; &lt;appender-ref ref="STDOUT"/&gt; &lt;appender-ref ref="ALL"/&gt; &lt;/logger&gt; &lt;!-- 根logger的设置--&gt; &lt;root&gt; &lt;level value="warn"/&gt; &lt;appender-ref ref="STDOUT"/&gt; &lt;/root&gt;&lt;/log4j:configuration&gt; 参考 slf4 官方文档 logback 官方文档 log4j 官方文档 commons-logging 官方文档 http://blog.csdn.net/yycdaizi/article/details/8276265]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dozer 使用小结]]></title>
    <url>%2Fblog%2F2016%2F10%2F12%2Fjava%2Fjavalib%2Fdozer%2F</url>
    <content type="text"><![CDATA[Dozer 使用小结 这篇文章是本人在阅读 Dozer 官方文档（5.5.1 版本，官网已经一年多没更新了）的过程中，整理下来我认为比较基础的应用场景。 本文中提到的例子应该能覆盖 JavaBean 映射的大部分场景，希望对你有所帮助。 概述 Dozer 是什么? Dozer 是一个 JavaBean 映射工具库。 它支持简单的属性映射，复杂类型映射，双向映射，隐式显式的映射，以及递归映射。 它支持三种映射方式：注解、API、XML。 它是开源的，遵从Apache 2.0 协议 安装 引入 jar 包 maven 方式 如果你的项目使用 maven，添加以下依赖到你的 pom.xml 即可： &lt;dependency&gt; &lt;groupId&gt;net.sf.dozer&lt;/groupId&gt; &lt;artifactId&gt;dozer&lt;/artifactId&gt; &lt;version&gt;5.4.0&lt;/version&gt;&lt;/dependency&gt; 非 maven 方式 如果你的项目不使用 maven，那就只能发扬不怕苦不怕累的精神了。 使用 Dozer 需要引入 Dozer 的 jar 包以及其依赖的第三方 jar 包。 Dozer Dozer 依赖的第三方 jar 包 Eclipse 插件 Dozer 有插件可以在 Eclipse 中使用(不知道是否好用，反正我没用过) 插件地址: http://dozer.sourceforge.net/eclipse-plugin 使用 将 Dozer 引入到工程中后，我们就可以来小试一番了。 实践出真知，先以一个最简单的例子来展示 Dozer 映射的处理过程。 准备 我们先准备两个要互相映射的类 NotSameAttributeA.java public class NotSameAttributeA &#123; private long id; private String name; private Date date; // 省略getter/setter&#125; NotSameAttributeB.java public class NotSameAttributeB &#123; private long id; private String value; private Date date; // 省略getter/setter&#125; 这两个类存在属性名不完全相同的情况：name 和 value。 Dozer 的配置 为什么要有映射配置? 如果要映射的两个对象有完全相同的属性名，那么一切都很简单。 只需要直接使用 Dozer 的 API 即可： Mapper mapper = new DozerBeanMapper();DestinationObject destObject = mapper.map(sourceObject, DestinationObject.class); 但实际映射时，往往存在属性名不同的情况。 所以，你需要一些配置来告诉 Dozer 应该转换什么，怎么转换。 注：官网着重建议：在现实应用中，最好不要每次映射对象时都创建一个Mapper实例来工作，这样会产生不必要的开销。如果你不使用 IoC 容器（如：spring）来管理你的项目，那么，最好将Mapper定义为单例模式。 映射配置文件 在src/test/resources目录下添加dozer/dozer-mapping.xml文件。 &lt;mapping&gt;标签中允许你定义&lt;class-a&gt;和&lt;class-b&gt;，对应着相互映射的类。 &lt;field&gt;标签里定义要映射的特殊属性。需要注意&lt;a&gt;和&lt;class-a&gt;对应，&lt;b&gt;和&lt;class-b&gt;对应，聪明的你，猜也猜出来了吧。 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;mappings xmlns="http://dozer.sourceforge.net" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dozer.sourceforge.net http://dozer.sourceforge.net/schema/beanmapping.xsd"&gt; &lt;mapping date-format="yyyy-MM-dd"&gt; &lt;class-a&gt;org.zp.notes.spring.common.dozer.vo.NotSameAttributeA&lt;/class-a&gt; &lt;class-b&gt;org.zp.notes.spring.common.dozer.vo.NotSameAttributeB&lt;/class-b&gt; &lt;field&gt; &lt;a&gt;name&lt;/a&gt; &lt;b&gt;value&lt;/b&gt; &lt;/field&gt; &lt;/mapping&gt;&lt;/mappings&gt; 与 Spring 整合 配置 DozerBeanMapperFactoryBean 在src/test/resources目录下添加spring/spring-dozer.xml文件。 Dozer 与 Spring 的整合很便利，你只需要声明一个DozerBeanMapperFactoryBean， 将所有的 dozer 映射配置文件作为属性注入到mappingFiles， DozerBeanMapperFactoryBean会加载这些规则。 spring-dozer.xml 文件范例 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.0.xsd" default-autowire="byName" default-lazy-init="false"&gt; &lt;bean id="mapper" class="org.dozer.spring.DozerBeanMapperFactoryBean"&gt; &lt;property name="mappingFiles"&gt; &lt;list&gt; &lt;value&gt;classpath*:dozer/dozer-mapping.xml&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 自动装配 至此，万事具备，你只需要自动装配mapper。 RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &#123;"classpath:spring/spring-dozer.xml"&#125;)@TransactionConfiguration(defaultRollback = false)public class DozerTest extends TestCase &#123; @Autowired Mapper mapper; @Test public void testNotSameAttributeMapping() &#123; NotSameAttributeA src = new NotSameAttributeA(); src.setId(007); src.setName("邦德"); src.setDate(new Date()); NotSameAttributeB desc = mapper.map(src, NotSameAttributeB.class); Assert.assertNotNull(desc); &#125;&#125; 运行一下单元测试，绿灯通过。 Dozer 支持的数据类型转换 Dozer 可以自动做数据类型转换。当前，Dozer 支持以下数据类型转换（都是双向的） Primitive to Primitive Wrapper 原型(int、long 等)和原型包装类(Integer、Long) Primitive to Custom Wrapper 原型和定制的包装 Primitive Wrapper to Primitive Wrapper 原型包装类和包装类 Primitive to Primitive 原型和原型 Complex Type to Complex Type 复杂类型和复杂类型 String to Primitive 字符串和原型 String to Primitive Wrapper 字符串和原型包装类 String to Complex Type if the Complex Type contains a String constructor 字符串和有字符串构造器的复杂类型（类） String to Map 字符串和 Map Collection to Collection 集合和集合 Collection to Array 集合和数组 Map to Complex Type Map 和复杂类型 Map to Custom Map Type Map 和定制 Map 类型 Enum to Enum 枚举和枚举 Each of these can be mapped to one another: java.util.Date, java.sql.Date, java.sql.Time, java.sql.Timestamp, java.util.Calendar, java.util.GregorianCalendar 这些时间相关的常见类可以互换：java.util.Date, java.sql.Date, java.sql.Time, java.sql.Timestamp, java.util.Calendar, java.util.GregorianCalendar String to any of the supported Date/Calendar Objects. 字符串和支持 Date/Calendar 的对象 **Objects containing a toString() method that produces a long representing time in (ms) to any supported Date/Calendar object. ** 如果一个对象的 toString()方法返回的是一个代表 long 型的时间数值（单位：ms），就可以和任何支持 Date/Calendar 的对象转换。 Dozer 的映射配置 在前面的简单例子中，我们体验了一把 Dozer 的映射流程。但是两个类进行映射，有很多复杂的情况，相应的，你也需要一些更复杂的配置。 Dozer 有三种映射配置方式： 注解方式 API 方式 XML 方式 用注解来配置映射 Dozer 5.3.2版本开始支持注解方式配置映射（只有一个注解：@Mapping）。可以应对一些简单的映射处理，复杂的就玩不转了。 看一下@Mapping的声明就可以知道，这个注解只能用于元素和方法。 @Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.FIELD, ElementType.METHOD&#125;)public @interface Mapping &#123; String value() default "";&#125; 让我们来试试吧： TargetBean.java public class SourceBean &#123; private Long id; private String name; @Mapping("binaryData") private String data; @Mapping("pk") public Long getId() &#123; return this.id; &#125; //其余getter/setter方法略&#125; TargetBean.java public class TargetBean &#123; private String pk; private String name; private String binaryData; //getter/setter方法略&#125; 定义了两个相互映射的 Java 类，只需要在源类中用@Mapping标记和目标类中对应的属性就可以了。 @Testpublic void testAnnotationMapping() &#123; SourceBean src = new SourceBean(); src.setId(7L); src.setName("邦德"); src.setData("00000111"); TargetBean desc = mapper.map(src, TargetBean.class); Assert.assertNotNull(desc);&#125; 测试一下，绿灯通过。 官方文档说，虽然当前版本（文档的版本对应 Dozer 5.5.1）仅支持@Mapping，但是在未来的发布版本会提供其他的注解功能，那就敬请期待吧（再次吐槽一下：一年多没更新了）。 用 API 来配置映射 个人觉得这种方式比较麻烦，不推荐，也不想多做介绍，就是这么任性。 用 XML 来配置映射 需要强调的是：如果两个类的所有属性都能很好的互转，可以你中有我，我中有你，不分彼此，那么就不要画蛇添足的在 xml 中去声明映射规则了。 属性名不同时的映射(Basic Property Mapping) Dozer 会自动映射属性名相同的属性，所以不必添加在 xml 文件中。 &lt;field&gt; &lt;a&gt;one&lt;/a&gt; &lt;b&gt;onePrime&lt;/b&gt;&lt;/field&gt; 字符串和日期映射(String to Date Mapping) 字符串在和日期进行映射时，允许用户指定日期的格式。 格式的设置分为三个作用域级别： 属性级别 对当前属性有效（这个属性必须是日期字符串） &lt;field&gt; &lt;a date-format="MM/dd/yyyy HH:mm:ss:SS"&gt;dateString&lt;/a&gt; &lt;b&gt;dateObject&lt;/b&gt;&lt;/field&gt; 类级别 对这个类中的所有日期相关的属性有效 &lt;mapping date-format="MM-dd-yyyy HH:mm:ss"&gt; &lt;class-a&gt;org.dozer.vo.TestObject&lt;/class-a&gt; &lt;class-b&gt;org.dozer.vo.TestObjectPrime&lt;/class-b&gt; &lt;field&gt; &lt;a&gt;dateString&lt;/a&gt; &lt;b&gt;dateObject&lt;/b&gt; &lt;/field&gt;&lt;/mapping&gt; 全局级别 对整个文件中的所有日期相关的属性有效。 &lt;mappings&gt; &lt;configuration&gt; &lt;date-format&gt;MM/dd/yyyy HH:mm&lt;/date-format&gt; &lt;/configuration&gt; &lt;mapping wildcard="true"&gt; &lt;class-a&gt;org.dozer.vo.TestObject&lt;/class-a&gt; &lt;class-b&gt;org.dozer.vo.TestObjectPrime&lt;/class-b&gt; &lt;field&gt; &lt;a&gt;dateString&lt;/a&gt; &lt;b&gt;dateObject&lt;/b&gt; &lt;/field&gt; &lt;/mapping&gt;&lt;/mappings&gt; 集合和数组映射(Collection and Array Mapping) Dozer 可以自动处理以下类型的双向转换。 List to List List to Array Array to Array Set to Set Set to Array Set to List 使用 hint 如果使用泛型或数组，没有必要使用 hint。 如果不使用泛型或数组。在处理集合或数组之间的转换时，你需要用hint指定目标列表的数据类型。 若你不指定hint，Dozer 将认为目标集合和源集合的类型是一致的。 使用 Hints 的范例： &lt;field&gt; &lt;a&gt;hintList&lt;/a&gt; &lt;b&gt;hintList&lt;/b&gt; &lt;b-hint&gt;org.dozer.vo.TheFirstSubClassPrime&lt;/b-hint&gt;&lt;/field&gt; 累计映射和非累计映射（Cumulative vs. Non-Cumulative List Mapping） 如果你要转换的目标类已经初始化，你可以选择让 Dozer 添加或更新对象到你的集合中。 而这取决于relationship-type配置，默认是累计。 它的设置有作用域级别： 全局级 &lt;mappings&gt; &lt;configuration&gt; &lt;relationship-type&gt;non-cumulative&lt;/relationship-type&gt; &lt;/configuration&gt;&lt;/mappings&gt; 类级别 &lt;mappings&gt; &lt;mapping relationship-type="non-cumulative"&gt; &lt;!-- 省略 --&gt; &lt;/mapping&gt;&lt;/mappings&gt; 属性级别 &lt;field relationship-type="cumulative"&gt; &lt;a&gt;hintList&lt;/a&gt; &lt;b&gt;hintList&lt;/b&gt; &lt;a-hint&gt;org.dozer.vo.TheFirstSubClass&lt;/a-hint&gt; &lt;b-hint&gt;org.dozer.vo.TheFirstSubClassPrime&lt;/b-hint&gt;&lt;/field&gt; 移动孤儿(Removing Orphans) 这里的孤儿是指目标集合中存在，但是源集合中不存在的元素。 你可以使用remove-orphans开关来选择是否移除这样的元素。 &lt;field remove-orphans="true"&gt; &lt;a&gt;srcList&lt;/a&gt; &lt;b&gt;destList&lt;/b&gt;&lt;/field&gt; 深度映射(Deep Mapping) 所谓深度映射，是指允许你指定属性的属性（比如一个类的属性本身也是一个类）。举例来说 Source.java public class Source &#123; private long id; private String info;&#125; Dest.java public class Dest &#123; private long id; private Info info;&#125; public class Info &#123; private String content;&#125; 映射规则 &lt;mapping&gt; &lt;class-a&gt;org.zp.notes.spring.common.dozer.vo.Source&lt;/class-a&gt; &lt;class-b&gt;org.zp.notes.spring.common.dozer.vo.Dest&lt;/class-b&gt; &lt;field&gt; &lt;a&gt;info&lt;/a&gt; &lt;b&gt;info.content&lt;/b&gt; &lt;/field&gt;&lt;/mapping&gt; 排除属性(Excluding Fields) 就像任何团体都有捣乱分子，类之间转换时也有想要排除的因子。 如何在做类型转换时，自动排除一些属性，Dozer 提供了几种方法，这里只介绍一种比较通用的方法。 更多详情参考官网。 field-exclude 可以排除不需要映射的属性。 &lt;field-exclude&gt; &lt;a&gt;fieldToExclude&lt;/a&gt; &lt;b&gt;fieldToExclude&lt;/b&gt;&lt;/field-exclude&gt; 单向映射(One-Way Mapping) 注：本文的映射方式，无特殊说明，都是双向映射的。 有的场景可能希望转换过程不可逆，即单向转换。 单向转换可以通过使用one-way来开启 类级别 &lt;mapping type="one-way"&gt; &lt;class-a&gt;org.dozer.vo.TestObjectFoo&lt;/class-a&gt; &lt;class-b&gt;org.dozer.vo.TestObjectFooPrime&lt;/class-b&gt; &lt;field&gt; &lt;a&gt;oneFoo&lt;/a&gt; &lt;b&gt;oneFooPrime&lt;/b&gt; &lt;/field&gt;&lt;/mapping&gt; 属性级别 &lt;mapping&gt; &lt;class-a&gt;org.dozer.vo.TestObjectFoo2&lt;/class-a&gt; &lt;class-b&gt;org.dozer.vo.TestObjectFooPrime2&lt;/class-b&gt; &lt;field type="one-way"&gt; &lt;a&gt;oneFoo2&lt;/a&gt; &lt;b&gt;oneFooPrime2&lt;/b&gt; &lt;/field&gt; &lt;field type="one-way"&gt; &lt;a&gt;oneFoo3.prime&lt;/a&gt; &lt;b&gt;oneFooPrime3&lt;/b&gt; &lt;/field&gt; 全局配置(Global Configuration) 全局配置用来设置全局的配置信息。此外，任何定制转换都是在这里定义的。 全局配置都是可选的。 &lt;date-format&gt;表示日期格式 &lt;stop-on-errors&gt;错误处理开关 &lt;wildcard&gt;通配符 &lt;trim-strings&gt;裁剪字符串开关 &lt;configuration &gt; &lt;date-format&gt;MM/dd/yyyy HH:mm&lt;/date-format&gt; &lt;stop-on-errors&gt;true&lt;/stop-on-errors&gt; &lt;wildcard&gt;true&lt;/wildcard&gt; &lt;trim-strings&gt;false&lt;/trim-strings&gt; &lt;custom-converters&gt; &lt;!-- these are always bi-directional --&gt; &lt;converter type="org.dozer.converters.TestCustomConverter" &gt; &lt;class-a&gt;org.dozer.vo.TestCustomConverterObject&lt;/class-a&gt; &lt;class-b&gt;another.type.to.Associate&lt;/class-b&gt; &lt;/converter&gt; &lt;/custom-converters&gt;&lt;/configuration&gt; 全局配置的作用是帮助你少配置一些参数，如果个别类的映射规则需要变更，你可以 mapping 中覆盖它。 覆盖的范例如下 &lt;mapping date-format="MM-dd-yyyy HH:mm:ss"&gt; &lt;!-- 省略 --&gt;&lt;/mapping&gt;&lt;mapping wildcard="false"&gt; &lt;!-- 省略 --&gt;&lt;/mapping&gt;&lt;mapping stop-on-errors="false"&gt; &lt;!-- 省略 --&gt;&lt;/mapping&gt;&lt;mapping trim-strings="true"&gt; &lt;!-- 省略 --&gt;&lt;/mapping&gt; 定制转换(Custom Converters) 如果 Dozer 默认的转换规则不能满足实际需要，你可以选择定制转换。 定制转换通过配置 XML 来告诉 Dozer 如何去转换两个指定的类。当 Dozer 转换这两个指定类的时候，会调用你的映射规则去替换标准映射规则。 为了让 Dozer 识别，你必须实现org.dozer.CustomConverter接口。否则，Dozer 会抛异常。 具体做法： (1) 创建一个类实现org.dozer.CustomConverter接口。 public class TestCustomConverter implements CustomConverter &#123; public Object convert(Object destination, Object source, Class destClass, Class sourceClass) &#123; if (source == null) &#123; return null; &#125; CustomDoubleObject dest = null; if (source instanceof Double) &#123; // check to see if the object already exists if (destination == null) &#123; dest = new CustomDoubleObject(); &#125; else &#123; dest = (CustomDoubleObject) destination; &#125; dest.setTheDouble(((Double) source).doubleValue()); return dest; &#125; else if (source instanceof CustomDoubleObject) &#123; double sourceObj = ((CustomDoubleObject) source).getTheDouble(); return new Double(sourceObj); &#125; else &#123; throw new MappingException("Converter TestCustomConverter " + "used incorrectly. Arguments passed in were:" + destination + " and " + source); &#125; &#125; (2) 在 xml 中引用定制的映射规则 引用定制的映射规则也是分级的，你可以酌情使用。 全局级 &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;mappings xmlns="http://dozer.sourceforge.net" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://dozer.sourceforge.net http://dozer.sourceforge.net/schema/beanmapping.xsd"&gt; &lt;configuration&gt; &lt;!-- 总是双向转换的 --&gt; &lt;custom-converters&gt; &lt;converter type="org.dozer.converters.TestCustomConverter" &gt; &lt;class-a&gt;org.dozer.vo.CustomDoubleObject&lt;/class-a&gt; &lt;class-b&gt;java.lang.Double&lt;/class-b&gt; &lt;/converter&gt; &lt;!-- You are responsible for mapping everything between ClassA and ClassB --&gt; &lt;converter type="org.dozer.converters.TestCustomHashMapConverter" &gt; &lt;class-a&gt;org.dozer.vo.TestCustomConverterHashMapObject&lt;/class-a&gt; &lt;class-b&gt;org.dozer.vo.TestCustomConverterHashMapPrimeObject&lt;/class-b&gt; &lt;/converter&gt; &lt;/custom-converters&gt; &lt;/configuration&gt;&lt;/mappings&gt; 属性级 &lt;mapping&gt; &lt;class-a&gt;org.dozer.vo.SimpleObj&lt;/class-a&gt; &lt;class-b&gt;org.dozer.vo.SimpleObjPrime2&lt;/class-b&gt; &lt;field custom-converter= "org.dozer.converters.TestCustomConverter"&gt; &lt;a&gt;field1&lt;/a&gt; &lt;b&gt;field1Prime&lt;/b&gt; &lt;/field&gt;&lt;/mapping&gt; 映射的继承(Inheritance Mapping) Dozer 支持映射规则的继承机制。 属性如果有着相同的名字则不需要在 xml 中配置，除非使用了hint 我们来看一个例子 &lt;mapping&gt; &lt;class-a&gt;org.dozer.vo.SuperClass&lt;/class-a&gt; &lt;class-b&gt;org.dozer.vo.SuperClassPrime&lt;/class-b&gt; &lt;field&gt; &lt;a&gt;superAttribute&lt;/a&gt; &lt;b&gt;superAttr&lt;/b&gt; &lt;/field&gt;&lt;/mapping&gt;&lt;mapping&gt; &lt;class-a&gt;org.dozer.vo.SubClass&lt;/class-a&gt; &lt;class-b&gt;org.dozer.vo.SubClassPrime&lt;/class-b&gt; &lt;field&gt; &lt;a&gt;attribute&lt;/a&gt; &lt;b&gt;attributePrime&lt;/b&gt; &lt;/field&gt;&lt;/mapping&gt;&lt;mapping&gt; &lt;class-a&gt;org.dozer.vo.SubClass2&lt;/class-a&gt; &lt;class-b&gt;org.dozer.vo.SubClassPrime2&lt;/class-b&gt; &lt;field&gt; &lt;a&gt;attribute2&lt;/a&gt; &lt;b&gt;attributePrime2&lt;/b&gt; &lt;/field&gt;&lt;/mapping&gt; 在上面的例子中 SubClass、SubClass2 是 SuperClass 的子类； SubClassPrime 和 SubClassPrime2 是 SuperClassPrime 的子类。 superAttribute 和 superAttr 的映射规则会被子类所继承，所以不必再重复的在子类中去声明。 参考 Dozer 官方文档 | Dozer 源码地址]]></content>
      <categories>
        <category>java</category>
        <category>javalib</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javalib</tag>
        <tag>bean</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2Fblog%2F2016%2F10%2F10%2Ftools%2Fregex%2F</url>
    <content type="text"><![CDATA[正则表达式 📓 本文已归档到：「blog」 简介 基本元字符 等价字符 元字符优先级顺序 捕获与非捕获 反向引用 非捕获组 零宽断言 贪婪与懒惰 最实用的正则 速查元字符字典 参考资料 简介 为了理解下面章节的内容，你需要先了解一些基本概念。 正则表达式 - 正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。 元字符 - 元字符(metacharacters)就是正则表达式中具有特殊意义的专用字符。 普通字符 - 普通字符包括没有显式指定为元字符的所有可打印和不可打印字符。这包括所有大写和小写字母、所有数字、所有标点符号和一些其他符号。 基本元字符 正则表达式的元字符难以记忆，很大程度上是因为有很多为了简化表达而出现的等价字符。 而实际上最基本的元字符，并没有那么多。对于大部分的场景，基本元字符都可以搞定。 让我们从一个个实例出发，由浅入深的去体会正则的奥妙。 多选 - | 例 匹配一个确定的字符串 checkMatches("abc", "abc"); 如果要匹配一个确定的字符串，非常简单，如例 1 所示。 如果你不确定要匹配的字符串，希望有多个选择，怎么办？ 答案是：使用元字符| ，它的含义是或。 例 匹配多个可选的字符串 // 测试正则表达式字符：|Assert.assertTrue(checkMatches("yes|no", "yes"));Assert.assertTrue(checkMatches("yes|no", "no"));Assert.assertFalse(checkMatches("yes|no", "right")); 输出 yes matches： yes|nono matches： yes|noright not matches： yes|no 分组 - () 如果你希望表达式由多个子表达式组成，你可以使用 ()。 例 匹配组合字符串 Assert.assertTrue(checkMatches("(play|end)(ing|ed)", "ended"));Assert.assertTrue(checkMatches("(play|end)(ing|ed)", "ending"));Assert.assertTrue(checkMatches("(play|end)(ing|ed)", "playing"));Assert.assertTrue(checkMatches("(play|end)(ing|ed)", "played")); 输出 ended matches： (play|end)(ing|ed)ending matches： (play|end)(ing|ed)playing matches： (play|end)(ing|ed)played matches： (play|end)(ing|ed) 指定单字符有效范围 - [] 前面展示了如何匹配字符串，但是很多时候你需要精确的匹配一个字符，这时可以使用[] 。 例 字符在指定范围 // 测试正则表达式字符：[]Assert.assertTrue(checkMatches("[abc]", "b")); // 字符只能是a、b、cAssert.assertTrue(checkMatches("[a-z]", "m")); // 字符只能是a - zAssert.assertTrue(checkMatches("[A-Z]", "O")); // 字符只能是A - ZAssert.assertTrue(checkMatches("[a-zA-Z]", "K")); // 字符只能是a - z和A - ZAssert.assertTrue(checkMatches("[a-zA-Z]", "k"));Assert.assertTrue(checkMatches("[0-9]", "5")); // 字符只能是0 - 9 输出 b matches： [abc]m matches： [a-z]O matches： [A-Z]K matches： [a-zA-Z]k matches： [a-zA-Z]5 matches： [0-9] 指定单字符无效范围 - [^] 例 字符不能在指定范围 如果需要匹配一个字符的逆操作，即字符不能在指定范围，可以使用[^]。 // 测试正则表达式字符：[^]Assert.assertFalse(checkMatches("[^abc]", "b")); // 字符不能是a、b、cAssert.assertFalse(checkMatches("[^a-z]", "m")); // 字符不能是a - zAssert.assertFalse(checkMatches("[^A-Z]", "O")); // 字符不能是A - ZAssert.assertFalse(checkMatches("[^a-zA-Z]", "K")); // 字符不能是a - z和A - ZAssert.assertFalse(checkMatches("[^a-zA-Z]", "k"));Assert.assertFalse(checkMatches("[^0-9]", "5")); // 字符不能是0 - 9 输出 b not matches： [^abc]m not matches： [^a-z]O not matches： [^A-Z]K not matches： [^a-zA-Z]k not matches： [^a-zA-Z]5 not matches： [^0-9] 限制字符数量 - {} 如果想要控制字符出现的次数，可以使用{}。 字符 描述 {n} n 是一个非负整数。匹配确定的 n 次。 {n,} n 是一个非负整数。至少匹配 n 次。 {n,m} m 和 n 均为非负整数，其中 n &lt;= m。最少匹配 n 次且最多匹配 m 次。 例 限制字符出现次数 // &#123;n&#125;: n 是一个非负整数。匹配确定的 n 次。checkMatches("ap&#123;1&#125;", "a");checkMatches("ap&#123;1&#125;", "ap");checkMatches("ap&#123;1&#125;", "app");checkMatches("ap&#123;1&#125;", "apppppppppp");// &#123;n,&#125;: n 是一个非负整数。至少匹配 n 次。checkMatches("ap&#123;1,&#125;", "a");checkMatches("ap&#123;1,&#125;", "ap");checkMatches("ap&#123;1,&#125;", "app");checkMatches("ap&#123;1,&#125;", "apppppppppp");// &#123;n,m&#125;: m 和 n 均为非负整数，其中 n &lt;= m。最少匹配 n 次且最多匹配 m 次。checkMatches("ap&#123;2,5&#125;", "a");checkMatches("ap&#123;2,5&#125;", "ap");checkMatches("ap&#123;2,5&#125;", "app");checkMatches("ap&#123;2,5&#125;", "apppppppppp"); 输出 a not matches： ap&#123;1&#125;ap matches： ap&#123;1&#125;app not matches： ap&#123;1&#125;apppppppppp not matches： ap&#123;1&#125;a not matches： ap&#123;1,&#125;ap matches： ap&#123;1,&#125;app matches： ap&#123;1,&#125;apppppppppp matches： ap&#123;1,&#125;a not matches： ap&#123;2,5&#125;ap not matches： ap&#123;2,5&#125;app matches： ap&#123;2,5&#125;apppppppppp not matches： ap&#123;2,5&#125; 转义字符 - / 如果想要查找元字符本身，你需要使用转义符，使得正则引擎将其视作一个普通字符，而不是一个元字符去处理。 * 的转义字符：\*+ 的转义字符：\+? 的转义字符：\?^ 的转义字符：\^$ 的转义字符：\$. 的转义字符：\. 如果是转义符\本身，你也需要使用\\ 。 指定表达式字符串的开始和结尾 - ^、$ 如果希望匹配的字符串必须以特定字符串开头，可以使用^ 。 注：请特别留意，这里的^ 一定要和 [^] 中的 “^” 区分。 例 限制字符串头部 Assert.assertTrue(checkMatches("^app[a-z]&#123;0,&#125;", "apple")); // 字符串必须以app开头Assert.assertFalse(checkMatches("^app[a-z]&#123;0,&#125;", "aplause")); 输出 apple matches： ^app[a-z]&#123;0,&#125;aplause not matches： ^app[a-z]&#123;0,&#125; 如果希望匹配的字符串必须以特定字符串开头，可以使用$ 。 例 限制字符串尾部 Assert.assertTrue(checkMatches("[a-z]&#123;0,&#125;ing$", "playing")); // 字符串必须以ing结尾Assert.assertFalse(checkMatches("[a-z]&#123;0,&#125;ing$", "long")); 输出 playing matches： [a-z]&#123;0,&#125;ing$long not matches： [a-z]&#123;0,&#125;ing$ 等价字符 等价字符，顾名思义，就是对于基本元字符表达的一种简化（等价字符的功能都可以通过基本元字符来实现）。 在没有掌握基本元字符之前，可以先不用理会，因为很容易把人绕晕。 等价字符的好处在于简化了基本元字符的写法。 表示某一类型字符的等价字符 下表中的等价字符都表示某一类型的字符。 字符 描述 . 匹配除“\n”之外的任何单个字符。 \d 匹配一个数字字符。等价于[0-9]。 \D 匹配一个非数字字符。等价于[^0-9]。 \w 匹配包括下划线的任何单词字符。类似但不等价于“[A-Za-z0-9_]”，这里的单词字符指的是 Unicode 字符集。 \W 匹配任何非单词字符。 \s 匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]。 \S 匹配任何可见字符。等价于[ \f\n\r\t\v]。 案例 基本等价字符的用法 // 匹配除“\n”之外的任何单个字符Assert.assertTrue(checkMatches(".&#123;1,&#125;", "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_"));Assert.assertTrue(checkMatches(".&#123;1,&#125;", "~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\"));Assert.assertFalse(checkMatches(".", "\n"));Assert.assertFalse(checkMatches("[^\n]", "\n"));// 匹配一个数字字符。等价于[0-9]Assert.assertTrue(checkMatches("\\d&#123;1,&#125;", "0123456789"));// 匹配一个非数字字符。等价于[^0-9]Assert.assertFalse(checkMatches("\\D&#123;1,&#125;", "0123456789"));// 匹配包括下划线的任何单词字符。类似但不等价于“[A-Za-z0-9_]”，这里的单词字符指的是Unicode字符集Assert.assertTrue(checkMatches("\\w&#123;1,&#125;", "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_"));Assert.assertFalse(checkMatches("\\w&#123;1,&#125;", "~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\"));// 匹配任何非单词字符Assert.assertFalse(checkMatches("\\W&#123;1,&#125;", "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_"));Assert.assertTrue(checkMatches("\\W&#123;1,&#125;", "~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\"));// 匹配任何不可见字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]Assert.assertTrue(checkMatches("\\s&#123;1,&#125;", " \f\r\n\t"));// 匹配任何可见字符。等价于[^ \f\n\r\t\v]Assert.assertFalse(checkMatches("\\S&#123;1,&#125;", " \f\r\n\t")); 输出 ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_ matches： .&#123;1,&#125;~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\ matches： .&#123;1,&#125;\n not matches： .\n not matches： [^\n]0123456789 matches： \\d&#123;1,&#125;0123456789 not matches： \\D&#123;1,&#125;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_ matches： \\w&#123;1,&#125;~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\ not matches： \\w&#123;1,&#125;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_ not matches： \\W&#123;1,&#125;~!@#$%^&amp;*()+`-=[]&#123;&#125;;:&lt;&gt;,./?|\\ matches： \\W&#123;1,&#125; \f\r\n\t matches： \\s&#123;1,&#125; \f\r\n\t not matches： \\S&#123;1,&#125; 限制字符数量的等价字符 在基本元字符章节中，已经介绍了限制字符数量的基本元字符 - {} 。 此外，还有 *、+、? 这个三个为了简化写法而出现的等价字符，我们来认识一下。 字符 描述 * 匹配前面的子表达式零次或多次。等价于{0,}。 + 匹配前面的子表达式一次或多次。等价于{1,}。 ? 匹配前面的子表达式零次或一次。等价于 {0,1}。 案例 限制字符数量的等价字符 // *: 匹配前面的子表达式零次或多次。* 等价于&#123;0,&#125;。checkMatches("ap*", "a");checkMatches("ap*", "ap");checkMatches("ap*", "app");checkMatches("ap*", "apppppppppp");// +: 匹配前面的子表达式一次或多次。+ 等价于 &#123;1,&#125;。checkMatches("ap+", "a");checkMatches("ap+", "ap");checkMatches("ap+", "app");checkMatches("ap+", "apppppppppp");// ?: 匹配前面的子表达式零次或一次。? 等价于 &#123;0,1&#125;。checkMatches("ap?", "a");checkMatches("ap?", "ap");checkMatches("ap?", "app");checkMatches("ap?", "apppppppppp"); 输出 a matches： ap*ap matches： ap*app matches： ap*apppppppppp matches： ap*a not matches： ap+ap matches： ap+app matches： ap+apppppppppp matches： ap+a matches： ap?ap matches： ap?app not matches： ap?apppppppppp not matches： ap? 元字符优先级顺序 正则表达式从左到右进行计算，并遵循优先级顺序，这与算术表达式非常类似。 下表从最高到最低说明了各种正则表达式运算符的优先级顺序： 运算符 说明 \ 转义符 (), (?😃, (?=), [] 括号和中括号 *, +, ?, {n}, {n,}, {n,m} 限定符 ^, $, *任何元字符、任何字符* 定位点和序列 | 替换 字符具有高于替换运算符的优先级，使得“m|food”匹配“m”或“food”。若要匹配“mood”或“food”，请使用括号创建子表达式，从而产生“(m|f)ood”。 分组构造 在基本元字符章节，提到了 () 字符可以用来对表达式分组。实际上分组还有更多复杂的用法。 所谓分组构造，是用来描述正则表达式的子表达式，用于捕获字符串中的子字符串。 捕获与非捕获 下表为分组构造中的捕获和非捕获分类。 表达式 描述 捕获或非捕获 (exp) 匹配的子表达式 捕获 (?&lt;name&gt;exp) 命名的反向引用 捕获 (?:exp) 非捕获组 非捕获 (?=exp) 零宽度正预测先行断言 非捕获 (?!exp) 零宽度负预测先行断言 非捕获 (?&lt;=exp) 零宽度正回顾后发断言 非捕获 (?&lt;!exp) 零宽度负回顾后发断言 非捕获 注：Java 正则引擎不支持平衡组。 反向引用 带编号的反向引用 带编号的反向引用使用以下语法：\number 其中number 是正则表达式中捕获组的序号位置。 例如，\4 匹配第四个捕获组的内容。 如果正则表达式模式中未定义number，则将发生分析错误 例 匹配重复的单词和紧随每个重复的单词的单词(不命名子表达式) // (\w+)\s\1\W(\w+) 匹配重复的单词和紧随每个重复的单词的单词Assert.assertTrue(findAll("(\\w+)\\s\\1\\W(\\w+)", "He said that that was the the correct answer.") &gt; 0); 输出 regex = (\w+)\s\1\W(\w+), content: He said that that was the the correct answer.[1th] start: 8, end: 21, group: that that was[2th] start: 22, end: 37, group: the the correct 说明 (\w+): 匹配一个或多个单词字符。 \s: 与空白字符匹配。 \1: 匹配第一个组，即(\w+)。 \W: 匹配包括空格和标点符号的一个非单词字符。 这样可以防止正则表达式模式匹配从第一个捕获组的单词开头的单词。 命名的反向引用 命名后向引用通过使用下面的语法进行定义：\k&lt;name &gt; 例 匹配重复的单词和紧随每个重复的单词的单词(命名子表达式) // (?&lt;duplicateWord&gt;\w+)\s\k&lt;duplicateWord&gt;\W(?&lt;nextWord&gt;\w+) 匹配重复的单词和紧随每个重复的单词的单词Assert.assertTrue(findAll("(?&lt;duplicateWord&gt;\\w+)\\s\\k&lt;duplicateWord&gt;\\W(?&lt;nextWord&gt;\\w+)", "He said that that was the the correct answer.") &gt; 0); 输出 regex = (?&lt;duplicateWord&gt;\w+)\s\k&lt;duplicateWord&gt;\W(?&lt;nextWord&gt;\w+), content: He said that that was the the correct answer.[1th] start: 8, end: 21, group: that that was[2th] start: 22, end: 37, group: the the correct 说明 (?\w+): 匹配一个或多个单词字符。 命名此捕获组 duplicateWord。 \s: 与空白字符匹配。 \k: 匹配名为 duplicateWord 的捕获的组。 \W: 匹配包括空格和标点符号的一个非单词字符。 这样可以防止正则表达式模式匹配从第一个捕获组的单词开头的单词。 (?\w+): 匹配一个或多个单词字符。 命名此捕获组 nextWord。 非捕获组 (?:exp) 表示当一个限定符应用到一个组，但组捕获的子字符串并非所需时，通常会使用非捕获组构造。 例 匹配以.结束的语句。 // 匹配由句号终止的语句。Assert.assertTrue(findAll("(?:\\b(?:\\w+)\\W*)+\\.", "This is a short sentence. Never end") &gt; 0); 输出 regex = (?:\b(?:\w+)\W*)+\., content: This is a short sentence. Never end[1th] start: 0, end: 25, group: This is a short sentence. 零宽断言 用于查找在某些内容(但并不包括这些内容)之前或之后的东西，也就是说它们像\b,^,$那样用于指定一个位置，这个位置应该满足一定的条件(即断言)，因此它们也被称为零宽断言。 表达式 描述 (?=exp) 匹配 exp 前面的位置 (?&lt;=exp) 匹配 exp 后面的位置 (?!exp) 匹配后面跟的不是 exp 的位置 (?&lt;!exp) 匹配前面不是 exp 的位置 匹配 exp 前面的位置 (?=exp) 表示输入字符串必须匹配子表达式中的正则表达式模式，尽管匹配的子字符串未包含在匹配结果中。 // \b\w+(?=\sis\b) 表示要捕获is之前的单词Assert.assertTrue(findAll("\\b\\w+(?=\\sis\\b)", "The dog is a Malamute.") &gt; 0);Assert.assertFalse(findAll("\\b\\w+(?=\\sis\\b)", "The island has beautiful birds.") &gt; 0);Assert.assertFalse(findAll("\\b\\w+(?=\\sis\\b)", "The pitch missed home plate.") &gt; 0);Assert.assertTrue(findAll("\\b\\w+(?=\\sis\\b)", "Sunday is a weekend day.") &gt; 0); 输出 regex = \b\w+(?=\sis\b), content: The dog is a Malamute.[1th] start: 4, end: 7, group: dogregex = \b\w+(?=\sis\b), content: The island has beautiful birds.not foundregex = \b\w+(?=\sis\b), content: The pitch missed home plate.not foundregex = \b\w+(?=\sis\b), content: Sunday is a weekend day.[1th] start: 0, end: 6, group: Sunday 说明 \b: 在单词边界处开始匹配。 \w+: 匹配一个或多个单词字符。 (?=\sis\b): 确定单词字符是否后接空白字符和字符串“is”，其在单词边界处结束。 如果如此，则匹配成功。 匹配 exp 后面的位置 (?&lt;=exp) 表示子表达式不得在输入字符串当前位置左侧出现，尽管子表达式未包含在匹配结果中。零宽度正回顾后发断言不会回溯。 // (?&lt;=\b20)\d&#123;2&#125;\b 表示要捕获以20开头的数字的后面部分Assert.assertTrue(findAll("(?&lt;=\\b20)\\d&#123;2&#125;\\b", "2010 1999 1861 2140 2009") &gt; 0); 输出 regex = (?&lt;=\b20)\d&#123;2&#125;\b, content: 2010 1999 1861 2140 2009[1th] start: 2, end: 4, group: 10[2th] start: 22, end: 24, group: 09 说明 \d{2}: 匹配两个十进制数字。 {?&lt;=\b20): 如果两个十进制数字的字边界以小数位数“20”开头，则继续匹配。 \b: 在单词边界处结束匹配。 匹配后面跟的不是 exp 的位置 (?!exp) 表示输入字符串不得匹配子表达式中的正则表达式模式，尽管匹配的子字符串未包含在匹配结果中。 例 捕获未以“un”开头的单词 // \b(?!un)\w+\b 表示要捕获未以“un”开头的单词Assert.assertTrue(findAll("\\b(?!un)\\w+\\b", "unite one unethical ethics use untie ultimate") &gt; 0); 输出 regex = \b(?!un)\w+\b, content: unite one unethical ethics use untie ultimate[1th] start: 6, end: 9, group: one[2th] start: 20, end: 26, group: ethics[3th] start: 27, end: 30, group: use[4th] start: 37, end: 45, group: ultimate 说明 \b: 在单词边界处开始匹配。 (?!un): 确定接下来的两个的字符是否为“un”。 如果没有，则可能匹配。 \w+: 匹配一个或多个单词字符。 \b: 在单词边界处结束匹配。 匹配前面不是 exp 的位置 (?&lt;!exp) 表示子表达式不得在输入字符串当前位置的左侧出现。 但是，任何不匹配子表达式 的子字符串不包含在匹配结果中。 例 捕获任意工作日 // (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b 表示要捕获任意工作日（即周一到周五）Assert.assertTrue(findAll("(?&lt;!(Saturday|Sunday) )\\b\\w+ \\d&#123;1,2&#125;, \\d&#123;4&#125;\\b", "Monday February 1, 2010") &gt; 0);Assert.assertTrue(findAll("(?&lt;!(Saturday|Sunday) )\\b\\w+ \\d&#123;1,2&#125;, \\d&#123;4&#125;\\b", "Wednesday February 3, 2010") &gt; 0);Assert.assertFalse(findAll("(?&lt;!(Saturday|Sunday) )\\b\\w+ \\d&#123;1,2&#125;, \\d&#123;4&#125;\\b", "Saturday February 6, 2010") &gt; 0);Assert.assertFalse(findAll("(?&lt;!(Saturday|Sunday) )\\b\\w+ \\d&#123;1,2&#125;, \\d&#123;4&#125;\\b", "Sunday February 7, 2010") &gt; 0);Assert.assertTrue(findAll("(?&lt;!(Saturday|Sunday) )\\b\\w+ \\d&#123;1,2&#125;, \\d&#123;4&#125;\\b", "Monday, February 8, 2010") &gt; 0); 输出 regex = (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b, content: Monday February 1, 2010[1th] start: 7, end: 23, group: February 1, 2010regex = (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b, content: Wednesday February 3, 2010[1th] start: 10, end: 26, group: February 3, 2010regex = (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b, content: Saturday February 6, 2010not foundregex = (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b, content: Sunday February 7, 2010not foundregex = (?&lt;!(Saturday|Sunday) )\b\w+ \d&#123;1,2&#125;, \d&#123;4&#125;\b, content: Monday, February 8, 2010[1th] start: 8, end: 24, group: February 8, 2010 贪婪与懒惰 当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能多的字符。以这个表达式为例：a.*b，它将会匹配最长的以 a 开始，以 b 结束的字符串。如果用它来搜索 aabab 的话，它会匹配整个字符串 aabab。这被称为贪婪匹配。 有时，我们更需要懒惰匹配，也就是匹配尽可能少的字符。前面给出的限定符都可以被转化为懒惰匹配模式，只要在它后面加上一个问号?。这样.*?就意味着匹配任意数量的重复，但是在能使整个匹配成功的前提下使用最少的重复。 表达式 描述 *? 重复任意次，但尽可能少重复 +? 重复 1 次或更多次，但尽可能少重复 ?? 重复 0 次或 1 次，但尽可能少重复 {n,m}? 重复 n 到 m 次，但尽可能少重复 {n,}? 重复 n 次以上，但尽可能少重复 例 Java 正则中贪婪与懒惰的示例 // 贪婪匹配Assert.assertTrue(findAll("a\\w*b", "abaabaaabaaaab") &gt; 0);// 懒惰匹配Assert.assertTrue(findAll("a\\w*?b", "abaabaaabaaaab") &gt; 0);Assert.assertTrue(findAll("a\\w+?b", "abaabaaabaaaab") &gt; 0);Assert.assertTrue(findAll("a\\w??b", "abaabaaabaaaab") &gt; 0);Assert.assertTrue(findAll("a\\w&#123;0,4&#125;?b", "abaabaaabaaaab") &gt; 0);Assert.assertTrue(findAll("a\\w&#123;3,&#125;?b", "abaabaaabaaaab") &gt; 0); 输出 regex = a\w*b, content: abaabaaabaaaab[1th] start: 0, end: 14, group: abaabaaabaaaabregex = a\w*?b, content: abaabaaabaaaab[1th] start: 0, end: 2, group: ab[2th] start: 2, end: 5, group: aab[3th] start: 5, end: 9, group: aaab[4th] start: 9, end: 14, group: aaaabregex = a\w+?b, content: abaabaaabaaaab[1th] start: 0, end: 5, group: abaab[2th] start: 5, end: 9, group: aaab[3th] start: 9, end: 14, group: aaaabregex = a\w??b, content: abaabaaabaaaab[1th] start: 0, end: 2, group: ab[2th] start: 2, end: 5, group: aab[3th] start: 6, end: 9, group: aab[4th] start: 11, end: 14, group: aabregex = a\w&#123;0,4&#125;?b, content: abaabaaabaaaab[1th] start: 0, end: 2, group: ab[2th] start: 2, end: 5, group: aab[3th] start: 5, end: 9, group: aaab[4th] start: 9, end: 14, group: aaaabregex = a\w&#123;3,&#125;?b, content: abaabaaabaaaab[1th] start: 0, end: 5, group: abaab[2th] start: 5, end: 14, group: aaabaaaab 说明 本例中代码展示的是使用不同贪婪或懒惰策略去查找字符串&quot;abaabaaabaaaab&quot; 中匹配以&quot;a&quot;开头，以&quot;b&quot;结尾的所有子字符串。 请从输出结果中，细细体味使用不同的贪婪或懒惰策略，对于匹配子字符串有什么影响。 最实用的正则 校验中文 **描述：**校验字符串中只能有中文字符（不包括中文标点符号）。中文字符的 Unicode 编码范围是\u4e00 到 \u9fa5。 如有兴趣，可以参考百度百科-Unicode 。 ^[\u4e00-\u9fa5]+$ 匹配： 春眠不觉晓 **不匹配：**春眠不觉晓， 校验身份证号码 **描述：**身份证为 15 位或 18 位。15 位是第一代身份证。从 1999 年 10 月 1 日起，全国实行公民身份证号码制度，居民身份证编号由原 15 位升至 18 位。 15 位身份证 **描述：**由 15 位数字组成。排列顺序从左至右依次为：六位数字地区码；六位数字出生日期；三位顺序号，其中 15 位男为单数，女为双数。 18 位身份证 **描述：**由十七位数字本体码和一位数字校验码组成。排列顺序从左至右依次为：六位数字地区码；八位数字出生日期；三位数字顺序码和一位数字校验码（也可能是 X）。 身份证号含义详情请见：百度百科-居民身份证号码 地区码（6 位） (1[1-5]|2[1-3]|3[1-7]|4[1-3]|5[0-4]|6[1-5])\d&#123;4&#125; 出生日期（8 位） 注：下面的是 18 位身份证的有效出生日期，如果是 15 位身份证，只要将第一个\d{4}改为\d{2}即可。 ((\d&#123;4&#125;((0[13578]|1[02])(0[1-9]|[12]\d|3[01])|(0[13456789]|1[012])(0[1-9]|[12]\d|30)|02(0[1-9]|1\d|2[0-8])))|([02468][048]|[13579][26])0229) 15 位有效身份证 ^((1[1-5]|2[1-3]|3[1-7]|4[1-3]|5[0-4]|6[1-5])\d&#123;4&#125;)((\d&#123;2&#125;((0[13578]|1[02])(0[1-9]|[12]\d|3[01])|(0[13456789]|1[012])(0[1-9]|[12]\d|30)|02(0[1-9]|1\d|2[0-8])))|([02468][048]|[13579][26])0229)(\d&#123;3&#125;)$ **匹配：**110001700101031 **不匹配：**110001701501031 18 位有效身份证 ^((1[1-5]|2[1-3]|3[1-7]|4[1-3]|5[0-4]|6[1-5])\d&#123;4&#125;)((\d&#123;4&#125;((0[13578]|1[02])(0[1-9]|[12]\d|3[01])|(0[13456789]|1[012])(0[1-9]|[12]\d|30)|02(0[1-9]|1\d|2[0-8])))|([02468][048]|[13579][26])0229)(\d&#123;3&#125;(\d|X))$ **匹配：**110001199001010310 | 11000019900101015X **不匹配：**990000199001010310 | 110001199013010310 校验有效用户名、密码 **描述：**长度为 6-18 个字符，允许输入字母、数字、下划线，首字符必须为字母。 ^[a-zA-Z]\w&#123;5,17&#125;$ **匹配：**he_llo@worl.d.com | hel.l-o@wor-ld.museum | h1ello@123.com **不匹配：**hello@worl_d.com | he&amp;llo@world.co1 | .hello@wor#.co.uk 校验邮箱 **描述：**不允许使用 IP 作为域名，如 : hello@154.145.68.12 @符号前的邮箱用户和.符号前的域名(domain)必须满足以下条件： 字符只能是英文字母、数字、下划线_、.、- ； 首字符必须为字母或数字； _、.、- 不能连续出现。 域名的根域只能为字母，且至少为两个字符。 ^[A-Za-z0-9](([_\.\-]?[a-zA-Z0-9]+)*)@([A-Za-z0-9]+)(([\.\-]?[a-zA-Z0-9]+)*)\.([A-Za-z]&#123;2,&#125;)$ **匹配：**he_llo@worl.d.com | hel.l-o@wor-ld.museum | h1ello@123.com **不匹配：**hello@worl_d.com | he&amp;llo@world.co1 | .hello@wor#.co.uk 校验 URL **描述：**校验 URL。支持 http、https、ftp、ftps。 ^(ht|f)(tp|tps)\://[a-zA-Z0-9\-\.]+\.([a-zA-Z]&#123;2,3&#125;)?(/\S*)?$ **匹配：**http://google.com/help/me | http://www.google.com/help/me/ | https://www.google.com/help.asp | ftp://www.google.com | ftps://google.org **不匹配：**http://un/www.google.com/index.asp 校验时间 **描述：**校验时间。时、分、秒必须是有效数字，如果数值不是两位数，十位需要补零。 ^([0-1][0-9]|[2][0-3]):([0-5][0-9])$ **匹配：**00:00:00 | 23:59:59 | 17:06:30 **不匹配：**17:6:30 | 24:16:30 校验日期 **描述：**校验日期。日期满足以下条件： 格式 yyyy-MM-dd 或 yyyy-M-d 连字符可以没有或是“-”、“/”、“.”之一 闰年的二月可以有 29 日；而平年不可以。 一、三、五、七、八、十、十二月为 31 日。四、六、九、十一月为 30 日。 ^(?:(?!0000)[0-9]&#123;4&#125;([-/.]?)(?:(?:0?[1-9]|1[0-2])\1(?:0?[1-9]|1[0-9]|2[0-8])|(?:0?[13-9]|1[0-2])\1(?:29|30)|(?:0?[13578]|1[02])\1(?:31))|(?:[0-9]&#123;2&#125;(?:0[48]|[2468][048]|[13579][26])|(?:0[48]|[2468][048]|[13579][26])00)([-/.]?)0?2\2(?:29))$ **匹配：**2016/1/1 | 2016/01/01 | 20160101 | 2016-01-01 | 2016.01.01 | 2000-02-29 **不匹配：**2001-02-29 | 2016/12/32 | 2016/6/31 | 2016/13/1 | 2016/0/1 校验中国手机号码 **描述：**中国手机号码正确格式：11 位数字。 移动有 16 个号段：134、135、136、137、138、139、147、150、151、152、157、158、159、182、187、188。其中 147、157、188 是 3G 号段，其他都是 2G 号段。联通有 7 种号段：130、131、132、155、156、185、186。其中 186 是 3G（WCDMA）号段，其余为 2G 号段。电信有 4 个号段：133、153、180、189。其中 189 是 3G 号段（CDMA2000），133 号段主要用作无线网卡号。总结：13 开头手机号 0-9；15 开头手机号 0-3、5-9；18 开头手机号 0、2、5-9。 此外，中国在国际上的区号为 86，所以手机号开头有+86、86 也是合法的。 以上信息来源于 百度百科-手机号 ^((\+)?86\s*)?((13[0-9])|(15([0-3]|[5-9]))|(18[0,2,5-9]))\d&#123;8&#125;$ 匹配：+86 18012345678 | 86 18012345678 | 15812345678 **不匹配：**15412345678 | 12912345678 | 180123456789 校验中国固话号码 **描述：**固话号码，必须加区号（以 0 开头）。 3 位有效区号：010、020~029，固话位数为 8 位。 4 位有效区号：03xx 开头到 09xx，固话位数为 7。 如果想了解更详细的信息，请参考 百度百科-电话区号 。 ^(010|02[0-9])(\s|-)\d&#123;8&#125;|(0[3-9]\d&#123;2&#125;)(\s|-)\d&#123;7&#125;$ **匹配：**010-12345678 | 010 12345678 | 0512-1234567 | 0512 1234567 **不匹配：**1234567 | 12345678 校验 IPv4 地址 **描述：**IP 地址是一个 32 位的二进制数，通常被分割为 4 个“8 位二进制数”（也就是 4 个字节）。IP 地址通常用“点分十进制”表示成（a.b.c.d）的形式，其中，a,b,c,d 都是 0~255 之间的十进制整数。 ^([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])\.([01]?\d\d?|2[0-4]\d|25[0-5])$ **匹配：**0.0.0.0 | 255.255.255.255 | 127.0.0.1 **不匹配：**10.10.10 | 10.10.10.256 校验 IPv6 地址 **描述：**IPv6 的 128 位地址通常写成 8 组，每组为四个十六进制数的形式。 IPv6 地址可以表示为以下形式： IPv6 地址 零压缩 IPv6 地址(section 2.2 of rfc5952) 带有本地链接区域索引的 IPv6 地址 (section 11 of rfc4007) 嵌入 IPv4 的 IPv6 地址(section 2 of rfc6052 映射 IPv4 的 IPv6 地址 (section 2.1 of rfc2765) 翻译 IPv4 的 IPv6 地址 (section 2.1 of rfc2765) 显然，IPv6 地址的表示方式很复杂。你也可以参考 百度百科-IPv6 Stack overflow 上的 IPv6 正则表达高票答案 (([0-9a-fA-F]&#123;1,4&#125;:)&#123;7,7&#125;[0-9a-fA-F]&#123;1,4&#125;|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,7&#125;:|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,6&#125;:[0-9a-fA-F]&#123;1,4&#125;|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,5&#125;(:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,2&#125;|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,4&#125;(:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,3&#125;|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,3&#125;(:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,4&#125;|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,2&#125;(:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,5&#125;|[0-9a-fA-F]&#123;1,4&#125;:((:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,6&#125;)|:((:[0-9a-fA-F]&#123;1,4&#125;)&#123;1,7&#125;|:)|fe80:(:[0-9a-fA-F]&#123;0,4&#125;)&#123;0,4&#125;%[0-9a-zA-Z]&#123;1,&#125;|::(ffff(:0&#123;1,4&#125;)&#123;0,1&#125;:)&#123;0,1&#125;((25[0-5]|(2[0-4]|1&#123;0,1&#125;[0-9])&#123;0,1&#125;[0-9])\.)&#123;3,3&#125;(25[0-5]|(2[0-4]|1&#123;0,1&#125;[0-9])&#123;0,1&#125;[0-9])|([0-9a-fA-F]&#123;1,4&#125;:)&#123;1,4&#125;:((25[0-5]|(2[0-4]|1&#123;0,1&#125;[0-9])&#123;0,1&#125;[0-9])\.)&#123;3,3&#125;(25[0-5]|(2[0-4]|1&#123;0,1&#125;[0-9])&#123;0,1&#125;[0-9])) **匹配：**1:2:3:4:5:6:7:8 | 1:: | 1::8 | 1::6:7:8 | 1::5:6:7:8 | 1::4:5:6:7:8 | 1::3:4:5:6:7:8 | ::2:3:4:5:6:7:8 | 1:2:3:4:5:6:7:: | 1:2:3:4:5:6::8 | 1:2:3:4:5::8 | 1:2:3:4::8 | 1:2:3::8 | 1:2::8 | 1::8 | ::8 | fe80::7:8%1 | ::255.255.255.255 | 2001:db8:3:4::192.0.2.33 | 64:ff9b::192.0.2.33 **不匹配：**1.2.3.4.5.6.7.8 | 1::2::3 特定字符 匹配长度为 3 的字符串：^.{3}$。 匹配由 26 个英文字母组成的字符串：^[A-Za-z]+$。 匹配由 26 个大写英文字母组成的字符串：^[A-Z]+$。 匹配由 26 个小写英文字母组成的字符串：^[a-z]+$。 匹配由数字和 26 个英文字母组成的字符串：^[A-Za-z0-9]+$。 匹配由数字、26 个英文字母或者下划线组成的字符串：^\w+$。 特定数字 匹配正整数：^[1-9]\d*$ 匹配负整数：^-[1-9]\d*$ 匹配整数：^(-?[1-9]\d*)|0$ 匹配正浮点数：^[1-9]\d*\.\d+|0\.\d+$ 匹配负浮点数：^-([1-9]\d*\.\d*|0\.\d*[1-9]\d*)$ 匹配浮点数：^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$ 速查元字符字典 为了方便快查正则的元字符含义，在本节根据元字符的功能集中罗列正则的各种元字符。 限定符 字符 描述 * 匹配前面的子表达式零次或多次。例如，zo* 能匹配 “z” 以及 “zoo”。* 等价于{0,}。 + 匹配前面的子表达式一次或多次。例如，‘zo+’ 能匹配 “zo” 以及 “zoo”，但不能匹配 “z”。+ 等价于 {1,}。 ? 匹配前面的子表达式零次或一次。例如，“do(es)?” 可以匹配 “do” 或 “does” 中的&quot;do&quot; 。? 等价于 {0,1}。 {n} n 是一个非负整数。匹配确定的 n 次。例如，‘o{2}’ 不能匹配 “Bob” 中的 ‘o’，但是能匹配 “food” 中的两个 o。 {n,} n 是一个非负整数。至少匹配 n 次。例如，‘o{2,}’ 不能匹配 “Bob” 中的 ‘o’，但能匹配 “foooood” 中的所有 o。‘o{1,}’ 等价于 ‘o+’。‘o{0,}’ 则等价于 ‘o*’。 {n,m} m 和 n 均为非负整数，其中 n &lt;= m。最少匹配 n 次且最多匹配 m 次。例如，“o{1,3}” 将匹配 “fooooood” 中的前三个 o。‘o{0,1}’ 等价于 ‘o?’。请注意在逗号和两个数之间不能有空格。 定位符 字符 描述 ^ 匹配输入字符串开始的位置。如果设置了 RegExp 对象的 Multiline 属性，^ 还会与 \n 或 \r 之后的位置匹配。 $ 匹配输入字符串结尾的位置。如果设置了 RegExp 对象的 Multiline 属性，$ 还会与 \n 或 \r 之前的位置匹配。 \b 匹配一个字边界，即字与空格间的位置。 \B 非字边界匹配。 非打印字符 字符 描述 \cx 匹配由 x 指明的控制字符。例如， \cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。 \f 匹配一个换页符。等价于 \x0c 和 \cL。 \n 匹配一个换行符。等价于 \x0a 和 \cJ。 \r 匹配一个回车符。等价于 \x0d 和 \cM。 \s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。 \S 匹配任何非空白字符。等价于 [ \f\n\r\t\v]。 \t 匹配一个制表符。等价于 \x09 和 \cI。 \v 匹配一个垂直制表符。等价于 \x0b 和 \cK。 分组 表达式 描述 (exp) 匹配的子表达式。()中的内容就是子表达式。 (?&lt;name&gt;exp) 命名的子表达式（反向引用）。 (?:exp) 非捕获组，表示当一个限定符应用到一个组，但组捕获的子字符串并非所需时，通常会使用非捕获组构造。 (?=exp) 匹配 exp 前面的位置。 (?&lt;=exp) 匹配 exp 后面的位置。 (?!exp) 匹配后面跟的不是 exp 的位置。 (?&lt;!exp) 匹配前面不是 exp 的位置。 特殊符号 字符 描述 \ 将下一个字符标记为或特殊字符、或原义字符、或向后引用、或八进制转义符。例如， ‘n’ 匹配字符 ‘n’。’\n’ 匹配换行符。序列 ‘\’ 匹配 “”，而 ‘(’ 则匹配 “(”。 \| 指明两项之间的一个选择。 [] 匹配方括号范围内的任意一个字符。形式如：[xyz]、[xyz]、[a-z]、[a-z]、[x,y,z] 参考资料 正则表达式 30 分钟入门教程 msdn 正则表达式教程 正则应用之——日期正则表达式 http://www.regexlib.com/]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 极简教程]]></title>
    <url>%2Fblog%2F2016%2F10%2F10%2Ftools%2Fnginx%2F</url>
    <content type="text"><![CDATA[Nginx 极简教程 本文是一个 Nginx 极简教程，目的在于帮助新手快速入门 Nginx。 我在 Github 上创建了一个 Nginx 教程项目： nginx-tutorial。教程中提供了一些常用场景的 Nginx 示例，示例可以通过脚本一键式启动，方便新手学习。 📓 本文已归档到：「blog」 概述 安装 Windows 安装 Linux 安装 Linux 开机自启动 使用 nginx 配置实战 http 反向代理配置 负载均衡配置 网站有多个 webapp 的配置 https 反向代理配置 静态站点配置 搭建文件服务器 跨域解决方案 参考资料 概述 什么是 Nginx? Nginx (engine x) 是一款轻量级的 Web 服务器 、反向代理服务器及电子邮件（IMAP/POP3）代理服务器。 什么是反向代理？ 反向代理（Reverse Proxy）方式是指以代理服务器来接受 internet 上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给 internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 安装 Windows 安装 （1）进入官方下载地址，选择合适版本（nginx/Windows-xxx）。 （2）解压到本地 （3）启动 下面以 C 盘根目录为例说明下： cd C:cd C:\nginx-0.8.54 start nginx 注：Nginx / Win32 是运行在一个控制台程序，而非 windows 服务方式的。服务器方式目前还是开发尝试中。 Linux 安装 rpm 包方式（推荐） （1）进入下载页面，选择合适版本下载。 $ wget http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm （2）安装 nginx rpm 包 nginx rpm 包实际上安装的是 nginx 的 yum 源。 $ rpm -ivh nginx-*.rpm （3）正式安装 rpm 包 $ yum install nginx （4）关闭防火墙 $ firewall-cmd --zone=public --add-port=80/tcp --permanent$ firewall-cmd --reload 源码编译方式 安装编译工具及库文件 Nginx 源码的编译依赖于 gcc 以及一些库文件，所以必须提前安装。 $ yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel Nginx 依赖 pcre 库，安装步骤如下： （1）下载解压到本地 进入pcre 官网下载页面，选择合适的版本下载。 我选择的是 8.35 版本： wget -O /opt/pcre/pcre-8.35.tar.gz http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gzcd /opt/pcretar zxvf pcre-8.35.tar.gz （2）编译安装 执行以下命令： cd /opt/pcre/pcre-8.35./configuremake &amp;&amp; make install （3）检验是否安装成功 执行 pcre-config --version 命令。 安装 Nginx 安装步骤如下： （1）下载解压到本地 进入官网下载地址：http://nginx.org/en/download.html ，选择合适的版本下载。 我选择的是 1.12.2 版本：http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz wget -O /opt/nginx/nginx-1.12.2.tar.gz http://nginx.org/download/nginx-1.12.2.tar.gzcd /opt/nginxtar zxvf nginx-1.12.2.tar.gz （2）编译安装 执行以下命令： cd /opt/nginx/nginx-1.12.2./configure --with-http_stub_status_module --with-http_ssl_module --with-pcre=/opt/pcre/pcre-8.35 （3）关闭防火墙 $ firewall-cmd --zone=public --add-port=80/tcp --permanent$ firewall-cmd --reload （4） 启动 Nginx 安装成功后，直接执行 nginx 命令即可启动 nginx。 Linux 开机自启动 Centos7 以上是用 Systemd 进行系统初始化的，Systemd 是 Linux 系统中最新的初始化系统（init），它主要的设计目标是克服 sysvinit 固有的缺点，提高系统的启动速度。Systemd 服务文件以 .service 结尾。 rpm 包方式 如果是通过 rpm 包安装的，会自动创建 nginx.service 文件。 直接用命令： $ systemctl enable nginx.service 设置开机启动即可。 源码编译方式 如果采用源码编译方式，需要手动创建 nginx.service 文件。 使用 nginx 的使用比较简单，就是几条命令。 常用到的命令如下： nginx -s stop 快速关闭Nginx，可能不保存相关信息，并迅速终止web服务。nginx -s quit 平稳关闭Nginx，保存相关信息，有安排的结束web服务。nginx -s reload 因改变了Nginx相关配置，需要重新加载配置而重载。nginx -s reopen 重新打开日志文件。nginx -c filename 为 Nginx 指定一个配置文件，来代替缺省的。nginx -t 不运行，而仅仅测试配置文件。nginx 将检查配置文件的语法的正确性，并尝试打开配置文件中所引用到的文件。nginx -v 显示 nginx 的版本。nginx -V 显示 nginx 的版本，编译器版本和配置参数。 如果不想每次都敲命令，可以在 nginx 安装目录下新添一个启动批处理文件startup.bat，双击即可运行。内容如下： @echo offrem 如果启动前已经启动nginx并记录下pid文件，会kill指定进程nginx.exe -s stoprem 测试配置文件语法正确性nginx.exe -t -c conf/nginx.confrem 显示版本信息nginx.exe -vrem 按照指定配置去启动nginxnginx.exe -c conf/nginx.conf 如果是运行在 Linux 下，写一个 shell 脚本，大同小异。 nginx 配置实战 我始终认为，各种开发工具的配置还是结合实战来讲述，会让人更易理解。 http 反向代理配置 我们先实现一个小目标：不考虑复杂的配置，仅仅是完成一个 http 反向代理。 nginx.conf 配置文件如下： 注：conf / nginx.conf 是 nginx 的默认配置文件。你也可以使用 nginx -c 指定你的配置文件 #运行用户#user somebody;#启动进程,通常设置成和cpu的数量相等worker_processes 1;#全局错误日志error_log D:/Tools/nginx-1.10.1/logs/error.log;error_log D:/Tools/nginx-1.10.1/logs/notice.log notice;error_log D:/Tools/nginx-1.10.1/logs/info.log info;#PID文件，记录当前启动的nginx的进程IDpid D:/Tools/nginx-1.10.1/logs/nginx.pid;#工作模式及连接数上限events &#123; worker_connections 1024; #单个后台worker process进程的最大并发链接数&#125;#设定http服务器，利用它的反向代理功能提供负载均衡支持http &#123; #设定mime类型(邮件支持类型),类型由mime.types文件定义 include D:/Tools/nginx-1.10.1/conf/mime.types; default_type application/octet-stream; #设定日志 log_format main '[$remote_addr] - [$remote_user] [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log D:/Tools/nginx-1.10.1/logs/access.log main; rewrite_log on; #sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件，对于普通应用， #必须设为 on,如果用来进行下载等应用磁盘IO重负载应用，可设置为 off，以平衡磁盘与网络I/O处理速度，降低系统的uptime. sendfile on; #tcp_nopush on; #连接超时时间 keepalive_timeout 120; tcp_nodelay on; #gzip压缩开关 #gzip on; #设定实际的服务器列表 upstream zp_server1&#123; server 127.0.0.1:8089; &#125; #HTTP服务器 server &#123; #监听80端口，80端口是知名端口号，用于HTTP协议 listen 80; #定义使用www.xx.com访问 server_name www.helloworld.com; #首页 index index.html #指向webapp的目录 root D:\01_Workspace\Project\github\zp\SpringNotes\spring-security\spring-shiro\src\main\webapp; #编码格式 charset utf-8; #代理配置参数 proxy_connect_timeout 180; proxy_send_timeout 180; proxy_read_timeout 180; proxy_set_header Host $host; proxy_set_header X-Forwarder-For $remote_addr; #反向代理的路径（和upstream绑定），location 后面设置映射的路径 location / &#123; proxy_pass http://zp_server1; &#125; #静态文件，nginx自己处理 location ~ ^/(images|javascript|js|css|flash|media|static)/ &#123; root D:\01_Workspace\Project\github\zp\SpringNotes\spring-security\spring-shiro\src\main\webapp\views; #过期30天，静态文件不怎么更新，过期可以设大一点，如果频繁更新，则可以设置得小一点。 expires 30d; &#125; #设定查看Nginx状态的地址 location /NginxStatus &#123; stub_status on; access_log on; auth_basic "NginxStatus"; auth_basic_user_file conf/htpasswd; &#125; #禁止访问 .htxxx 文件 location ~ /\.ht &#123; deny all; &#125; #错误处理页面（可选择性配置） #error_page 404 /404.html; #error_page 500 502 503 504 /50x.html; #location = /50x.html &#123; # root html; #&#125; &#125;&#125; 好了，让我们来试试吧： 启动 webapp，注意启动绑定的端口要和 nginx 中的 upstream 设置的端口保持一致。 更改 host：在 C:\Windows\System32\drivers\etc 目录下的 host 文件中添加一条 DNS 记录 127.0.0.1 www.helloworld.com 启动前文中 startup.bat 的命令 在浏览器中访问 www.helloworld.com，不出意外，已经可以访问了。 负载均衡配置 上一个例子中，代理仅仅指向一个服务器。 但是，网站在实际运营过程中，多半都是有多台服务器运行着同样的 app，这时需要使用负载均衡来分流。 nginx 也可以实现简单的负载均衡功能。 假设这样一个应用场景：将应用部署在 192.168.1.11:80、192.168.1.12:80、192.168.1.13:80 三台 linux 环境的服务器上。网站域名叫 www.helloworld.com，公网 IP 为 192.168.1.11。在公网 IP 所在的服务器上部署 nginx，对所有请求做负载均衡处理。 nginx.conf 配置如下： http &#123; #设定mime类型,类型由mime.type文件定义 include /etc/nginx/mime.types; default_type application/octet-stream; #设定日志格式 access_log /var/log/nginx/access.log; #设定负载均衡的服务器列表 upstream load_balance_server &#123; #weigth参数表示权值，权值越高被分配到的几率越大 server 192.168.1.11:80 weight=5; server 192.168.1.12:80 weight=1; server 192.168.1.13:80 weight=6; &#125; #HTTP服务器 server &#123; #侦听80端口 listen 80; #定义使用www.xx.com访问 server_name www.helloworld.com; #对所有请求进行负载均衡请求 location / &#123; root /root; #定义服务器的默认网站根目录位置 index index.html index.htm; #定义首页索引文件的名称 proxy_pass http://load_balance_server ;#请求转向load_balance_server 定义的服务器列表 #以下是一些反向代理的配置(可选择性配置) #proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $remote_addr; proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时) proxy_read_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传 client_max_body_size 10m; #允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数 &#125; &#125;&#125; 网站有多个 webapp 的配置 当一个网站功能越来越丰富时，往往需要将一些功能相对独立的模块剥离出来，独立维护。这样的话，通常，会有多个 webapp。 举个例子：假如 www.helloworld.com 站点有好几个 webapp，finance（金融）、product（产品）、admin（用户中心）。访问这些应用的方式通过上下文(context)来进行区分: www.helloworld.com/finance/ www.helloworld.com/product/ www.helloworld.com/admin/ 我们知道，http 的默认端口号是 80，如果在一台服务器上同时启动这 3 个 webapp 应用，都用 80 端口，肯定是不成的。所以，这三个应用需要分别绑定不同的端口号。 那么，问题来了，用户在实际访问 www.helloworld.com 站点时，访问不同 webapp，总不会还带着对应的端口号去访问吧。所以，你再次需要用到反向代理来做处理。 配置也不难，来看看怎么做吧： http &#123; #此处省略一些基本配置 upstream product_server&#123; server www.helloworld.com:8081; &#125; upstream admin_server&#123; server www.helloworld.com:8082; &#125; upstream finance_server&#123; server www.helloworld.com:8083; &#125; server &#123; #此处省略一些基本配置 #默认指向product的server location / &#123; proxy_pass http://product_server; &#125; location /product/&#123; proxy_pass http://product_server; &#125; location /admin/ &#123; proxy_pass http://admin_server; &#125; location /finance/ &#123; proxy_pass http://finance_server; &#125; &#125;&#125; https 反向代理配置 一些对安全性要求比较高的站点，可能会使用 HTTPS（一种使用 ssl 通信标准的安全 HTTP 协议）。 这里不科普 HTTP 协议和 SSL 标准。但是，使用 nginx 配置 https 需要知道几点： HTTPS 的固定端口号是 443，不同于 HTTP 的 80 端口 SSL 标准需要引入安全证书，所以在 nginx.conf 中你需要指定证书和它对应的 key 其他和 http 反向代理基本一样，只是在 Server 部分配置有些不同。 #HTTP服务器server &#123; #监听443端口。443为知名端口号，主要用于HTTPS协议 listen 443 ssl; #定义使用www.xx.com访问 server_name www.helloworld.com; #ssl证书文件位置(常见证书文件格式为：crt/pem) ssl_certificate cert.pem; #ssl证书key位置 ssl_certificate_key cert.key; #ssl配置参数（选择性配置） ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; #数字签名，此处使用MD5 ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / &#123; root /root; index index.html index.htm; &#125;&#125; 静态站点配置 有时候，我们需要配置静态站点(即 html 文件和一堆静态资源)。 举例来说：如果所有的静态资源都放在了 /app/dist 目录下，我们只需要在 nginx.conf 中指定首页以及这个站点的 host 即可。 配置如下： worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; gzip on; gzip_types text/plain application/x-javascript text/css application/xml text/javascript application/javascript image/jpeg image/gif image/png; gzip_vary on; server &#123; listen 80; server_name static.zp.cn; location / &#123; root /app/dist; index index.html; #转发任何请求到 index.html &#125; &#125;&#125; 然后，添加 HOST： 127.0.0.1 static.zp.cn 此时，在本地浏览器访问 static.zp.cn ，就可以访问静态站点了。 搭建文件服务器 有时候，团队需要归档一些数据或资料，那么文件服务器必不可少。使用 Nginx 可以非常快速便捷的搭建一个简易的文件服务。 Nginx 中的配置要点： 将 autoindex 开启可以显示目录，默认不开启。 将 autoindex_exact_size 开启可以显示文件的大小。 将 autoindex_localtime 开启可以显示文件的修改时间。 root 用来设置开放为文件服务的根路径。 charset 设置为 charset utf-8,gbk;，可以避免中文乱码问题（windows 服务器下设置后，依然乱码，本人暂时没有找到解决方法）。 一个最简化的配置如下： autoindex on;# 显示目录autoindex_exact_size on;# 显示文件大小autoindex_localtime on;# 显示文件时间server &#123; charset utf-8,gbk; # windows 服务器下设置后，依然乱码，暂时无解 listen 9050 default_server; listen [::]:9050 default_server; server_name _; root /share/fs;&#125; 跨域解决方案 web 领域开发中，经常采用前后端分离模式。这种模式下，前端和后端分别是独立的 web 应用程序，例如：后端是 Java 程序，前端是 React 或 Vue 应用。 各自独立的 web app 在互相访问时，势必存在跨域问题。解决跨域问题一般有两种思路： CORS 在后端服务器设置 HTTP 响应头，把你需要运行访问的域名加入加入 Access-Control-Allow-Origin 中。 jsonp 把后端根据请求，构造 json 数据，并返回，前端用 jsonp 跨域。 这两种思路，本文不展开讨论。 需要说明的是，nginx 根据第一种思路，也提供了一种解决跨域的解决方案。 举例：www.helloworld.com 网站是由一个前端 app ，一个后端 app 组成的。前端端口号为 9000， 后端端口号为 8080。 前端和后端如果使用 http 进行交互时，请求会被拒绝，因为存在跨域问题。来看看，nginx 是怎么解决的吧： 首先，在 enable-cors.conf 文件中设置 cors ： # allow origin listset $ACAO '*';# set single originif ($http_origin ~* (www.helloworld.com)$) &#123; set $ACAO $http_origin;&#125;if ($cors = "trueget") &#123; add_header 'Access-Control-Allow-Origin' "$http_origin"; add_header 'Access-Control-Allow-Credentials' 'true'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS'; add_header 'Access-Control-Allow-Headers' 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type';&#125;if ($request_method = 'OPTIONS') &#123; set $cors "$&#123;cors&#125;options";&#125;if ($request_method = 'GET') &#123; set $cors "$&#123;cors&#125;get";&#125;if ($request_method = 'POST') &#123; set $cors "$&#123;cors&#125;post";&#125; 接下来，在你的服务器中 include enable-cors.conf 来引入跨域配置： # ----------------------------------------------------# 此文件为项目 nginx 配置片段# 可以直接在 nginx config 中 include（推荐）# 或者 copy 到现有 nginx 中，自行配置# www.helloworld.com 域名需配合 dns hosts 进行配置# 其中，api 开启了 cors，需配合本目录下另一份配置文件# ----------------------------------------------------upstream front_server&#123; server www.helloworld.com:9000;&#125;upstream api_server&#123; server www.helloworld.com:8080;&#125;server &#123; listen 80; server_name www.helloworld.com; location ~ ^/api/ &#123; include enable-cors.conf; proxy_pass http://api_server; rewrite "^/api/(.*)$" /$1 break; &#125; location ~ ^/ &#123; proxy_pass http://front_server; &#125;&#125; 到此，就完成了。 参考资料 Nginx 的中文维基]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>load balance</tag>
        <tag>proxy</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 编码和加密]]></title>
    <url>%2Fblog%2F2016%2F07%2F20%2Fjava%2Fjavacore%2Fadvanced%2FJava%E7%BC%96%E7%A0%81%E5%92%8C%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[Java 编码和加密 📓 本文已归档到：「blog」 术语 Base64 编码 算法简述 算法实现 对称加密 算法简述 算法实现 非对称加密 算法简述 算法实现 消息摘要 算法简述 算法实现 数字签名 算法简述 算法实现 参考资料 术语 明文(Plaintext)：指待加密信息。明文可以是文本文件、图片文件、二进制数据等。 密文(Ciphertext)：指经过加密后的明文。密文通常以文本、二进制等形式存在。 加密(Encryption)：指将明文转换为密文的过程。 解密(Decryption)：指将密文转换为明文的过程。 加密密钥(Encryption Key)：指通过加密算法进行加密操作用的密钥。 解密密钥(Decryption Key)：指通过解密算法进行解密操作用的密钥。 信道(Channel)：通信的通道，是信号传输的媒介。 Base64 编码 算法简述 定义 Base64 内容传送编码是一种以任意 8 位字节序列组合的描述形式，这种形式不易被人直接识别。 Base64 是一种很常见的编码规范，其作用是将二进制序列转换为人类可读的 ASCII 字符序列，常用在需用通过文本协议（比如 HTTP 和 SMTP）来传输二进制数据的情况下。Base64 并不是加密解密算法，尽管我们有时也听到使用 Base64 来加密解密的说法，但这里所说的加密与解密实际是指**编码（encode）和解码（decode）**的过程，其变换是非常简单的，仅仅能够避免信息被直接识别。 原理 Base64 算法主要是将给定的字符以字符编码(如 ASCII 码，UTF-8 码)对应的十进制数为基准，做编码操作： 将给定的字符串以字符为单位，转换为对应的字符编码。 将获得字符编码转换为二进制 对二进制码做分组转换，每 3 个字节为一组，转换为每 4 个 6 位二进制位一组（不足 6 位时低位补 0）。这是一个分组变化的过程，3 个 8 位二进制码和 4 个 6 位二进制码的长度都是 24 位（38 = 46 = 24）。 对获得的 4-6 二进制码补位，向 6 位二进制码添加 2 位高位 0，组成 4 个 8 位二进制码。 对获得的 4-8 二进制码转换为十进制码。 将获得的十进制码转换为 Base64 字符表中对应的字符。 Base64 编码表 索引 对应字符 索引 对应字符 索引 对应字符 索引 对应字符 0 A 17 R 34 i 51 z 1 B 18 S 35 j 52 0 2 C 19 T 36 k 53 1 3 D 20 U 37 l 54 2 4 E 21 V 38 m 55 3 5 F 22 W 39 n 56 4 6 G 23 X 40 o 57 5 7 H 24 Y 41 p 58 6 8 I 25 Z 42 q 59 7 9 J 26 a 43 r 60 8 10 K 27 b 44 s 61 9 11 L 28 c 45 t 62 + 12 M 29 d 46 u 63 / 13 N 30 e 47 v 14 O 31 f 48 w 15 P 32 g 49 x 16 Q 33 h 50 y 应用 Base64 编码可用于在 HTTP 环境下传递较长的标识信息。在其他应用程序中，也常常需要把二进制数据编码为适合放在 URL(包括隐藏表单域)中的形式。此时，采用 Base64 编码具有不可读性，即所编码的数据不会被人用肉眼所直接看到，算是起到一个加密的作用。 然而，标准的 Base64 并不适合直接放在 URL 里传输，因为 URL 编码器会把标准 Base64 中的“/”和“+”字符变为形如“%XX”的形式，而这些“%”号在存入数据库时还需要再进行转换，因为 ANSI SQL 中已将“%”号用作通配符。 为解决此问题，可采用一种用于 URL 的改进 Base64 编码，它不仅在末尾填充’='号，并将标准 Base64 中的“+”和“/”分别改成了“-”和“_”，这样就免去了在 URL 编解码和数据库存储时所要作的转换，避免了编码信息长度在此过程中的增加，并统一了数据库、表单等处对象标识符的格式。 另有一种用于正则表达式的改进 Base64 变种，它将“+”和“/”改成了“!”和“-”，因为“+”,“*”以及前面在 IRCu 中用到的“[”和“]”在正则表达式中都可能具有特殊含义。 此外还有一些变种，它们将“+/”改为“-”或“.”（用作编程语言中的标识符名称）或“.-”（用于 XML 中的 Nmtoken）甚至“_:”（用于 XML 中的 Name）。 算法实现 commons-codec开源包提供了对于 Base64 的实现，推荐使用。 请在 maven 工程中添加依赖： &lt;dependency&gt; &lt;groupId&gt;commons-codec&lt;/groupId&gt; &lt;artifactId&gt;commons-codec&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt;&lt;/dependency&gt; 范例 注：在 commons-codec 包中的 Base64 这个类中提供了 Base64 的编码、解码方式。 其中，encodeBase64提供的是标准的 Base64 编码方式；encodeBase64URLSafe提供了 URL 安全的 Base64 编码方式（将+ 和 /替换为 - 和 _）。 package org.zp.javase.security.encrypt;import org.apache.commons.codec.binary.Base64;import java.io.UnsupportedEncodingException;public class Base64Demo &#123; public static void main(String[] args) throws UnsupportedEncodingException &#123; String url = "https://www.baidu.com/s?wd=Base64&amp;rsv_spt=1&amp;rsv_iqid=0xa9188d560005131f&amp;issp=1&amp;f=3&amp;rsv_bp=0&amp;rsv_idx=2&amp;ie=utf-8&amp;tn=baiduhome_pg&amp;rsv_enter=1&amp;rsv_sug3=1&amp;rsv_sug1=1&amp;rsv_sug7=001&amp;rsv_sug2=1&amp;rsp=0&amp;rsv_sug9=es_2_1&amp;rsv_sug4=2153&amp;rsv_sug=9"; // byte[] encoded = Base64.encodeBase64(url.getBytes("UTF8")); // 标准的Base64编码 byte[] encoded = Base64.encodeBase64URLSafe(url.getBytes("UTF8")); // URL安全的Base64编码 byte[] decoded = Base64.decodeBase64(encoded); System.out.println("url:" + url); System.out.println("encoded:" + new String(encoded)); System.out.println("decoded:" + new String(decoded)); &#125;&#125; 对称加密 算法简述 对称加密算法是应用较早的加密算法，技术成熟。在对称加密算法中，数据发信方将明文（原始数据）和加密密钥（mi yao）一起经过特殊加密算法处理后，使其变成复杂的加密密文发送出去。收信方收到密文后，若想解读原文，则需要使用加密用过的密钥及相同算法的逆算法对密文进行解密，才能使其恢复成可读明文。在对称加密算法中，使用的密钥只有一个，发收信双方都使用这个密钥对数据进行加密和解密，这就要求解密方事先必须知道加密密钥。 特点 优点： 计算量小、加密速度快、加密效率高。 缺点： 算法是公开的，安全性得不到保证。 通信双方每次使用对称加密算法时，都需要使用其他人不知道的惟一密钥，这会使得通信双方所拥有的密钥数量呈几何级数增长，密钥管理成为用户的负担。对称加密算法在分布式网络系统上使用较为困难，主要是因为密钥管理困难，使用成本较高。 而与公钥、密钥加密算法比起来，对称加密算法能够提供加密和认证却缺乏了签名功能，使得使用范围有所缩小。 原理 对称加密要求加密与解密使用同一个密钥，解密是加密的逆运算。由于加密、解密使用同一个密钥，这要求通信双方必须在通信前商定该密钥，并妥善保存该密钥。 对称加密体制分为两种： 一种是对明文的单个位（或字节）进行运算，称为流密码，也称为序列密码； 一种是把明文信息划分为不同的组（或块）结构，分别对每个组（或块）进行加密、解密，称为分组密码。 假设甲乙方作为通信双方。假定甲乙双方在消息传递前已商定加密算法，欲完成一次消息传递需要经过如下步骤。 工作模式 以 DES 算法的工作模式为例，DES 算法根据其加密算法所定义的明文分组的大小（56 位），将数据分割成若干 56 位的加密区块，再以加密区块为单位，分别进行加密处理。如果最后剩下不足一个区块的大小，称之为短块。短块的处理方法有填充法、流密码加密法、密文挪用技术。 根据数据加密时每个加密区块见得关联方式来区分，可以分为以下种工作模式： (1) 电子密码本模式(Electronic Code Book, ECB) 用途：适合加密密钥，随机数等短数据。例如，安全地传递 DES 密钥，ECB 是最合适的模式。 (2) 密文链接模式(Cipher Booki Chaining, CBC) 用途：可加密任意长度的数据，适用于计算产生检测数据完整性的消息认证 MAC。 (3) 密文反馈模式(Cipher Feed Back, CFB) 用途：因错误传播无界，可以用于检查发现明文密文的篡改。 (4) 输出反馈模式(Output Feed Back, OFB) 用途：使用于加密冗余性较大的数据，比如语音和图像数据。 AES 算法除了以上 4 中模式外，还有一种新的工作模式： (5) 计数器模式(Counter, CTR) 用途：适用于各种加密应用。 本文对于各种工作模式的原理展开描述。个人认为，作为工程应用，了解其用途即可。 填充方法 Java 中对称加密对于短块的处理，一般是采用填充方式。 常采用的是：NoPadding（不填充）、Zeros 填充（0 填充）、PKCS5Padding 填充。 ZerosPadding 方式：全部填充为 0 的字节 结果如下： F1 F2 F3 F4 F5 F6 F7 F8 //第一块 F9 00 00 00 00 00 00 00 //第二块 PKCS5Padding 方式：每个填充的字节都记录了填充的总字节数 结果如下： F1 F2 F3 F4 F5 F6 F7 F8 //第一块 F9 07 07 07 07 07 07 07 //第二块 常用算法 对称加密算法主要有 DES、3DES（TripleDES）、AES、IDEA、RC2、RC4、RC5 和 Blowfish 等。 算法实现 基于密钥加密的流程（DES、DESede、AES 和 IDEA） DES、DESede、AES 和 IDEA 等算法都是基于密钥加密的对称加密算法，它们的实现流程也基本一致。步骤如下： （1）生成密钥 KeyGenerator kg = KeyGenerator.getInstance("DES");SecureRandom random = new SecureRandom();kg.init(random);SecretKey secretKey = kg.generateKey(); 建议使用随机数来初始化密钥的生成。 （2）初始化密码对象 Cipher cipher = Cipher.getInstance("DES/ECB/PKCS5Padding");cipher.init(Cipher.ENCRYPT_MODE, secretKey); ENCRYPT_MODE：加密模式 DECRYPT_MODE：解密模式 （3）执行 String plaintext = "Hello World";byte[] ciphertext = cipher.doFinal(plaintext.getBytes()); 完整实例 一个完整的 DES 加密解密范例 import org.bouncycastle.util.encoders.Base64;import org.zp.javase.security.encode.Encode;import javax.crypto.BadPaddingException;import javax.crypto.Cipher;import javax.crypto.IllegalBlockSizeException;import javax.crypto.KeyGenerator;import javax.crypto.NoSuchPaddingException;import javax.crypto.spec.IvParameterSpec;import java.security.InvalidAlgorithmParameterException;import java.security.InvalidKeyException;import java.security.Key;import java.security.NoSuchAlgorithmException;import java.security.NoSuchProviderException;import java.security.SecureRandom;/** * @Title DESCoder * @Description DES安全编码：是经典的对称加密算法。密钥仅56位，且迭代次数偏少。已被视为并不安全的加密算法。 * @Author Victor Zhang * @Date 2016年7月14日 */public class DESCoder implements Encode &#123; public static final String KEY_ALGORITHM_DES = "DES"; public static final String CIPHER_DES_DEFAULT = "DES"; public static final String CIPHER_DES_ECB_PKCS5PADDING = "DES/ECB/PKCS5Padding"; // 算法/模式/补码方式 public static final String CIPHER_DES_CBC_PKCS5PADDING = "DES/CBC/PKCS5Padding"; public static final String CIPHER_DES_CBC_NOPADDING = "DES/CBC/NoPadding"; private static final String SEED = "%%%today is nice***"; // 用于生成随机数的种子 private Key key; private Cipher cipher; private String transformation; public DESCoder() throws NoSuchAlgorithmException, NoSuchPaddingException, NoSuchProviderException &#123; this.key = initKey(); this.cipher = Cipher.getInstance(CIPHER_DES_DEFAULT); this.transformation = CIPHER_DES_DEFAULT; &#125; public DESCoder(String transformation) throws NoSuchAlgorithmException, NoSuchPaddingException, NoSuchProviderException &#123; this.key = initKey(); this.cipher = Cipher.getInstance(transformation); this.transformation = transformation; &#125; /** * @Title decrypt * @Description 解密 * @Author Victor Zhang * @Date 2016年7月20日 * @param input 密文 * @return byte[] 明文 * @throws InvalidKeyException * @throws IllegalBlockSizeException * @throws BadPaddingException * @throws InvalidAlgorithmParameterException */ public byte[] decrypt(byte[] input) throws InvalidKeyException, IllegalBlockSizeException, BadPaddingException, InvalidAlgorithmParameterException &#123; if (transformation.equals(CIPHER_DES_CBC_PKCS5PADDING) || transformation.equals(CIPHER_DES_CBC_NOPADDING)) &#123; cipher.init(Cipher.DECRYPT_MODE, key, new IvParameterSpec(getIV())); &#125; else &#123; cipher.init(Cipher.DECRYPT_MODE, key); &#125; return cipher.doFinal(input); &#125; /** * @Title encrypt * @Description 加密 * @Author Victor Zhang * @Date 2016年7月20日 * @param input 明文 * @return byte[] 密文 * @throws InvalidKeyException * @throws IllegalBlockSizeException * @throws BadPaddingException * @throws InvalidAlgorithmParameterException */ public byte[] encrypt(byte[] input) throws InvalidKeyException, IllegalBlockSizeException, BadPaddingException, InvalidAlgorithmParameterException &#123; if (transformation.equals(CIPHER_DES_CBC_PKCS5PADDING) || transformation.equals(CIPHER_DES_CBC_NOPADDING)) &#123; cipher.init(Cipher.ENCRYPT_MODE, key, new IvParameterSpec(getIV())); &#125; else &#123; cipher.init(Cipher.ENCRYPT_MODE, key); &#125; return cipher.doFinal(input); &#125; /** * @Title initKey * @Description 根据随机数种子生成一个密钥 * @Author Victor Zhang * @Date 2016年7月14日 * @Return Key * @throws NoSuchAlgorithmException * @throws NoSuchProviderException */ private Key initKey() throws NoSuchAlgorithmException, NoSuchProviderException &#123; // 根据种子生成一个安全的随机数 SecureRandom secureRandom = null; secureRandom = new SecureRandom(SEED.getBytes()); KeyGenerator keyGen = KeyGenerator.getInstance(KEY_ALGORITHM_DES); keyGen.init(secureRandom); return keyGen.generateKey(); &#125; private byte[] getIV() &#123; String iv = "01234567"; // IV length: must be 8 bytes long return iv.getBytes(); &#125; public static void main(String[] args) throws Exception &#123; DESCoder aes = new DESCoder(CIPHER_DES_CBC_PKCS5PADDING); String msg = "Hello World!"; System.out.println("原文: " + msg); byte[] encoded = aes.encrypt(msg.getBytes("UTF8")); String encodedBase64 = Base64.toBase64String(encoded); System.out.println("密文: " + encodedBase64); byte[] decodedBase64 = Base64.decode(encodedBase64); byte[] decoded = aes.decrypt(decodedBase64); System.out.println("明文: " + new String(decoded)); &#125;&#125; 输出 原文: Hello World!密文: TtnEu9ezNQtxFKpmq/37Qw==明文: Hello World! 基于口令加密的流程（PBE） DES、DESede、AES、IDEA 这几种算法的应用模型几乎如出一辙。 但是，并非所有对称加密算法都是如此。 基于口令加密(Password Based Encryption, PBE)是一种基于口令加密的算法。其特点是：口令由用户自己掌管，采用随机数（这里叫做盐）杂凑多重加密等方法保证数据的安全性。 PBE 没有密钥概念，密钥在其他对称加密算法中是经过计算得出的，PBE 则使用口令替代了密钥。 流程： 步骤如下： （1）产生盐 SecureRandom secureRandom = new SecureRandom();byte[] salt = secureRandom.generateSeed(8); // 盐长度必须为8字节 （2）根据密码产生 Key String password = "123456";PBEKeySpec keySpec = new PBEKeySpec(password.toCharArray());SecretKeyFactory keyFactory = SecretKeyFactory.getInstance(KEY_ALGORITHM);SecretKey secretKey = keyFactory.generateSecret(keySpec); （3）初始化加密或解密对象 PBEParameterSpec paramSpec = new PBEParameterSpec(salt, ITERATION_COUNT);Cipher cipher = Cipher.getInstance(KEY_ALGORITHM);cipher.init(Cipher.ENCRYPT_MODE, secretKey, paramSpec); （4）执行 byte[] plaintext = "Hello World".getBytes();byte[] ciphertext = cipher.doFinal(plaintext); 非对称加密 算法简述 非对称加密算法和对称加密算法的主要差别在于非对称加密算法用于加密和解密的密钥是不同的。一个公开，称为公钥（public key）；一个保密，称为私钥（private key）。因此，非对称加密算法也称为双钥加密算法或公钥加密算法。 特点 优点 非对称加密算法解决了对称加密算法的密钥分配问题，并极大地提高了算法安全性。 缺点 算法比对称算法更复杂，因此加密、解密速度都比对称算法慢很多。 原理 非对称加密算法实现机密信息交换的基本过程是：甲方生成一对密钥并将其中的一把作为公用密钥向其它方公开；得到该公用密钥的乙方使用该密钥对机密信息进行加密后再发送给甲方；甲方再用自己保存的另一把专用密钥对加密后的信息进行解密。 另一方面，甲方可以使用乙方的公钥对机密信息进行签名后再发送给乙方；乙方再用自己的私匙对数据进行验证。 甲方只能用其私钥解密，由其公钥加密后的任何信息。 非对称加密算法的保密性比较好，它消除了最终用户交换密钥的需要。 常用算法 DH(Diffie-Hellman，密钥交换算法)、RSA 算法实现 完整范例 import org.apache.commons.codec.binary.Base64;import javax.crypto.Cipher;import java.security.KeyFactory;import java.security.KeyPair;import java.security.KeyPairGenerator;import java.security.PrivateKey;import java.security.PublicKey;import java.security.spec.PKCS8EncodedKeySpec;import java.security.spec.X509EncodedKeySpec;public class RSACoder &#123; private final static String KEY_ALGORITHM = "RSA"; private KeyPair keyPair; public RSACoder() throws Exception &#123; this.keyPair = initKeyPair(); &#125; public byte[] encryptByPublicKey(byte[] plaintext, byte[] key) throws Exception &#123; X509EncodedKeySpec keySpec = new X509EncodedKeySpec(key); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); PublicKey publicKey = keyFactory.generatePublic(keySpec); Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm()); cipher.init(Cipher.ENCRYPT_MODE, publicKey); return cipher.doFinal(plaintext); &#125; public byte[] encryptByPrivateKey(byte[] plaintext, byte[] key) throws Exception &#123; PKCS8EncodedKeySpec keySpec = new PKCS8EncodedKeySpec(key); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); PrivateKey privateKey = keyFactory.generatePrivate(keySpec); Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm()); cipher.init(Cipher.ENCRYPT_MODE, privateKey); return cipher.doFinal(plaintext); &#125; public byte[] decryptByPublicKey(byte[] ciphertext, byte[] key) throws Exception &#123; X509EncodedKeySpec keySpec = new X509EncodedKeySpec(key); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); PublicKey publicKey = keyFactory.generatePublic(keySpec); Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm()); cipher.init(Cipher.DECRYPT_MODE, publicKey); return cipher.doFinal(ciphertext); &#125; public byte[] decryptByPrivateKey(byte[] ciphertext, byte[] key) throws Exception &#123; PKCS8EncodedKeySpec keySpec = new PKCS8EncodedKeySpec(key); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); PrivateKey privateKey = keyFactory.generatePrivate(keySpec); Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm()); cipher.init(Cipher.DECRYPT_MODE, privateKey); return cipher.doFinal(ciphertext); &#125; private KeyPair initKeyPair() throws Exception &#123; // KeyPairGenerator类用于生成公钥和私钥对，基于RSA算法生成对象 KeyPairGenerator keyPairGen = KeyPairGenerator.getInstance(KEY_ALGORITHM); // 初始化密钥对生成器，密钥大小为1024位 keyPairGen.initialize(1024); // 生成一个密钥对 return keyPairGen.genKeyPair(); &#125; public static void main(String[] args) throws Exception &#123; String msg = "Hello World!"; RSACoder coder = new RSACoder(); // 私钥加密，公钥解密 byte[] ciphertext = coder.encryptByPrivateKey(msg.getBytes("UTF8"), coder.keyPair.getPrivate().getEncoded()); byte[] plaintext = coder.decryptByPublicKey(ciphertext, coder.keyPair.getPublic().getEncoded()); // 公钥加密，私钥解密 byte[] ciphertext2 = coder.encryptByPublicKey(msg.getBytes(), coder.keyPair.getPublic().getEncoded()); byte[] plaintext2 = coder.decryptByPrivateKey(ciphertext2, coder.keyPair.getPrivate().getEncoded()); System.out.println("原文：" + msg); System.out.println("公钥：" + Base64.encodeBase64URLSafeString(coder.keyPair.getPublic().getEncoded())); System.out.println("私钥：" + Base64.encodeBase64URLSafeString(coder.keyPair.getPrivate().getEncoded())); System.out.println("============== 私钥加密，公钥解密 =============="); System.out.println("密文：" + Base64.encodeBase64URLSafeString(ciphertext)); System.out.println("明文：" + new String(plaintext)); System.out.println("============== 公钥加密，私钥解密 =============="); System.out.println("密文：" + Base64.encodeBase64URLSafeString(ciphertext2)); System.out.println("明文：" + new String(plaintext2)); &#125;&#125; 输出 原文：Hello World!公钥：MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCVN2mWAMdatpo2l8dwavaX2VC8mRleVTdjwjyahsyCE6UxkdqHsKD6Ecq3OBbuJhEfHxnr7MAD_zoE6zalFs7_si09XTgpVFsFCztPXJpPw-rpQdvaaxYEXJHkY07M_DBrxh1URg2gQl9dEDaruIFrZ12ugTwwEkLA1K_LN7yZrwIDAQAB私钥：MIICdQIBADANBgkqhkiG9w0BAQEFAASCAl8wggJbAgEAAoGBAJU3aZYAx1q2mjaXx3Bq9pfZULyZGV5VN2PCPJqGzIITpTGR2oewoPoRyrc4Fu4mER8fGevswAP_OgTrNqUWzv-yLT1dOClUWwULO09cmk_D6ulB29prFgRckeRjTsz8MGvGHVRGDaBCX10QNqu4gWtnXa6BPDASQsDUr8s3vJmvAgMBAAECgYBvU1M8LcKOJFQzzNNoRPVLX0AEJXkuzwcvL1hFtbJYjc2eiQHwYFAJokKKpZc-ADqf7HVLdmvfz4h66P3w925hYHjSF3cs6jiibI7fc9lrdrJLMpv44phPlRCiIanD-U6pyN3bZxRl4Giuz5uGL0SVU6Dxh2Sw7mtnvUBbHCyyaQJBAOixpR-t81Qnpdy4jlbZL8ufTTF1TzlSh0NEDB4tlpHlVolVmZB2M-rdJ3nP8fJXazdsGZMP0q38vgiN2HHMtxsCQQCkKWAaA6KxKNbj0mJDSP1p4qUJ4EAcgXBz4B_PKMZa3ZU2CdmFlhlLRRTOIjZX2VC6IjHKWssa-3V2EqBzCSz9AkBsiK9kH1anofaTBOIfUB4i86KltvnE2uGMVmjwioL4eefrFqoR35BHs-99uag4SN4Rc7JaDb9Ot9xLUR3rtniRAkB8dFXEQx9Teie4HmaapjpgzQ_b9eQE-GjdoHvdHQeMGdMmXb9IVGwmsV-9ixhx73IROx1OURkMArmhYyu7KqitAkBkeQ-7AYOIROJnTUSQTMUELUmZFF1Io_SJGXyRYLgDqz7JCmmhfH7sNm8Gcn6f2VWg-U2D9-G5IHO-vHfz2DS6============== 私钥加密，公钥解密 ==============密文：U2otXypy1Fg4wcXK187xAuOxWM88oORVDJfaNxvG74Q_rqZ-sT4fEZYLZO80KmsWiufkJbD9Gskgkg7dRPRCwG90pRaU3PD9_sTmksN0v8MUwCX2p80zUeG3gWU6BJwMMUZrltJaHFbKn-BhzoNrn3Q-4BJA8lt6-cKtH0TPeN4明文：Hello World!============== 公钥加密，私钥解密 ==============密文：O_rknvo12qaFfWieyTI_Ay8_ph49y3V4jJVs1BykpI81GM3ozCPSnOjHbtdWdjPtgJHFfCjbspAnIT2eM4PtJldIJg6k_2HZCmCCaheUj2pxcvkrhb6GdhSlH-K2FhFGAnlxUAp-3tZpYpxzAteEw1-suldelHdikrCV_uXxAEM明文：Hello World! 消息摘要 算法简述 定义 它是一个唯一对应一个消息或文本的固定长度的值，它由一个单向 Hash 加密函数对消息进行作用而产生。如果消息在途中改变了，则接收者通过对收到消息的新产生的摘要与原摘要比较，就可知道消息是否被改变了。因此消息摘要保证了消息的完整性。消息摘要采用单向 Hash 函数将需加密的明文&quot;摘要&quot;成一串密文，这一串密文亦称为数字指纹(Finger Print)。它有固定的长度，且不同的明文摘要成密文，其结果总是不同的，而同样的明文其摘要必定一致。这样这串摘要便可成为验证明文是否是&quot;真身&quot;的&quot;指纹&quot;了。 特点 消息摘要具有以下特点： 唯一性：数据只要有一点改变，那么再通过消息摘要算法得到的摘要也会发生变化。虽然理论上有可能会发生碰撞，但是概率极其低。 不可逆：消息摘要算法的密文无法被解密。 不需要密钥，可使用于分布式网络。 无论输入的明文有多长，计算出来的消息摘要的长度总是固定的。 原理 消息摘要，其实就是将需要摘要的数据作为参数，经过哈希函数(Hash)的计算，得到的散列值。 常用算法 消息摘要算法包括MD(Message Digest，消息摘要算法)、SHA(Secure Hash Algorithm，安全散列算法)、**MAC(Message AuthenticationCode，消息认证码算法)**共 3 大系列，常用于验证数据的完整性，是数字签名算法的核心算法。 MD5和SHA1分别是MD、SHA算法系列中最有代表性的算法。 如今，MD5 已被发现有许多漏洞，从而不再安全。SHA 算法比 MD 算法的摘要长度更长，也更加安全。 算法实现 MD5、SHA 的范例 JDK 中使用 MD5 和 SHA 这两种消息摘要的方式基本一致，步骤如下： 初始化 MessageDigest 对象 更新要计算的内容 生成摘要 范例 importjava.io.UnsupportedEncodingException;import java.security.MessageDigest;import java.security.NoSuchAlgorithmException;import org.apache.commons.codec.binary.Base64;public class MsgDigestDemo&#123; public static void main(String args[]) throws NoSuchAlgorithmException, UnsupportedEncodingException &#123; String msg = "Hello World!"; MessageDigest md5Digest = MessageDigest.getInstance("MD5"); // 更新要计算的内容 md5Digest.update(msg.getBytes()); // 完成哈希计算，得到摘要 byte[] md5Encoded = md5Digest.digest(); MessageDigest shaDigest = MessageDigest.getInstance("SHA"); // 更新要计算的内容 shaDigest.update(msg.getBytes()); // 完成哈希计算，得到摘要 byte[] shaEncoded = shaDigest.digest(); System.out.println("原文: " + msg); System.out.println("MD5摘要: " + Base64.encodeBase64URLSafeString(md5Encoded)); System.out.println("SHA摘要: " + Base64.encodeBase64URLSafeString(shaEncoded)); &#125;&#125; 输出 原文:Hello World!MD5摘要: 7Qdih1MuhjZehB6Sv8UNjASHA摘要:Lve95gjOVATpfV8EL5X4nxwjKHE HMAC 的范例 importjavax.crypto.Mac;import javax.crypto.spec.SecretKeySpec;import org.apache.commons.codec.binary.Base64;public class HmacCoder&#123; /** * JDK支持HmacMD5, HmacSHA1,HmacSHA256, HmacSHA384, HmacSHA512 */ public enum HmacTypeEn &#123; HmacMD5, HmacSHA1, HmacSHA256, HmacSHA384, HmacSHA512; &#125; public static byte[] encode(byte[] plaintext, byte[] secretKey, HmacTypeEn type) throwsException &#123; SecretKeySpec keySpec = new SecretKeySpec(secretKey, type.name()); Mac mac = Mac.getInstance(keySpec.getAlgorithm()); mac.init(keySpec); return mac.doFinal(plaintext); &#125; public static void main(String[] args) throws Exception &#123; String msg = "Hello World!"; byte[] secretKey = "Secret_Key".getBytes("UTF8"); byte[] digest = HmacCoder.encode(msg.getBytes(), secretKey, HmacTypeEn.HmacSHA256); System.out.println("原文: " + msg); System.out.println("摘要: " + Base64.encodeBase64URLSafeString(digest)); &#125;&#125; 输出 原文:Hello World!摘要: b8-eUifaOJ5OUFweOoq08HbGAMsIpC3Nt-Yv-S91Yr4 数字签名 算法简述 数字签名算法可以看做是一种带有密钥的消息摘要算法，并且这种密钥包含了公钥和私钥。也就是说，数字签名算法是非对称加密算法和消息摘要算法的结合体。 特点 数字签名算法要求能够验证数据完整性、认证数据来源，并起到抗否认的作用。 原理 数字签名算法包含签名和验证两项操作，遵循私钥签名，公钥验证的方式。 签名时要使用私钥和待签名数据，验证时则需要公钥、签名值和待签名数据，其核心算法主要是消息摘要算法。 常用算法 RSA、DSA、ECDSA 算法实现 DSA 的范例 数字签名有两个流程：签名和验证。 它们的前提都是要有一个公钥、密钥对。 签名 用私钥为消息计算签名 范例 用公钥验证摘要 importjava.security.KeyFactory;import java.security.KeyPair;import java.security.KeyPairGenerator;import java.security.PrivateKey;import java.security.PublicKey;import java.security.Signature;import java.security.spec.PKCS8EncodedKeySpec;import java.security.spec.X509EncodedKeySpec;import org.apache.commons.codec.binary.Base64;public class DsaCoder&#123; public static final String KEY_ALGORITHM = "DSA"; public enum DsaTypeEn &#123; MD5withDSA, SHA1withDSA &#125; /** * DSA密钥长度默认1024位。 密钥长度必须是64的整数倍，范围在512~1024之间 */ private static final int KEY_SIZE = 1024; private KeyPair keyPair; public DsaCoder() throws Exception &#123; keyPair = initKey(); &#125; public byte[] signature(byte[] data, byte[] privateKey) throws Exception &#123; PKCS8EncodedKeySpec keySpec = new PKCS8EncodedKeySpec(privateKey); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); PrivateKey key =keyFactory.generatePrivate(keySpec); Signature signature = Signature.getInstance(DsaTypeEn.SHA1withDSA.name()); signature.initSign(key); signature.update(data); return signature.sign(); &#125; public boolean verify(byte[] data, byte[] publicKey, byte[] sign) throws Exception &#123; X509EncodedKeySpec keySpec = new X509EncodedKeySpec(publicKey); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); PublicKey key =keyFactory.generatePublic(keySpec); Signature signature = Signature.getInstance(DsaTypeEn.SHA1withDSA.name()); signature.initVerify(key); signature.update(data); return signature.verify(sign); &#125; private KeyPair initKey() throws Exception &#123; // 初始化密钥对生成器 KeyPairGenerator keyPairGen = KeyPairGenerator.getInstance(KEY_ALGORITHM); // 实例化密钥对生成器 keyPairGen.initialize(KEY_SIZE); // 实例化密钥对 return keyPairGen.genKeyPair(); &#125; public byte[] getPublicKey() &#123; return keyPair.getPublic().getEncoded(); &#125; public byte[] getPrivateKey() &#123; return keyPair.getPrivate().getEncoded(); &#125; public static void main(String[] args) throws Exception &#123; String msg = "Hello World"; DsaCoder dsa = new DsaCoder(); byte[] sign = dsa.signature(msg.getBytes(), dsa.getPrivateKey()); boolean flag = dsa.verify(msg.getBytes(), dsa.getPublicKey(), sign); String result = flag ? "数字签名匹配" : "数字签名不匹配"; System.out.println("数字签名：" + Base64.encodeBase64URLSafeString(sign)); System.out.println("验证结果：" + result); &#125;&#125; 参考资料 《Core Java Volume2》 《Java 加密与解密技术》]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>advanced</tag>
        <tag>encrypt</tag>
        <tag>encode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 教程之入门指南]]></title>
    <url>%2Fblog%2F2016%2F06%2F16%2Fjava%2Fjavatool%2Fbuild%2Fmaven%2Fmaven-quickstart%2F</url>
    <content type="text"><![CDATA[Maven 教程之入门指南 📓 本文已归档到：「blog」 简介 Maven 是什么 Maven 的生命周期 Maven 的标准工程结构 Maven 的&quot;约定优于配置&quot; Maven 的版本规范 安装 配置环境变量 本地仓储配置 第一个 Maven 工程 在 Intellij 中创建 Maven 工程 在 Eclipse 中创建 Maven 工程 使用指导 如何添加外部依赖 jar 包 如何寻找 jar 包 如何使用 Maven 插件(Plugin) 如何一次编译多个工程 常用 Maven 插件 常用 Maven 命令 常见问题 dependencies 和 dependencyManagement，plugins 和 pluginManagement 有什么区别？ IDEA 修改 JDK 版本后编译报错 重复引入依赖 如何打包一个可以直接运行的 Spring Boot jar 包 最佳实践 通过 bom 统一管理版本 引用和引申 简介 Maven 是什么 Maven 是一个项目管理工具。它负责管理项目开发过程中的几乎所有的东西。 版本 - maven 有自己的版本定义和规则。 构建 - maven 支持许多种的应用程序类型，对于每一种支持的应用程序类型都定义好了一组构建规则和工具集。 输出物管理 - maven 可以管理项目构建的产物，并将其加入到用户库中。这个功能可以用于项目组和其他部门之间的交付行为。 依赖关系 - maven 对依赖关系的特性进行细致的分析和划分，避免开发过程中的依赖混乱和相互污染行为 文档和构建结果 - maven 的 site 命令支持各种文档信息的发布，包括构建过程的各种输出，javadoc，产品文档等。 项目关系 - 一个大型的项目通常有几个小项目或者模块组成，用 maven 可以很方便地管理。 移植性管理 - maven 可以针对不同的开发场景，输出不同种类的输出结果。 Maven 的生命周期 maven 把项目的构建划分为不同的生命周期(lifecycle)。粗略一点的话，它这个过程(phase)包括：编译、测试、打包、集成测试、验证、部署。maven 中所有的执行动作(goal)都需要指明自己在这个过程中的执行位置，然后 maven 执行的时候，就依照过程的发展依次调用这些 goal 进行各种处理。 这个也是 maven 的一个基本调度机制。一般来说，位置稍后的过程都会依赖于之前的过程。当然，maven 同样提供了配置文件，可以依照用户要求，跳过某些阶段。 Maven 的标准工程结构 Maven 的标准工程结构如下： |-- pom.xml(maven的核心配置文件)|-- src|-- main |-- java(java源代码目录) |-- resources(资源文件目录)|-- test |-- java(单元测试代码目录)|-- target(输出目录，所有的输出物都存放在这个目录下) |-- classes(编译后的class文件存放处) Maven 的&quot;约定优于配置&quot; 所谓的&quot;约定优于配置&quot;，在 maven 中并不是完全不可以修改的，他们只是一些配置的默认值而已。但是除非必要，并不需要去修改那些约定内容。maven 默认的文件存放结构如下： 每一个阶段的任务都知道怎么正确完成自己的工作，比如 compile 任务就知道从 src/main/java 下编译所有的 java 文件，并把它的输出 class 文件存放到 target/classes 中。 对 maven 来说，采用&quot;约定优于配置&quot;的策略可以减少修改配置的工作量，也可以降低学习成本，更重要的是，给项目引入了统一的规范。 Maven 的版本规范 maven 使用如下几个要素来唯一定位某一个输出物： groupId - 团体、组织的标识符。团体标识的约定是，它以创建这个项目的组织名称的逆向域名(reverse domain name)开头。一般对应着 JAVA 的包的结构。例如 org.apache artifactId - 单独项目的唯一标识符。比如我们的 tomcat, commons 等。不要在 artifactId 中包含点号(.)。 version - 一个项目的特定版本。 packaging - 项目的类型，默认是 jar，描述了项目打包后的输出。类型为 jar 的项目产生一个 JAR 文件，类型为 war 的项目产生一个 web 应用。 maven 有自己的版本规范，一般是如下定义 &lt;major version&gt;、&lt;minor version&gt;、&lt;incremental version&gt;-&lt;qualifier&gt; ，比如 1.2.3-beta-01。要说明的是，maven 自己判断版本的算法是 major,minor,incremental 部分用数字比 较，qualifier 部分用字符串比较，所以要小心 alpha-2 和 alpha-15 的比较关系，最好用 alpha-02 的格式。 maven 在版本管理时候可以使用几个特殊的字符串 SNAPSHOT，LATEST，RELEASE。比如&quot;1.0-SNAPSHOT&quot;。各个部分的含义和处理逻辑如下说明： SNAPSHOT - 这个版本一般用于开发过程中，表示不稳定的版本。 LATEST - 指某个特定构件的最新发布，这个发布可能是一个发布版，也可能是一个 snapshot 版，具体看哪个时间最后。 RELEASE - 指最后一个发布版。 安装 官网下载地址 Linux 环境安装可以使用我写一键安装脚本：https://github.com/dunwu/os-tutorial/tree/master/codes/linux/ops/service/maven 配置环境变量 注意：安装 maven 之前，必须先确保你的机器中已经安装了 JDK。 （1）解压压缩包（以 apache-maven-3.3.9-bin.zip 为例） （2）添加环境变量 MAVEN_HOME，值为 apache-maven-3.3.9 的安装路径 （3）在 Path 环境变量的变量值末尾添加%MAVEN_HOME%\bin （4）在 cmd 输入 mvn –version，如果出现 maven 的版本信息，说明配置成功。 本地仓储配置 从中央仓库下载的 jar 包，都会统一存放到本地仓库中。我们需要配置本地仓库的位置。 打开 maven 安装目录，打开 conf 目录下的 setting.xml 文件。 可以参照下图配置本地仓储位置。 第一个 Maven 工程 在 Intellij 中创建 Maven 工程 （1）创建 Maven 工程 依次点击 File -&gt; New -&gt; Project 打开创建工程对话框，选择 Maven 工程。 （2）输入项目信息 （3）点击 Intellij 侧边栏中的 Maven 工具界面，有几个可以直接使用的 maven 命令，可以帮助你进行构建。 在 Eclipse 中创建 Maven 工程 （1）Maven 插件 在 Eclipse 中创建 Maven 工程，需要安装 Maven 插件。 一般较新版本的 Eclipse 都会带有 Maven 插件，如果你的 Eclipse 中已经有 Maven 插件，可以跳过这一步骤。 点击 Help -&gt; Eclipse Marketplace，搜索 maven 关键字，选择安装红框对应的 Maven 插件。 （2）Maven 环境配置 点击 Window -&gt; Preferences 如下图所示，配置 settings.xml 文件的位置 （3）创建 Maven 工程 File -&gt; New -&gt; Maven Project -&gt; Next，在接下来的窗口中会看到一大堆的项目模板，选择合适的模板。 接下来设置项目的参数，如下： groupId是项目组织唯一的标识符，实际对应 JAVA 的包的结构，是 main 目录里 java 的目录结构。 artifactId就是项目的唯一的标识符，实际对应项目的名称，就是项目根目录的名称。 点击 Finish，Eclipse 会创建一个 Maven 工程。 （4）使用 Maven 进行构建 Eclipse 中构建方式 在 Elipse 项目上右击 -&gt; Run As 就能看到很多 Maven 操作。这些操作和 maven 命令是等效的。例如 Maven clean，等同于 mvn clean 命令。 你也可以点击 Maven build，输入组合命令，并保存下来。如下图： Maven 命令构建方式 当然，你也可以直接使用 maven 命令进行构建。 进入工程所在目录，输入 maven 命令就可以了。 使用指导 如何添加外部依赖 jar 包 在 Maven 工程中添加依赖 jar 包，很简单，只要在 POM 文件中引入对应的&lt;dependency&gt;标签即可。 参考下例： &lt;project xmlns="http://maven.apache.org/POM/4.0.0"xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zp.maven&lt;/groupId&gt; &lt;artifactId&gt;MavenDemo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;MavenDemo&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;junit.version&gt;3.8.1&lt;/junit.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; &lt;dependency&gt; 标签最常用的四个属性标签： &lt;groupId&gt; - 项目组织唯一的标识符，实际对应 JAVA 的包的结构。 &lt;artifactId&gt; - 项目唯一的标识符，实际对应项目的名称，就是项目根目录的名称。 &lt;version&gt; - jar 包的版本号。可以直接填版本数字，也可以在 properties 标签中设置属性值。 &lt;scope&gt; - jar 包的作用范围。可以填写 compile、runtime、test、system 和 provided。用来在编译、测试等场景下选择对应的 classpath。 如何寻找 jar 包 可以在 http://mvnrepository.com/ 站点搜寻你想要的 jar 包版本 例如，想要使用 log4j，可以找到需要的版本号，然后拷贝对应的 maven 标签信息，将其添加到 pom .xml 文件中。 如何使用 Maven 插件(Plugin) 要添加 Maven 插件，可以在 pom.xml 文件中添加 &lt;plugin&gt; 标签。 &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; &lt;configuration&gt; 标签用来配置插件的一些使用参数。 如何一次编译多个工程 假设要创建一个父 maven 工程，它有两个子工程：my-app 和 my-webapp： +- pom.xml+- my-app| +- pom.xml| +- src| +- main| +- java+- my-webapp| +- pom.xml| +- src| +- main| +- webapp app 工程的 pom.xml 如下，重点在于在 modules 中引入两个子 module： &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;modules&gt; &lt;module&gt;my-app&lt;/module&gt; &lt;module&gt;my-webapp&lt;/module&gt; &lt;/modules&gt;&lt;/project&gt; 选择编译 XXX 时，会依次对它的所有 Module 执行相同操作。 常用 Maven 插件 更多详情请参考：https://maven.apache.org/plugins/ maven-antrun-plugin maven-antrun-plugin 能让用户在 Maven 项目中运行 Ant 任务。用户可以直接在该插件的配置以 Ant 的方式编写 Target， 然后交给该插件的 run 目标去执行。在一些由 Ant 往 Maven 迁移的项目中，该插件尤其有用。此外当你发现需要编写一些自定义程度很高的任务，同时又觉 得 Maven 不够灵活时，也可以以 Ant 的方式实现之。maven-antrun-plugin 的 run 目标通常与生命周期绑定运行。 maven-archetype-plugin Archtype 指项目的骨架，Maven 初学者最开始执行的 Maven 命令可能就是mvn archetype:generate，这实际上就是让 maven-archetype-plugin 生成一个很简单的项目骨架，帮助开发者快速上手。可能也有人看到一些文档写了mvn archetype:create， 但实际上 create 目标已经被弃用了，取而代之的是 generate 目标，该目标使用交互式的方式提示用户输入必要的信息以创建项目，体验更好。 maven-archetype-plugin 还有一些其他目标帮助用户自己定义项目原型，例如你由一个产品需要交付给很多客户进行二次开发，你就可以为 他们提供一个 Archtype，帮助他们快速上手。 maven-assembly-plugin maven-assembly-plugin 的用途是将项目打包，该包可能包含了项目的可执行文件、源代码、readme、平台脚本等等。 maven-assembly-plugin 支持各种主流的格式如 zip、tar.gz、jar 和 war 等，具体打包哪些文件是高度可控的，例如用户可以 按文件级别的粒度、文件集级别的粒度、模块级别的粒度、以及依赖级别的粒度控制打包，此外，包含和排除配置也是支持的。maven-assembly- plugin 要求用户使用一个名为assembly.xml的元数据文件来表述打包，它的 single 目标可以直接在命令行调用，也可以被绑定至生命周期。 maven-dependency-plugin maven-dependency-plugin 最大的用途是帮助分析项目依赖，dependency:list能够列出项目最终解析到的依赖列表，dependency:tree能进一步的描绘项目依赖树，dependency:analyze可以告诉你项目依赖潜在的问题，如果你有直接使用到的却未声明的依赖，该目标就会发出警告。maven-dependency-plugin 还有很多目标帮助你操作依赖文件，例如dependency:copy-dependencies能将项目依赖从本地 Maven 仓库复制到某个特定的文件夹下面。 maven-enforcer-plugin 在一个稍大一点的组织或团队中，你无法保证所有成员都熟悉 Maven，那他们做一些比较愚蠢的事情就会变得很正常，例如给项目引入了外部的 SNAPSHOT 依赖而导致构建不稳定，使用了一个与大家不一致的 Maven 版本而经常抱怨构建出现诡异问题。maven-enforcer- plugin 能够帮助你避免之类问题，它允许你创建一系列规则强制大家遵守，包括设定 Java 版本、设定 Maven 版本、禁止某些依赖、禁止 SNAPSHOT 依赖。只要在一个父 POM 配置规则，然后让大家继承，当规则遭到破坏的时候，Maven 就会报错。除了标准的规则之外，你还可以扩展该插 件，编写自己的规则。maven-enforcer-plugin 的 enforce 目标负责检查规则，它默认绑定到生命周期的 validate 阶段。 maven-help-plugin maven-help-plugin 是一个小巧的辅助工具，最简单的help:system可以打印所有可用的环境变量和 Java 系统属性。help:effective-pom和help:effective-settings最 为有用，它们分别打印项目的有效 POM 和有效 settings，有效 POM 是指合并了所有父 POM（包括 Super POM）后的 XML，当你不确定 POM 的某些信息从何而来时，就可以查看有效 POM。有效 settings 同理，特别是当你发现自己配置的 settings.xml 没有生效时，就可以用help:effective-settings来验证。此外，maven-help-plugin 的 describe 目标可以帮助你描述任何一个 Maven 插件的信息，还有 all-profiles 目标和 active-profiles 目标帮助查看项目的 Profile。 maven-release-plugin maven-release-plugin 的用途是帮助自动化项目版本发布，它依赖于 POM 中的 SCM 信息。release:prepare用来准备版本发布，具体的工作包括检查是否有未提交代码、检查是否有 SNAPSHOT 依赖、升级项目的 SNAPSHOT 版本至 RELEASE 版本、为项目打标签等等。release:perform则 是签出标签中的 RELEASE 源码，构建并发布。版本发布是非常琐碎的工作，它涉及了各种检查，而且由于该工作仅仅是偶尔需要，因此手动操作很容易遗漏一 些细节，maven-release-plugin 让该工作变得非常快速简便，不易出错。maven-release-plugin 的各种目标通常直接在 命令行调用，因为版本发布显然不是日常构建生命周期的一部分。 maven-resources-plugin 为了使项目结构更为清晰，Maven 区别对待 Java 代码文件和资源文件，maven-compiler-plugin 用来编译 Java 代码，maven-resources-plugin 则用来处理资源文件。默认的主资源文件目录是src/main/resources，很多用户会需要添加额外的资源文件目录，这个时候就可以通过配置 maven-resources-plugin 来实现。此外，资源文件过滤也是 Maven 的一大特性，你可以在资源文件中使用*${propertyName}*形式的 Maven 属性，然后配置 maven-resources-plugin 开启对资源文件的过滤，之后就可以针对不同环境通过命令行或者 Profile 传入属性的值，以实现更为灵活的构建。 maven-surefire-plugin 可能是由于历史的原因，Maven 2.3 中用于执行测试的插件不是 maven-test-plugin，而是 maven-surefire-plugin。其实大部分时间内，只要你的测试 类遵循通用的命令约定（以 Test 结尾、以 TestCase 结尾、或者以 Test 开头），就几乎不用知晓该插件的存在。然而在当你想要跳过测试、排除某些 测试类、或者使用一些 TestNG 特性的时候，了解 maven-surefire-plugin 的一些配置选项就很有用了。例如 mvn test -Dtest=FooTest 这样一条命令的效果是仅运行 FooTest 测试类，这是通过控制 maven-surefire-plugin 的 test 参数实现的。 build-helper-maven-plugin Maven 默认只允许指定一个主 Java 代码目录和一个测试 Java 代码目录，虽然这其实是个应当尽量遵守的约定，但偶尔你还是会希望能够指定多个 源码目录（例如为了应对遗留项目），build-helper-maven-plugin 的 add-source 目标就是服务于这个目的，通常它被绑定到 默认生命周期的 generate-sources 阶段以添加额外的源码目录。需要强调的是，这种做法还是不推荐的，因为它破坏了 Maven 的约定，而且可能会遇到其他严格遵守约定的插件工具无法正确识别额外的源码目录。 build-helper-maven-plugin 的另一个非常有用的目标是 attach-artifact，使用该目标你可以以 classifier 的形式选取部分项目文件生成附属构件，并同时 install 到本地仓库，也可以 deploy 到远程仓库。 exec-maven-plugin exec-maven-plugin 很好理解，顾名思义，它能让你运行任何本地的系统程序，在某些特定情况下，运行一个 Maven 外部的程序可能就是最简单的问题解决方案，这就是exec:exec的 用途，当然，该插件还允许你配置相关的程序运行参数。除了 exec 目标之外，exec-maven-plugin 还提供了一个 java 目标，该目标要求你 提供一个 mainClass 参数，然后它能够利用当前项目的依赖作为 classpath，在同一个 JVM 中运行该 mainClass。有时候，为了简单的 演示一个命令行 Java 程序，你可以在 POM 中配置好 exec-maven-plugin 的相关运行参数，然后直接在命令运行mvn exec:java 以查看运行效果。 jetty-maven-plugin 在进行 Web 开发的时候，打开浏览器对应用进行手动的测试几乎是无法避免的，这种测试方法通常就是将项目打包成 war 文件，然后部署到 Web 容器 中，再启动容器进行验证，这显然十分耗时。为了帮助开发者节省时间，jetty-maven-plugin 应运而生，它完全兼容 Maven 项目的目录结构，能够周期性地检查源文件，一旦发现变更后自动更新到内置的 Jetty Web 容器中。做一些基本配置后（例如 Web 应用的 contextPath 和自动扫描变更的时间间隔），你只要执行 mvn jetty:run ，然后在 IDE 中修改代码，代码经 IDE 自动编译后产生变更，再由 jetty-maven-plugin 侦测到后更新至 Jetty 容器，这时你就可以直接 测试 Web 页面了。需要注意的是，jetty-maven-plugin 并不是宿主于 Apache 或 Codehaus 的官方插件，因此使用的时候需要额外 的配置settings.xml的 pluginGroups 元素，将 org.mortbay.jetty 这个 pluginGroup 加入。 versions-maven-plugin 很多 Maven 用户遇到过这样一个问题，当项目包含大量模块的时候，为他们集体更新版本就变成一件烦人的事情，到底有没有自动化工具能帮助完成这件 事情呢？（当然你可以使用 sed 之类的文本操作工具，不过不在本文讨论范围）答案是肯定的，versions-maven- plugin 提供了很多目标帮助你管理 Maven 项目的各种版本信息。例如最常用的，命令 mvn versions:set -DnewVersion=1.1-SNAPSHOT 就能帮助你把所有模块的版本更新到 1.1-SNAPSHOT。该插件还提供了其他一些很有用的目标，display-dependency- updates 能告诉你项目依赖有哪些可用的更新；类似的 display-plugin-updates 能告诉你可用的插件更新；然后 use- latest-versions 能自动帮你将所有依赖升级到最新版本。最后，如果你对所做的更改满意，则可以使用 mvn versions:commit 提交，不满意的话也可以使用 mvn versions:revert 进行撤销。 常用 Maven 命令 更详细命令说明请参考：https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html 生命周期 阶段描述 mvn validate 验证项目是否正确，以及所有为了完整构建必要的信息是否可用 mvn generate-sources 生成所有需要包含在编译过程中的源代码 mvn process-sources 处理源代码，比如过滤一些值 mvn generate-resources 生成所有需要包含在打包过程中的资源文件 mvn process-resources 复制并处理资源文件至目标目录，准备打包 mvn compile 编译项目的源代码 mvn process-classes 后处理编译生成的文件，例如对 Java 类进行字节码增强（bytecode enhancement） mvn generate-test-sources 生成所有包含在测试编译过程中的测试源码 mvn process-test-sources 处理测试源码，比如过滤一些值 mvn generate-test-resources 生成测试需要的资源文件 mvn process-test-resources 复制并处理测试资源文件至测试目标目录 mvn test-compile 编译测试源码至测试目标目录 mvn test 使用合适的单元测试框架运行测试。这些测试应该不需要代码被打包或发布 mvn prepare-package 在真正的打包之前，执行一些准备打包必要的操作。这通常会产生一个包的展开的处理过的版本（将会在 Maven 2.1+中实现） mvn package 将编译好的代码打包成可分发的格式，如 JAR，WAR，或者 EAR mvn pre-integration-test 执行一些在集成测试运行之前需要的动作。如建立集成测试需要的环境 mvn integration-test 如果有必要的话，处理包并发布至集成测试可以运行的环境 mvn post-integration-test 执行一些在集成测试运行之后需要的动作。如清理集成测试环境。 mvn verify 执行所有检查，验证包是有效的，符合质量规范 mvn install 安装包至本地仓库，以备本地的其它项目作为依赖使用 mvn deploy 复制最终的包至远程仓库，共享给其它开发人员和项目（通常和一次正式的发布相关） 使用参数 -Dmaven.test.skip=true: 跳过单元测试(eg: mvn clean package -Dmaven.test.skip=true) 常见问题 dependencies 和 dependencyManagement，plugins 和 pluginManagement 有什么区别？ dependencyManagement 是表示依赖 jar 包的声明，即你在项目中的 dependencyManagement 下声明了依赖，maven 不会加载该依赖，dependencyManagement 声明可以被继承。 dependencyManagement 的一个使用案例是当有父子项目的时候，父项目中可以利用 dependencyManagement 声明子项目中需要用到的依赖 jar 包，之后，当某个或者某几个子项目需要加载该插件的时候，就可以在子项目中 dependencies 节点只配置 groupId 和 artifactId 就可以完成插件的引用。 dependencyManagement 主要是为了统一管理插件，确保所有子项目使用的插件版本保持一致，类似的还有 plugins 和 pluginManagement。 IDEA 修改 JDK 版本后编译报错 错误现象 修改 JDK 版本，指定 maven-compiler-plugin 的 source 和 target 为 1.8 。 然后，在 Intellij IDEA 中执行 maven 指令，报错： [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.0:compile (default-compile) on project apollo-common: Fatal error compiling: 无效的目标版本： 1.8 -&gt; [Help 1] 错误原因 maven 的 JDK 源与指定的 JDK 编译版本不符。 排错手段 查看 Project Settings Project SDK 是否正确 SDK 路径是否正确 查看 Settings &gt; Maven 的配置 JDK for importer 是否正确 Runner 是否正确 重复引入依赖 在 Idea 中，选中 Module，使用 Ctrl+Alt+Shift+U，打开依赖图，检索是否存在重复引用的情况。如果存在重复引用，可以将多余的引用删除。 如何打包一个可以直接运行的 Spring Boot jar 包 可以使用 spring-boot-maven-plugin 插件 &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 如果引入了第三方 jar 包，如何打包？ 首先，要添加依赖 &lt;dependency&gt; &lt;groupId&gt;io.github.dunwu&lt;/groupId&gt; &lt;artifactId&gt;dunwu-common&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;project.basedir&#125;/src/main/resources/lib/dunwu-common-1.0.0.jar&lt;/systemPath&gt;&lt;/dependency&gt; 接着，需要配置 spring-boot-maven-plugin 插件： &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;includeSystemScope&gt;true&lt;/includeSystemScope&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 最佳实践 通过 bom 统一管理版本 采用类似 spring-boot-dependencies 的方式统一管理依赖版本。 spring-boot-dependencies 的 pom.xml 形式： &lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt;&lt;version&gt;2.1.4.RELEASE&lt;/version&gt;&lt;packaging&gt;pom&lt;/packaging&gt;&lt;!-- 省略 --&gt;&lt;!-- 依赖包版本管理 --&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 省略 --&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;build&gt;&lt;!-- 插件版本管理 --&gt;&lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- 省略 --&gt; &lt;/plugins&gt;&lt;/pluginManagement&gt;&lt;/build&gt;&lt;/project&gt; 其他项目引入 spring-boot-dependencies 来管理依赖版本的方式： &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-boot.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 引用和引申 官方文档 http://www.oschina.net/question/158170_29368 http://www.cnblogs.com/crazy-fox/archive/2012/02/09/2343722.html]]></content>
      <categories>
        <category>java</category>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>build</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eclipse 使用小结]]></title>
    <url>%2Fblog%2F2016%2F03%2F20%2Fjava%2Fjavatool%2Fide%2Feclipse%2F</url>
    <content type="text"><![CDATA[Eclipse 使用小结 代码智能提示 插件安装 基本设置 设置文本文件及 JSP 文件编码 设置 JDK 本地 JavaDOC API 路径及源码路径 设置 Servlet 源码或其它 Jar 包源码 [反编译插件 JD-Eclipse](#反编译插件- jd-eclipse) Validate 优化 常用快捷键 代码智能提示 Java 智能提示 Window -&gt; Preferences -&gt; Java -&gt; Editor -&gt; Content Assist -&gt; Auto Activation delay 是自动弹出提示框的延时时间，我们可以修改成 100 毫秒；triggers 这里默认是&quot;.&quot;，只要加上&quot;abcdefghijklmnopqrstuvwxyz&quot;或者&quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ&quot;，嘿嘿！这下就能做到和 VS 一样的输入每个字母都能提示啦： 其它类型的文件比如 HTML、JavaScript、JSP 如果也能提供提示那不是更爽了？有了第二点设置的基础，其实这些设置都是一样的。 JavaScript 智能提示 Window -&gt; Preferences -&gt; JavaScript-&gt; Editor -&gt; Content Assist -&gt; Auto-Activation HTML 智能提示 Window -&gt; Preferences -&gt; Web -&gt; HTML Files -&gt; Editor -&gt; Content Assist -&gt; Auto-Activation 保存后，我们再来输入看看，感觉真是不错呀： 插件安装 很多教科书上说到 Eclipse 的插件安装都是通过 Help -&gt; Install New SoftWare 这种自动检索的方式，操作起来固然是方便，不过当我们不需要某种插件时不太容易找到要删除哪些内容，而且以后 Eclipse 版本升级的时候，通过这种方式安装过的插件都得再重新装一次。另外一种通过 Link 链接方式，就可以解决这些问题。 我们以 Eclipse 的中文汉化包插件为例，先到官方提供的汉化包地址下载一个：http://www.eclipse.org/babel/downloads.php，注意选好自己的 Eclipse 版本： 我的版本是 Kepler，然后进入下载页面，单击红框框中的链接，即可下载汉化包了： 下载完解压缩后，会有个包含 features 和 plugin 目录的 eclipse 文件夹，把这个 eclipse 放在我们的 Eclipse 安装根目录，也就是和 eclipse.exe 同一级目录下。然后仍然在这一级目录下，新建一个 links 文件夹，并在该文件夹内，建一个 language.link 的文本文件。该文本文件的名字是可以任取的，后缀名是.link，而不是.txt 哟。好了，最后一步，编辑该文件，在里面写入刚才放入的语言包的地址，并用“\”表示路径，一定要有 path= 这个前缀。 保存文件后，重新打开 Eclipse，熟悉的中文界面终于看到了。虽然汉化不完全，不过也够用了不是么。如果仍然出现的是英文，说明汉化失败，重新检查下 language.link 文件中配置的信息是否和汉化包的目录一致。 其它的插件安装方法也是如此，当不需要某个插件时，只需删除存放插件的目录和 links 目录下相应的 link 文件，或者改变下 link 文件里面的路径变成无效路径即可；对 Eclipse 做高版本升级时，也只需把老版存放插件的目录和 links 目录复制过去就行了。 基本设置 在 Preference 的搜索项中搜索 Text Editors。 可以参考我的设置： Show line numbers Show print margin Insert spaces for tabs 设置代码的字体类型和大小： Window -&gt; Preferences -&gt; General -&gt; Appearance -&gt; Content Assist -&gt; Colors and Fornts，只需修改 Basic 里面的 Text Font 就可以了。 推荐 Courier New。 设置文本文件及 JSP 文件编码 Window -&gt; Preferences -&gt; General -&gt; Workspace -&gt; Text file encoding -&gt; Other： 设置 JDK 本地 JavaDOC API 路径及源码路径 还都生成的是无意义的变量名，这样可能会对含有相同类型的变量参数的调用顺序造成干扰； 这种问题，我们把 JDK 或者相应 Jar 包的源码导入进去就能避免了： Window -&gt; Preferences -&gt; Java -&gt; Installed JREs -&gt; Edit： 选中设置好的 JRE 目录，编辑，然后全选 JRE system libraries 下的所有 Jar 包，点击右边的 Source Attachment； External location 下，选中 JDK 安装目录下的 src.zip 文件，一路 OK 下来。 设置完，我们再来看看，幸福来的好突然有木有！ 设置 Servlet 源码或其它 Jar 包源码 上一步已经设置过了 JDK 的源码或 JavaDoc 路径，为啥现在又出来了呢？其实这个不难理解，因为我们使用到的类的源码并不在 JDK 的源码包中。 仔细看，我们会发现这些 Jar 包其实都在 Tomcat 根目录下的 lib 文件夹中，但是翻遍了 Tomcat 目录也没有相应的 jar 或 zip 文件呀。既然本地没有，那就去官网上找找： http://tomcat.apache.org/download-70.cgi这里有Tomcat的安装包和源码包； 可以自定义一个专门用于存放 JavaSource 和 JavaDoc 的文件夹，把下载文件放到该目录下， 然后再切换到 Eclipse 下，选中没有代码提示的类或者函数， 按下 F3，点击 Change Attached Source： 选择我们刚才下载好的 tomcat 源码文件，一路 OK。 然后再回过头看看我们的代码提示，友好多了： 其它 Jar 包源码的设置方式也一样。 反编译插件 JD-Eclipse 无论是开发还是调试，反编译必不可少，每次都用 jd-gui 打开去看，多麻烦，干脆配置下 JD 插件，自动关联.class： 先从 http://jd.benow.ca/ 上下载离线安装包 jdeclipse_update_site.zip，解压缩后把 features、plugins 这 2 个文件夹复制到 新建文件夹 jdeclipse，然后把 jdeclipse 文件夹整个复制到 Eclipse 根目录的 dropins 文件夹下，重启 Eclipse 即可。这种方式是不是比建 link 文件更方便了？ 打开 Eclipse，Window -&gt; Preferences -&gt; General - &gt; Editors ，把 .class 文件设置关联成 jd 插件的 editor Validate 优化 我们在 eclipse 里经常看到这个进程，validating… 逐个的检查每一个文件。那么如何关闭一些 validate 操作呢？ 打开 eclipse，点击【window】菜单，选择【preferences】选项。 在左侧点击【validation】选项，在右侧可以看到 eclipse 进行的自动检查都有哪些内容。 将 Manual（手动）保持不动，将 build 里面只留下 classpath dependency Validator，其他的全部去掉。 最后点击【OK】按钮，保存设置。 以后如果需要对文件进行校验检查的时候，在文件上点击右键，点击【Validate】进行检查。 常用快捷键 快捷键 描述 Ctrl+1 快速修复（最经典的快捷键,就不用多说了，可以解决很多问题，比如 import 类、try catch 包围等） Ctrl+Shift+F 格式化当前代码 Ctrl+Shift+M 添加类的 import 导入 Ctrl+Shift+O 组织类的 import 导入（既有 Ctrl+Shift+M 的作用，又可以帮你去除没用的导入，很有用） Ctrl+Y 重做（与撤销 Ctrl+Z 相反） Alt+/ 内容辅助（帮你省了多少次键盘敲打，太常用了） Ctrl+D 删除当前行或者多行 Alt+↓ 当前行和下面一行交互位置（特别实用,可以省去先剪切,再粘贴了） Alt+↑ 当前行和上面一行交互位置（同上） Ctrl+Alt+↓ 复制当前行到下一行（复制增加） Ctrl+Alt+↑ 复制当前行到上一行（复制增加） Shift+Enter 在当前行的下一行插入空行（这时鼠标可以在当前行的任一位置,不一定是最后） Ctrl+/ 注释当前行,再按则取消注释 Alt+Shift+↑ 选择封装元素 Alt+Shift+← 选择上一个元素 Alt+Shift+→ 选择下一个元素 Shift+← 从光标处开始往左选择字符 Shift+→ 从光标处开始往右选择字符 Ctrl+Shift+← 选中光标左边的单词 Ctrl+Shift+→ 选中光标又边的单词 Ctrl+← 光标移到左边单词的开头，相当于 vim 的 b Ctrl+→ 光标移到右边单词的末尾，相当于 vim 的 e Ctrl+K 参照选中的 Word 快速定位到下一个（如果没有选中 word，则搜索上一次使用搜索的 word） Ctrl+Shift+K 参照选中的 Word 快速定位到上一个 Ctrl+J 正向增量查找（按下 Ctrl+J 后,你所输入的每个字母编辑器都提供快速匹配定位到某个单词,如果没有,则在状态栏中显示没有找到了,查一个单词时,特别实用,要退出这个模式，按 escape 建） Ctrl+Shift+J 反向增量查找（和上条相同,只不过是从后往前查） Ctrl+Shift+U 列出所有包含字符串的行 Ctrl+H 打开搜索对话框 Ctrl+G 工作区中的声明 Ctrl+Shift+G 工作区中的引用 Ctrl+Shift+T 搜索类（包括工程和关联的第三 jar 包） Ctrl+Shift+R 搜索工程中的文件 Ctrl+E 快速显示当前 Editer 的下拉列表（如果当前页面没有显示的用黑体表示） F4 打开类型层次结构 F3 跳转到声明处 Alt+← 前一个编辑的页面 Alt+→ 下一个编辑的页面（当然是针对上面那条来说了） Ctrl+PageUp/PageDown 在编辑器中，切换已经打开的文件 F5 单步跳入 F6 单步跳过 F7 单步返回 F8 继续 Ctrl+Shift+D 显示变量的值 Ctrl+Shift+B 在当前行设置或者去掉断点 Ctrl+R 运行至行(超好用，可以节省好多的断点) Alt+Shift+R 重命名方法名、属性或者变量名 （是我自己最爱用的一个了,尤其是变量和类的 Rename,比手工方法能节省很多劳动力） Alt+Shift+M 把一段函数内的代码抽取成方法 （这是重构里面最常用的方法之一了,尤其是对一大堆泥团代码有用） Alt+Shift+C 修改函数结构（比较实用,有 N 个函数调用了这个方法,修改一次搞定） Alt+Shift+L 抽取本地变量（ 可以直接把一些魔法数字和字符串抽取成一个变量,尤其是多处调用的时候） Alt+Shift+F 把 Class 中的 local 变量变为 field 变量 （比较实用的功能） Alt+Shift+I 合并变量（可能这样说有点不妥 Inline） Alt+Shift+V 移动函数和变量（不怎么常用） Alt+Shift+Z 重构的后悔药（Undo） Alt+Enter 显示当前选择资源的属性，windows 下的查看文件的属性就是这个快捷键，通常用来查看文件在 windows 中的实际路径 Ctrl+↑ 文本编辑器 上滚行 Ctrl+↓ 文本编辑器 下滚行 Ctrl+M 最大化当前的 Edit 或 View （再按则反之） Ctrl+O 快速显示 OutLine Ctrl+T 快速显示当前类的继承结构 Ctrl+W 关闭当前 Editer Ctrl+L 文本编辑器 转至行 F2 显示工具提示描述]]></content>
      <categories>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>IDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA 使用小结]]></title>
    <url>%2Fblog%2F2016%2F03%2F20%2Fjava%2Fjavatool%2Fide%2Fintellij%2F</url>
    <content type="text"><![CDATA[Intellij IDEA 使用小结 快捷键 插件 个性化 破解 参考资料 快捷键 核心快捷键 IntelliJ IDEA 作为一个以快捷键为中心的 IDE，为大多数操作建议了键盘快捷键。在这个主题中，您可以找到最不可缺少的列表，使 IntelliJ IDEA 轻松实现第一步。 核心快捷键表 操作 快捷键 根据名称查找操作 Ctrl+Shift+A 显示可用 意图操作 列表 Alt+Enter 切换视图 (Project,Structure, etc.). Alt+F1 切换工具窗口和在编辑器中打开的文件 Ctrl+Tab 显示 导航栏. Alt+Home 插入代码模板. Ctrl+J 在周围插入代码模板. Ctrl+Alt+J Edit an item from the Project or another tree view. F4 注释 Ctrl+/ Ctrl+Shift+/ 根据名称查找类或文件. Ctrl+N Ctrl+Shift+N 拷贝当前行或指定的行. Ctrl+D 增加或减少选中的表达式. Ctrl+W and Ctrl+Shift+W 在当前文件查找或替换. Ctrl+F Ctrl+R 在项目中或指定的目录中查找或替换 Ctrl+Shift+F Ctrl+Shift+R 全局搜索 双击 Shift 快速查看选中对象的引用. Ctrl+Shift+F7 展开或折叠编辑器中的代码块. Ctrl+NumPad Plus Ctrl+NumPad - 调用代码完成. Ctrl+Space 智能声明完成. Ctrl+Shift+Enter 智能补全代码 Ctrl+Shift+Space 显示可用的重构方法列表 Ctrl+Shift+Alt+T 快捷键分类 Tradition 快捷键 介绍 Ctrl + Z 撤销 Ctrl + Shift + Z 取消撤销 Ctrl + X 剪切 Ctrl + C 复制 Ctrl + S 保存 Tab 缩进 Shift + Tab 取消缩进 Shift + Home/End 选中光标到当前行头位置/行尾位置 Ctrl + Home/End 跳到文件头/文件尾 Editing 快捷键 介绍 Ctrl + Space 基础代码补全，默认在 Windows 系统上被输入法占用，需要进行修改，建议修改为 Ctrl + 逗号（必备） Ctrl + Alt + Space 类名自动完成 Ctrl + Shift + Enter 自动结束代码，行末自动添加分号（必备） Ctrl + P 方法参数提示显示 Ctrl + Q 光标所在的变量/类名/方法名等上面（也可以在提示补充的时候按），显示文档内容 Shift + F1 如果有外部文档可以连接外部文档 Ctrl + F1 在光标所在的错误代码处显示错误信息（必备） Alt + Insert 代码自动生成，如生成对象的 set/get 方法，构造函数，toString() 等（必备） Ctrl + O 选择可重写的方法 Ctrl + I 选择可继承的方法 Ctrl + Alt + T 对选中的代码弹出环绕选项弹出层（必备） Ctrl + / 注释光标所在行代码，会根据当前不同文件类型使用不同的注释符号（必备） Ctrl + Shift + / 代码块注释（必备） Ctrl + W 递进式选择代码块。可选中光标所在的单词或段落，连续按会在原有选中的基础上再扩展选中范围（必备） Ctrl + Shift + W 递进式取消选择代码块。可选中光标所在的单词或段落，连续按会在原有选中的基础上再扩展取消选中范围（必备） Alt + Q 弹出一个提示，显示当前类的声明/上下文信息 Alt + Enter IntelliJ IDEA 根据光标所在问题，提供快速修复选择，光标放在的位置不同提示的结果也不同（必备） Ctrl + Alt + L 格式化代码，可以对当前文件和整个包目录使用（必备） Ctrl + Alt + O 优化导入的类，可以对当前文件和整个包目录使用（必备） Ctrl + Alt + I 光标所在行 或 选中部分进行自动代码缩进，有点类似格式化 Ctrl + Shift + C 复制当前文件磁盘路径到剪贴板（必备） Ctrl + Shift + V 弹出缓存的最近拷贝的内容管理器弹出层 Ctrl + Alt + Shift + C 复制参考信息 Ctrl + Alt + Shift + V 无格式黏贴（必备） Ctrl + D 复制光标所在行 或 复制选择内容，并把复制内容插入光标位置下面（必备） Ctrl + Y 删除光标所在行 或 删除选中的行（必备） Ctrl + Shift + J 自动将下一行合并到当前行末尾（必备） Shift + Enter 开始新一行。光标所在行下空出一行，光标定位到新行位置（必备） Ctrl + Shift + U 对选中的代码进行大/小写轮流转换（必备） Ctrl + Shift + ]/[ 选中从光标所在位置到它的底部/顶部的中括号位置（必备） Ctrl + Delete 删除光标后面的单词或是中文句（必备） Ctrl + BackSpace 删除光标前面的单词或是中文句（必备） Ctrl + +/- 展开/折叠代码块 Ctrl + Shift + +/- 展开/折叠所有代码（必备） Ctrl + F4 关闭当前编辑文件 Ctrl + Shift + Up/Down 光标放在方法名上，将方法移动到上一个/下一个方法前面，调整方法排序（必备） Alt + Shift + Up/Down 移动光标所在行向上移动/向下移动（必备） Ctrl + Shift + 左键单击 把光标放在某个类变量上，按此快捷键可以直接定位到该类中（必备） Alt + Shift + 左键双击 选择被双击的单词/中文句，按住不放，可以同时选择其他单词/中文句（必备） Ctrl + Shift + T 对当前类生成单元测试类，如果已经存在的单元测试类则可以进行选择（必备） Search/Replace 快捷键 介绍 Double Shift 弹出 Search Everywhere 弹出层 F3 在查找模式下，定位到下一个匹配处 Shift + F3 在查找模式下，查找匹配上一个 Ctrl + F 在当前文件进行文本查找（必备） Ctrl + R 在当前文件进行文本替换（必备） Ctrl + Shift + F 根据输入内容查找整个项目 或 指定目录内文件（必备） Ctrl + Shift + R 根据输入内容替换对应内容，范围为整个项目 或 指定目录内文件（必备） Usage Search 快捷键 介绍 Alt + F7 查找光标所在的方法/变量/类被调用的地方 Ctrl + Alt + F7 显示使用的地方。寻找被该类或是变量被调用的地方，用弹出框的方式找出来 Ctrl + Shift + F7 高亮显示所有该选中文本，按 Esc 高亮消失（必备） Compile and Run 快捷键 介绍 Ctrl + F9 执行 Make Project 操作 Ctrl + Shift + F9 编译选中的文件/包/Module Shift + F9 Debug Shift + F10 Run Alt + Shift + F9 弹出 Debug 的可选择菜单 Alt + Shift + F10 弹出 Run 的可选择菜单 Debugging 快捷键 介绍 F7 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则进入当前方法体内，如果该方法体还有方法，则不会进入该内嵌的方法中 F8 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则不进入当前方法体内 Shift + F7 在 Debug 模式下，智能步入。断点所在行上有多个方法调用，会弹出进入哪个方法 Shift + F8 在 Debug 模式下，跳出，表现出来的效果跟 F9 一样 Alt + F8 在 Debug 模式下，选中对象，弹出可输入计算表达式调试框，查看该输入内容的调试结果 Alt + F9 在 Debug 模式下，执行到光标处 F9 在 Debug 模式下，恢复程序运行，但是如果该断点下面代码还有断点则停在下一个断点上 Ctrl + F8 在 Debug 模式下，设置光标当前行为断点，如果当前已经是断点则去掉断点 Ctrl + Shift + F8 在 Debug 模式下，指定断点进入条件 Navigation 快捷键 介绍 Ctrl + N 跳转到类（必备） Ctrl + Shift + N 跳转到文件（必备） Ctrl + Alt + Shift + N 跳转到符号（必备） Alt + Left/Right 切换当前已打开的窗口中的子视图，比如 Debug 窗口中有 Output、Debugger 等子视图，用此快捷键就可以在子视图中切换（必备） F12 回到前一个工具窗口（必备） ESC 从工具窗口进入代码文件窗口（必备） Shift + ESC 隐藏当前 或 最后一个激活的工具窗口 Ctrl + G 跳转到当前文件的指定行处 Ctrl + E 显示最近打开的文件记录列表（必备） Ctrl + Shift + E 显示最近编辑的文件记录列表（必备） Ctrl + Alt + Left/Right 跳转到上一个/下一个操作的地方（必备） Ctrl + Shift + Backspace 退回到上次修改的地方（必备） Alt + F1 显示当前文件选择目标弹出层，弹出层中有很多目标可以进行选择（必备） Ctrl + B/Ctrl + 左键单击 跳转到声明处 Ctrl + Alt + B 在某个调用的方法名上使用会跳到具体的实现处，可以跳过接口 Ctrl + Shift + B 跳转到类型声明处（必备） Ctrl + Shift + I 快速查看光标所在的方法 或 类的定义 Ctrl + U 前往当前光标所在的方法的父类的方法/接口定义（必备） Alt + Up/Down 跳转到当前文件的前一个/后一个方法（必备） Ctrl + ]/[ 跳转到当前所在代码的花括号结束位置/开始位置 Ctrl + F12 弹出当前文件结构层，可以在弹出的层上直接输入，进行筛选 Ctrl + H 显示当前类的层次结构 Ctrl + Shift + H 显示方法层次结构 Ctrl + Alt + H 调用层次 F2/Shift + F2 跳转到下一个/上一个高亮错误 或 警告位置（必备） F4 编辑源（必备） Alt + Home 定位/显示到当前文件的 Navigation Bar F11 添加书签（必备） Ctrl + F11 选中文件/文件夹，使用助记符设定/取消书签（必备） Shift + F11 弹出书签显示层（必备） Alt + 1,2,3…9 显示对应数值的选项卡，其中 1 是 Project 用得最多（必备） Ctrl + 1,2,3…9 定位到对应数值的书签位置（必备） Refactoring 快捷键 介绍 Shift + F6 对文件/文件夹 重命名（必备） Ctrl + Alt + Shift + T 打开重构菜单（必备） VCS/Local History 快捷键 介绍 Ctrl + K 版本控制提交项目，需要此项目有加入到版本控制才可用 Ctrl + T 版本控制更新项目，需要此项目有加入到版本控制才可用 Alt + | 显示版本控制常用操作菜单弹出层（必备）` Alt + Shift + C 查看最近操作项目的变化情况列表 Alt + Shift + N 选择/添加 task（必备） Live Templates 快捷键 介绍 Ctrl + J 插入自定义动态代码模板（必备） Ctrl + Alt + J 弹出模板选择窗口，将选定的代码加入动态模板中 General 快捷键 介绍 Ctrl + Tab 编辑窗口切换，如果在切换的过程又加按上 delete，则是关闭对应选中的窗口 Ctrl + Alt + Y 同步、刷新 Ctrl + Alt + S 打开 IntelliJ IDEA 系统设置（必备） Ctrl + Alt + Shift + S 打开当前项目设置（必备） Ctrl + Shift + A 查找动作/设置（必备） Ctrl + Shift + F12 编辑器最大化（必备） Alt + Shift + F 显示添加到收藏夹弹出层/添加到收藏夹 Alt + Shift + I 查看项目当前文件 Intellij IDEA 官方快捷键表 插件 推荐几个比较好用的插件 Key promoter 快捷键提示 https://plugins.jetbrains.com/plugin/4455?pr=idea CamelCase 驼峰式命名和下划线命名交替变化 CheckStyle-IDEA 代码规范检查 FindBugs-IDEA潜在 Bug 检查 MetricsReloaded 代码复杂度检查 Statistic 代码统计 JRebel Plugin 热部署 GsonFormat 把 JSON 字符串直接实例化成类 Eclipse Code Formatter 如果你以前用的是 IDE，并有自己的一套代码风格配置，可以通过此插件导入到 IDEA Alibaba Java Coding Guidelines 阿里 Java 开发规范的静态检查工具 IDE Features Trainer 官方的新手训练插件 Markdown Navigator Markdown 插件，适用于喜欢用 markdown 写文档的人 个性化 颜色主题 intellij-colors-solarized 个人觉得这种色彩搭配十分优雅 下载地址：https://github.com/altercation/solarized 破解 Intellij 是一个收费的 IDE，坦白说有点小贵，买不起。 所以，很惭愧，只好用下破解方法了。网上有很多使用注册码的网文，但是注册码不稳定，随时可能被封。还是自行搭建一个注册服务器比较稳定。我使用了 ilanyu 博文 IntelliJ IDEA License Server 本地搭建教程 的方法，亲测十分有效。 我的备用地址：百度云盘 下载并解压文中的压缩包到本地，选择适合操作系统的版本运行。 如果是在 Linux 上运行，推荐创建一个脚本，代码如下： # 使用 nohup 创建守护进程，运行 IntelliJIDEALicenseServer_linux_amd64# 如果运行在其他 Linux 发行版本，替换执行的脚本即可nohup sh IntelliJIDEALicenseServer_linux_amd64 2&gt;&amp;1 这样做是因为：大部分人使用 linux 是使用仿真器连接虚拟机，如果断开连接，进程也会被 kill，每次启动这个注册服务器很麻烦不是吗？而启动了守护进程，则不会出现这种情况，只有你主动 kill 进程才能将其干掉。 Windows 版本是 exe 程序，将其设为开机自动启动即可，别告诉我你不知道怎么设置开机自动启动。 参考资料 IntelliJ-IDEA-Tutorial 极客学院 - Intellij IDEA 使用教程]]></content>
      <categories>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>IDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 面向对象]]></title>
    <url>%2Fblog%2F2016%2F02%2F04%2Fjava%2Fjavacore%2Fbasics%2FJava%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[Java 面向对象 在深入理解 Java 基本数据类型中我们了解 Java 中支持的基本数据类型（值类型）。本文开始讲解 Java 中重要的引用类型——类。 - [封装](#封装) - [继承](#继承) - [多态](#多态) 参考资料 封装 封装（Encapsulation）是指一种将抽象性函式接口的实现细节部份包装、隐藏起来的方法。 封装最主要的作用在于我们能修改自己的实现代码，而不用修改那些调用我们代码的程序片段。 适当的封装可以让程式码更容易理解与维护，也加强了程式码的安全性。 封装的优点： 良好的封装能够减少耦合。 类内部的结构可以自由修改。 可以对成员变量进行更精确的控制。 隐藏信息，实现细节。 实现封装的步骤： 修改属性的可见性来限制对属性的访问（一般限制为 private）。 对每个值属性提供对外的公共方法访问，也就是创建一对赋取值方法，用于对私有属性的访问。 继承 继承是 java 面向对象编程技术的一块基石，因为它允许创建分等级层次的类。 继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具有父类相同的行为。 现实中的例子： 狗和鸟都是动物。如果将狗、鸟作为类，它们可以继承动物类。 类的继承形式： class 父类 &#123;&#125;class 子类 extends 父类 &#123;&#125; 继承类型 继承的特性 子类拥有父类非 private 的属性、方法。 子类可以拥有自己的属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。 Java 的继承是单继承，但是可以多重继承，单继承就是一个子类只能继承一个父类，多重继承就是，例如 A 类继承 B 类，B 类继承 C 类，所以按照关系就是 C 类是 B 类的父类，B 类是 A 类的父类，这是 Java 继承区别于 C++ 继承的一个特性。 提高了类之间的耦合性（继承的缺点，耦合度高就会造成代码之间的联系越紧密，代码独立性越差）。 继承关键字 继承可以使用 extends 和 implements 这两个关键字来实现继承，而且所有的类都是继承于 java.lang.Object，当一个类没有继承的两个关键字，则默认继承 object（这个类在 java.lang 包中，所以不需要 import）祖先类。 多态 刚开始学习面向对象编程时，容易被各种术语弄得云里雾里。所以，很多人会死记硬背书中对于术语的定义。 但是，随着应用和理解的深入，应该会渐渐有更进一步的认识，将其融汇贯通的理解。 学习类之前，先让我们思考一个问题：Java 中为什么要引入类机制，设计的初衷是什么？ Java 中提供的基本数据类型，只能表示单一的数值，这用于数值计算，还 OK。但是，如果要抽象模拟现实中更复杂的事物，则无法做到。 试想，如果要让你抽象狗的数据模型，怎么做？狗有眼耳口鼻等器官，有腿，狗有大小，毛色，这些都是它的状态，狗会跑、会叫、会吃东西，这些是它的行为。 类的引入，就是为了抽象这种相对复杂的事物。 对象是用于计算机语言对问题域中事物的描述。对象通过方法和属性来分别描述事物所具有的行为和状态。 类是用于描述同一类的对象的一个抽象的概念，类中定义了这一类对象所具有的行为和状态。 类可以看成是创建 Java 对象的模板。 什么是方法？扩展阅读：面向对象编程的弊端是什么？ - invalid s 的回答 参考资料 书 Java 编程思想 JAVA 核心技术（卷 1） Head First Java 文章 面向对象编程的弊端是什么？ - invalid s 的回答]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
        <tag>oop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Java方法]]></title>
    <url>%2Fblog%2F2016%2F02%2F04%2Fjava%2Fjavacore%2Fbasics%2FJava%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[深入理解 Java 方法 方法（有的人喜欢叫函数）是一段可重用的代码段。 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「javacore」 方法的使用 方法定义 方法的调用 方法参数 方法修饰符 访问控制修饰符 static final default abstract synchronized 特殊方法 main 方法 构造方法 变参方法 finalize() 方法 覆写和重载 小结 参考资料 方法的使用 方法定义 方法定义语法格式： [修饰符] 返回值类型 方法名([参数类型 参数名])&#123; ... 方法体 ... return 返回值;&#125; 示例： public static void main(String[] args) &#123; System.out.println("Hello World");&#125; 方法包含一个方法头和一个方法体。下面是一个方法的所有部分： 修饰符 - 修饰符是可选的，它告诉编译器如何调用该方法。定义了该方法的访问类型。 返回值类型 - 返回值类型表示方法执行结束后，返回结果的数据类型。如果没有返回值，应设为 void。 方法名 - 是方法的实际名称。方法名和参数表共同构成方法签名。 参数类型 - 参数像是一个占位符。当方法被调用时，传递值给参数。参数列表是指方法的参数类型、顺序和参数的个数。参数是可选的，方法可以不包含任何参数。 方法体 - 方法体包含具体的语句，定义该方法的功能。 return - 必须返回声明方法时返回值类型相同的数据类型。在 void 方法中，return 语句可有可无，如果要写 return，则只能是 return; 这种形式。 方法的调用 当程序调用一个方法时，程序的控制权交给了被调用的方法。当被调用方法的返回语句执行或者到达方法体闭括号时候交还控制权给程序。 Java 支持两种调用方法的方式，根据方法是否有返回值来选择。 有返回值方法 - 有返回值方法通常被用来给一个变量赋值或代入到运算表达式中进行计算。 int larger = max(30, 40); 无返回值方法 - 无返回值方法只能是一条语句。 System.out.println("Hello World"); 递归调用 Java 支持方法的递归调用（即方法调用自身）。 注意： 递归方法必须有明确的结束条件。 尽量避免使用递归调用。因为递归调用如果处理不当，可能导致栈溢出。 斐波那契数列（一个典型的递归算法）示例： public class RecursionMethodDemo &#123; public static int fib(int num) &#123; if (num == 1 || num == 2) &#123; return 1; &#125; else &#123; return fib(num - 2) + fib(num - 1); &#125; &#125; public static void main(String[] args) &#123; for (int i = 1; i &lt; 10; i++) &#123; System.out.print(fib(i) + "\t"); &#125; &#125;&#125; 方法参数 在 C/C++ 等编程语言中，方法的参数传递一般有两种形式： 值传递 - 值传递的参数被称为形参。值传递时，传入的参数，在方法中的修改，不会在方法外部生效。 引用传递 - 引用传递的参数被称为实参。引用传递时，传入的参数，在方法中的修改，会在方法外部生效。 那么，Java 中是怎样的呢？ Java 中只有值传递。 示例一： public class MethodParamDemo &#123; public static void method(int value) &#123; value = value + 1; &#125; public static void main(String[] args) &#123; int num = 0; method(num); System.out.println("num = [" + num + "]"); method(num); System.out.println("num = [" + num + "]"); &#125;&#125;// Output:// num = [0]// num = [0] 示例二： public class MethodParamDemo2 &#123; public static void method(StringBuilder sb) &#123; sb = new StringBuilder("B"); &#125; public static void main(String[] args) &#123; StringBuilder sb = new StringBuilder("A"); System.out.println("sb = [" + sb.toString() + "]"); method(sb); System.out.println("sb = [" + sb.toString() + "]"); sb = new StringBuilder("C"); System.out.println("sb = [" + sb.toString() + "]"); &#125;&#125;// Output:// sb = [A]// sb = [A]// sb = [C] 说明： 以上两个示例，无论向方法中传入的是基础数据类型，还是引用类型，在方法中修改的值，在外部都未生效。 Java 对于基本数据类型，会直接拷贝值传递到方法中；对于引用数据类型，拷贝当前对象的引用地址，然后把该地址传递过去，所以也是值传递。 扩展阅读： 图解 Java 中的参数传递 方法修饰符 前面提到了，Java 方法的修饰符是可选的，它告诉编译器如何调用该方法。定义了该方法的访问类型。 Java 方法有好几个修饰符，让我们一一来认识一下： 访问控制修饰符 访问权限控制的等级，从最大权限到最小权限依次为： public &gt; protected &gt; 包访问权限（没有任何关键字）&gt; private public - 表示任何类都可以访问； 包访问权限 - 包访问权限，没有任何关键字。它表示当前包中的所有其他类都可以访问，但是其它包的类无法访问。 protected - 表示子类可以访问，此外，同一个包内的其他类也可以访问，即使这些类不是子类。 private - 表示其它任何类都无法访问。 static 被 static 修饰的方法被称为静态方法。 静态方法相比于普通的实例方法，主要有以下区别： 在外部调用静态方法时，可以使用 类名.方法名 的方式，也可以使用 对象名.方法名 的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制。 静态方法常被用于各种工具类、工厂方法类。 final 被 final 修饰的方法不能被子类覆写（Override）。 final 方法示例： public class FinalMethodDemo &#123; static class Father &#123; protected final void print() &#123; System.out.println("call Father print()"); &#125;; &#125; static class Son extends Father &#123; @Override protected void print() &#123; System.out.println("call print()"); &#125; &#125; public static void main(String[] args) &#123; Father demo = new Son(); demo.print(); &#125;&#125;// 编译时会报错 说明： 上面示例中，父类 Father 中定义了一个 final 方法 print()，则其子类不能 Override 这个 final 方法，否则会编译报错。 default JDK8 开始，支持在接口 Interface 中定义 default 方法。default 方法只能出现在接口 Interface 中。 接口中被 default 修饰的方法被称为默认方法，实现此接口的类如果没 Override 此方法，则直接继承这个方法，不再强制必须实现此方法。 default 方法语法的出现，是为了既有的成千上万的 Java 类库的类增加新的功能， 且不必对这些类重新进行设计。 举例来说，JDK8 中 Collection 类中有一个非常方便的 stream() 方法，就是被修饰为 default，Collection 的一大堆 List、Set 子类就直接继承了这个方法 I，不必再为每个子类都注意添加这个方法。 default 方法示例： public class DefaultMethodDemo &#123; interface MyInterface &#123; default void print() &#123; System.out.println("Hello World"); &#125; &#125; static class MyClass implements MyInterface &#123;&#125; public static void main(String[] args) &#123; MyInterface obj = new MyClass(); obj.print(); &#125;&#125;// Output:// Hello World abstract 被 abstract 修饰的方法被称为抽象方法，方法不能有实体。抽象方法只能出现抽象类中。 抽象方法示例： public class AbstractMethodDemo &#123; static abstract class AbstractClass &#123; abstract void print(); &#125; static class ConcreteClass extends AbstractClass &#123; @Override void print() &#123; System.out.println("call print()"); &#125; &#125; public static void main(String[] args) &#123; AbstractClass demo = new ConcreteClass(); demo.print(); &#125;&#125;// Outpu:// call print() synchronized synchronized 用于并发编程。被 synchronized 修饰的方法在一个时刻，只允许一个线程执行。 在 Java 的同步容器（Vector、Stack、HashTable）中，你会见到大量的 synchronized 方法。不过，请记住：在 Java 并发编程中，synchronized 方法并不是一个好的选择，大多数情况下，我们会选择更加轻量级的锁 。 特殊方法 Java 中，有一些较为特殊的方法，分别使用于特殊的场景。 main 方法 Java 中的 main 方法是一种特殊的静态方法，因为所有的 Java 程序都是由 public static void main(String[] args) 方法开始执行。 有很多新手虽然一直用 main 方法，却不知道 main 方法中的 args 有什么用。实际上，这是用来接收接收命令行输入参数的。 示例： public class MainMethodDemo &#123; public static void main(String[] args) &#123; for (String arg : args) &#123; System.out.println("arg = [" + arg + "]"); &#125; &#125;&#125; 依次执行 javac MainMethodDemo.javajava MainMethodDemo A B C 控制台会打印输出参数： arg = [A]arg = [B]arg = [C] 构造方法 任何类都有构造方法，构造方法的作用就是在初始化类实例时，设置实例的状态。 每个类都有构造方法。如果没有显式地为类定义任何构造方法，Java 编译器将会为该类提供一个默认构造方法。 在创建一个对象的时候，至少要调用一个构造方法。构造方法的名称必须与类同名，一个类可以有多个构造方法。 public class ConstructorMethodDemo &#123; static class Person &#123; private String name; public Person(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; public static void main(String[] args) &#123; Person person = new Person("jack"); System.out.println("person name is " + person.getName()); &#125;&#125; 注意，构造方法除了使用 public，也可以使用 private 修饰，这种情况下，类无法调用此构造方法去实例化对象，这常常用于设计模式中的单例模式。 变参方法 JDK5 开始，Java 支持传递同类型的可变参数给一个方法。在方法声明中，在指定参数类型后加一个省略号 ...。一个方法中只能指定一个可变参数，它必须是方法的最后一个参数。任何普通的参数必须在它之前声明。 变参方法示例： public class VarargsDemo &#123; public static void method(String... params) &#123; System.out.println("params.length = " + params.length); for (String param : params) &#123; System.out.println("params = [" + param + "]"); &#125; &#125; public static void main(String[] args) &#123; method("red"); method("red", "yellow"); method("red", "yellow", "blue"); &#125;&#125;// Output:// params.length = 1// params = [red]// params.length = 2// params = [red]// params = [yellow]// params.length = 3// params = [red]// params = [yellow]// params = [blue] finalize() 方法 finalize 在对象被垃圾收集器析构(回收)之前调用，用来清除回收对象。 finalize 是在 java.lang.Object 里定义的，也就是说每一个对象都有这么个方法。这个方法在 GC 启动，该对象被回收的时候被调用。 finalizer() 通常是不可预测的，也是很危险的，一般情况下是不必要的。使用终结方法会导致行为不稳定、降低性能，以及可移植性问题。 请记住：应该尽量避免使用 finalizer()。千万不要把它当成是 C/C++ 中的析构函数来用。原因是：Finalizer 线程会和我们的主线程进行竞争，不过由于它的优先级较低，获取到的 CPU 时间较少，因此它永远也赶不上主线程的步伐。所以最后可能会发生 OutOfMemoryError 异常。 扩展阅读： 下面两篇文章比较详细的讲述了 finalizer() 可能会造成的问题及原因。 Java 的 Finalizer 引发的内存溢出 重载 Finalize 引发的内存泄露 覆写和重载 覆写（Override）是指子类定义了与父类中同名的方法，但是在方法覆写时必须考虑到访问权限，子类覆写的方法不能拥有比父类更加严格的访问权限。 子类要覆写的方法如果要访问父类的方法，可以使用 super 关键字。 覆写示例： public class MethodOverrideDemo &#123; static class Animal &#123; public void move() &#123; System.out.println("会动"); &#125; &#125; static class Dog extends Animal &#123; @Override public void move() &#123; super.move(); System.out.println("会跑"); &#125; &#125; public static void main(String[] args) &#123; Animal dog = new Dog(); dog.move(); &#125;&#125;// Output:// 会动// 会跑 方法的重载（Overload）是指方法名称相同，但参数的类型或参数的个数不同。通过传递参数的个数及类型的不同可以完成不同功能的方法调用。 注意： 重载一定是方法的参数不完全相同。如果方法的参数完全相同，仅仅是返回值不同，Java 是无法编译通过的。 重载示例： public class MethodOverloadDemo &#123; public static void add(int x, int y) &#123; System.out.println("x + y = " + (x + y)); &#125; public static void add(double x, double y) &#123; System.out.println("x + y = " + (x + y)); &#125; public static void main(String[] args) &#123; add(10, 20); add(1.0, 2.0); &#125;&#125;// Output:// x + y = 30// x + y = 3.0 小结 参考资料 Java 编程思想 JAVA 核心技术（卷 1） Head First Java 图解 Java 中的参数传递 Java 的 Finalizer 引发的内存溢出 重载 Finalize 引发的内存泄露]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>method</tag>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 面向对象]]></title>
    <url>%2Fblog%2F2016%2F02%2F04%2Fjava%2Fjavacore%2Fbasics%2FJava%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[Java 面向对象 在深入理解 Java 基本数据类型中我们了解 Java 中支持的基本数据类型（值类型）。本文开始讲解 Java 中重要的引用类型——类。 对象 类 构造方法 创建对象 访问实例变量和方法 访问权限控制 Java 包 访问权限修饰关键字 接口 抽象类 内部类 参考资料 对象 每种编程语言，都有自己的操纵内存中元素的方式。 Java 中提供了基本数据类型，但这还不能满足编写程序时，需要抽象更加复杂数据类型的需要。因此，Java 中，允许开发者通过类（类的机制下面会讲到）创建自定义类型。 有了自定义类型，那么数据类型自然会千变万化，所以，必须要有一定的机制，使得它们仍然保持一些必要的、通用的特性。 Java 世界有一句名言：一切皆为对象。这句话，你可能第一天学 Java 时，就听过了。这不仅仅是一句口号，也体现在 Java 的设计上。 首先，所有 Java 类都继承自 Object 类（从这个名字，就可见一斑）。 几乎所有 Java 对象初始化时，都要使用 new 创建对象（基本数据类型、String、枚举特殊处理），对象存储在堆中。 // 下面两String s = "abc";String s = new String("abc"); 其中，String s 定义了一个名为 s 的引用，它指向一个 String 类型的对象，而实际的对象是 “abc” 字符串。这就像是，使用遥控器（引用）来操纵电视机（对象）。 与 C/C++ 这类语言不同，程序员只需要通过 new 创建一个对象，但不必负责销毁或结束一个对象。负责运行 Java 程序的 Java 虚拟机有一个垃圾回收器，它会监视 new 创建的对象，一旦发现对象不再被引用，则会释放对象的内存空间。 类 与大多数面向对象编程语言一样，Java 使用 class （类）关键字来表示自定义类型。自定义类型是为了更容易抽象现实事物。 在一个类中，可以设置一静一动两种元素：属性（静）和方法（动）。 属性（有的人喜欢称为成员、字段） - 属性抽象的是事物的状态。类属性可以是任何类型的对象。 方法（有的人喜欢称为函数） - 方法抽象的是事物的行为。 类的形式如下： 方法 方法定义 修饰符 返回值类型 方法名(参数类型 参数名)&#123; ... 方法体 ... return 返回值;&#125; 方法包含一个方法头和一个方法体。下面是一个方法的所有部分： **修饰符：**修饰符，这是可选的，告诉编译器如何调用该方法。定义了该方法的访问类型。 **返回值类型 ：**方法可能有返回值。如果没有返回值，这种情况下，返回值类型应设为 void。 **方法名：**是方法的实际名称。方法名和参数表共同构成方法签名。 **参数类型：**参数像是一个占位符。当方法被调用时，传递值给参数。这个值被称为实参或变量。参数列表是指方法的参数类型、顺序和参数的个数。参数是可选的，方法可以不包含任何参数。 **方法体：**方法体包含具体的语句，定义该方法的功能。 示例： public static int add(int x, int y) &#123; return x + y;&#125; 方法调用 Java 支持两种调用方法的方式，根据方法是否返回值来选择。 当程序调用一个方法时，程序的控制权交给了被调用的方法。当被调用方法的返回语句执行或者到达方法体闭括号时候交还控制权给程序。 当方法返回一个值的时候，方法调用通常被当做一个值。例如： int larger = max(30, 40); 如果方法返回值是 void，方法调用一定是一条语句。例如，方法 println 返回 void。下面的调用是个语句： System.out.println("Hello World"); 构造方法 每个类都有构造方法。如果没有显式地为类定义任何构造方法，Java 编译器将会为该类提供一个默认构造方法。 在创建一个对象的时候，至少要调用一个构造方法。构造方法的名称必须与类同名，一个类可以有多个构造方法。 public class Puppy&#123; public Puppy()&#123; &#125; public Puppy(String name)&#123; // 这个构造器仅有一个参数：name &#125;&#125; 变量 Java 支持的变量类型有： 局部变量 - 类方法中的变量。 实例变量（也叫成员变量） - 类方法外的变量，不过没有 static 修饰。 类变量（也叫静态变量） - 类方法外的变量，用 static 修饰。 特性对比： 局部变量 实例变量（也叫成员变量） 类变量（也叫静态变量） 局部变量声明在方法、构造方法或者语句块中。 实例变量声明在方法、构造方法和语句块之外。 类变量声明在方法、构造方法和语句块之外。并且以 static 修饰。 局部变量在方法、构造方法、或者语句块被执行的时候创建，当它们执行完成后，变量将会被销毁。 实例变量在对象创建的时候创建，在对象被销毁的时候销毁。 类变量在第一次被访问时创建，在程序结束时销毁。 局部变量没有默认值，所以必须经过初始化，才可以使用。 实例变量具有默认值。数值型变量的默认值是 0，布尔型变量的默认值是 false，引用类型变量的默认值是 null。变量的值可以在声明时指定，也可以在构造方法中指定。 类变量具有默认值。数值型变量的默认值是 0，布尔型变量的默认值是 false，引用类型变量的默认值是 null。变量的值可以在声明时指定，也可以在构造方法中指定。此外，静态变量还可以在静态语句块中初始化。 对于局部变量，如果是基本类型，会把值直接存储在栈；如果是引用类型，会把其对象存储在堆，而把这个对象的引用（指针）存储在栈。 实例变量存储在堆。 类变量存储在静态存储区。 访问修饰符不能用于局部变量。 访问修饰符可以用于实例变量。 访问修饰符可以用于类变量。 局部变量只在声明它的方法、构造方法或者语句块中可见。 实例变量对于类中的方法、构造方法或者语句块是可见的。一般情况下应该把实例变量设为私有。通过使用访问修饰符可以使实例变量对子类可见。 与实例变量具有相似的可见性。但为了对类的使用者可见，大多数静态变量声明为 public 类型。 实例变量可以直接通过变量名访问。但在静态方法以及其他类中，就应该使用完全限定名：ObejectReference.VariableName。 静态变量可以通过：ClassName.VariableName 的方式访问。 无论一个类创建了多少个对象，类只拥有类变量的一份拷贝。 类变量除了被声明为常量外很少使用。 变量修饰符 访问级别修饰符 - 如果变量是实例变量或类变量，可以添加访问级别修饰符（public/protected/private） 静态修饰符 - 如果变量是类变量，需要添加 static 修饰 final - 如果变量使用 fianl 修饰符，就表示这是一个常量，不能被修改。 创建对象 对象是根据类创建的。在 Java 中，使用关键字 new 来创建一个新的对象。创建对象需要以下三步： 声明：声明一个对象，包括对象名称和对象类型。 实例化：使用关键字 new 来创建一个对象。 初始化：使用 new 创建对象时，会调用构造方法初始化对象。 public class Puppy&#123; public Puppy(String name)&#123; //这个构造器仅有一个参数：name System.out.println("小狗的名字是 : " + name ); &#125; public static void main(String[] args)&#123; // 下面的语句将创建一个Puppy对象 Puppy myPuppy = new Puppy( "tommy" ); &#125;&#125; 访问实例变量和方法 /* 实例化对象 */ObjectReference = new Constructor();/* 访问类中的变量 */ObjectReference.variableName;/* 访问类中的方法 */ObjectReference.methodName(); 访问权限控制 代码组织 当编译一个 .java 文件时，在 .java 文件中的每个类都会输出一个与类同名的 .class 文件。 MultiClassDemo.java 示例： class MultiClass1 &#123;&#125;class MultiClass2 &#123;&#125;class MultiClass3 &#123;&#125;public class MultiClassDemo &#123;&#125; 执行 javac MultiClassDemo.java 命令，本地会生成 MultiClass1.class、MultiClass2.class、MultiClass3.class、MultiClassDemo.class 四个文件。 Java 可运行程序是由一组 .class 文件打包并压缩成的一个 .jar 文件。Java 解释器负责这些文件的查找、装载和解释。Java 类库实际上是一组类文件（.java 文件）。 其中每个文件允许有一个 public 类，以及任意数量的非 public 类。 public 类名必须和 .java 文件名完全相同，包括大小写。 程序一般不止一个人编写，会调用系统提供的代码、第三方库中的代码、项目中其他人写的代码等，不同的人因为不同的目的可能定义同样的类名/接口名，这就是命名冲突。 Java 中为了解决命名冲突问题，提供了包（package）和导入（import）机制。 package 包（package）的原则： 包类似于文件夹，文件放在文件夹中，类和接口则放在包中。为了便于组织，文件夹一般是一个有层次的树形结构，包也类似。 包名以逗号 . 分隔，表示层次结构。 Java 中命名包名的一个惯例是使用域名作为前缀，因为域名是唯一的，一般按照域名的反序来定义包名，比如，域名是：apache.org，包名就以 org.apache 开头。 **包名和文件目录结构必须完全匹配。**Java 解释器运行过程如下： 找出环境变量 CLASSPATH，作为 .class 文件的根目录。 从根目录开始，获取包名称，并将逗号 . 替换为文件分隔符（反斜杠 /），通过这个路径名称去查找 Java 类。 import 同一个包下的类之间互相引用是不需要包名的，可以直接使用。但如果类不在同一个包内，则必须要知道其所在的包，使用有两种方式： 通过类的完全限定名 通过 import 将用到的类引入到当前类 通过类的完全限定名示例： public class PackageDemo &#123; public static void main (String[]args)&#123; System.out.println(new java.util.Date()); System.out.println(new java.util.Date()); &#125;&#125; 通过 import 导入其它包的类到当前类： import java.util.Date;public class PackageDemo2 &#123; public static void main(String[] args) &#123; System.out.println(new Date()); System.out.println(new Date()); &#125;&#125; 说明：以上两个示例比较起来，显然是 import 方式，代码更加整洁。 扩展阅读：https://www.cnblogs.com/swiftma/p/5628762.html 访问权限修饰关键字 访问权限控制的等级，从最大权限到最小权限依次为： public &gt; protected &gt; 包访问权限（没有任何关键字）&gt; private public - 表示任何类都可以访问； 包访问权限 - 包访问权限，没有任何关键字。它表示当前包中的所有其他类都可以访问，但是其它包的类无法访问。 protected - 表示子类可以访问，此外，同一个包内的其他类也可以访问，即使这些类不是子类。 private - 表示其它任何类都无法访问。 接口 public interface Comparable&lt;T&gt; &#123; public int compareTo(T o);&#125; 抽象类 抽象类不能被实例化(初学者很容易犯的错)，如果被实例化，就会报错，编译无法通过。只有抽象类的非抽象子类可以创建对象。 抽象类中不一定包含抽象方法，但是有抽象方法的类必定是抽象类。 抽象类中的抽象方法只是声明，不包含方法体，就是不给出方法的具体实现也就是方法的具体功能。 构造方法，类方法（用 static 修饰的方法）不能声明为抽象方法。 抽象类的子类必须给出抽象类中的抽象方法的具体实现，除非该子类也是抽象类。 内部类 参考资料 书 Java 编程思想 JAVA 核心技术（卷 1） Head First Java 文章 面向对象编程的弊端是什么？ - invalid s 的回答 https://www.cnblogs.com/swiftma/p/5628762.html]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
        <tag>oop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 本地化]]></title>
    <url>%2Fblog%2F2016%2F01%2F20%2Fjava%2Fjavacore%2Fadvanced%2FJava%E6%9C%AC%E5%9C%B0%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Java 本地化 📓 本文已归档到：「blog」 背景知识 语言编码、国家/地区编码 字符编码 Java 中实现本地化 定义不同语种的模板 选择语种 加载指定语种的模板 支持本地化的工具类 NumberFormat DateFormat MessageFormat 背景知识 通讯的发达，使得世界各地交流越来越紧密。许多的软件产品也要面向世界上不同国家的用户。其中，语言障碍显然是产品在不同语种用户中进行推广的一个重要问题。 本文围绕本地化这一主题，先介绍国际标准的语言编码，然后讲解在 Java 应用中如何去实现本地化。 语言编码、国家/地区编码 做 web 开发的朋友可能多多少少接触过类似 zh-cn, en-us 这样的编码字样。 这些编码是用来表示指定的国家地区的语言类型的。那么，这些含有特殊含义的编码是如何产生的呢？ ISO-639 标准使用编码定义了国际上常见的语言，每一种语言由两个小写字母表示。 ISO-3166 标准使用编码定义了国家/地区，每个国家/地区由两个大写字母表示。 下表列举了一些常见国家、地区的语言编码： 国家/地区 语言编码 国家/地区 语言编码 简体中文(中国) zh-cn 繁体中文(台湾地区) zh-tw 繁体中文(香港) zh-hk 英语(香港) en-hk 英语(美国) en-us 英语(英国) en-gb 英语(全球) en-ww 英语(加拿大) en-ca 英语(澳大利亚) en-au 英语(爱尔兰) en-ie 英语(芬兰) en-fi 芬兰语(芬兰) fi-fi 英语(丹麦) en-dk 丹麦语(丹麦) da-dk 英语(以色列) en-il 希伯来语(以色列) he-il 英语(南非) en-za 英语(印度) en-in 英语(挪威) en-no 英语(新加坡) en-sg 英语(新西兰) en-nz 英语(印度尼西亚) en-id 英语(菲律宾) en-ph 英语(泰国) en-th 英语(马来西亚) en-my 英语(阿拉伯) en-xa 韩文(韩国) ko-kr 日语(日本) ja-jp 荷兰语(荷兰) nl-nl 荷兰语(比利时) nl-be 葡萄牙语(葡萄牙) pt-pt 葡萄牙语(巴西) pt-br 法语(法国) fr-fr 法语(卢森堡) fr-lu 法语(瑞士) fr-ch 法语(比利时) fr-be 法语(加拿大) fr-ca 西班牙语(拉丁美洲) es-la 西班牙语(西班牙) es-es 西班牙语(阿根廷) es-ar 西班牙语(美国) es-us 西班牙语(墨西哥) es-mx 西班牙语(哥伦比亚) es-co 西班牙语(波多黎各) es-pr 德语(德国) de-de 德语(奥地利) de-at 德语(瑞士) de-ch 俄语(俄罗斯) ru-ru 意大利语(意大利) it-it 希腊语(希腊) el-gr 挪威语(挪威) no-no 匈牙利语(匈牙利) hu-hu 土耳其语(土耳其) tr-tr 捷克语(捷克共和国) cs-cz 斯洛文尼亚语 sl-sl 波兰语(波兰) pl-pl 瑞典语(瑞典) sv-se 注：由表中可以看出语言、国家/地区编码一般都是英文单词的缩写。 字符编码 在此处，引申一下字符编码的概念。 是不是有了语言、国家/地区编码，计算机就可以识别各种语言了？ 答案是否。作为程序员，相信每个人都会遇到过这样的情况：期望打印中文，结果输出的却是乱码。 这种情况，往往是因为字符编码的问题。 计算机在设计之初，并没有考虑多个国家，多种不同语言的应用场景。当时定义一种ASCII码，将字母、数字和其他符号编号用 7 比特的二进制数来表示。后来，计算机在世界开始普及，为了适应多种文字，出现了多种编码格式，例如中文汉字一般使用的编码格式为GB2312、GBK。 由此，又产生了一个问题，不同字符编码之间互相无法识别。于是，为了一统江湖，出现了 unicode 编码。它为每种语言的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台的文本转换需求。 有人不禁要问，既然 Unicode 可以支持所有语言的字符，那还要其他字符编码做什么？ Unicode 有一个缺点：为了支持所有语言的字符，所以它需要用更多位数去表示，比如 ASCII 表示一个英文字符只需要一个字节，而 Unicode 则需要两个字节。很明显，如果字符数多，这样的效率会很低。 为了解决这个问题，有出现了一些中间格式的字符编码：如 UTF-8、UTF-16、UTF-32 等（中国的程序员一般使用UTF-8编码）。 Java 中实现本地化 本地化的实现原理很简单： 先定义好不同语种的模板； 选择语种； 加载指定语种的模板。 接下来，本文会按照步骤逐一讲解实现本地化的具体步骤 定义不同语种的模板 Java 中将多语言文本存储在格式为 properties 的资源文件中。 它必须遵照以下的命名规范： &lt;资源名&gt;_&lt;语言代码&gt;_&lt;国家/地区编码&gt;.properties 其中，语言编码和国家/地区编码都是可选的。 注：&lt;资源名&gt;.properties 命名的本地化资源文件是默认的资源文件，即某个本地化类型在系统中找不到对应的资源文件，就采用这个默认的资源文件。 定义 properties 文件 在src/main/resources/locales 路径下定义名为 content 的不同语种资源文件： content_en_US.properties helloWorld = HelloWorld!time = The current time is %s. content_zh_CN.properties helloWorld = \u4e16\u754c\uff0c\u4f60\u597d\uff01time = \u5f53\u524d\u65f6\u95f4\u662f\u0025\u0073\u3002 可以看到：几个资源文件中，定义的 Key 完全一致，只是 Value 是对应语言的字符串。 虽然属性值各不相同，但属性名却是相同的，这样应用程序就可以通过 Locale 对象和属性名精确调用到某个具体的属性值了。 Unicode 转换工具 上一节中，我们定义的中文资源文件中的属性值都是以\u 开头的四位 16 进制数。其实，这表示的是一个 Unicode 编码。 helloWorld = \u4e16\u754c\uff0c\u4f60\u597d\uff01time = \u5f53\u524d\u65f6\u95f4\u662f\u0025\u0073\u3002 本文的字符编码中提到了，为了达到跨编码也正常显示的目的，有必要将非 ASCII 字符转为 Unicode 编码。上面的中文资源文件就是中文转为 Unicode 的结果。 怎么将非 ASCII 字符转为 Unicode 编码呢？ JDK 在 bin 目录下为我们提供了一个转换工具：native2ascii。 它可以将中文字符的资源文件转换为 Unicode 代码格式的文件，命令格式如下： native2ascii [-reverse] [-encoding 编码] [输入文件 [输出文件]] 假设content_zh_CN.properties 在 d:\ 目录。执行以下命令可以新建一个名为 content_zh_CN_new.properties 的文件，其中的内容就中文字符转为 UTF-8 编码格式的结果。 native2ascii -encoding utf-8 d:\content_zh_CN.properties d:\content_zh_CN_new.properties 选择语种 定义了多语言资源文件，第二步就是根据本地语种选择模板文件了。 Locale 在 Java 中，一个 java.util.Locale 对象表示了特定的地理、政治和文化地区。需要 Locale 来执行其任务的操作称为语言环境敏感的操作，它使用 Locale 为用户量身定制本地信息。 它有三个构造方法 Locale(String language) ：根据语言编码初始化 Locale(String language, String country) ：根据语言编码、国家编码初始化 Locale(String language, String country, String variant) ：根据语言编码、国家编码、变体初始化 此外，Locale 定义了一些常用的 Locale 常量：Locale.ENGLISH、Locale.CHINESE 等。 // 初始化一个通用英语的locale.Locale locale1 = new Locale("en");// 初始化一个加拿大英语的locale.Locale locale2 = new Locale("en", "CA");// 初始化一个美式英语变种硅谷英语的localeLocale locale3 = new Locale("en", "US", "SiliconValley");// 根据Locale常量初始化一个简体中文Locale locale4 = Locale.SIMPLIFIED_CHINESE; 加载指定语种的模板 ResourceBoundle Java 为我们提供了用于加载本地化资源文件的工具类：java.util.ResourceBoundle。 ResourceBoundle 提供了多个名为 getBundle 的静态重载方法，这些方法的作用是用来根据资源名、Locale 选择指定语种的资源文件。需要说明的是： getBundle 方法的第一个参数一般都是baseName ，这个参数表示资源文件名。 ResourceBoundle 还提供了名为 getString 的方法，用来获取资源文件中 key 对应的 value。 public static void main(String[] args) &#123; // 根据语言+地区编码初始化 ResourceBundle rbUS = ResourceBundle.getBundle("locales.content", new Locale("en", "US")); // 根据Locale常量初始化 ResourceBundle rbZhCN = ResourceBundle.getBundle("locales.content", Locale.SIMPLIFIED_CHINESE); // 获取本地系统默认的Locale初始化 ResourceBundle rbDefault = ResourceBundle.getBundle("locales.content"); // ResourceBundle rbDefault =ResourceBundle.getBundle("locales.content", Locale.getDefault()); // 与上行代码等价 System.out.println("us-US:" + rbUS.getString("helloWorld")); System.out.println("us-US:" + String.format(rbUS.getString("time"), "08:00")); System.out.println("zh-CN：" + rbZhCN.getString("helloWorld")); System.out.println("zh-CN：" + String.format(rbZhCN.getString("time"), "08:00")); System.out.println("default：" + rbDefault.getString("helloWorld")); System.out.println("default：" + String.format(rbDefault.getString("time"), "08:00"));&#125; 输出 us-US:HelloWorld!us-US:The current time is 08:00.zh-CN：世界，你好！zh-CN：当前时间是08:00。default：世界，你好！default：当前时间是08:00。 注：在加载资源时，如果指定的本地化资源文件不存在，它会尝试按下面的顺序加载其他的资源：本地系统默认本地化对象对应的资源 -&gt; 默认的资源。如果指定错误，Java 会提示找不到资源文件。 支持本地化的工具类 Java 中也提供了几个支持本地化的格式化工具类。例如：NumberFormat、DateFormat、MessageFormat NumberFormat NumberFormat 是所有数字格式类的基类。它提供格式化和解析数字的接口。它也提供了决定数字所属语言类型的方法。 public static void main(String[] args) &#123; double num = 123456.78; NumberFormat format = NumberFormat.getCurrencyInstance(Locale.SIMPLIFIED_CHINESE); System.out.format("%f 的本地化（%s）结果: %s", num, Locale.SIMPLIFIED_CHINESE, format.format(num));&#125; DateFormat DateFormat 是日期、时间格式化类的抽象类。它支持基于语言习惯的日期、时间格式。 public static void main(String[] args) &#123; Date date = new Date(); DateFormat df = DateFormat.getDateInstance(DateFormat.MEDIUM, Locale.ENGLISH); DateFormat df2 = DateFormat.getDateInstance(DateFormat.MEDIUM, Locale.SIMPLIFIED_CHINESE); System.out.format("%s 的本地化（%s）结果: %s\n", date, Locale.SIMPLIFIED_CHINESE, df.format(date)); System.out.format("%s 的本地化（%s）结果: %s\n", date, Locale.SIMPLIFIED_CHINESE, df2.format(date));&#125; MessageFormat Messageformat 提供一种与语言无关的拼接消息的方式。通过这种拼接方式，将最终呈现返回给使用者。 public static void main(String[] args) &#123; String pattern1 = "&#123;0&#125;，你好！你于 &#123;1&#125; 消费 &#123;2&#125; 元。"; String pattern2 = "At &#123;1,time,short&#125; On &#123;1,date,long&#125;，&#123;0&#125; paid &#123;2,number, currency&#125;."; Object[] params = &#123;"Jack", new GregorianCalendar().getTime(), 8888&#125;; String msg1 = MessageFormat.format(pattern1, params); MessageFormat mf = new MessageFormat(pattern2, Locale.US); String msg2 = mf.format(params); System.out.println(msg1); System.out.println(msg2);&#125;]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>advanced</tag>
        <tag>locale</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络协议之 HTTP]]></title>
    <url>%2Fblog%2F2016%2F01%2F08%2Fcommunication%2Fhttp%2F</url>
    <content type="text"><![CDATA[网络协议之 HTTP 📓 本文已归档到：「blog」 HTTP 是什么？ 实例 工作原理 特点 客户端请求消息 服务器响应消息 HTTP 状态码 更多内容 HTTP 是什么？ HTTP（HyperText Transfer Protocol，超文本传输协议）是 WWW (World Wide Web)实现数据通信的基石。 HTTP 是由 IETF(Internet Engineering Task Force，互联网工程工作小组) 和 W3C(World Wide Web Consortium，万维网协会) 共同合作制订的，它们发布了一系列的RFC(Request For Comments)，其中最著名的是 RFC 2616，它定义了HTTP /1.1。 它是一种应用层协议（OSI 七层模型的最顶层），它基于 TCP/IP 通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。 实例 如果你学习过计算机网络，熟悉 OSI 模型，那么你可以跳过这个实例了。 而不了解 OSI 模型的朋友，不妨通过一个实例来对 HTTP 报文有一个感性的认识。 以下是使用 wireshark 抓取的一个实际访问百度首页的 HTTP GET 报文： 可以清楚的看到它的层级结构如下图，经过了层层的包装。 工作原理 HTTP 工作于 Client/Server 模型上。 客户端和服务器之间的通信采用 request/response 机制。 客户端是终端（可以是浏览器、爬虫程序等），服务器是网站的 Web 服务器。 一次 HTTP 操作称为一个事务，其工作过程大致可分为四步： 建立连接 - 首先，客户端和服务器需要建立一个到服务器指定端口（默认端口号为 80）的 TCP 连接（注：虽然 HTTP 采用 TCP 连接是最流行的方式，但是 RFC 并没有指定一定要采用这种网络传输方式。）。 发送请求信息 - 客户端向服务器发送请求。请求方式的格式为，统一资源标识符、协议版本号，后边是 MIME 信息包括请求修饰符 发送响应信息 - 服务器监听指定接口是否收到请求，一旦发现请求，处理后，返回响应结果给客户端。其格式为一个状态行包括信息的协议版本号、一个成功或错误的代码，后边是 MIME 信息包括服务器信息、实体信息和可能的内容。 关闭连接 - 客户端根据响应，显示结果给用户，最后关闭连接。 特点 无连接的 - 无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。 无状态的 - HTTP 协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。 媒体独立的 - 这意味着，只要客户端和服务器知道如何处理的数据内容，任何类型的数据都可以通过 HTTP 发送。客户端以及服务器指定使用适合的 MIME-type 内容类型。 C/S 模型的 - 基于 Client/Server 模型工作。 HTTP 消息结构 HTTP 是基于客户端/服务端（C/S）的架构模型，通过一个可靠的链接来交换信息，是一个无状态的请求/响应协议。 一个 HTTP&quot;客户端&quot;是一个应用程序（Web 浏览器或其他任何客户端），通过连接到服务器达到向服务器发送一个或多个 HTTP 的请求的目的。 一个 HTTP&quot;服务器&quot;同样也是一个应用程序（通常是一个 Web 服务，如 Apache Web 服务器或 IIS 服务器等），通过接收客户端的请求并向客户端发送 HTTP 响应数据。 HTTP 使用统一资源标识符（Uniform Resource Identifiers, URI）来传输数据和建立连接。 一旦建立连接后，数据消息就通过类似 Internet 邮件所使用的格式[RFC5322]和多用途 Internet 邮件扩展（MIME）[RFC2045]来传送。 客户端请求消息 客户端发送一个 HTTP 请求到服务器的请求消息包括以下格式：请求行（request line）、请求头部（header）、空行和请求数据四个部分组成，下图给出了请求报文的一般格式。 服务器响应消息 HTTP 响应也由四个部分组成，分别是：状态行、消息报头、空行和响应正文。 HTTP 请求 根据 HTTP 标准，HTTP 请求可以使用多种请求方法。 HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法。 HTTP1.1新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT方法。 方法 描述 GET 请求指定的页面信息，并返回实体主体。 HEAD 类似于 get 请求，只不过返回的响应中没有具体的内容，用于获取报头 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE 请求服务器删除指定的页面。 CONNECT HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。 OPTIONS 允许客户端查看服务器的性能。 TRACE 回显服务器收到的请求，主要用于测试或诊断。 HTTP 请求消息头 请求消息头 说明 Accept 浏览器支持的格式 Accept-Encoding 支持的编码格式，如（UTF-8，GBK） Accept-Language 支持的语言类型 User-Agent 浏览器信息 Cookie 记录的是用户当前的状态 Referer 指从哪个页面单击链接进入的页面 HOST 目的地址对应的主机名 Connection 连接类型。如 Keep-Alive 表示长连接，不会断开 Content-Length 内容长度 Content-Type 内容类型 HTTP 响应 响应消息头 说明 Allow 服务器支持哪些请求方法（如 GET、POST 等）。 Content-Encoding 文档的编码（Encode）方法。只有在解码之后才可以得到 Content-Type 头指定的内容类型。利用 gzip 压缩文档能够显著地减少 HTML 文档的下载时间。Java 的 GZIPOutputStream 可以很方便地进行 gzip 压缩，但只有 Unix 上的 Netscape 和 Windows 上的 IE 4、IE 5 才支持它。因此，Servlet 应该通过查看 Accept-Encoding 头（即 request.getHeader(“Accept-Encoding”)）检查浏览器是否支持 gzip，为支持 gzip 的浏览器返回经 gzip 压缩的 HTML 页面，为其他浏览器返回普通页面。 Content-Length 表示内容长度。只有当浏览器使用持久 HTTP 连接时才需要这个数据。如果你想要利用持久连接的优势，可以把输出文档写入 ByteArrayOutputStram，完成后查看其大小，然后把该值放入 Content-Length 头，最后通过byteArrayStream.writeTo(response.getOutputStream() 发送内容。 Content-Type 表示后面的文档属于什么 MIME 类型。Servlet 默认为 text/plain，但通常需要显式地指定为 text/html。由于经常要设置 Content-Type，因此 HttpServletResponse 提供了一个专用的方法 setContentType。 Date 当前的 GMT 时间。你可以用 setDateHeader 来设置这个头以避免转换时间格式的麻烦。 Expires 应该在什么时候认为文档已经过期，从而不再缓存它？ Last-Modified 文档的最后改动时间。客户可以通过 If-Modified-Since 请求头提供一个日期，该请求将被视为一个条件 GET，只有改动时间迟于指定时间的文档才会返回，否则返回一个 304（Not Modified）状态。Last-Modified 也可用 setDateHeader 方法来设置。 Location 表示客户应当到哪里去提取文档。Location 通常不是直接设置的，而是通过 HttpServletResponse 的 sendRedirect 方法，该方法同时设置状态代码为 302。 Refresh 表示浏览器应该在多少时间之后刷新文档，以秒计。除了刷新当前文档之外，你还可以通过 response.setHeader(&quot;Refresh&quot;, &quot;5;URL=http://host/path&quot;)让浏览器读取指定的页面。 注意这种功能通常是通过设置 HTML 页面 HEAD 区的 &lt;META HTTP-EQUIV=&quot;Refresh&quot; CONTENT=&quot;5;URL=http://host/path&quot;&gt;实现，这是因为，自动刷新或重定向对于那些不能使用 CGI 或 Servlet 的 HTML 编写者十分重要。但是，对于 Servlet 来说，直接设置 Refresh 头更加方便。 注意 Refresh 的意义是&quot;N 秒之后刷新本页面或访问指定页面&quot;，而不是&quot;每隔 N 秒刷新本页面或访问指定页面&quot;。因此，连续刷新要求每次都发送一个 Refresh 头，而发送 204 状态代码则可以阻止浏览器继续刷新，不管是使用 Refresh 头还是 &lt;META HTTP-EQUIV=&quot;Refresh&quot; ...&gt;。 注意 Refresh 头不属于 HTTP 1.1 正式规范的一部分，而是一个扩展，但 Netscape 和 IE 都支持它。 Server 服务器名字。Servlet 一般不设置这个值，而是由 Web 服务器自己设置。 Set-Cookie 设置和页面关联的 Cookie。Servlet 不应使用response.setHeader(&quot;Set-Cookie&quot;, ...)，而是应使用 HttpServletResponse 提供的专用方法 addCookie。参见下文有关 Cookie 设置的讨论。 WWW-Authenticate 客户应该在 Authorization 头中提供什么类型的授权信息？在包含 401（Unauthorized）状态行的应答中这个头是必需的。例如，response.setHeader(&quot;WWW-Authenticate&quot;, &quot;BASIC realm=＼&quot;executives＼&quot;&quot;)。 注意 Servlet 一般不进行这方面的处理，而是让 Web 服务器的专门机制来控制受密码保护页面的访问（例如.htaccess）。 HTTP 状态码 当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含 HTTP 状态码的信息头（server header）用以响应浏览器的请求。 HTTP 状态码的英文为 HTTP Status Code。 下面是常见的 HTTP 状态码： 200 - 请求成功 301 - 资源（网页等）被永久转移到其它 URL 404 - 请求的资源（网页等）不存在 500 - 内部服务器错误 HTTP 状态码分类 HTTP 状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，后两个数字没有分类的作用。HTTP 状态码共分为 5 种类型： 分类 分类描述 1 信息，服务器收到请求，需要请求者继续执行操作 2 成功，操作被成功接收并处理 3 重定向，需要进一步的操作以完成请求 4 客户端错误，请求包含语法错误或无法完成请求 5 服务器错误，服务器在处理请求的过程中发生了错误 HTTP 状态列表： 状态码 状态码英文名称 100 Continue 101 Switching Protocols 200 OK 201 Created 202 Accepted 203 Non-Authoritative Information 204 No Content 205 Reset Content 206 Partial Content 300 Multiple Choices 301 Moved Permanently 302 Found 303 See Other 304 Not Modified 305 Use Proxy 306 Unused 307 Temporary Redirect 400 Bad Request 401 Unauthorized 402 Payment Required 403 Forbidden 404 Not Found 405 Method Not Allowed 406 Not Acceptable 407 Proxy Authentication Required 408 Request Time-out 409 Conflict 410 Gone 411 Length Required 412 Precondition Failed 413 Request Entity Too Large 414 Request-URI Too Large 415 Unsupported Media Type 416 Requested range not satisfiable 417 Expectation Failed 500 Internal Server Error 501 Not Implemented 502 Bad Gateway 503 Service Unavailable 504 Gateway Time-out 505 HTTP Version not supported 更多内容 http://blog.csdn.net/gueter/article/details/1524447 http://www.runoob.com/http/http-intro.html https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol]]></content>
      <categories>
        <category>communication</category>
      </categories>
      <tags>
        <tag>communication</tag>
        <tag>network</tag>
        <tag>application</tag>
        <tag>protocol</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 控制语句]]></title>
    <url>%2Fblog%2F2015%2F05%2F28%2Fjava%2Fjavacore%2Fbasics%2FJava%E6%8E%A7%E5%88%B6%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[Java 控制语句 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「javacore」 选择语句 if 语句 if…else 语句 if…else if…else 语句 嵌套的 if…else 语句 switch 语句 循环语句 while 循环 do while 循环 for 循环 foreach 循环 中断语句 break 关键字 continue 关键字 return 关键字 最佳实践 小结 参考资料 Java 控制语句大致可分为三大类： 选择语句 if, else-if, else switch 循环语句 while do…while for foreach 终端语句 break continue return 选择语句 if 语句 if 语句会判断括号中的条件是否成立，如果成立则执行 if 语句中的代码块，否则跳过代码块继续执行。 语法 if(布尔表达式) &#123; //如果布尔表达式为true将执行的语句&#125; 示例 public class IfDemo &#123; public static void main(String args[]) &#123; int x = 10; if (x &lt; 20) &#123; System.out.print("这是 if 语句"); &#125; &#125;&#125;// output:// 这是 if 语句 if…else 语句 if 语句后面可以跟 else 语句，当 if 语句的布尔表达式值为 false 时，else 语句块会被执行。 语法 if(布尔表达式) &#123; //如果布尔表达式的值为true&#125; else &#123; //如果布尔表达式的值为false&#125; 示例 public class IfElseDemo &#123; public static void main(String args[]) &#123; int x = 30; if (x &lt; 20) &#123; System.out.print("这是 if 语句"); &#125; else &#123; System.out.print("这是 else 语句"); &#125; &#125;&#125;// output:// 这是 else 语句 if…else if…else 语句 if 语句至多有 1 个 else 语句，else 语句在所有的 else if 语句之后。 If 语句可以有若干个 else if 语句，它们必须在 else 语句之前。 一旦其中一个 else if 语句检测为 true，其他的 else if 以及 else 语句都将跳过执行。 语法 if (布尔表达式 1) &#123; //如果布尔表达式 1的值为true执行代码&#125; else if (布尔表达式 2) &#123; //如果布尔表达式 2的值为true执行代码&#125; else if (布尔表达式 3) &#123; //如果布尔表达式 3的值为true执行代码&#125; else &#123; //如果以上布尔表达式都不为true执行代码&#125; 示例 public class IfElseifElseDemo &#123; public static void main(String args[]) &#123; int x = 3; if (x == 1) &#123; System.out.print("Value of X is 1"); &#125; else if (x == 2) &#123; System.out.print("Value of X is 2"); &#125; else if (x == 3) &#123; System.out.print("Value of X is 3"); &#125; else &#123; System.out.print("This is else statement"); &#125; &#125;&#125;// output:// Value of X is 3 嵌套的 if…else 语句 使用嵌套的 if else 语句是合法的。也就是说你可以在另一个 if 或者 else if 语句中使用 if 或者 else if 语句。 语法 if (布尔表达式 1) &#123; ////如果布尔表达式 1的值为true执行代码 if (布尔表达式 2) &#123; ////如果布尔表达式 2的值为true执行代码 &#125;&#125; 示例 public class IfNestDemo &#123; public static void main(String args[]) &#123; int x = 30; int y = 10; if (x == 30) &#123; if (y == 10) &#123; System.out.print("X = 30 and Y = 10"); &#125; &#125; &#125;&#125;// output:// X = 30 and Y = 10 switch 语句 switch 语句判断一个变量与一系列值中某个值是否相等，每个值称为一个分支。 switch 语句有如下规则： switch 语句中的变量类型只能为 byte、short、int、char 或者 String。 switch 语句可以拥有多个 case 语句。每个 case 后面跟一个要比较的值和冒号。 case 语句中的值的数据类型必须与变量的数据类型相同，而且只能是常量或者字面常量。 当变量的值与 case 语句的值相等时，那么 case 语句之后的语句开始执行，直到 break 语句出现才会跳出 switch 语句。 当遇到 break 语句时，switch 语句终止。程序跳转到 switch 语句后面的语句执行。case 语句不必须要包含 break 语句。如果没有 break 语句出现，程序会继续执行下一条 case 语句，直到出现 break 语句。 switch 语句可以包含一个 default 分支，该分支必须是 switch 语句的最后一个分支。default 在没有 case 语句的值和变量值相等的时候执行。default 分支不需要 break 语句。 语法 switch(expression)&#123; case value : //语句 break; //可选 case value : //语句 break; //可选 //你可以有任意数量的case语句 default : //可选 //语句 break; //可选，但一般建议加上&#125; 示例 public class SwitchDemo &#123; public static void main(String args[]) &#123; char grade = 'C'; switch (grade) &#123; case 'A': System.out.println("Excellent!"); break; case 'B': case 'C': System.out.println("Well done"); break; case 'D': System.out.println("You passed"); case 'F': System.out.println("Better try again"); break; default: System.out.println("Invalid grade"); break; &#125; System.out.println("Your grade is " + grade); &#125;&#125;// output:// Well done// Your grade is C 循环语句 while 循环 只要布尔表达式为 true，while 循环体会一直执行下去。 语法 while( 布尔表达式 ) &#123; //循环内容&#125; 示例 public class WhileDemo &#123; public static void main(String args[]) &#123; int x = 10; while (x &lt; 20) &#123; System.out.print("value of x : " + x); x++; System.out.print("\n"); &#125; &#125;&#125;// output:// value of x : 10// value of x : 11// value of x : 12// value of x : 13// value of x : 14// value of x : 15// value of x : 16// value of x : 17// value of x : 18// value of x : 19 do while 循环 对于 while 语句而言，如果不满足条件，则不能进入循环。但有时候我们需要即使不满足条件，也至少执行一次。 do while 循环和 while 循环相似，不同的是，do while 循环至少会执行一次。 语法 do &#123; //代码语句&#125; while (布尔表达式); 布尔表达式在循环体的后面，所以语句块在检测布尔表达式之前已经执行了。 如果布尔表达式的值为 true，则语句块一直执行，直到布尔表达式的值为 false。 示例 public class DoWhileDemo &#123; public static void main(String args[]) &#123; int x = 10; do &#123; System.out.print("value of x : " + x); x++; System.out.print("\n"); &#125; while (x &lt; 20); &#125;&#125;// output:// value of x:10// value of x:11// value of x:12// value of x:13// value of x:14// value of x:15// value of x:16// value of x:17// value of x:18// value of x:19 for 循环 虽然所有循环结构都可以用 while 或者 do while 表示，但 Java 提供了另一种语句 —— for 循环，使一些循环结构变得更加简单。 for 循环执行的次数是在执行前就确定的。 语法 for (初始化; 布尔表达式; 更新) &#123; //代码语句&#125; 最先执行初始化步骤。可以声明一种类型，但可初始化一个或多个循环控制变量，也可以是空语句。 然后，检测布尔表达式的值。如果为 true，循环体被执行。如果为 false，循环终止，开始执行循环体后面的语句。 执行一次循环后，更新循环控制变量。 再次检测布尔表达式。循环执行上面的过程。 示例 public class ForDemo &#123; public static void main(String args[]) &#123; for (int x = 10; x &lt; 20; x = x + 1) &#123; System.out.print("value of x : " + x); System.out.print("\n"); &#125; &#125;&#125;// output:// value of x : 10// value of x : 11// value of x : 12// value of x : 13// value of x : 14// value of x : 15// value of x : 16// value of x : 17// value of x : 18// value of x : 19 foreach 循环 Java5 引入了一种主要用于数组的增强型 for 循环。 语法 for (声明语句 : 表达式) &#123; //代码句子&#125; 声明语句：声明新的局部变量，该变量的类型必须和数组元素的类型匹配。其作用域限定在循环语句块，其值与此时数组元素的值相等。 表达式：表达式是要访问的数组名，或者是返回值为数组的方法。 示例 public class ForeachDemo &#123; public static void main(String args[]) &#123; int[] numbers = &#123; 10, 20, 30, 40, 50 &#125;; for (int x : numbers) &#123; System.out.print(x); System.out.print(","); &#125; System.out.print("\n"); String[] names = &#123; "James", "Larry", "Tom", "Lacy" &#125;; for (String name : names) &#123; System.out.print(name); System.out.print(","); &#125; &#125;&#125;// output:// 10,20,30,40,50,// James,Larry,Tom,Lacy, 中断语句 break 关键字 break 主要用在循环语句或者 switch 语句中，用来跳出整个语句块。 break 跳出最里层的循环，并且继续执行该循环下面的语句。 示例 public class BreakDemo &#123; public static void main(String args[]) &#123; int[] numbers = &#123; 10, 20, 30, 40, 50 &#125;; for (int x : numbers) &#123; if (x == 30) &#123; break; &#125; System.out.print(x); System.out.print("\n"); &#125; System.out.println("break 示例结束"); &#125;&#125;// output:// 10// 20// break 示例结束 continue 关键字 continue 适用于任何循环控制结构中。作用是让程序立刻跳转到下一次循环的迭代。在 for 循环中，continue 语句使程序立即跳转到更新语句。在 while 或者 do while 循环中，程序立即跳转到布尔表达式的判断语句。 示例 public class ContinueDemo &#123; public static void main(String args[]) &#123; int[] numbers = &#123; 10, 20, 30, 40, 50 &#125;; for (int x : numbers) &#123; if (x == 30) &#123; continue; &#125; System.out.print(x); System.out.print("\n"); &#125; &#125;&#125;// output:// 10// 20// 40// 50 return 关键字 跳出整个函数体，函数体后面的部分不再执行。 示例 public class ReturnDemo &#123; public static void main(String args[]) &#123; int[] numbers = &#123; 10, 20, 30, 40, 50 &#125;; for (int x : numbers) &#123; if (x == 30) &#123; return; &#125; System.out.print(x); System.out.print("\n"); &#125; System.out.println("return 示例结束"); &#125;&#125;// output:// 10// 20 注意：请仔细体会一下 return 和 break 的区别。 最佳实践 选择分支特别多的情况下，switch 语句优于 if...else if...else 语句。 switch 语句不要吝啬使用 default。 switch 语句中的 default 要放在最后。 foreach 循环优先于传统的 for 循环 不要循环遍历容器元素，然后删除特定元素。正确姿势应该是遍历容器的迭代器（Iterator），删除元素。 小结 参考资料 Java 编程思想 JAVA 核心技术（卷 1）]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
        <tag>datatype</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 异常]]></title>
    <url>%2Fblog%2F2015%2F04%2F25%2Fjava%2Fjavacore%2Fbasics%2FJava%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[深入理解 Java 异常 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「javacore」 异常框架 Throwable Error Exception RuntimeException 自定义异常 抛出异常 捕获异常 异常链 异常注意事项 finally 覆盖异常 覆盖抛出异常的方法 异常和线程 最佳实践 小结 参考资料 异常框架 Throwable Throwable 是 Java 语言中所有错误（Error）和异常（Exception）的超类。 Throwable 包含了其线程创建时线程执行堆栈的快照，它提供了 printStackTrace() 等接口用于获取堆栈跟踪数据等信息。 主要方法： fillInStackTrace - 用当前的调用栈层次填充 Throwable 对象栈层次，添加到栈层次任何先前信息中。 getMessage - 返回关于发生的异常的详细信息。这个消息在 Throwable 类的构造函数中初始化了。 getCause - 返回一个 Throwable 对象代表异常原因。 getStackTrace - 返回一个包含堆栈层次的数组。下标为 0 的元素代表栈顶，最后一个元素代表方法调用堆栈的栈底。 printStackTrace - 打印 toString() 结果和栈层次到 System.err，即错误输出流。 toString - 使用 getMessage 的结果返回代表 Throwable 对象的字符串。 Error Error 是 Throwable 的一个子类。Error 表示合理的应用程序不应该尝试捕获的严重问题。大多数此类错误都是异常情况。编译器不会检查 Error。 常见 Error： AssertionError - 断言错误。 VirtualMachineError - 虚拟机错误。 UnsupportedClassVersionError - Java 类版本错误。 StackOverflowError - 栈溢出错误。 OutOfMemoryError - 内存溢出错误。 Exception Exception 是 Throwable 的一个子类。Exception 表示合理的应用程序可能想要捕获的条件。 **编译器会检查 Exception 异常。**此类异常，要么通过 throws 进行声明抛出，要么通过 try catch 进行捕获处理，否则不能通过编译。 常见 Exception： ClassNotFoundException - 应用程序试图加载类时，找不到相应的类，抛出该异常。 CloneNotSupportedException - 当调用 Object 类中的 clone 方法克隆对象，但该对象的类无法实现 Cloneable 接口时，抛出该异常。 IllegalAccessException - 拒绝访问一个类的时候，抛出该异常。 InstantiationException - 当试图使用 Class 类中的 newInstance 方法创建一个类的实例，而指定的类对象因为是一个接口或是一个抽象类而无法实例化时，抛出该异常。 InterruptedException - 一个线程被另一个线程中断，抛出该异常。 NoSuchFieldException - 请求的变量不存在。 NoSuchMethodException - 请求的方法不存在。 示例： public class ExceptionDemo &#123; public static void main(String[] args) &#123; Method method = String.class.getMethod("toString", int.class); &#125;&#125;; 试图编译运行时会报错： Error:(7, 47) java: 未报告的异常错误java.lang.NoSuchMethodException; 必须对其进行捕获或声明以便抛出 RuntimeException RuntimeException 是 Exception 的一个子类。RuntimeException 是那些可能在 Java 虚拟机正常运行期间抛出的异常的超类。 **编译器不会检查 RuntimeException 异常。**当程序中可能出现这类异常时，倘若既没有通过 throws 声明抛出它，也没有用 try catch 语句捕获它，程序还是会编译通过。 示例： public class RuntimeExceptionDemo &#123; public static void main(String[] args) &#123; // 此处产生了异常 int result = 10 / 0; System.out.println("两个数字相除的结果：" + result); System.out.println("----------------------------"); &#125;&#125;; 运行时输出： Exception in thread "main" java.lang.ArithmeticException: / by zero at io.github.dunwu.javacore.exception.RumtimeExceptionDemo01.main(RumtimeExceptionDemo01.java:6) 常见 RuntimeException： ArrayIndexOutOfBoundsException - 用非法索引访问数组时抛出的异常。如果索引为负或大于等于数组大小，则该索引为非法索引。 ArrayStoreException - 试图将错误类型的对象存储到一个对象数组时抛出的异常。 ClassCastException - 当试图将对象强制转换为不是实例的子类时，抛出该异常。 IllegalArgumentException - 抛出的异常表明向方法传递了一个不合法或不正确的参数。 IllegalMonitorStateException - 抛出的异常表明某一线程已经试图等待对象的监视器，或者试图通知其他正在等待对象的监视器而本身没有指定监视器的线程。 IllegalStateException - 在非法或不适当的时间调用方法时产生的信号。换句话说，即 Java 环境或 Java 应用程序没有处于请求操作所要求的适当状态下。 IllegalThreadStateException - 线程没有处于请求操作所要求的适当状态时抛出的异常。 IndexOutOfBoundsException - 指示某排序索引（例如对数组、字符串或向量的排序）超出范围时抛出。 NegativeArraySizeException - 如果应用程序试图创建大小为负的数组，则抛出该异常。 NullPointerException - 当应用程序试图在需要对象的地方使用 null 时，抛出该异常 NumberFormatException - 当应用程序试图将字符串转换成一种数值类型，但该字符串不能转换为适当格式时，抛出该异常。 SecurityException - 由安全管理器抛出的异常，指示存在安全侵犯。 StringIndexOutOfBoundsException - 此异常由 String 方法抛出，指示索引或者为负，或者超出字符串的大小。 UnsupportedOperationException - 当不支持请求的操作时，抛出该异常。 自定义异常 自定义一个异常类，只需要继承 Exception 或 RuntimeException 即可。 示例： public class MyExceptionDemo &#123; public static void main(String[] args) &#123; throw new MyException("自定义异常"); &#125; static class MyException extends RuntimeException &#123; public MyException(String message) &#123; super(message); &#125; &#125;&#125; 输出： Exception in thread "main" io.github.dunwu.javacore.exception.MyExceptionDemo$MyException: 自定义异常 at io.github.dunwu.javacore.exception.MyExceptionDemo.main(MyExceptionDemo.java:9) 抛出异常 如果想在程序中明确地抛出异常，需要用到 throw 和 throws 。 如果一个方法没有捕获一个检查性异常，那么该方法必须使用 throws 关键字来声明。throws 关键字放在方法签名的尾部。 throw 示例： public class ThrowDemo &#123; public static void f() &#123; try &#123; throw new RuntimeException("抛出一个异常"); &#125; catch (Exception e) &#123; System.out.println(e); &#125; &#125; public static void main(String[] args) &#123; f(); &#125;&#125;; 输出： java.lang.RuntimeException: 抛出一个异常 也可以使用 throw 关键字抛出一个异常，无论它是新实例化的还是刚捕获到的。 throws 示例： public class ThrowsDemo &#123; public static void f1() throws NoSuchMethodException, NoSuchFieldException &#123; Field field = Integer.class.getDeclaredField("digits"); if (field != null) &#123; System.out.println("反射获取 digits 方法成功"); &#125; Method method = String.class.getMethod("toString", int.class); if (method != null) &#123; System.out.println("反射获取 toString 方法成功"); &#125; &#125; public static void f2() &#123; try &#123; // 调用 f1 处，如果不用 try catch ，编译时会报错 f1(); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; catch (NoSuchFieldException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; f2(); &#125;&#125;; 输出： 反射获取 digits 方法成功java.lang.NoSuchMethodException: java.lang.String.toString(int) at java.lang.Class.getMethod(Class.java:1786) at io.github.dunwu.javacore.exception.ThrowsDemo.f1(ThrowsDemo.java:12) at io.github.dunwu.javacore.exception.ThrowsDemo.f2(ThrowsDemo.java:21) at io.github.dunwu.javacore.exception.ThrowsDemo.main(ThrowsDemo.java:30) throw 和 throws 的区别： throws 使用在函数上，throw 使用在函数内。 throws 后面跟异常类，可以跟多个，用逗号区别；throw 后面跟的是异常对象。 捕获异常 使用 try 和 catch 关键字可以捕获异常。try catch 代码块放在异常可能发生的地方。 它的语法形式如下： try &#123; // 可能会发生异常的代码块&#125; catch (Exception e1) &#123; // 捕获并处理try抛出的异常类型Exception&#125; catch (Exception2 e2) &#123; // 捕获并处理try抛出的异常类型Exception2&#125; finally &#123; // 无论是否发生异常，都将执行的代码块&#125; 此外，JDK7 以后，catch 多种异常时，也可以像下面这样简化代码： try &#123; // 可能会发生异常的代码块&#125; catch (Exception | Exception2 e) &#123; // 捕获并处理try抛出的异常类型&#125; finally &#123; // 无论是否发生异常，都将执行的代码块&#125; try - try 语句用于监听。将要被监听的代码(可能抛出异常的代码)放在 try 语句块之内，当 try 语句块内发生异常时，异常就被抛出。 catch - catch 语句包含要捕获异常类型的声明。当保护代码块中发生一个异常时，try 后面的 catch 块就会被检查。 finally - finally 语句块总是会被执行，无论是否出现异常。try catch 语句后不一定非要finally 语句。finally 常用于这样的场景：由于finally 语句块总是会被执行，所以那些在 try 代码块中打开的，并且必须回收的物理资源(如数据库连接、网络连接和文件)，一般会放在finally 语句块中释放资源。 try、catch、finally 三个代码块中的局部变量不可共享使用。 catch 块尝试捕获异常时，是按照 catch 块的声明顺序从上往下寻找的，一旦匹配，就不会再向下执行。因此，如果同一个 try 块下的多个 catch 异常类型有父子关系，应该将子类异常放在前面，父类异常放在后面。 示例： public class TryCatchFinallyDemo &#123; public static void main(String[] args) &#123; try &#123; // 此处产生了异常 int temp = 10 / 0; System.out.println("两个数字相除的结果：" + temp); System.out.println("----------------------------"); &#125; catch (ArithmeticException e) &#123; System.out.println("出现异常了：" + e); &#125; finally &#123; System.out.println("不管是否出现异常，都执行此代码"); &#125; &#125;&#125;; 运行时输出： 出现异常了：java.lang.ArithmeticException: / by zero不管是否出现异常，都执行此代码 异常链 异常链是以一个异常对象为参数构造新的异常对象，新的异常对象将包含先前异常的信息。 通过使用异常链，我们可以提高代码的可理解性、系统的可维护性和友好性。 我们有两种方式处理异常，一是 throws 抛出交给上级处理，二是 try…catch 做具体处理。try…catch 的 catch 块我们可以不需要做任何处理，仅仅只用 throw 这个关键字将我们封装异常信息主动抛出来。然后在通过关键字 throws 继续抛出该方法异常。它的上层也可以做这样的处理，以此类推就会产生一条由异常构成的异常链。 示例： public class ExceptionChainDemo &#123; static class MyException1 extends Exception &#123; public MyException1(String message) &#123; super(message); &#125; &#125; static class MyException2 extends Exception &#123; public MyException2(String message, Throwable cause) &#123; super(message, cause); &#125; &#125; public static void f1() throws MyException1 &#123; throw new MyException1("出现 MyException1"); &#125; public static void f2() throws MyException2 &#123; try &#123; f1(); &#125; catch (MyException1 e) &#123; throw new MyException2("出现 MyException2", e); &#125; &#125; public static void main(String[] args) throws MyException2 &#123; f2(); &#125;&#125; 输出： Exception in thread "main" io.github.dunwu.javacore.exception.ExceptionChainDemo$MyException2: 出现 MyException2 at io.github.dunwu.javacore.exception.ExceptionChainDemo.f2(ExceptionChainDemo.java:29) at io.github.dunwu.javacore.exception.ExceptionChainDemo.main(ExceptionChainDemo.java:34)Caused by: io.github.dunwu.javacore.exception.ExceptionChainDemo$MyException1: 出现 MyException1 at io.github.dunwu.javacore.exception.ExceptionChainDemo.f1(ExceptionChainDemo.java:22) at io.github.dunwu.javacore.exception.ExceptionChainDemo.f2(ExceptionChainDemo.java:27) ... 1 more 扩展阅读：https://juejin.im/post/5b6d61e55188251b38129f9a#heading-10 这篇文章中对于异常链讲解比较详细。 异常注意事项 finally 覆盖异常 Java 异常处理中 finally 中的 return 会覆盖 catch 代码块中的 return 语句和 throw 语句，所以 Java 不建议在 finally 中使用 return 语句。 此外 finally 中的 throw 语句也会覆盖 catch 代码块中的 return 语句和 throw 语句。 示例： public class FinallyOverrideExceptionDemo &#123; static void f() throws Exception &#123; try &#123; throw new Exception("A"); &#125; catch (Exception e) &#123; throw new Exception("B"); &#125; finally &#123; throw new Exception("C"); &#125; &#125; public static void main(String[] args) &#123; try &#123; f(); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125;&#125; 输出：C 覆盖抛出异常的方法 当子类重写父类带有 throws 声明的函数时，其 throws 声明的异常必须在父类异常的可控范围内——用于处理父类的 throws 方法的异常处理器，必须也适用于子类的这个带 throws 方法 。这是为了支持多态。 示例： public class ExceptionOverrideDemo &#123; static class Father &#123; public void start() throws IOException &#123; throw new IOException(); &#125; &#125; static class Son extends Father &#123; @Override public void start() throws SQLException &#123; throw new SQLException(); &#125; &#125; public static void main(String[] args) &#123; Father obj1 = new Father(); Father obj2 = new Son(); try &#123; obj1.start(); obj2.start(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 上面的示例编译时会报错，原因在于： 因为 Son 类抛出异常的实质是 SQLException，而 IOException 无法处理它。那么这里的 try catch 就不能处理 Son 中的异常了。多态就不能实现了。 异常和线程 如果 Java 程序只有一个线程，那么没有被任何代码处理的异常会导致程序终止。如果 Java 程序是多线程的，那么没有被任何代码处理的异常仅仅会导致异常所在的线程结束。 最佳实践 对可恢复的情况使用检查性异常（Exception），对编程错误使用运行时异常（RuntimeException） 优先使用 Java 标准的异常 抛出与抽象相对应的异常 在细节消息中包含能捕获失败的信息 尽可能减少 try 代码块的大小 尽量缩小异常范围。例如，如果明知尝试捕获的是一个 ArithmeticException，就应该 catch ArithmeticException，而不是 catch 范围较大的 RuntimeException，甚至是 Exception。 尽量不要在 finally 块抛出异常或者返回值 不要忽略异常，一旦捕获异常，就应该处理，而非丢弃 异常处理效率很低，所以不要用异常进行业务逻辑处理 各类异常必须要有单独的日志记录，将异常分级，分类管理，因为有的时候仅仅想给第三方运维看到逻辑异常，而不是更细节的信息。 如何对异常进行分类 逻辑异常，这类异常用于描述业务无法按照预期的情况处理下去，属于用户制造的意外。 代码错误，这类异常用于描述开发的代码错误，例如 NPE，ILLARG，都属于程序员制造的 BUG。 专有异常，多用于特定业务场景，用于描述指定作业出现意外情况无法预先处理。 扩展阅读： Effective java 中文版 之 第九章 异常 优雅的处理你的 Java 异常 小结 参考资料 Java 编程思想 JAVA 核心技术（卷 1） Effective java 中文版 之 第九章 异常 优雅的处理你的 Java 异常 https://juejin.im/post/5b6d61e55188251b38129f9a#heading-17 https://www.cnblogs.com/skywang12345/p/3544168.html http://www.importnew.com/26613.html]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
        <tag>exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ant 简易教程]]></title>
    <url>%2Fblog%2F2015%2F04%2F19%2Fjava%2Fjavatool%2Fbuild%2Fant%2F</url>
    <content type="text"><![CDATA[Ant 简易教程 前言 Apache Ant 是一个将软件编译、测试、部署等步骤联系在一起加以自动化的一个工具，大多用于 Java 环境中的软件开发。由 Apache 软件基金会所提供。 Ant 是纯 Java 语言编写的，所以具有很好的跨平台性。 下载和安装 下载 ant 的官方下载地址：http://ant.apache.org/bindownload.cgi 进入页面后，在下图的红色方框中可以下载最新版本。笔者下载的版本是 apache-ant-1.9.4。 配置环境变量 配置环境变量（我的电脑 -&gt; 属性 -&gt; 高级 -&gt; 环境变量）。 设置 ant 环境变量： ANT_HOME C:/ apache-ant-1.9.4 **path ** C:/ apache-ant-1.9.4/bin classpath C:/apache-ant-1.9.4/lib 验证 点击 开始 -&gt; 运行 -&gt; 输入 cmd 执行构建文件 输入如下命令：ant 如果出现如下内容，说明安装成功： Buildfile: build.xml does not exist! Build failed 注意：因为 ant 默认运行 build.xml 文件，这个文件需要我们创建。 如果不想命名为 build.xml，运行时可以使用 ant -buildfile test.xml 命令指明要运行的构建文件。 查看版本信息 输入 ant -version，可以查看版本信息。 但如果出现 ‘ant’ 不是内部或外部命令，也不是可运行的程序或批处理文件，说明安装失败：（可以重复前述步骤，直至安装成功。） 例子 在安装和配置成功后，我们就可以使用 ant 了。 为了让读者对 ant 有一个直观的认识，首先以 Ant 官方手册上的一个简单例子做一个说明。 以下是一个 build.xml 文件的内容： &lt;project name="MyProject" default="dist" basedir="."&gt; &lt;description&gt; simple example build file &lt;/description&gt; &lt;!-- set global properties for this build --&gt; &lt;property name="src" location="src"/&gt; &lt;property name="build" location="build"/&gt; &lt;property name="dist" location="dist"/&gt; &lt;target name="init"&gt; &lt;!-- Create the time stamp --&gt; &lt;tstamp/&gt; &lt;!-- Create the build directory structure used by compile --&gt; &lt;mkdir dir="$&#123;build&#125;"/&gt; &lt;/target&gt; &lt;target name="compile" depends="init" description="compile the source " &gt; &lt;!-- Compile the java code from $&#123;src&#125; into $&#123;build&#125; --&gt; &lt;javac srcdir="$&#123;src&#125;" destdir="$&#123;build&#125;"/&gt; &lt;/target&gt; &lt;target name="dist" depends="compile" description="generate the distribution" &gt; &lt;!-- Create the distribution directory --&gt; &lt;mkdir dir="$&#123;dist&#125;/lib"/&gt; &lt;!-- Put everything in $&#123;build&#125; into the MyProject-$&#123;DSTAMP&#125;.jar file --&gt; &lt;jar jarfile="$&#123;dist&#125;/lib/MyProject-$&#123;DSTAMP&#125;.jar" basedir="$&#123;build&#125;"/&gt; &lt;/target&gt; &lt;target name="clean" description="clean up" &gt; &lt;!-- Delete the $&#123;build&#125; and $&#123;dist&#125; directory trees --&gt; &lt;delete dir="$&#123;build&#125;"/&gt; &lt;delete dir="$&#123;dist&#125;"/&gt; &lt;/target&gt;&lt;/project&gt; 在这个 xml 文件中，有几个 target 标签，每个 target 对应一个执行目标。 我们将这个 build.xml 放在 D:\Temp\ant_test 路径下，然后在 dos 界面下进行测试。 ant init 在 D:\Temp\ant_test 路径下创建了一个 build 目录，执行成功。 ant compile 提示错误，原来是在 build.xml 的所在目录下找不到 src 目录。好的，我们直接创建一个 src 目录，然后再次尝试。这次，执行成功。 **ant dist ** 在 D:\Temp\ant_test 路径下创建了一个 dist 目录，执行成功。 ant clean 清除创建的 build 和 dist 目录，执行成功。 一个细节 细心的读者，想必已经发现一个问题——在执行 ant compile 和 ant dist 命令的时候把前面的命令也执行了。这是为什么呢？ 请留意一下 build.xml 中的内容。有部分 target 标签中含有 depends 关键字。 这表明，当前的 target 在执行时需要依赖其他的 target，必须先执行依赖的 target，然后再执行。 关键元素 Ant 的构件文件都是 XML 格式的。每个构件文件包含一个 project 元素和至少一个 target。 target 元素可以包含多个 task 元素。 Project 元素 project 元素是构建文件的根元素。 一个 project 元素可以有多个 target 元素，一个 target 元素可以有多个 task。 在上节的例子中，project 标签里有三个属性。 &lt;project name="MyProject" default="dist" basedir="."&gt; name 属性，指示 project 元素的名字。例子中的名字就是 MyProject。 default 属性，指示这个 project 默认执行的 target。在本文的例子中，默认执行的 target 为 dist。 如果我们输入命令 ant 时，不指定 target 参数，默认会执行 dist 这个 target。 basedir 属性，指定根路径的位置。该属性没有指定时，使用 Ant 的构件文件的所在目录作为根目录。 Target 元素 target 元素是 task 的容器，也就是 Ant 的一个基本执行单元。 以上节例子中的 compile 来举例。 &lt;target name="compile" depends="init" description="compile the source " &gt; &lt;!-- Compile the java code from $&#123;src&#125; into $&#123;build&#125; --&gt; &lt;javac srcdir="$&#123;src&#125;" destdir="$&#123;build&#125;"/&gt;&lt;/target&gt; 这个 target 中出现了几个属性。 name 属性，指示 target 元素的名称。 这个属性在一个 project 元素中必须是唯一的。这很好理解，如果出现重复，Ant 就不知道具体该执行哪个 target 了。 depends 属性，指示依赖的 target，当前的 target 必须在依赖的 target 之后执行。 description 属性，是关于 target 的简短说明。 此外，还有其他几个未出现在构建文件中的属性。 if 属性，验证指定的属性是否存在，若不存在，所在 target 将不会被执行。 unless 属性，正好和 if 属性相反，验证指定的属性是否存在，若存在，所在 target 将不会被执行。**** extensionOf 属性，添加当前 target 到 extension-point 依赖列表。——Ant1.8.0 新特性。 extension-point 元素和 target 元素十分类似，都可以指定依赖的 target。但是不同的是，extension-point 中不能包含任何 task。 请看以下实例： &lt;target name="create-directory-layout"&gt; ...&lt;/target&gt;&lt;extension-point name="ready-to-compile" depends="create-directory-layout"/&gt;&lt;target name="compile" depends="ready-to-compile"&gt; ...&lt;/target&gt; 调用 target 顺序: create-directory-layout --&gt; ‘empty slot’ --&gt; compile &lt;target name="generate-sources" extensionOf="ready-to-compile"&gt; ...&lt;/target&gt; 调用 target 顺序: create-directory-layout --&gt; generate-sources --&gt; compile onMissingExtensionPoint 属性：当无法找到一个 extension-point 时，target 尝试去做的动作(“fail”, “warn”, “ignore”)。——Ant1.8.2 新特性 Task 元素 task 是一段可以被执行的代码。 一个 task 可以有多个属性， 一个属性可以包含对一个 property 的引用。 task 的通常结构为 &lt;name attribute1="value1" attribute2="value2" ... /&gt; 其中，name 是 task 的名字， attributeN 是属性名， valueN 是这个属性的值。 还是以 compile 做为例子： &lt;target name="compile" depends="init" description="compile the source " &gt; &lt;!-- Compile the java code from srcintosrcinto&#123;build&#125; --&gt; &lt;javac srcdir="$&#123;src&#125;" destdir="$&#123;build&#125;"/&gt;&lt;/target&gt; 在 compile 这个 target 标签中包含了一个任务。 这个任务的动作是：执行 JAVA 编译，编译 src 下的代码，并把编译生成的文件放在 build 目录中。 **常用 task ** javac：用于编译一个或者多个 Java 源文件，通常需要 srcdir 和 destdir 两个属性，用于指定 Java 源文件的位置和编译后 class 文件的保存位置。 &lt;javac srcdir="$&#123;src&#125;" destdir="$&#123;build&#125;" classpath="abc.jar" debug="on" source="1.7" /&gt; java：用于运行某个 Java 类，通常需要 classname 属性，用于指定需要运行哪个类。 &lt;java classname="test.Main"&gt; &lt;arg value="-h" /&gt; &lt;classpath&gt; &lt;pathelement location="dist/test.jar" /&gt; &lt;/classpath&gt;&lt;/java&gt; jar：用于生成 JAR 包，通常需要指定 destfile 属性，用于指定所创建 JAR 包的文件名。除此之外，通常还应指定一个文件集，表明需要将哪些文件打包到 JAR 包里。 &lt;jar jarfile="dist/lib/MyProject−dist/lib/MyProject−&#123;DSTAMP&#125;.jar" basedir="$&#123;build&#125;"/&gt; echo：输出某个字符串。 &lt;echo message="Building to $&#123;builddir&#125;"/&gt;&lt;echo&gt;You are using version $&#123;java.version&#125; of Java! This message spans two lines.&lt;/echo&gt; copy：用于复制文件或路径。 &lt;copy todir="$&#123;builddir&#125;/srccopy"&gt; &lt;fileset dir="$&#123;srcdir&#125;"&gt; &lt;include name="**/*.java"/&gt; &lt;/fileset&gt; &lt;filterset&gt; &lt;filter token="VERSION" value="$&#123;app.version&#125;"/&gt; &lt;/filterset&gt;&lt;/copy&gt; delete：用于删除文件或路径。 &lt;copy todir="$&#123;builddir&#125;/srccopy"&gt; &lt;fileset dir="$&#123;srcdir&#125;"&gt; &lt;include name="**/*.java"/&gt; &lt;/fileset&gt; &lt;filterset&gt; &lt;filter token="VERSION" value="$&#123;app.version&#125;"/&gt; &lt;/filterset&gt;&lt;/copy&gt; mkdir：用于创建文件夹。 &lt;mkdir dir="$&#123;dist&#125;/lib" /&gt; move：用户移动文件和路径。 &lt;move todir="some/new/dir"&gt; &lt;fileset dir="my/src/dir"&gt; &lt;include name="**/*.jar" /&gt; &lt;exclude name="**/ant.jar" /&gt; &lt;/fileset&gt;&lt;/move&gt; Property 元素 Property 是对参数的定义。 project 的属性可以通过 property 元素来设定，也可在 Ant 之外设定。若要在外部引入某文件，例如 build.properties 文件，可以通过如下内容将其引入：&lt;property file=” build.properties”/&gt;。 property 元素可用作 task 的属性值。在 task 中是通过将属性名放在“${”和“}”之间，并放在 task 属性值的位置来实现的。 例如 complile 例子中，使用了前面定义的 src 作为源目录。 &lt;javac srcdir="$&#123;src&#125;" destdir="$&#123;build&#125;"/&gt; Ant 提供了一些内置的属性，它能得到的系统属性的列表与 Java 文档中 System.getPropertis()方法得到的属性一致，这些系统属性可参考 sun 网站的说明。 extension-point 元素 和 target 元素十分类似，都可以指定依赖的 target。但是不同的是，extension-point 中不能包含任何 task。 ——Ant1.8.0 新增特性。 在 target 元素中的例子里已提到过，不再赘述。 参考资料 ant 官方手册 http://www.blogjava.net/amigoxie/archive/2007/11/09/159413.html]]></content>
      <categories>
        <category>java</category>
        <category>javatool</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javatool</tag>
        <tag>build</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组]]></title>
    <url>%2Fblog%2F2015%2F04%2F10%2Falgorithm%2Fdata-structure%2Farray%2F</url>
    <content type="text"><![CDATA[数组 所谓数组，是有序的元素序列。若将有限个类型相同的变量的集合命名，那么这个名称为数组名。组成数组的各个变量称为数组的分量，也称为数组的元素，有时也称为下标变量。用于区分数组的各个元素的数字编号称为下标。数组是在程序设计中，为了处理方便， 把具有相同类型的若干元素按无序的形式组织起来的一种形式。这些无序排列的同类数据元素的集合称为数组。 简介 一维数组 二维数组 多维数组 数组的特性 数组中的操作 更多内容 简介 数组是一种基本的数据结构，用于按顺序存储元素的集合。但是元素可以随机存取，因为数组中的每个元素都可以通过数组索引来识别。 数组可以有一个或多个维度。 一维数组 一维（或单维）数组是一种线性数组，其中元素的访问是以行或列索引的单一下标表示。 这里有一个例子： 在上面的例子中，数组 A 中有 6 个元素。也就是说，A 的长度是 6 。我们可以使用 A[0] 来表示数组中的第一个元素。因此，A[0] = 6 。类似地，A[1] = 3，A[2] = 8，依此类推。 二维数组 类似于一维数组，二维数组也是由元素的序列组成。但是这些元素可以排列在矩形网格中而不是直线上。 类似于一维数组，二维数组也是由元素的序列组成。但是这些元素可以排列在矩形网格中而不是直线上。 在一些语言中，多维数组实际上是在内部作为一维数组实现的，而在其他一些语言中，实际上根本没有多维数组。 1. C++ 将二维数组存储为一维数组。 下图显示了大小为 M * N 的数组 A 的实际结构： 因此，如果我们将 A 定义为也包含 M * N 个元素的一维数组，那么实际上 A[i][j] 就等于 A[i * N + j]。 2. 在 Java 中，二维数组实际上是包含着 M 个元素的一维数组，每个元素都是包含有 N 个整数的数组。 下图显示了 Java 中二维数组 A 的实际结构： 二维数组示例： public class TwoDimensionArray &#123; private static void printArray(int[][] a) &#123; for (int i = 0; i &lt; a.length; ++i) &#123; System.out.println(a[i]); &#125; for (int i = 0; i &lt; a.length; ++i) &#123; for (int j = 0; a[i] != null &amp;&amp; j &lt; a[i].length; ++j) &#123; System.out.print(a[i][j] + " "); &#125; System.out.println(); &#125; &#125; public static void main(String[] args) &#123; System.out.println("Example I:"); int[][] a = new int[2][5]; printArray(a); System.out.println("Example II:"); int[][] b = new int[2][]; printArray(b); System.out.println("Example III:"); b[0] = new int[3]; b[1] = new int[5]; printArray(b); &#125;&#125; 多维数组 普通数组采用一个整数来作下标。多维数组（高维数组）的概念特别是在数值计算和图形应用方面非常有用。我们在多维数组之中采用一系列有序的整数来标注，如在[ 3,1,5 ] 。这种整数列表之中整数的个数始终相同，且被称为数组的“维度”。关于每个数组维度的边界称为“维”。维度为 k 的数组通常被称为 k 维。 多维数组的数组名字，在表达式中自动转换为数组首元素地址值，但这个首元素实际上是去除数组下标第一维之后的数组剩余部分。 数组的特性 数组设计之初是在形式上依赖内存分配而成的，所以必须在使用前预先请求空间。这使得数组有以下特性： 请求空间以后大小固定，不能再改变（数据溢出问题）； 在内存中有空间连续性的表现，中间不会存在其他程序需要调用的数据，为此数组的专用内存空间； 在旧式编程语言中（如有中阶语言之称的 C），程序不会对数组的操作做下界判断，也就有潜在的越界操作的风险（比如会把数据写在运行中程序需要调用的核心部分的内存上）。 因为简单数组强烈倚赖计算机硬件之内存，所以不适用于现代的程序设计。欲使用可变大小、硬件无关性的数据类型，Java 等程序设计语言均提供了更高级的数据结构：ArrayList、Vector 等动态数组。 数组中的操作 public class Main &#123; public static void main(String[] args) &#123; // 1. Initialize int[] a0 = new int[5]; int[] a1 = &#123;1, 2, 3&#125;; // 2. Get Length System.out.println("The size of a1 is: " + a1.length); // 3. Access Element System.out.println("The first element is: " + a1[0]); // 4. Iterate all Elements System.out.print("[Version 1] The contents of a1 are:"); for (int i = 0; i &lt; a1.length; ++i) &#123; System.out.print(" " + a1[i]); &#125; System.out.println(); System.out.print("[Version 2] The contents of a1 are:"); for (int item: a1) &#123; System.out.print(" " + item); &#125; System.out.println(); // 5. Modify Element a1[0] = 4; // 6. Sort Arrays.sort(a1); &#125;&#125; 更多内容 https://zh.wikipedia.org/wiki/数组]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性表]]></title>
    <url>%2Fblog%2F2015%2F04%2F09%2Falgorithm%2Fdata-structure%2Flist%2F</url>
    <content type="text"><![CDATA[线性表 单链表 单链表中的每个结点不仅包含值，还包含链接到下一个结点的引用字段。通过这种方式，单链表将所有结点按顺序组织起来。、 下面是一个单链表的例子： 蓝色箭头显示单个链接列表中的结点是如何组合在一起的。 与数组不同，我们无法在常量时间内访问单链表中的随机元素。 如果我们想要获得第 i 个元素，我们必须从头结点逐个遍历。 我们按索引来访问元素平均要花费 O(N)时间，其中 N 是链表的长度。 双链表 双链表以类似的方式工作，但还有一个引用字段，称为“prev”字段。有了这个额外的字段，您就能够知道当前结点的前一个结点。 让我们看一个例子： 绿色箭头表示我们的“prev”字段是如何工作的。]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图]]></title>
    <url>%2Fblog%2F2015%2F03%2F24%2Falgorithm%2Fdata-structure%2Fgraph%2F</url>
    <content type="text"><![CDATA[图 在计算机科学中，一个图就是一些顶点的集合，这些顶点通过一系列边结对（连接）。顶点用圆圈表示，边就是这些圆圈之间的连线。顶点之间通过边连接。 术语 阶（Order） - 图 G 中点集 V 的大小称作图 G 的阶。 子图（Sub-Graph） - 当图 G’=(V’,E’)其中 V‘包含于 V，E’包含于 E，则 G’称作图 G=(V,E)的子图。每个图都是本身的子图。 生成子图（Spanning Sub-Graph） - 指满足条件 V(G’) = V(G)的 G 的子图 G’。 导出子图（Induced Subgraph） - 以图 G 的顶点集 V 的非空子集V1 为顶点集，以两端点均在 V1 中的全体边为边集的 G 的子图，称为 V1 导出的导出子图；以图 G 的边集 E 的非空子集 E1 为边集，以 E1 中边关联的顶点的全体为顶点集的 G 的子图，称为 E1 导出的导出子图。 有向图 - 如果给图的每条边规定一个方向，那么得到的图称为有向图。 无向图 - 边没有方向的图称为无向图。 度（Degree） - 一个顶点的度是指与该顶点相关联的边的条数，顶点 v 的度记作 d(v)。 入度（In-degree）和出度（Out-degree） - 对于有向图来说，一个顶点的度可细分为入度和出度。一个顶点的入度是指与其关联的各边之中，以其为终点的边数；出度则是相对的概念，指以该顶点为起点的边数。 自环（Loop） - 若一条边的两个顶点为同一顶点，则此边称作自环。 路径（Path） - 从 u 到 v 的一条路径是指一个序列 v0,e1,v1,e2,v2,…ek,vk，其中 ei 的顶点为 vi 及 vi - 1，k 称作路径的长度。如果它的起止顶点相同，该路径是“闭”的，反之，则称为“开”的。一条路径称为一简单路径(simple path)，如果路径中除起始与终止顶点可以重合外，所有顶点两两不等。 行迹（Trace） - 如果路径 P(u,v)中的边各不相同，则该路径称为 u 到 v 的一条行迹。闭的行迹称作回路（Circuit）。 轨迹（Track） - 如果路径 P(u,v)中的顶点各不相同，则该路径称为 u 到 v 的一条轨迹。闭的轨迹称作圈（Cycle）。 桥（Bridge） - 若去掉一条边，便会使得整个图不连通，该边称为桥。 图的基本操作 创建一个图结构 - CreateGraph(G) 检索给定顶点 - LocateVex(G,elem) 获取图中某个顶点 - GetVex(G,v) 为图中顶点赋值 - PutVex(G,v,value) 返回第一个邻接点 - FirstAdjVex(G,v) 返回下一个邻接点 - NextAdjVex(G,v,w) 插入一个顶点 - InsertVex(G,v) 删除一个顶点 - DeleteVex(G,v) 插入一条边 - InsertEdge(G,v,w) 删除一条边 - DeleteEdge(G,v,w) 遍历图 - Traverse(G,v)]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[哈希表]]></title>
    <url>%2Fblog%2F2015%2F03%2F16%2Falgorithm%2Fdata-structure%2Fhash%2F</url>
    <content type="text"><![CDATA[哈希表 关键词： hash, 哈希表, 哈希函数 简介 原理 更多内容 简介 哈希表是一种使用哈希函数组织数据，以支持快速插入和搜索的数据结构。 有两种不同类型的哈希表：哈希集合和哈希映射。 哈希集合是集合数据结构的实现之一，用于存储非重复值。 哈希映射是映射 数据结构的实现之一，用于存储(key, value)键值对。 在标准模板库的帮助下，哈希表是易于使用的。大多数常见语言（如 Java，C ++ 和 Python）都支持哈希集合和哈希映射。 通过选择合适的哈希函数，哈希表可以在插入和搜索方面实现出色的性能。 原理 哈希表的关键思想是使用哈希函数将键映射到存储桶。更确切地说， 当我们插入一个新的键时，哈希函数将决定该键应该分配到哪个桶中，并将该键存储在相应的桶中； 当我们想要搜索一个键时，哈希表将使用相同的哈希函数来查找对应的桶，并只在特定的桶中进行搜索。 哈希函数示例 在示例中，我们使用 y = x ％ 5 作为哈希函数。让我们使用这个例子来完成插入和搜索策略： 插入：我们通过哈希函数解析键，将它们映射到相应的桶中。 例如，1987 分配给桶 2，而 24 分配给桶 4。 搜索：我们通过相同的哈希函数解析键，并仅在特定存储桶中搜索。 如果我们搜索 1987，我们将使用相同的哈希函数将 1987 映射到 2。因此我们在桶 2 中搜索，我们在那个桶中成功找到了 1987。 例如，如果我们搜索 23，将映射 23 到 3，并在桶 3 中搜索。我们发现 23 不在桶 3 中，这意味着 23 不在哈希表中。 哈希表的关键 1. 哈希函数 哈希函数是哈希表中最重要的组件，该哈希表用于将键映射到特定的桶。在上一节的示例中，我们使用 y = x % 5 作为散列函数，其中 x 是键值，y 是分配的桶的索引。 散列函数将取决于键值的范围和桶的数量。 下面是一些哈希函数的示例： 哈希函数的设计是一个开放的问题。其思想是尽可能将键分配到桶中，理想情况下，完美的哈希函数将是键和桶之间的一对一映射。然而，在大多数情况下，哈希函数并不完美，它需要在桶的数量和桶的容量之间进行权衡。 2. 冲突解决 理想情况下，如果我们的哈希函数是完美的一对一映射，我们将不需要处理冲突。不幸的是，在大多数情况下，冲突几乎是不可避免的。例如，在我们之前的哈希函数（y = x ％ 5）中，1987 和 2 都分配给了桶 2，这是一个冲突。 冲突解决算法应该解决以下几个问题： 如何组织在同一个桶中的值？ 如果为同一个桶分配了太多的值，该怎么办？ 如何在特定的桶中搜索目标值？ 根据我们的哈希函数，这些问题与桶的容量和可能映射到同一个桶的键的数目有关。 让我们假设存储最大键数的桶有 N 个键。 通常，如果 N 是常数且很小，我们可以简单地使用一个数组将键存储在同一个桶中。如果 N 是可变的或很大，我们可能需要使用高度平衡的二叉树来代替。 更多内容 https://leetcode-cn.com/explore/learn/card/hash-table/]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>hash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hash 表的查找]]></title>
    <url>%2Fblog%2F2015%2F03%2F16%2Falgorithm%2Fdata-structure%2Fsearch%2Fhash-search%2F</url>
    <content type="text"><![CDATA[Hash 表的查找 要点 哈希表和哈希函数 在记录的存储位置和它的关键字之间是建立一个确定的对应关系（映射函数），使每个关键字和一个存储位置能唯一对应。这个映射函数称为哈希函数，根据这个原则建立的表称为哈希表(Hash Table)，也叫散列表。 以上描述，如果通过数学形式来描述就是： 若查找关键字为 key，则其值存放在 f(key) 的存储位置上。由此，不需比较便可直接取得所查记录。 注：哈希查找与线性表查找和树表查找最大的区别在于，不用数值比较。 冲突 若 key1 ≠ key2 ，而 f(key1) = f(key2)，这种情况称为冲突(Collision)。 根据哈希函数f(key)和处理冲突的方法将一组关键字映射到一个有限的连续的地址集（区间）上，并以关键字在地址集中的“像”作为记录在表中的存储位置，这一映射过程称为构造哈希表。 构造哈希表这个场景就像汽车找停车位，如果车位被人占了，只能找空的地方停。 构造哈希表 由以上内容可知，哈希查找本身其实不费吹灰之力，问题的关键在于如何构造哈希表和处理冲突。 常见的构造哈希表的方法有 5 种： 直接定址法 说白了，就是小学时学过的一元一次方程。 即 f(key) = a * key + b。其中，a和b 是常数。 数字分析法 假设关键字是R进制数（如十进制）。并且哈希表中可能出现的关键字都是事先知道的，则可选取关键字的若干数位组成哈希地址。 选取的原则是使得到的哈希地址尽量避免冲突，即所选数位上的数字尽可能是随机的。 平方取中法 取关键字平方后的中间几位为哈希地址。通常在选定哈希函数时不一定能知道关键字的全部情况，仅取其中的几位为地址不一定合适； 而一个数平方后的中间几位数和数的每一位都相关， 由此得到的哈希地址随机性更大。取的位数由表长决定。 除留余数法 取关键字被某个不大于哈希表表长 m 的数 p 除后所得的余数为哈希地址。 即 f(key) = key % p (p ≤ m) 这是一种最简单、最常用的方法，它不仅可以对关键字直接取模，也可在折叠、平方取中等运算之后取模。 注意：p的选择很重要，如果选的不好，容易产生冲突。根据经验，一般情况下可以选p为素数。 随机数法 选择一个随机函数，取关键字的随机函数值为它的哈希地址，即 f(key) = random(key)。 通常，在关键字长度不等时采用此法构造哈希函数较为恰当。 解决冲突 设计合理的哈希函数可以减少冲突，但不能完全避免冲突。 所以需要有解决冲突的方法，常见有两类： 开放定址法 如果两个数据元素的哈希值相同，则在哈希表中为后插入的数据元素另外选择一个表项。 当程序查找哈希表时，如果没有在第一个对应的哈希表项中找到符合查找要求的数据元素，程序就会继续往后查找，直到找到一个符合查找要求的数据元素，或者遇到一个空的表项。 示例 若要将一组关键字序列 {1, 9, 25, 11, 12, 35, 17, 29} 存放到哈希表中。 采用除留余数法构造哈希表；采用开放定址法处理冲突。 不妨设选取的p和m为13，由 f(key) = key % 13 可以得到下表。 需要注意的是，在上图中有两个关键字的探查次数为 2 ，其他都是1。 这个过程是这样的： a. 12 % 13 结果是12，而它的前面有个 25 ，25 % 13 也是12，存在冲突。 我们使用开放定址法 (12 + 1) % 13 = 0，没有冲突，完成。 b. 35 % 13 结果是 9，而它的前面有个 9，9 % 13也是 9，存在冲突。 我们使用开放定址法 (9 + 1) % 13 = 10，没有冲突，完成。 拉链法 将哈希值相同的数据元素存放在一个链表中，在查找哈希表的过程中，当查找到这个链表时，必须采用线性查找方法。 在这种方法中，哈希表中每个单元存放的不再是记录本身，而是相应同义词单链表的头指针。 示例 如果对开放定址法示例中提到的序列使用拉链法，得到的结果如下图所示： 实现一个哈希表 假设要实现一个哈希表，要求 a. 哈希函数采用除留余数法，即 f(key) = key % p (p ≤ m) b. 解决冲突采用开放定址法，即 f2(key) = (f(key)+i) % size (p ≤ m) （1）定义哈希表的数据结构 class HashTable &#123; public int key = 0; // 关键字 public int data = 0; // 数值 public int count = 0; // 探查次数&#125; （2）在哈希表中查找关键字key 根据设定的哈希函数，计算哈希地址。如果出现地址冲突，则按设定的处理冲突的方法寻找下一个地址。 如此反复，直到不冲突为止（查找成功）或某个地址为空（查找失败）。 /** * 查找哈希表 * 构造哈希表采用除留取余法，即f(key) = key mod p (p ≤ size) * 解决冲突采用开放定址法，即f2(key) = (f(key) + i) mod p (1 ≤ i ≤ size-1) * ha为哈希表，p为模，size为哈希表大小，key为要查找的关键字 */public int searchHashTable(HashTable[] ha, int p, int size, int key) &#123; int addr = key % p; // 采用除留取余法找哈希地址 // 若发生冲突，用开放定址法找下一个哈希地址 while (ha[addr].key != NULLKEY &amp;&amp; ha[addr].key != key) &#123; addr = (addr + 1) % size; &#125; if (ha[addr].key == key) &#123; return addr; // 查找成功 &#125; else &#123; return FAILED; // 查找失败 &#125;&#125; （3）删除关键字为key的记录 在采用开放定址法处理冲突的哈希表上执行删除操作，只能在被删记录上做删除标记，而不能真正删除记录。 找到要删除的记录，将关键字置为删除标记DELKEY。 public int deleteHashTable(HashTable[] ha, int p, int size, int key) &#123; int addr = 0; addr = searchHashTable(ha, p, size, key); if (FAILED != addr) &#123; // 找到记录 ha[addr].key = DELKEY; // 将该位置的关键字置为DELKEY return SUCCESS; &#125; else &#123; return NULLKEY; // 查找不到记录，直接返回NULLKEY &#125;&#125; （4）插入关键字为key的记录 将待插入的关键字key插入哈希表 先调用查找算法，若在表中找到待插入的关键字，则插入失败； 若在表中找到一个开放地址，则将待插入的结点插入到其中，则插入成功。 public void insertHashTable(HashTable[] ha, int p, int size, int key) &#123; int i = 1; int addr = 0; addr = key % p; // 通过哈希函数获取哈希地址 if (ha[addr].key == NULLKEY || ha[addr].key == DELKEY) &#123; // 如果没有冲突，直接插入 ha[addr].key = key; ha[addr].count = 1; &#125; else &#123; // 如果有冲突，使用开放定址法处理冲突 do &#123; addr = (addr + 1) % size; // 寻找下一个哈希地址 i++; &#125; while (ha[addr].key != NULLKEY &amp;&amp; ha[addr].key != DELKEY); ha[addr].key = key; ha[addr].count = i; &#125;&#125; （5）建立哈希表 先将哈希表中各关键字清空，使其地址为开放的，然后调用插入算法将给定的关键字序列依次插入。 public void insertHashTable(HashTable[] ha, int p, int size, int key) &#123; int i = 1; int addr = 0; addr = key % p; // 通过哈希函数获取哈希地址 if (ha[addr].key == NULLKEY || ha[addr].key == DELKEY) &#123; // 如果没有冲突，直接插入 ha[addr].key = key; ha[addr].count = 1; &#125; else &#123; // 如果有冲突，使用开放定址法处理冲突 do &#123; addr = (addr + 1) % size; // 寻找下一个哈希地址 i++; &#125; while (ha[addr].key != NULLKEY &amp;&amp; ha[addr].key != DELKEY); ha[addr].key = key; ha[addr].count = i; &#125;&#125; 完整示例 示例代码 资源 《数据结构习题与解析》（B级第3版）]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性表的查找]]></title>
    <url>%2Fblog%2F2015%2F03%2F10%2Falgorithm%2Fdata-structure%2Fsearch%2Flinear-list-search%2F</url>
    <content type="text"><![CDATA[线性表的查找 概念 什么是查找？ 查找是根据给定的某个值，在表中确定一个关键字的值等于给定值的记录或数据元素。 查找算法的分类 若在查找的同时对表记录做修改操作（如插入和删除），则相应的表称之为动态查找表； 否则，称之为静态查找表。 此外，如果查找的全过程都在内存中进行，称之为内查找； 反之，如果查找过程中需要访问外存，称之为外查找。 查找算法性能比较的标准 ——平均查找长度ASL（Average Search Length） 由于查找算法的主要运算是关键字的比较过程，所以通常把查找过程中对关键字需要执行的平均比较长度（也称为平均比较次数）作为衡量一个查找算法效率优劣的比较标准。 选取查找算法的因素 (1) 使用什么数据存储结构（如线性表、树形表等）。 (2) 表中的次序，即对无序表还是有序表进行查找。 顺序查找 要点 它是一种最简单的查找算法，效率也很低下。 存储结构 没有存储结构要求，可以无序，也可以有序。 基本思想 从数据结构线形表的一端开始，顺序扫描，依次将扫描到的结点关键字与给定值k相比较，若相等则表示查找成功； 若扫描结束仍没有找到关键字等于k的结点，表示查找失败。 核心代码 public int orderSearch(int[] list, int length, int key) &#123; // 从前往后扫描list数组，如果有元素的值与key相等，直接返回其位置 for (int i = 0; i &lt; length; i++) &#123; if (key == list[i]) &#123; return i; &#125; &#125; // 如果扫描完，说明没有元素的值匹配key，返回-1，表示查找失败 return -1;&#125; 算法分析 顺序查找算法最好的情况是，第一个记录即匹配关键字，则需要比较 1 次； 最坏的情况是，最后一个记录匹配关键字，则需要比较 N 次。 所以，顺序查找算法的平均查找长度为 ASL = (N + N-1 + … + 2 + 1) / N = (N+1) / 2 顺序查找的平均时间复杂度为O(N)。 二分查找 要点 二分查找又称折半查找，它是一种效率较高的查找方法。 存储结构 使用二分查找需要两个前提： (1) 必须是顺序存储结构。 (2) 必须是有序的表。 基本思想 首先，将表中间位置记录的关键字与查找关键字比较，如果两者相等，则查找成功； 否则利用中间位置记录将表分成前、后两个子表，如果中间位置记录的关键字大于查找关键字，则进一步查找前一子表，否则进一步查找后一子表。 重复以上过程，直到找到满足条件的记录，使查找成功，或直到子表不存在为止，此时查找不成功。 核心代码 public int binarySearch(int[] list, int length, int key) &#123; int low = 0, mid = 0, high = length - 1; while (low &lt;= high) &#123; mid = (low + high) / 2; if (list[mid] == key) &#123; return mid; // 查找成功，直接返回位置 &#125; else if (list[mid] &lt; key) &#123; low = mid + 1; // 关键字大于中间位置的值，则在大值区间[mid+1, high]继续查找 &#125; else &#123; high = mid - 1; // 关键字小于中间位置的值，则在小值区间[low, mid-1]继续查找 &#125; &#125; return -1;&#125; 算法分析 二分查找的过程可看成一个二叉树。 把查找区间的中间位置视为树的根，左区间和右区间视为根的左子树和右子树。 由此得到的二叉树，称为二分查找的判定树或比较树。 由此可知，二分查找的平均查找长度实际上就是树的高度O(log2N)。 分块查找 要点 分块查找(Blocking Search)又称索引顺序查找。它是一种性能介于顺序查找和二分查找之间的查找方法。 分块查找由于只要求索引表是有序的，对块内节点没有排序要求，因此特别适合于节点动态变化的情况。 存储结构 分块查找表是由**“分块有序”的线性表和索引表**两部分构成的。 所谓**“分块有序”的线性表**，是指： 假设要排序的表为R[0…N-1]，将表均匀分成b块，前b-1块中记录个数为s=N/b，最后一块记录数小于等于s； 每一块中的关键字不一定有序，但前一块中的最大关键字必须小于后一块中的最小关键字。 注：这是使用分块查找的前提条件。 如上将表均匀分成b块后，抽取各块中的最大关键字和起始位置构成一个索引表IDX[0…b-1]。 由于表R是分块有序的，所以索引表是一个递增有序表。 下图就是一个分块查找表的存储结构示意图 基本思想 分块查找算法有两个处理步骤： (1) 首先查找索引表 因为分块查找表是“分块有序”的，所以我们可以通过索引表来锁定关键字所在的区间。 又因为索引表是递增有序的，所以查找索引可以使用顺序查找或二分查找。 (2) 然后在已确定的块中进行顺序查找 因为块中不一定是有序的，所以只能使用顺序查找。 代码范例 class BlockSearch &#123; class IndexType &#123; public int key; // 分块中的最大值 public int link; // 分块的起始位置 &#125; // 建立索引方法，n 是线性表最大长度，gap是分块的最大长度 public IndexType[] createIndex(int[] list, int n, int gap) &#123; int i = 0, j = 0, max = 0; int num = n / gap; IndexType[] idxGroup = new IndexType[num]; // 根据步长数分配索引数组大小 while (i &lt; num) &#123; j = 0; idxGroup[i] = new IndexType(); idxGroup[i].link = gap * i; // 确定当前索引组的第一个元素位置 max = list[gap * i]; // 每次假设当前组的第一个数为最大值 // 遍历这个分块，找到最大值 while (j &lt; gap) &#123; if (max &lt; list[gap * i + j]) &#123; max = list[gap * i + j]; &#125; j++; &#125; idxGroup[i].key = max; i++; &#125; return idxGroup; &#125; // 分块查找算法 public int blockSearch(IndexType[] idxGroup, int m, int[] list, int n, int key) &#123; int mid = 0; int low = 0; int high = m -1; int gap = n / m; // 分块大小等于线性表长度除以组数 // 先在索引表中进行二分查找，找到的位置存放在 low 中 while (low &lt;= high) &#123; mid = (low + high) / 2; if (idxGroup[mid].key &gt;= key) &#123; high = mid - 1; &#125; else &#123; low = mid + 1; &#125; &#125; // 在索引表中查找成功后，再在线性表的指定块中进行顺序查找 if (low &lt; m) &#123; for (int i = idxGroup[low].link; i &lt; idxGroup[low].link + gap; i++) &#123; if (list[i] == key) return i; &#125; &#125; return -1; &#125; // 打印完整序列 public void printAll(int[] list) &#123; for (int value : list) &#123; System.out.print(value + " "); &#125; System.out.println(); &#125; // 打印索引表 public void printIDX(IndexType[] list) &#123; System.out.println("构造索引表如下："); for (IndexType elem : list) &#123; System.out.format("key = %d, link = %d\n", elem.key, elem.link); &#125; System.out.println(); &#125; public static void main(String[] args) &#123; int key = 85; int array[] = &#123; 8, 14, 6, 9, 10, 22, 34, 18, 19, 31, 40, 38, 54, 66, 46, 71, 78, 68, 80, 85 &#125;; BlockSearch search = new BlockSearch(); System.out.print("线性表: "); search.printAll(array); IndexType[] idxGroup = search.createIndex(array, array.length, 5); search.printIDX(idxGroup); int pos = search.blockSearch(idxGroup, idxGroup.length, array, array.length, key); if (-1 == pos) &#123; System.out.format("查找key = %d失败", key); &#125; else &#123; System.out.format("查找key = %d成功，位置为%d", key, pos); &#125; &#125;&#125; 运行结果 线性表: 8 14 6 9 10 22 34 18 19 31 40 38 54 66 46 71 78 68 80 85 构造索引表如下：key = 14, link = 0key = 34, link = 5key = 66, link = 10key = 85, link = 15查找key = 85成功，位置为19 算法分析 因为分块查找实际上是两次查找过程之和。若以二分查找来确定块，显然它的查找效率介于顺序查找和二分查找之间。 三种线性查找的PK (1) 以平均查找长度而言，二分查找 &gt; 分块查找 &gt; 顺序查找。 (2) 从适用性而言，顺序查找无限制条件，二分查找仅适用于有序表，分块查找要求“分块有序”。 (3) 从存储结构而言，顺序查找和分块查找既可用于顺序表也可用于链表；而二分查找只适用于顺序表。 (4) 分块查找综合了顺序查找和二分查找的优点，既可以较为快速，也能使用动态变化的要求。 资源 《数据结构习题与解析》（B级第3版）]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[细说排序算法]]></title>
    <url>%2Fblog%2F2015%2F03%2F03%2Falgorithm%2Fdata-structure%2Fsort%2F</url>
    <content type="text"><![CDATA[细说排序算法 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「algorithm-tutorial」 冒泡排序 要点 算法思想 算法分析 示例代码 快速排序 要点 算法思想 算法分析 示例代码 插入排序 要点 算法思想 算法分析 示例代码 希尔排序 要点 算法思想 算法分析 示例代码 简单选择排序 要点 算法思想 算法分析 示例代码 堆排序 要点 算法思想 算法分析 示例代码 归并排序 要点 算法思想 算法分析 示例代码 基数排序 要点 算法分析 示例代码 冒泡排序 要点 冒泡排序是一种交换排序。 什么是交换排序呢？ 交换排序：两两比较待排序的关键字，并交换不满足次序要求的那对数，直到整个表都满足次序要求为止。 算法思想 它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。 这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端，故名。 假设有一个大小为 N 的无序序列。冒泡排序就是要每趟排序过程中通过两两比较，找到第 i 个小（大）的元素，将其往上排。 以上图为例，演示一下冒泡排序的实际流程： 假设有一个无序序列 { 4. 3. 1. 2, 5 } 第一趟排序：通过两两比较，找到第一小的数值 1 ，将其放在序列的第一位。 第二趟排序：通过两两比较，找到第二小的数值 2 ，将其放在序列的第二位。 第三趟排序：通过两两比较，找到第三小的数值 3 ，将其放在序列的第三位。 至此，所有元素已经有序，排序结束。 要将以上流程转化为代码，我们需要像机器一样去思考，不然编译器可看不懂。 假设要对一个大小为 N 的无序序列进行升序排序（即从小到大）。 每趟排序过程中需要通过比较找到第 i 个小的元素。 所以，我们需要一个外部循环，从数组首端(下标 0) 开始，一直扫描到倒数第二个元素（即下标 N - 2) ，剩下最后一个元素，必然为最大。 假设是第 i 趟排序，可知，前 i-1 个元素已经有序。现在要找第 i 个元素，只需从数组末端开始，扫描到第 i 个元素，将它们两两比较即可。 所以，需要一个内部循环，从数组末端开始（下标 N - 1），扫描到 (下标 i + 1)。 核心代码 public void bubbleSort(int[] list) &#123; int temp = 0; // 用来交换的临时数 // 要遍历的次数 for (int i = 0; i &lt; list.length - 1; i++) &#123; // 从后向前依次的比较相邻两个数的大小，遍历一次后，把数组中第i小的数放在第i个位置上 for (int j = list.length - 1; j &gt; i; j--) &#123; // 比较相邻的元素，如果前面的数大于后面的数，则交换 if (list[j - 1] &gt; list[j]) &#123; temp = list[j - 1]; list[j - 1] = list[j]; list[j] = temp; &#125; &#125; System.out.format("第 %d 趟：\t", i); printAll(list); &#125;&#125; 算法分析 冒泡排序算法的性能 参数 结果 排序类别 交换排序 排序方法 冒泡排序 时间复杂度平均情况 O(N2) 时间复杂度最坏情况 O(N3) 时间复杂度最好情况 O(N) 空间复杂度 O(1) 稳定性 稳定 复杂性 简单 时间复杂度 若文件的初始状态是正序的，一趟扫描即可完成排序。所需的关键字比较次数 C 和记录移动次数 M 均达到最小值：Cmin = N - 1, Mmin = 0。所以，冒泡排序最好时间复杂度为 O(N)。 若初始文件是反序的，需要进行 N -1 趟排序。每趟排序要进行 N - i 次关键字的比较(1 ≤ i ≤ N - 1)，且每次比较都必须移动记录三次来达到交换记录位置。在这种情况下，比较和移动次数均达到最大值： Cmax = N(N-1)/2 = O(N2) Mmax = 3N(N-1)/2 = O(N2) 冒泡排序的最坏时间复杂度为 O(N2)。 因此，冒泡排序的平均时间复杂度为 O(N2)。 总结起来，其实就是一句话：当数据越接近正序时，冒泡排序性能越好。 算法稳定性 冒泡排序就是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。 所以相同元素的前后顺序并没有改变，所以冒泡排序是一种稳定排序算法。 优化 对冒泡排序常见的改进方法是加入标志性变量 exchange，用于标志某一趟排序过程中是否有数据交换。 如果进行某一趟排序时并没有进行数据交换，则说明所有数据已经有序，可立即结束排序，避免不必要的比较过程。 核心代码 // 对 bubbleSort 的优化算法public void bubbleSort_2(int[] list) &#123; int temp = 0; // 用来交换的临时数 boolean bChange = false; // 交换标志 // 要遍历的次数 for (int i = 0; i &lt; list.length - 1; i++) &#123; bChange = false; // 从后向前依次的比较相邻两个数的大小，遍历一次后，把数组中第i小的数放在第i个位置上 for (int j = list.length - 1; j &gt; i; j--) &#123; // 比较相邻的元素，如果前面的数大于后面的数，则交换 if (list[j - 1] &gt; list[j]) &#123; temp = list[j - 1]; list[j - 1] = list[j]; list[j] = temp; bChange = true; &#125; &#125; // 如果标志为false，说明本轮遍历没有交换，已经是有序数列，可以结束排序 if (false == bChange) break; System.out.format("第 %d 趟：\t", i); printAll(list); &#125;&#125; 示例代码 我的 Github 测试例 样本包含：数组个数为奇数、偶数的情况；元素重复或不重复的情况。且样本均为随机样本，实测有效。 快速排序 要点 快速排序是一种交换排序。 快速排序由 C. A. R. Hoare 在 1962 年提出。 算法思想 它的基本思想是： 通过一趟排序将要排序的数据分割成独立的两部分：分割点左边都是比它小的数，右边都是比它大的数。 然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 详细的图解往往比大堆的文字更有说明力，所以直接上图： 上图中，演示了快速排序的处理过程： 初始状态为一组无序的数组：2、4、5、1、3。 经过以上操作步骤后，完成了第一次的排序，得到新的数组：1、2、5、4、3。 新的数组中，以 2 为分割点，左边都是比 2 小的数，右边都是比 2 大的数。 因为 2 已经在数组中找到了合适的位置，所以不用再动。 2 左边的数组只有一个元素 1，所以显然不用再排序，位置也被确定。（注：这种情况时，left 指针和 right 指针显然是重合的。因此在代码中，我们可以通过设置判定条件 left 必须小于 right，如果不满足，则不用排序了）。 而对于 2 右边的数组 5、4、3，设置 left 指向 5，right 指向 3，开始继续重复图中的一、二、三、四步骤，对新的数组进行排序。 核心代码 public int division(int[] list, int left, int right) &#123; // 以最左边的数(left)为基准 int base = list[left]; while (left &lt; right) &#123; // 从序列右端开始，向左遍历，直到找到小于base的数 while (left &lt; right &amp;&amp; list[right] &gt;= base) right--; // 找到了比base小的元素，将这个元素放到最左边的位置 list[left] = list[right]; // 从序列左端开始，向右遍历，直到找到大于base的数 while (left &lt; right &amp;&amp; list[left] &lt;= base) left++; // 找到了比base大的元素，将这个元素放到最右边的位置 list[right] = list[left]; &#125; // 最后将base放到left位置。此时，left位置的左侧数值应该都比left小； // 而left位置的右侧数值应该都比left大。 list[left] = base; return left;&#125;private void quickSort(int[] list, int left, int right) &#123; // 左下标一定小于右下标，否则就越界了 if (left &lt; right) &#123; // 对数组进行分割，取出下次分割的基准标号 int base = division(list, left, right); System.out.format("base = %d:\t", list[base]); printPart(list, left, right); // 对“基准标号“左侧的一组数值进行递归的切割，以至于将这些数值完整的排序 quickSort(list, left, base - 1); // 对“基准标号“右侧的一组数值进行递归的切割，以至于将这些数值完整的排序 quickSort(list, base + 1, right); &#125;&#125; 算法分析 快速排序算法的性能 参数 结果 排序类别 交换排序 排序方法 快速排序 时间复杂度平均情况 O(Nlog2N) 时间复杂度最坏情况 O(N2) 时间复杂度最好情况 O(Nlog2N) 空间复杂度 O(Nlog2N) 稳定性 不稳定 复杂性 较复杂 时间复杂度 当数据有序时，以第一个关键字为基准分为两个子序列，前一个子序列为空，此时执行效率最差。 而当数据随机分布时，以第一个关键字为基准分为两个子序列，两个子序列的元素个数接近相等，此时执行效率最好。 所以，数据越随机分布时，快速排序性能越好；数据越接近有序，快速排序性能越差。 空间复杂度 快速排序在每次分割的过程中，需要 1 个空间存储基准值。而快速排序的大概需要 Nlog2N 次的分割处理，所以占用空间也是 Nlog2N 个。 算法稳定性 在快速排序中，相等元素可能会因为分区而交换顺序，所以它是不稳定的算法。 示例代码 我的 Github 测试例 样本包含：数组个数为奇数、偶数的情况；元素重复或不重复的情况。且样本均为随机样本，实测有效。 插入排序 要点 直接插入排序是一种最简单的插入排序。 插入排序：每一趟将一个待排序的记录，按照其关键字的大小插入到有序队列的合适位置里，知道全部插入完成。 算法思想 在讲解直接插入排序之前，先让我们脑补一下我们打牌的过程。 先拿一张 5 在手里， 再摸到一张 4，比 5 小，插到 5 前面， 摸到一张 6，嗯，比 5 大，插到 5 后面， 摸到一张 8，比 6 大，插到 6 后面， 。。。 最后一看，我靠，凑到的居然是同花顺，这下牛逼大了。 以上的过程，其实就是典型的直接插入排序，每次将一个新数据插入到有序队列中的合适位置里。 很简单吧，接下来，我们要将这个算法转化为编程语言。 假设有一组无序序列 R0, R1, … , RN-1。 我们先将这个序列中下标为 0 的元素视为元素个数为 1 的有序序列。 然后，我们要依次把 R1, R2, … , RN-1 插入到这个有序序列中。所以，我们需要一个外部循环，从下标 1 扫描到 N-1 。 接下来描述插入过程。假设这是要将 Ri 插入到前面有序的序列中。由前面所述，我们可知，插入 Ri 时，前 i-1 个数肯定已经是有序了。 所以我们需要将 Ri 和 R0 ~ Ri-1 进行比较，确定要插入的合适位置。这就需要一个内部循环，我们一般是从后往前比较，即从下标 i-1 开始向 0 进行扫描。 核心代码 public void insertSort(int[] list) &#123; // 打印第一个元素 System.out.format("i = %d:\t", 0); printPart(list, 0, 0); // 第1个数肯定是有序的，从第2个数开始遍历，依次插入有序序列 for (int i = 1; i &lt; list.length; i++) &#123; int j = 0; int temp = list[i]; // 取出第i个数，和前i-1个数比较后，插入合适位置 // 因为前i-1个数都是从小到大的有序序列，所以只要当前比较的数(list[j])比temp大，就把这个数后移一位 for (j = i - 1; j &gt;= 0 &amp;&amp; temp &lt; list[j]; j--) &#123; list[j + 1] = list[j]; &#125; list[j + 1] = temp; System.out.format("i = %d:\t", i); printPart(list, 0, i); &#125;&#125; 算法分析 直接插入排序的算法性能 参数 结果 排序类别 插入排序 排序方法 直接插入排序 时间复杂度平均情况 O(N2) 时间复杂度最坏情况 O(N2) 时间复杂度最好情况 O(N) 空间复杂度 O(1) 稳定性 稳定 复杂性 简单 时间复杂度 当数据正序时，执行效率最好，每次插入都不用移动前面的元素，时间复杂度为 O(N)。 当数据反序时，执行效率最差，每次插入都要前面的元素后移，时间复杂度为 O(N2)。 所以，数据越接近正序，直接插入排序的算法性能越好。 空间复杂度 由直接插入排序算法可知，我们在排序过程中，需要一个临时变量存储要插入的值，所以空间复杂度为 1 。 算法稳定性 直接插入排序的过程中，不需要改变相等数值元素的位置，所以它是稳定的算法。 示例代码 我的 Github 测试例 样本包含：数组个数为奇数、偶数的情况；元素重复或不重复的情况。且样本均为随机样本，实测有效。 希尔排序 要点 希尔(Shell)排序又称为缩小增量排序，它是一种插入排序。它是直接插入排序算法的一种威力加强版。 该方法因 DL．Shell 于 1959 年提出而得名。 算法思想 希尔排序的基本思想是： 把记录按步长 gap 分组，对每组记录采用直接插入排序方法进行排序。 随着步长逐渐减小，所分成的组包含的记录越来越多，当步长的值减小到 1 时，整个数据合成为一组，构成一组有序记录，则完成排序。 我们来通过演示图，更深入的理解一下这个过程。 在上面这幅图中： 初始时，有一个大小为 10 的无序序列。 在第一趟排序中，我们不妨设 gap1 = N / 2 = 5，即相隔距离为 5 的元素组成一组，可以分为 5 组。 接下来，按照直接插入排序的方法对每个组进行排序。 在** 第二趟排序中**，我们把上次的 gap 缩小一半，即 gap2 = gap1 / 2 = 2 (取整数)。这样每相隔距离为 2 的元素组成一组，可以分为 2 组。 按照直接插入排序的方法对每个组进行排序。 在第三趟排序中，再次把 gap 缩小一半，即 gap3 = gap2 / 2 = 1。 这样相隔距离为 1 的元素组成一组，即只有一组。 按照直接插入排序的方法对每个组进行排序。此时，排序已经结束。 需要注意一下的是，图中有两个相等数值的元素 5 和 5 。我们可以清楚的看到，在排序过程中，两个元素位置交换了。 所以，希尔排序是不稳定的算法。 核心代码 public void shellSort(int[] list) &#123; int gap = list.length / 2; while (1 &lt;= gap) &#123; // 把距离为 gap 的元素编为一个组，扫描所有组 for (int i = gap; i &lt; list.length; i++) &#123; int j = 0; int temp = list[i]; // 对距离为 gap 的元素组进行排序 for (j = i - gap; j &gt;= 0 &amp;&amp; temp &lt; list[j]; j = j - gap) &#123; list[j + gap] = list[j]; &#125; list[j + gap] = temp; &#125; System.out.format("gap = %d:\t", gap); printAll(list); gap = gap / 2; // 减小增量 &#125;&#125; 算法分析 希尔排序的算法性能 参数 结果 排序类别 插入排序 排序方法 希尔排序 时间复杂度平均情况 O(Nlog2N) 时间复杂度最坏情况 O(N1.5) 时间复杂度最好情况 空间复杂度 O(1) 稳定性 不稳定 复杂性 较复杂 时间复杂度 步长的选择是希尔排序的重要部分。只要最终步长为 1 任何步长序列都可以工作。 算法最开始以一定的步长进行排序。然后会继续以一定步长进行排序，最终算法以步长为 1 进行排序。当步长为 1 时，算法变为插入排序，这就保证了数据一定会被排序。 Donald Shell 最初建议步长选择为 N/2 并且对步长取半直到步长达到 1。虽然这样取可以比 O(N2)类的算法（插入排序）更好，但这样仍然有减少平均时间和最差时间的余地。可能希尔排序最重要的地方在于当用较小步长排序后，以前用的较大步长仍然是有序的。比如，如果一个数列以步长 5 进行了排序然后再以步长 3 进行排序，那么该数列不仅是以步长 3 有序，而且是以步长 5 有序。如果不是这样，那么算法在迭代过程中会打乱以前的顺序，那就不会以如此短的时间完成排序了。 已知的最好步长序列是由 Sedgewick 提出的(1, 5, 19, 41, 109,…)，该序列的项来自这两个算式。 这项研究也表明“比较在希尔排序中是最主要的操作，而不是交换。”用这样步长序列的希尔排序比插入排序和堆排序都要快，甚至在小数组中比快速排序还快，但是在涉及大量数据时希尔排序还是比快速排序慢。 算法稳定性 由上文的希尔排序算法演示图即可知，希尔排序中相等数据可能会交换位置，所以希尔排序是不稳定的算法。 直接插入排序和希尔排序的比较 直接插入排序是稳定的；而希尔排序是不稳定的。 直接插入排序更适合于原始记录基本有序的集合。 希尔排序的比较次数和移动次数都要比直接插入排序少，当 N 越大时，效果越明显。 在希尔排序中，增量序列 gap 的取法必须满足：**最后一个步长必须是 1 。 ** 直接插入排序也适用于链式存储结构；希尔排序不适用于链式结构。 示例代码 我的 Github 测试例 样本包含：数组个数为奇数、偶数的情况；元素重复或不重复的情况。且样本均为随机样本，实测有效。 简单选择排序 要点 简单选择排序是一种选择排序。 选择排序：每趟从待排序的记录中选出关键字最小的记录，顺序放在已排序的记录序列末尾，直到全部排序结束为止。 算法思想 从待排序序列中，找到关键字最小的元素； 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换； 从余下的 N - 1 个元素中，找出关键字最小的元素，重复 1、2 步，直到排序结束。 如图所示，每趟排序中，将当前**第 i 小的元素放在位置 i **上。 核心代码 算法分析 简单选择排序算法的性能 参数 结果 排序类别 选择排序 排序方法 简单选择排序 时间复杂度平均情况 O(N2) 时间复杂度最坏情况 O(N2) 时间复杂度最好情况 O(N2) 空间复杂度 O(1) 稳定性 不稳定 复杂性 简单 时间复杂度 简单选择排序的比较次数与序列的初始排序无关。 假设待排序的序列有 N 个元素，则**比较次数总是 N (N - 1) / 2 **。 而移动次数与序列的初始排序有关。当序列正序时，移动次数最少，为 0. 当序列反序时，移动次数最多，为 3N (N - 1) / 2。 所以，综合以上，简单排序的时间复杂度为 O(N2)。 空间复杂度 简单选择排序需要占用一个临时空间，在交换数值时使用。 示例代码 我的 Github 测试例 样本包含：数组个数为奇数、偶数的情况；元素重复或不重复的情况。且样本均为随机样本，实测有效。 堆排序 要点 在介绍堆排序之前，首先需要说明一下，堆是个什么玩意儿。 堆是一棵顺序存储的完全二叉树。 其中每个结点的关键字都不大于其孩子结点的关键字，这样的堆称为小根堆。 其中每个结点的关键字都不小于其孩子结点的关键字，这样的堆称为大根堆。 举例来说，对于 n 个元素的序列 {R0, R1, … , Rn} 当且仅当满足下列关系之一时，称之为堆： Ri &lt;= R2i+1 且 Ri &lt;= R2i+2 （小根堆） Ri &gt;= R2i+1 且 Ri &gt;= R2i+2 （大根堆） 其中 i=1,2,…,n/2 向下取整; 如上图所示，序列 R{3, 8,15, 31, 25} 是一个典型的小根堆。 堆中有两个父结点，元素 3 和元素 8。 元素 3 在数组中以 R[0] 表示，它的左孩子结点是 R[1]，右孩子结点是 R[2]。 元素 8 在数组中以 R[1] 表示，它的左孩子结点是 R[3]，右孩子结点是 R[4]，它的父结点是 R[0]。可以看出，它们满足以下规律： 设当前元素在数组中以 R[i] 表示，那么， 它的左孩子结点是：R[2*i+1]; 它的右孩子结点是：R[2*i+2]; 它的父结点是：R[(i-1)/2]; R[i] &lt;= R[2*i+1] 且 R[i] &lt;= R[2i+2]。 算法思想 首先，按堆的定义将数组 R[0…n]调整为堆（这个过程称为创建初始堆），交换 R[0]和 R[n]； 然后，将 R[0…n-1]调整为堆，交换 R[0]和 R[n-1]； 如此反复，直到交换了 R[0]和 R[1]为止。 以上思想可归纳为两个操作： 根据初始数组去构造初始堆（构建一个完全二叉树，保证所有的父结点都比它的孩子结点数值大）。 每次交换第一个和最后一个元素，输出最后一个元素（最大值），然后把剩下元素重新调整为大根堆。 当输出完最后一个元素后，这个数组已经是按照从小到大的顺序排列了。 先通过详细的实例图来看一下，如何构建初始堆。 设有一个无序序列 { 1, 3,4, 5, 2, 6, 9, 7, 8, 0 }。 构造了初始堆后，我们来看一下完整的堆排序处理： 还是针对前面提到的无序序列 { 1,3, 4, 5, 2, 6, 9, 7, 8, 0 } 来加以说明。 相信，通过以上两幅图，应该能很直观的演示堆排序的操作处理。 核心代码 public void HeapAdjust(int[] array, int parent, int length) &#123; int temp = array[parent]; // temp保存当前父节点 int child = 2 * parent + 1; // 先获得左孩子 while (child &lt; length) &#123; // 如果有右孩子结点，并且右孩子结点的值大于左孩子结点，则选取右孩子结点 if (child + 1 &lt; length &amp;&amp; array[child] &lt; array[child + 1]) &#123; child++; &#125; // 如果父结点的值已经大于孩子结点的值，则直接结束 if (temp &gt;= array[child]) break; // 把孩子结点的值赋给父结点 array[parent] = array[child]; // 选取孩子结点的左孩子结点,继续向下筛选 parent = child; child = 2 * child + 1; &#125; array[parent] = temp;&#125;public void heapSort(int[] list) &#123; // 循环建立初始堆 for (int i = list.length / 2; i &gt;= 0; i--) &#123; HeapAdjust(list, i, list.length); &#125; // 进行n-1次循环，完成排序 for (int i = list.length - 1; i &gt; 0; i--) &#123; // 最后一个元素和第一元素进行交换 int temp = list[i]; list[i] = list[0]; list[0] = temp; // 筛选 R[0] 结点，得到i-1个结点的堆 HeapAdjust(list, 0, i); System.out.format("第 %d 趟: \t", list.length - i); printPart(list, 0, list.length - 1); &#125;&#125; 算法分析 堆排序算法的总体情况 参数 结果 排序类别 选择排序 排序方法 堆排序 时间复杂度平均情况 O(nlog2n) 时间复杂度最坏情况 O(nlog2n) 时间复杂度最好情况 O(nlog2n) 空间复杂度 O(1) 稳定性 不稳定 复杂性 较复杂 时间复杂度 堆的存储表示是顺序的。因为堆所对应的二叉树为完全二叉树，而完全二叉树通常采用顺序存储方式。 当想得到一个序列中第 k 个最小的元素之前的部分排序序列，最好采用堆排序。 因为堆排序的时间复杂度是 O(n+klog2n)，若 k ≤ n/log2n，则可得到的时间复杂度为 O(n)。 算法稳定性 堆排序是一种不稳定的排序方法。 因为在堆的调整过程中，关键字进行比较和交换所走的是该结点到叶子结点的一条路径， 因此对于相同的关键字就可能出现排在后面的关键字被交换到前面来的情况。 示例代码 我的 Github 测试例 样本包含：数组个数为奇数、偶数的情况；元素重复或不重复的情况。且样本均为随机样本，实测有效。 归并排序 要点 归并排序是建立在归并操作上的一种有效的排序算法，该算法是采用**分治法（Divide and Conquer）**的一个非常典型的应用。 将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并。 算法思想 将待排序序列 R[0…n-1] 看成是 n 个长度为 1 的有序序列，将相邻的有序表成对归并，得到 n/2 个长度为 2 的有序表；将这些有序序列再次归并，得到 n/4 个长度为 4 的有序序列；如此反复进行下去，最后得到一个长度为 n 的有序序列。 综上可知： 归并排序其实要做两件事： “分解”——将序列每次折半划分。 “合并”——将划分后的序列段两两合并后排序。 我们先来考虑第二步，如何合并？ 在每次合并过程中，都是对两个有序的序列段进行合并，然后排序。 这两个有序序列段分别为 R[low, mid] 和 R[mid+1, high]。 先将他们合并到一个局部的暂存数组R2 中，带合并完成后再将 R2 复制回 R 中。 为了方便描述，我们称 R[low, mid] 第一段，R[mid+1, high] 为第二段。 每次从两个段中取出一个记录进行关键字的比较，将较小者放入 R2 中。最后将各段中余下的部分直接复制到 R2 中。 经过这样的过程，R2 已经是一个有序的序列，再将其复制回 R 中，一次合并排序就完成了。 核心代码 public void Merge(int[] array, int low, int mid, int high) &#123; int i = low; // i是第一段序列的下标 int j = mid + 1; // j是第二段序列的下标 int k = 0; // k是临时存放合并序列的下标 int[] array2 = new int[high - low + 1]; // array2是临时合并序列 // 扫描第一段和第二段序列，直到有一个扫描结束 while (i &lt;= mid &amp;&amp; j &lt;= high) &#123; // 判断第一段和第二段取出的数哪个更小，将其存入合并序列，并继续向下扫描 if (array[i] &lt;= array[j]) &#123; array2[k] = array[i]; i++; k++; &#125; else &#123; array2[k] = array[j]; j++; k++; &#125; &#125; // 若第一段序列还没扫描完，将其全部复制到合并序列 while (i &lt;= mid) &#123; array2[k] = array[i]; i++; k++; &#125; // 若第二段序列还没扫描完，将其全部复制到合并序列 while (j &lt;= high) &#123; array2[k] = array[j]; j++; k++; &#125; // 将合并序列复制到原始序列中 for (k = 0, i = low; i &lt;= high; i++, k++) &#123; array[i] = array2[k]; &#125;&#125; 掌握了合并的方法，接下来，让我们来了解如何分解。 在某趟归并中，设各子表的长度为 gap，则归并前 R[0…n-1] 中共有 n/gap 个有序的子表：R[0...gap-1], R[gap...2*gap-1], … , R[(n/gap)*gap ... n-1]。 调用 Merge 将相邻的子表归并时，必须对表的特殊情况进行特殊处理。 若子表个数为奇数，则最后一个子表无须和其他子表归并（即本趟处理轮空）：若子表个数为偶数，则要注意到最后一对子表中后一个子表区间的上限为 n-1。 核心代码 public void MergePass(int[] array, int gap, int length) &#123; int i = 0; // 归并gap长度的两个相邻子表 for (i = 0; i + 2 * gap - 1 &lt; length; i = i + 2 * gap) &#123; Merge(array, i, i + gap - 1, i + 2 * gap - 1); &#125; // 余下两个子表，后者长度小于gap if (i + gap - 1 &lt; length) &#123; Merge(array, i, i + gap - 1, length - 1); &#125;&#125;public int[] sort(int[] list) &#123; for (int gap = 1; gap &lt; list.length; gap = 2 * gap) &#123; MergePass(list, gap, list.length); System.out.print("gap = " + gap + ":\t"); this.printAll(list); &#125; return list;&#125; 算法分析 归并排序算法的性能 参数 结果 排序类别 归并排序 排序方法 归并排序 时间复杂度平均情况 O(nlog2n) 时间复杂度最坏情况 O(nlog2n) 时间复杂度最好情况 O(nlog2n) 空间复杂度 O(n) 稳定性 稳定 复杂性 较复杂 时间复杂度 归并排序的形式就是一棵二叉树，它需要遍历的次数就是二叉树的深度，而根据完全二叉树的可以得出它的时间复杂度是 O(n*log2n)。 空间复杂度 由前面的算法说明可知，算法处理过程中，需要一个大小为 n 的临时存储空间用以保存合并序列。 算法稳定性 在归并排序中，相等的元素的顺序不会改变，所以它是稳定的算法。 归并排序和堆排序、快速排序的比较 若从空间复杂度来考虑：首选堆排序，其次是快速排序，最后是归并排序。 若从稳定性来考虑，应选取归并排序，因为堆排序和快速排序都是不稳定的。 若从平均情况下的排序速度考虑，应该选择快速排序。 示例代码 我的 Github 测试例 样本包含：数组个数为奇数、偶数的情况；元素重复或不重复的情况。且样本均为随机样本，实测有效。 基数排序 要点 基数排序与本系列前面讲解的七种排序方法都不同，它不需要比较关键字的大小。 它是根据关键字中各位的值，通过对排序的 N 个元素进行若干趟“分配”与“收集”来实现排序的。 不妨通过一个具体的实例来展示一下，基数排序是如何进行的。 设有一个初始序列为: R {50, 123, 543, 187, 49, 30,0, 2, 11, 100}。 我们知道，任何一个阿拉伯数，它的各个位数上的基数都是以 0~9 来表示的。 所以我们不妨把 0~9 视为 10 个桶。 我们先根据序列的个位数的数字来进行分类，将其分到指定的桶中。例如：R[0] = 50，个位数上是 0，将这个数存入编号为 0 的桶中。 分类后，我们在从各个桶中，将这些数按照从编号 0 到编号 9 的顺序依次将所有数取出来。 这时，得到的序列就是个位数上呈递增趋势的序列。 按照个位数排序： {50, 30, 0, 100, 11, 2, 123,543, 187, 49}。 接下来，可以对十位数、百位数也按照这种方法进行排序，最后就能得到排序完成的序列。 算法分析 基数排序的性能 参数 结果 排序类别 基数排序 排序方法 基数排序 时间复杂度平均情况 O(d(n+r)) 时间复杂度最坏情况 O(d(n+r)) 时间复杂度最好情况 O(d(n+r)) 空间复杂度 O(n+r) 稳定性 稳定 复杂性 较复杂 时间复杂度 通过上文可知，假设在基数排序中，r 为基数，d 为位数。则基数排序的时间复杂度为 O(d(n+r))。 我们可以看出，基数排序的效率和初始序列是否有序没有关联。 空间复杂度 在基数排序过程中，对于任何位数上的基数进行“装桶”操作时，都需要 n+r 个临时空间。 算法稳定性 在基数排序过程中，每次都是将当前位数上相同数值的元素统一“装桶”，并不需要交换位置。所以基数排序是稳定的算法。 示例代码 我的 Github 测试例 样本包含：数组个数为奇数、偶数的情况；元素重复或不重复的情况。且样本均为随机样本，实测有效。]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 Java 数组]]></title>
    <url>%2Fblog%2F2014%2F10%2F14%2Fjava%2Fjavacore%2Fbasics%2FJava%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[深入理解 Java 数组 📓 本文已归档到：「blog」 ⌨️ 本文中的示例代码已归档到：「javacore」 简介 数组的特性 数组和容器 Java 数组的本质是对象 Java 数组和内存 声明数组 创建数组 数组维度的形式 数组维度的大小 访问数组 数组的引用 泛型和数组 多维数组 Arrays 类 小结 参考资料 简介 数组的特性 数组对于每一门编程语言来说都是重要的数据结构之一，当然不同语言对数组的实现及处理也不尽相同。几乎所有程序设计语言都支持数组。 数组代表一系列对象或者基本数据类型，所有相同的类型都封装到一起，采用一个统一的标识符名称。 数组的定义和使用需要通过方括号 []。 Java 中，数组是一种引用类型。 Java 中，数组是用来存储固定大小的同类型元素。 数组和容器 Java 中，既然有了强大的容器，是不是就不需要数组了？ 答案是不。 诚然，大多数情况下，应该选择容器存储数据。 但是，数组也不是毫无是处： Java 中，数组是一种效率最高的存储和随机访问对象引用序列的方式。数组的效率要高于容器（如 ArrayList）。 数组可以持有值类型，而容器则不能（这时，就必须用到包装类）。 Java 数组的本质是对象 Java 数组的本质是对象。它具有 Java 中其他对象的一些基本特点：封装了一些数据，可以访问属性，也可以调用方法。所以，数组是对象。 如果有两个类 A 和 B，如果 B 继承（extends）了 A，那么 A[] 类型的引用就可以指向 B[] 类型的对象。 扩展阅读：Java 中数组的特性 如果想要论证 Java 数组本质是对象，不妨一读这篇文章。 Java 数组和内存 Java 数组在内存中的存储是这样的： 数组对象（这里可以看成一个指针）存储在栈中。 数组元素存储在堆中。 如下图所示：只有当 JVM 执行 new String[] 时，才会在堆中开辟相应的内存区域。数组对象 array 可以视为一个指针，指向这块内存的存储地址。 声明数组 声明数组变量的语法如下： int[] arr1; // 推荐风格int arr2[]; // 效果相同 创建数组 Java 语言使用 new 操作符来创建数组。有两种创建数组方式： 指定数组维度 为数组开辟指定大小的数组维度。 如果数组元素是基础数据类型，会将每个元素设为默认值；如果是引用类型，元素值为 null。 不指定数组维度 用花括号中的实际元素初始化数组，数组大小与元素数相同。 示例 1： public class ArrayDemo &#123; public static void main(String[] args) &#123; int[] array1 = new int[2]; // 指定数组维度 int[] array2 = new int[] &#123; 1, 2 &#125;; // 不指定数组维度 System.out.println("array1 size is " + array1.length); for (int item : array1) &#123; System.out.println(item); &#125; System.out.println("array2 size is " + array1.length); for (int item : array2) &#123; System.out.println(item); &#125; &#125;&#125;// Output:// array1 size is 2// 0// 0// array2 size is 2// 1// 2 💡 说明 请注意数组 array1 中的元素虽然没有初始化，但是 length 和指定的数组维度是一样的。这表明指定数组维度后，无论后面是否初始化数组中的元素，数组都已经开辟了相应的内存。 数组 array1 中的元素都被设为默认值。 示例 2： public class ArrayDemo2 &#123; static class User &#123;&#125; public static void main(String[] args) &#123; User[] array1 = new User[2]; // 指定数组维度 User[] array2 = new User[] &#123;new User(), new User()&#125;; // 不指定数组维度 System.out.println("array1: "); for (User item : array1) &#123; System.out.println(item); &#125; System.out.println("array2: "); for (User item : array2) &#123; System.out.println(item); &#125; &#125;&#125;// Output:// array1:// null// null// array2:// io.github.dunwu.javacore.array.ArrayDemo2$User@4141d797// io.github.dunwu.javacore.array.ArrayDemo2$User@68f7aae2 💡 说明 请将本例与示例 1 比较，可以发现：如果使用指定数组维度方式创建数组，且数组元素为引用类型，则数组中的元素元素值为 null。 数组维度的形式 创建数组时，指定的数组维度可以有多种形式： 数组维度可以是整数、字符。 数组维度可以是整数型、字符型变量。 数组维度可以是计算结果为整数或字符的表达式。 示例： public class ArrayDemo3 &#123; public static void main(String[] args) &#123; int length = 3; // 放开被注掉的代码，编译器会报错 // int[] array = new int[4.0]; // int[] array2 = new int["test"]; int[] array3 = new int['a']; int[] array4 = new int[length]; int[] array5 = new int[length + 2]; int[] array6 = new int['a' + 2]; // int[] array7 = new int[length + 2.1]; System.out.println("array3.length = [" + array3.length + "]"); System.out.println("array4.length = [" + array4.length + "]"); System.out.println("array5.length = [" + array5.length + "]"); System.out.println("array6.length = [" + array6.length + "]"); &#125;&#125;// Output:// array3.length = [97]// array4.length = [3]// array5.length = [5]// array6.length = [99] 💡 说明 当指定的数组维度是字符时，Java 会将其转为整数。如字符 a 的 ASCII 码是 97。 综上，Java 数组的数组维度可以是常量、变量、表达式，只要转换为整数即可。 请留意，有些编程语言则不支持这点，如 C/C++ 语言，只允许数组维度是常量。 数组维度的大小 数组维度并非没有上限的，如果数值过大，编译时会报错。 int[] array = new int[6553612431]; // 数组维度过大，编译报错 此外，数组过大，可能会导致栈溢出。 访问数组 Java 中，可以通过在 [] 中指定下标，访问数组元素，下标位置从 0 开始。 public class ArrayDemo4 &#123; public static void main(String[] args) &#123; int[] array = &#123;1, 2, 3&#125;; for (int i = 0; i &lt; array.length; i++) &#123; array[i]++; System.out.println(String.format("array[%d] = %d", i, array[i])); &#125; &#125;&#125;// Output:// array[0] = 2// array[1] = 3// array[2] = 4 💡 说明 上面的示例中，从 0 开始，使用下标遍历数组 array 的所有元素，为每个元素值加 1 。 数组的引用 Java 中，数组类型是一种引用类型。 因此，它可以作为引用，被 Java 函数作为函数入参或返回值。 数组作为函数入参的示例： public class ArrayRefDemo &#123; private static void fun(int[] array) &#123; for (int i : array) &#123; System.out.print(i + "\t"); &#125; &#125; public static void main(String[] args) &#123; int[] array = new int[] &#123;1, 3, 5&#125;; fun(array); &#125;&#125;// Output:// 1 3 5 数组作为函数返回值的示例： public class ArrayRefDemo2 &#123; /** * 返回一个数组 */ private static int[] fun() &#123; return new int[] &#123;1, 3, 5&#125;; &#125; public static void main(String[] args) &#123; int[] array = fun(); System.out.println(Arrays.toString(array)); &#125;&#125;// Output:// [1, 3, 5] 泛型和数组 通常，数组和泛型不能很好地结合。你不能实例化具有参数化类型的数组。 Peel&lt;Banana&gt;[] peels = new Pell&lt;Banana&gt;[10]; // 这行代码非法 Java 中不允许直接创建泛型数组。但是，可以通过创建一个类型擦除的数组，然后转型的方式来创建泛型数组。 public class GenericArrayDemo&lt;T&gt; &#123; static class GenericArray&lt;T&gt; &#123; private T[] array; public GenericArray(int num) &#123; array = (T[]) new Object[num]; &#125; public void put(int index, T item) &#123; array[index] = item; &#125; public T get(int index) &#123; return array[index]; &#125; public T[] array() &#123; return array; &#125; &#125; public static void main(String[] args) &#123; GenericArray&lt;Integer&gt; genericArray = new GenericArray&lt;Integer&gt;(4); genericArray.put(0, 0); genericArray.put(1, 1); Object[] array = genericArray.array(); System.out.println(Arrays.deepToString(array)); &#125;&#125;// Output:// [0, 1, null, null] 扩展阅读：https://www.cnblogs.com/jiangzhaowei/p/7399522.html 我认为，对于泛型数组的理解，点到为止即可。实际上，真的需要存储泛型，还是使用容器更合适。 多维数组 多维数组可以看成是数组的数组，比如二维数组就是一个特殊的一维数组，其每一个元素都是一个一维数组。 Java 可以支持二维数组、三维数组、四维数组、五维数组。。。 但是，以正常人的理解能力，一般也就最多能理解三维数组。所以，请不要做反人类的事，去定义过多维度的数组。 多维数组使用示例： public class MultiArrayDemo &#123; public static void main(String[] args) &#123; Integer[][] a1 = &#123; // 自动装箱 &#123;1, 2, 3,&#125;, &#123;4, 5, 6,&#125;, &#125;; Double[][][] a2 = &#123; // 自动装箱 &#123; &#123;1.1, 2.2&#125;, &#123;3.3, 4.4&#125; &#125;, &#123; &#123;5.5, 6.6&#125;, &#123;7.7, 8.8&#125; &#125;, &#123; &#123;9.9, 1.2&#125;, &#123;2.3, 3.4&#125; &#125;, &#125;; String[][] a3 = &#123; &#123;"The", "Quick", "Sly", "Fox"&#125;, &#123;"Jumped", "Over"&#125;, &#123;"The", "Lazy", "Brown", "Dog", "and", "friend"&#125;, &#125;; System.out.println("a1: " + Arrays.deepToString(a1)); System.out.println("a2: " + Arrays.deepToString(a2)); System.out.println("a3: " + Arrays.deepToString(a3)); &#125;&#125;// Output:// a1: [[1, 2, 3], [4, 5, 6]]// a2: [[[1.1, 2.2], [3.3, 4.4]], [[5.5, 6.6], [7.7, 8.8]], [[9.9, 1.2], [2.3, 3.4]]]// a3: [[The, Quick, Sly, Fox], [Jumped, Over], [The, Lazy, Brown, Dog, and, friend]] Arrays 类 Java 中，提供了一个很有用的数组工具类：Arrays。 它提供的主要操作有： sort - 排序 binarySearch - 查找 equals - 比较 fill - 填充 asList - 转列表 hash - 哈希 toString - 转字符串 扩展阅读：https://juejin.im/post/5a6ade5c518825733e60acb8 小结 参考资料 Java 编程思想 JAVA 核心技术（卷 1） Java 中数组的特性 https://juejin.im/post/59cae3de6fb9a00a4551915b https://www.cnblogs.com/jiangzhaowei/p/7399522.html https://juejin.im/post/5a6ade5c518825733e60acb8]]></content>
      <categories>
        <category>java</category>
        <category>javacore</category>
      </categories>
      <tags>
        <tag>array</tag>
        <tag>java</tag>
        <tag>javacore</tag>
        <tag>basics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ICMP]]></title>
    <url>%2Fblog%2F2014%2F07%2F02%2Fcommunication%2Ficmp%2F</url>
    <content type="text"><![CDATA[ICMP 📓 本文已归档到：「blog」 ICMP 简介 ICMP 类型 目的不可达(Destination Unreachable Message) 超时(Time Exceeded Message) 参数错误报文(Parameter Problem Message) 源冷却(Source Quench Message) 重定向(Redirect Message) 请求回显或回显应答(Echo or Echo Reply Message) 时间戳或时间戳请求(Timestamp or Timestamp Reply Message) 信息请求或信息响应 总结 参考 ICMP 简介 ICMP 全名为(INTERNET CONTROL MESSAGE PROTOCOL)网络控制消息协议。 ICMP 的协议号为1。 ICMP 报文就像是 IP 报文的小弟，总顶着 IP 报文的名头出来混。因为 ICMP 报文是在 IP 报文内部的，如图： 图：IP 数据报 ICMP 类型 ICMP 报文主要有两大功能：查询报文和差错报文。 目的不可达(Destination Unreachable Message) 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Type | Code | Checksum | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | unused | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Internet Header + 64 bits of Original Data Datagram | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ 日常生活中，邮寄包裹会经过多个传递环节，任意一环如果无法传下去，都会返回寄件人，并附上无法邮寄的原因。同理，当路由器收到一个无法传递下去的 IP 报文时，会发送 ICMP目的不可达报文（Type 为 3）给 IP 报文的源发送方。报文中的 Code 就表示发送失败的原因。 Code 0 = net unreachable; 1 = host unreachable; 2 = protocol unreachable; 3 = port unreachable; 4 = fragmentation needed and DF set; 5 = source route failed. 超时(Time Exceeded Message) 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Type | Code | Checksum | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | unused | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Internet Header + 64 bits of Original Data Datagram | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ 网络传输 IP 数据报的过程中，如果 IP 数据包的 TTL 值逐渐递减为 0 时，需要丢弃数据报。这时，路由器需要向源发送方发送 ICMP 超时报文(Type 为 11)，Code 为 0，表示传输过程中超时了。 一个 IP 数据报可能会因为过大而被分片，然后在目的主机侧把所有的分片重组。如果主机迟迟没有等到所有的分片报文，就会向源发送方发送一个 ICMP 超时报文，Code 为 1，表示分片重组超时了。 参数错误报文(Parameter Problem Message) 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Type | Code | Checksum | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Pointer | unused | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Internet Header + 64 bits of Original Data Datagram | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ 当路由器或主机处理数据报时，发现因为报文头的参数错误而不得不丢弃报文时，需要向源发送方发送参数错误报文(Type 为 12)。当 Code 为 0 时，报文中的 Pointer 表示错误的字节位置。 源冷却(Source Quench Message) 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Type | Code | Checksum | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | unused | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Internet Header + 64 bits of Original Data Datagram | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ 路由器在处理报文时会有一个缓存队列。如果超过最大缓存队列，将无法处理，从而丢弃报文。并向源发送方发一个 ICMP 源冷却报文(Type 为 4)，告诉对方：“嘿，我这里客满了，你迟点再来。” 重定向(Redirect Message) 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Type | Code | Checksum | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Gateway Internet Address | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Internet Header + 64 bits of Original Data Datagram | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ 想像一下，在公司中，有人来你的项目组问你某某某在哪儿。你一想，我们组没有这人啊。你肯定就会说，我们组没有这号人，你去其他组看看。当路由收到 IP 数据报，发现数据报的目的地址在路由表上没有，它就会发 ICMP 重定向报文(Type 为 5)给源发送方，提醒它想要发送的地址不在，去其他地方找找吧。 请求回显或回显应答(Echo or Echo Reply Message) 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Type | Code | Checksum | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Identifier | Sequence Number | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Data … ±±±±± Type(8)是请求回显报文(Echo)；Type(0)是回显应答报文(Echo Reply)。 请求回显或回显应答报文属于查询报文。Ping 就是用这种报文进行查询和回应。 时间戳或时间戳请求(Timestamp or Timestamp Reply Message) 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Type | Code | Checksum | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Identifier | Sequence Number | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Originate Timestamp | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Receive Timestamp | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Transmit Timestamp | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ 时间戳报文是用来记录收发以及传输时间的报文。Originate Timestamp 记录的是发送方发送报文的时刻；Receive Timestamp记录的是接收方收到报文的时刻；Transmit Timestamp表示回显这最后发送报文的时刻。 信息请求或信息响应 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Type | Code | Checksum | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ | Identifier | Sequence Number | ±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±±+ 这种报文是用来找出一个主机所在的网络个数（一个主机可能会在多个网络中）。报文的 IP 消息头的目的地址会填为全 0，表示 this，源地址会填为源 IP 所在的网络 IP。 总结 图：ICMP 知识点思维导图 参考 RFC792]]></content>
      <categories>
        <category>communication</category>
      </categories>
      <tags>
        <tag>communication</tag>
        <tag>network</tag>
        <tag>protocol</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树]]></title>
    <url>%2Fblog%2F2014%2F06%2F15%2Falgorithm%2Fdata-structure%2Ftree%2Fbinary-tree%2F</url>
    <content type="text"><![CDATA[二叉树 简介 二叉树的性质 满二叉树 完全二叉树 简介 二叉树是 N 个节点的有限集合，它或者是空树，或者是由一个根节点及两棵不想交的且分别称为左右子树的二叉树所组成。 二叉树的性质 二叉树第 i 层上的结点数目最多为 2i-1 (i≥1)。 深度为 k 的二叉树至多有 2k-1 个结点(k≥1)。 包含 n 个结点的二叉树的高度至少为 log2(n+1)。 在任意一棵二叉树中，若终端结点的个数为 n0，度为 2 的结点数为 n2，则 n0=n2+1。 满二叉树 定义：高度为 h，并且由 2h–1 个结点的二叉树，被称为满二叉树。 完全二叉树 定义：一棵二叉树中，只有最下面两层结点的度可以小于 2，并且最下一层的叶结点集中在靠左的若干位置上。这样的二叉树称为完全二叉树。 特点：叶子结点只能出现在最下层和次下层，且最下层的叶子结点集中在树的左部。显然，一棵满二叉树必定是一棵完全二叉树，而完全二叉树未必是满二叉树。]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[队列]]></title>
    <url>%2Fblog%2F2014%2F06%2F11%2Falgorithm%2Fdata-structure%2Fqueue%2F</url>
    <content type="text"><![CDATA[队列 队列是元素的集合，其包含了两个基本操作：入队（enqueue） 操作可以用于将元素插入到队列中，而出队（dequeue）操作则是将元素从队列中移除。 遵循先入先出原则 (FIFO)。 时间复杂度： 索引: O(n) 搜索: O(n) 插入: O(1) 移除: O(1)]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆栈]]></title>
    <url>%2Fblog%2F2014%2F01%2F25%2Falgorithm%2Fdata-structure%2Fstack%2F</url>
    <content type="text"><![CDATA[堆栈 堆栈（英语：stack）又称为栈或堆叠，是计算机科学中一种特殊的串列形式的抽象数据类型，其特殊之处在于只能允许在链表或数组的一端（称为堆栈顶端指针，英语：top）进行加入数据（英语：push）和输出数据（英语：pop）的运算。另外堆栈也可以用一维数组或链表的形式来完成。堆栈的另外一个相对的操作方式称为队列。 由于堆栈数据结构只允许在一端进行操作，因而按照后进先出（LIFO, Last In First Out）的原理运作。 概念 应用 更多内容 概念 特点 堆栈的基本特点： 先入后出，后入先出。 除头尾节点之外，每个元素有一个前驱，一个后继。 操作 堆栈数据结构使用两种基本操作：推入（压栈，push）和弹出（弹栈，pop）： 推入 - 将数据放入堆栈的顶端（数组形式或串列形式），堆栈顶端 top 指针加一。 弹出 - 将顶端数据数据输出（回传），堆栈顶端数据减一。 应用 回溯 递归 深度优先搜索 更多内容 https://zh.wikipedia.org/wiki/堆栈]]></content>
      <categories>
        <category>algorithm</category>
        <category>data-structure</category>
      </categories>
      <tags>
        <tag>data-structure</tag>
        <tag>stack</tag>
      </tags>
  </entry>
</search>
